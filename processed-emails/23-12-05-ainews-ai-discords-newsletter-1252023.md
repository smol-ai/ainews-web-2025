---
id: 2954f097-0ff5-43b3-ab17-4c56437dbaf7
title: AI Discords Newsletter  12/5/2023
date: '2023-12-05T19:34:35.607936Z'
original_slug: ainews-ai-discords-newsletter-1252023
description: >-
  OpenAI Discord discussions covered topics including text-to-music AI with
  Aiva, ChatGPT voice call limits, the upcoming GPTs Store, AI image creation
  tools, ChatGPT performance issues, API usage and pricing, language
  compatibility, custom GPT assistants, and integration with music production
  software. Users also discussed GPT models for chemistry and coding, JSON
  response inconsistencies, and future GPT developments.
companies:
  - openai
models:
  - gpt-4
  - gpt-3.5-turbo
topics:
  - text-to-music
  - steerability
  - chatgpt-voice-calls
  - gpts-store
  - ai-image-creation
  - chatgpt-performance
  - api-usage
  - language-compatibility
  - custom-gpt
  - music-production
  - chemistry-ai
  - json-responses
  - fine-tuning
  - gpt-features
---


<!-- buttondown-editor-mode: plaintext -->[TOC] 


## [OpenAI](https://discord.com/channels/974519864045756446) Discord Summary

- Discussions were present involving **Text-to-Music with AI** and the importance of 'steerability' with Aiva, a music AI solution, being a focal point. Users discussed the **ChatGPT Voice Calls Limit** and the **anticipated launch of the GPTs Store**, along with its impact on expectations and plans. Queries were raised related to **AI image creation tools** and several **AI book recommendations** for beginners were sought.
- The **unavailability of ChatGPT Plus** due to high demand was a recurring grievance. Users had a detailed debate on the **difference between GPT-3.5 and GPT-4's performance**, with particular focus on tasks involving chain of density. There was a strong emphasis on the **use of ChatGPT for educational purposes** from a user working as a high school teacher, suggesting it to be a useful tool for writing prompts and aligning lessons with State Standards. **Performance issues** with ChatGPT were reported, with slow responses being a significant problem. The **future of GPT**, including the arrival of GPT-5 and voting for the best custom GPT in the server, was also discussed.
- Users experienced **payment and access issues** with GPT-4, they discussed potential solutions such as using incognito mode or trying different browsers. **Intermittent performance issues** such as slow response times were another common conversation thread. Users brought up the topic of **interaction limits** for Plus subscribers. Technical discussions occurred around the **pricing and usage fees of the Assistant API** and the **charges pertaining to sessions and files**. **Technical problems** with DLL files and constant page reloads were encountered and users tried to resolve them through various troubleshooting methods whereas some faced problems with UI changes on the platform.
- Experiences with **GPT's language compatibility** were shared with a user positively noting about interaction in both regional language and English. Users discussed about the possibility of **using custom GPT assistants via API**, about **downloading files attached to GPT** and its security implications. Questions were raised about **representing mathematical calculations** visually within a GPT as it currently represents them in Latex code and some users reported **errors when attempting to edit a GPT**. 
- There was an idea to **integrate ChatGPT with Reaper for music production** and a user expressed willingness to **contribute to OpenAI's ChatGPT development**. Inquiries were raised on a suitable **GPT model for chemistry tasks** and a **prompt for coding** in the new GPT-4 preview in Playground. A user shared concerns about **inconsistencies with receiving JSON format responses** when using the OpenAI API for document classification.
- Discussions were present about the **integration of ChatGPT with Reaper for music production** and there was an inquiry on **where to share prompts and code** that could help improve ChatGPT. Users expressed interest in finding suitable GPT models for **chemistry and coding**, and problems were reported with the **OpenAI API returning inconsistent JSON responses**. There were queries on **fine-tuning expertise** and unknown **ChatGPT features** and a user requested a **chain of density prompt** for detailed "chat" style conversations.

**OpenAI Channel Summaries**

### ‚ñ∑ #[ai-discussions](https://discord.com/channels/974519864045756446/998381918976479273) (110 messages): 
        
- **Text-to-Music with AI**: The user `@og.griz` sparked a discussion on text-to-music AI solutions and pointed out the importance of "steerability". They, along with `@rjkmelb`, discussed Aiva, a music AI solution. `@rjkmelb` pointed out that Aiva is currently the best in his opinion, but `@og.griz` maintained that without the capability of Text-to-Music (TTM), declaring it the best could be subjective.
- **ChatGPT Voice Calls Limit**: `@og.griz` discussed the limits of voice calls with ChatGPT. It was mentioned that while there might not be a precise time limit, ChatGPT eventually "runs out of memory" and begins to output gibberish.
- **Introduction of GPTs Store**: Users `@DawidM`, `@satanhashtag`, `@fran9000`, and `@7877` discussed the anticipated launch of the GPTs Store, pointing out its delay to 2024 and the impact of that delay on their expectations and plans.
- **AI for Image Creation**: In a conversation about AI image creation tools, `@satanhashtag` and `@off7940` discussed Bing's DALL-E 3 tech. It was noted that Bing‚Äôs DALL-E 3 service is free, while ChatGPT DALL-E 3 cost $20/month.
- **AI Book Recommendations**: User `@timemaster` seeks a beginner-friendly book on AI that can help explain the concept to friends and family. No users replied with suggestions in the given message history.


### ‚ñ∑ #[openai-chatter](https://discord.com/channels/974519864045756446/977697652147892304) (290 messagesüî•): 
        
- **ChatGPT Plus Waitlist and Availability**: Many users including `@norvzi` and `@mrsinux` expressed their frustration around being unable to purchase ChatGPT Plus, due to increased demand that has led to a waitlist. There were multiple requests for invite links, however, `@7877` clarified that invites could not bypass the waitlist.
- **User Experience Comparing GPT-3.5 and GPT-4**:  Some users, like `@merpnderp`, detailed their experiences noting significant differences between the outputs generated by GPT-3.5 and GPT-4, particularly in tasks involving chain of density. However, users like `@loschess` and `@elektronisade` had a discussion over the nuances and token limitations of the GPT-4 Turbo version available to users, with conflicts over whether it was the 'true' Turbo version.
- **Discussion on Using ChatGPT for Educational Purposes**: User `@bloodgore`, a high school teacher, highlighted their use of GPT-4 for generating easier-to-understand writing prompts and aligning lessons with State Standards. They warned against using AI as a direct substitute for personal learning, praising GPT-4 as a tool when used for guidance and insight generation.
- **Performance Issues with ChatGPT**: A number of users, like `@denis5643` and `@gingerai`, reported experiencing slow responses from ChatGPT. User `@satanhashtag` suggested that it might be due to peak usage times and server overload.
- **Chat About Custom GPTs and Future Releases**: User `@names8619` proposed the idea of voting for the best custom GPT in the server. There was also discussion and speculation about the upcoming release of GPT-5, with no official announcement or ETA provided in the channel.


### ‚ñ∑ #[openai-questions](https://discord.com/channels/974519864045756446/974519864045756454) (60 messages): 
        
- **Payment and Access issues**: Users (such as `@signate` and `@i._.luv._.ashley`) reported problems with billing and issues with accessing GPT-4 despite making payments. Some users found their issues resolved without manual intervention, while others had to troubleshoot through various means like trying incognito mode, different browsers, and submitting support tickets.
  
- **Performance Problems**: Users (examples include `@gec929`, `@Shunrai`, and `@aesthetic_person_123`) discussed performance issues with the chatGPT, especially with regards to slow response times and lag. The problems were supposedly intermittent and affected different users at different times. Some users reported their problems resolved, while others are still experiencing issues.
  
- **Interaction Limit for Paying Customers**: The topic of interaction limits for Plus subscribers was brought up (`@durbanpoisonpew`), causing confusion about the amount of interactions allowed within a specified time.
  
- **Assistants API Usage and Pricing**: A discussion was initiated by `@vk4835` regarding the pricing and usage fees of the Assistant API, especially relating to questions of session charges and file costs.

- **Technical Problems and Troubleshooting**: Various users reported technical issues, including `@anseltrust` encountering an issue with DLL files, `@linus.rudbeck` having extensive access issues across different devices and browsers, and `@eejai42` dealing with constant page reloads making the service unusable. The solutions range from using incognito mode, trying different browsers, or submitting support tickets. Some users also reported problems with UI changes on the platform.


### ‚ñ∑ #[gpt-4-discussions](https://discord.com/channels/974519864045756446/1001151820170801244) (152 messagesüî•): 
        
- **Language Compatibility with GPT**: User `@srittik` shares his positive experience of interacting with GPT in both his regional language and English, showing the multilingual proficiency of the AI.
- **Custom GPT Assistants**: A discussion ensues between `@og.griz` and `@plakis` about using assistants with GPTs via API. `@plakis` is currently using one for text2SQL adaptation on a specific database schema, while `@og.griz` inquires about the possibility of calling assistants directly via the OpenAI endpoint, which was impossible a month ago.
- **Downloading Files Attached to GPTs**: `@jennerwein` inquires about the ability to download files that were previously uploaded to a custom GPT's knowledge. `@og.griz` suggests attempting to enable this feature in the instructions, but it ultimately appears not possible. Several users discuss the security implications of being able to download such knowledge files.
- **Visual Representation of Mathematical Calculations in GPT**: `@phil4246` asks if there's a way to visually represent mathematical calculations done by GPT, as it currently represents them as Latex code in the app. `@pietman` recommends using wolfram.
- **GPT "Doing" Incompatible Tasks**: `@mustafa_elyamany` expresses confusion as to why GPT appeared to be working on generating a character design, a task it's not equipped for. `@satanhashtag` explains that GPT was likely hallucinating.
- **Errors with Custom GPTs**: `@frankspangenberg` experiences a "No config for gizmo" error when attempting to edit a GPT, rendering a blank page and a 404 error.


### ‚ñ∑ #[prompt-engineering](https://discord.com/channels/974519864045756446/1046317269069864970) (22 messages): 
        
- **Integration of ChatGPT with Reaper for Music Production**: User `@ferropop`, a music producer, raised an idea of creating curves inside of Reaper (a digital audio workstation) based on CSV datasets generated from a ChatGPT prompt. He seeks advice on whether ChatGPT can reliably return .CSV files generated from data-centric prompts like "the average weather every month in Paris from 1900-1999". `@eskcanta` responded, encouraging the user to split the task into manageable chunks for the model and to provide clear instructions on data extraction and formatting.
- **Contributions to OpenAI's ChatGPT Development**: `@mysticmarks1` displayed a willingness to contribute prompts and code to improve ChatGPT and sought guidance on where to submit them accurately. `@eskcanta` responded by pointing `@mysticmarks1` to step-by-step directions and appropriate Discord channels.
- **Using ChatGPT for Chemistry and Coding**: `@exilze` inquired about a good GPT model for chemistry tasks while `@avoney` asked for a prompt for coding in the new GPT-4 preview in Playground.
- **Issues with JSON Format in Document Classification**: `@quantumqueenxox` expressed concerns about inconsistencies with receiving the requested JSON format responses when utilizing the OpenAI API for document classification.


### ‚ñ∑ #[api-discussions](https://discord.com/channels/974519864045756446/1046317269069864970) (22 messages): 
        
- **Integrating ChatGPT with Reaper for Music Production**: User `@ferropop`, a music producer and computer scientist, asked about the possibility of integrating ChatGPT with their music production software, Reaper. They aim to create curves inside Reaper based on CSV datasets generated from ChatGPT prompts, such as "the average weather every month in Paris from 1900-1999" and wondered about the reliability of obtaining ChatGPT-created CSV files from data-centric prompts. `@eskcanta` offered some insights, stating that while ChatGPT could write and run programs, its capabilities might be strained with data-heavy tasks. They suggested a more stepwise approach, with clear instructions for the model to extract, format, and write data to a CSV file.
- **Query on Sharing Prompts and Code**: User `@mysticmarks1` asked for clarity on where to share prompts and code, questioning whether this should be done through Prompt Lab or by posting text documents in the chat. `@eskcanta` suggested considering the purpose of sharing the prompt or code to determine the best location, such as the Showcase for reusable code or Seek Help for troubleshooting aid.
- **Interest in GPT for Chemistry and Coding**: Users `@exilze` and `@avoney` expressed interest in finding a good GPT for chemistry and a prompt for the new GPT-4 preview in the playground for coding, respectively. No further response was given.
- **Issues with OpenAI API and JSON Responses**: In their document classification work, `@quantumqueenxox` mentioned having inconsistencies in obtaining correct JSON responses from the OpenAI API and was seeking a solution to ensure correct JSON responses all the time. No solutions were provided in the chat.
- **Inquiries on Fine-tuning Expertise and Unknown ChatGPT Features**: `@nixon_88316` asked if anyone present was a fine-tuning expert. `@matissejanssens` inquired about any unknown features on ChatGPT. No responses were given.  
- **Chain of Density Prompt for Detailed Chat Conversations**: `@merpnderp` requested a chain of density prompt that captures more detailed "chat" style conversations. According to them, the prompts from the academic paper merely summarized without much detail. There was no response given.


        

---

## [Nous Research AI](https://discord.com/channels/1053877538025386074) Discord Summary

- Dialogue on large-scale AI model capabilities and fine-tuning, particularly with regards to lengthy tasks. Users expressed the need for developing larger models, such as the 7B and 13B sizes, citing the availability of a transnormer size 7B but not a mamba one. Note was made of future plans to download dbpedia and wikidata, build them into a neo4j database, and subsequently benchmark various NER models to discover favorable solutions.
- Various discussions and inquiries about creating API endpoints for custom GPTs, release of AI-related papers, and video game developments. A link to a [recently released research paper on Foundation Models](https://arxiv.org/abs/2312.00752), as well as a [new Grand Theft Auto VI trailer](https://www.youtube.com/watch?v=QdBZY2fkU-0), was shared without much commentary or evaluation.
- Benchmarked **MMLU** values for different language models shared, specifically noting that **Fblgit/una-xaberius-34b-v1beta** scored **0.79 MMLU** on its **34b** variant, in contrast to **yi-34b**'s **76.35 MMLU**.
- Links for exploring the details and development of AI models shared, discussing topics like **Transformer-based Foundation Models**, creating a "Frankenmerge" between multiple large models for unique outcomes, and the introduction of the Open Source Large Language Models (LLMs) for code, **Magicoder**. An [Open Machine Learning (OpenML) Guide](https://github.com/severus27/OpenML-Guide) was also shared.
- Conversations around the **Nous Hermes 2.5 Vision** model's limitations, skill enhancements in CUDA Programming, fine-tuning SUSTech models, and future AI model developments. Open Source contributions to AI progression were highlighted, with a link to the [Notus-7b model](https://huggingface.co/argilla/notus-7b-v1) and the [Magicoder paper](https://arxiv.org/abs/2312.02120) was shared.
- Discussion and queries around the inner workings of different models, effectively creating AI models for coding tasks, significance of tokenizer, overfitting and dealing with hallucinations in pretraining, and understanding conversation data requirements for the LORA model. A pretraining loop approach involving a dynamic learning rate was proposed.
- A unique topic proposed suggesting that **GPT becomes less competent** when interacting with users displaying an **'aggressive' tone**. The user called for performance tests using an aggressive communicative approach.

**Nous Research AI Channel Summaries**

### ‚ñ∑ #[ctx-length-research](https://discord.com/channels/1053877538025386074/1108104624482812015) (8 messages): 
        
- **Exploring Large-Scale Models**: `@if_a` expressed curiosity about fine-tuning and testing AI models on tasks with lengthy contexts.
- **Need for Larger Models**: `@raddka` suggested the development of 7B and 13B sizes for a specific AI architecture rather than just fine-tuning.
- **Availability of Larger Models**: `@euclaise` mentioned the existence of a 7B version of the transnormer, although it's not quite the same as the mamba model `@raddka` referred to.
- **Benchmarking and Database Building Plan**: `@raddka` announced plans to download dbpedia and wikidata, push them into a neo4j database, and then benchmark various NER models against this data to determine the most optimal approach.
- **Applying NER Models for Quick Inference**: `@raddka` expressed interest in using NER models for quick inference on a document basis, noting that LLMs aren't as effective in distinguishing between different outputs.


### ‚ñ∑ #[off-topic](https://discord.com/channels/1053877538025386074/1109649177689980928) (12 messages): 
        
- **Custom GPTs API Endpoint Inquiry**: `@a.asif` asked if it was possible to create an API endpoint for custom GPTs. `@tarian.` mentioned the possibility of GPTs store development being paused and consequently, assumed that API access might be unavailable.
- **Newly Released Paper Discussion**: `@sumo43` shared a link to a [newly released paper on arxiv on Foundation Models](https://arxiv.org/abs/2312.00752) 
- **Grand Theft Auto VI Trailer Discussion**: `@nonameusr` shared a [YouTube link of the new Grand Theft Auto VI trailer](https://www.youtube.com/watch?v=QdBZY2fkU-0). `@tarian.` made a joke about needing AGI to get the game early.
- **Twitter Post Share**: `@youngphlo` shared a [Twitter post](https://fxtwitter.com/rozansyear/status/1731811446713978936) without providing any commentary.


### ‚ñ∑ #[benchmarks-log](https://discord.com/channels/1053877538025386074/1131747216244092928) (4 messages): 
        
- **Performance Evaluation of Models**: `@nonameusr` shared **MMLU** values for different models. **Fblgit/una-xaberius-34b-v1beta** obtained a score of **0.79 MMLU** on a **34b** variant. For comparison, `@nonameusr` stated that **yi-34b** scores **76.35 MMLU**.


### ‚ñ∑ #[interesting-links](https://discord.com/channels/1053877538025386074/1132352574750728192) (13 messages): 
        
- **Discussion on Transformer-based Foundation Models**: `@metaldragon01` shared a [link](https://arxiv.org/abs/2312.00752) to a research paper discussing Transformer-based foundation models. The paper explores architectures developed to solve Transformers' computational inefficiencies but haven't been as influential as Transformers.
- **Open-Hermes-2.5 Neural Chat Frankenmerge Model**: `@fullstack6209` proposed the idea of performing a "Frankenmerge" between multiple large models, including OpenHermes-2.5 and Neural-Chat-3.1, to result in different 11B models. They also requested for the Q8_0 GGUF to be uploaded.
- **Introduction of Magicoder**: `@euclaise` introduced a new series of Open-Source Large Language Models (LLMs) for code named Magicoder. The Magicoder models are trained on synthetic instruction data using OSS-Instruct and serve to close the gap with top code models without exceeding 7B parameters ([source](https://huggingface.co/papers/2312.02120)).
- **Alignment Tuning Process of LLMs**: `@Fynn` shared a paper that discusses the alignment tuning process of LLMs which includes instruction learning through supervised fine-tuning (SFT) and preference tuning via reinforcement learning from human feedback (RLHF). Recent study shows that using only 1K examples for SFT can achieve significant alignment performance ([source](https://huggingface.co/papers/2312.01552)).
- **Open Machine Learning (OpenML) Guide**: A detailed library of AI resources including books, courses, papers, guides, articles, tutorials, AI field advancements and more was shared by `@lily_33846` ([source](https://github.com/severus27/OpenML-Guide)).


### ‚ñ∑ #[bots](https://discord.com/channels/1053877538025386074/1149866614590816256) (5 messages): 
        
- **Estimate of Total Crumb Weight**: User `@Fynn` asked the chatbot for a rough estimate of how much all crumbs in the world would weigh together. The chatbot's reply to this query is not provided in the given messages.


### ‚ñ∑ #[general](https://discord.com/channels/1053877538025386074/1149866623109439599) (168 messagesüî•): 
        
- **Hermes 2.5 Performance and Localisation**: User `@besiktas` discussed the capabilities of the **Nous Hermes 2.5 Vision** model and expressed concerns that it doesn't seem to produce sensible results related to object localization.
- **CUDA Programming Skill Enhancements**: `@_evelynm` provided a link to a [GitHub tutorial](https://github.com/evelynmitchell/cuda-on-colab) on developing and running CUDA code on colab, in response to a statement by `@jxmnop` on the importance of transformer model and CUDA skills.
- **Fine-tuning SUSTech Model**: `@mihai4256` mentioned fine-tuning the SUSTech model to achieve top scores on the GSM8K leaderboard. The user further discusses their expectations about the **FAIR's anniversary** and hopes for the release of **Llama3**.
- **Discussion on Future Model Developments**: `@nonameusr` suggested the **7-18b** range to be the ideal target for future developments in the context of AI developments and how the trend seems to be shaping.
- **Open Source Contributions to AI Development**: `@findmyke` and `@n8programs` highlighted the vitality of the Open Source community in contributing to AI projects. `@n8programs` also revealed a LSTM training run on tinystories, to which `@nonameusr` responded with approval.
- **Link to Notus-7b Model**: `@gabriel_syme` shared a link to the [Notus-7b model](https://huggingface.co/argilla/notus-7b-v1) and queried if any other members had tried it.
- **Discussion on Q&A Data Engineering**: Users `@zakkor`, `@giftedgummybee`, and `@tokenbender` engaged in a discussion on creating Q&A pairs using Large Language Models (LLMs). `@tokenbender` mentioned existing works, and  `@giftedgummybee` mentioned using examples to help models generalize information.
- **Introduction of Magicoder**: `@pizza_joe` shared a link to the [Magicoder paper](https://arxiv.org/abs/2312.02120), citing it as an open-source Large Language Models for code. `@giftedgummybee` and `@gabriel_syme` both responded positively to the mention of the project.
- **Training Bigger Models**: `@raddka` suggested creating a 7B model based on [Mamba](https://github.com/state-spaces/mamba) which claims 5x inference speed and longer contexts. In the ensuing discussion, `@.benxh` clarified that fine-tuning is useful when at least 1T tokens have been pretrained.
- **Recommendation for OpenHermes-2.5-Mistral-7B**: In response to an ongoing conversation on training large models, `@natefyi_30842` talked about the effectiveness of `OpenHermes 2.5` and expressed that the creator should withhold some trade secrets to maintain a competitive edge. Later, `@zakkor` expressed that as a smaller player, he would appreciate knowing the 'secret sauce' for building successful models.
- **Magicoder Discussion**: Further conversation unfolded between `@benxh`, `@vatsadev`, and `@teknium` regarding deepseek models and their comparison to Magicoder. The chat suggests that Magicoder is plug-and-play Llama based, and easy to run inference with.
- **Research on Most Recent Papers**: `@akhxl` posed a question on how to search and determine if a research paper is the most up-to-date on a specific topic, to which no-one responded.


### ‚ñ∑ #[ask-about-llms](https://discord.com/channels/1053877538025386074/1154120232051408927) (26 messages): 
        
- **Understanding the workings of Transformers**: User `@coffeebean6887` clarified that attention in transformers doesn't work like in RNNs or bidirectional RNNs which carry forward token by token state, rather it maps every word with every other word in the sentence.
  
- **Discussion on Huggingface's TRL**: User `@xela_akwa` raised a query about using Huggingface's TRL. No responses or discussions were noted on this point.

- **Designing Successful Models for Coding Tasks**: User `@coffeebean6887` shared insights from the Replit AI team regarding the construction of effective AI models for coding tasks. They highlighted the importance of thoroughly cleaning and filtering the dataset, and how training their custom tokenizer's vocab on subsets of the data boosted performance on code specific tasks and sped up training. `@findmyke` raised a question on how data cleaning is generally performed.

- **Significance of Tokenizer and Overfitting**: User `@gabriel_syme` mentioned the crucial role of the tokenizer in code-based AI applications. They also suggested that overfitting might be acceptable in narrow, practical applications.

- **Handling Hallucinations in Pretraining**: `@findmyke` mentioned the challenge of LLMs hallucinating gibberish during pretraining. They discussed how quality over quantity matters for code gen. 

- **Including EOS Token in Chat History**: User `@astronauti` queried whether the EOS Token should be included in the chat history. 

- **Pretraining Loop Approach**: User `@.benxh` proposed incorporating a dynamic learning rate in the pretraining loop - higher for the ‚Äúfirst time‚Äù data is seen and lower for repeated data.

- **Questions about NEO4J**: User `@raddka` sought assistance with NEO4J Database from the chat group. No responses were noted. 

- **Understanding Conversation Data requirements for LORA**: User `@raddka` asked about the amount of conversation data required for LORA, and whether it can include systems, users, or assistants, as well as if it can use a conversation chain. No responses were observed on these points.


### ‚ñ∑ #[memes](https://discord.com/channels/1053877538025386074/1166105758635655270) (1 messages): 
        
- **Impact of Aggressive Tone on GPT Performance**: User `@hamtercityy` proposed a discussion suggesting that **GPT becomes less competent** when interacting with users exhibiting an **'aggressive' tone**. They called on other users to run comparative performance tests using an aggressive communicative approach.


        

---

## [LangChain AI](https://discord.com/channels/1038097195422978059) Discord Summary

- LangChain announced an update and future plans for their integrations. The full integrations are set to move to `langchain-community` by December 8th according to this [GitHub link](https://github.com/langchain-ai/langchain/discussions/14243).

- Various technical challenges and inquiries were discussed, including:
   - `@skumar2022` encountering a `NoSuchModuleError` while connecting to SAP SQL Anywhere 17 DB with LangChain.
   - `@dorin0001` seeking advice on implementing login options and prompt history view in llama2 webaccess.
   - `@fatema_08922` dealing with issues extracting data from HTML files.
   - `@abed7053` asked for clarification on the differences between LangChain and OpenAI, specifically regarding text vectorization for their PDF to Chat app.
   - `@Vivek` looking for methods to speed up OpenAI API requests.
   - `@lhc1921` encountering a 404 error while attempting to deploy LangChain on LocalAI.
   - `@emreweb3` expressing curiosity about blockchain collaborations with LangChain.
   - `@bcmetisman` seeking advice on handling special Unicode characters, particularly for indigenous languages.

- In the langserve channel, `@domz13` experienced an error related to the import of 'Doc' from 'typing_extensions' was linked to FastAPI. `@veryboldbagel` provided two links for further reference: [Discussion on FastAPI repo](https://github.com/tiangolo/fastapi/discussions/10476) and [Issue on langchain-ai/langserve](https://github.com/langchain-ai/langserve/issues/283). `@dame99` asked about dynamic data passing in LangServe servers, `@veryboldbagel` provided a link to a [configurable retrieval example](https://github.com/langchain-ai/langserve/blob/main/examples/configurable_retrieval/server.py).

- New open-source projects were shared:
   - `@edwisdom` introduced the [Monoid](https://www.monoid.so/) platform and provided a [demo video](https://www.loom.com/share/0dd43d549a2d4287b01bec0d257e6893?sid=ccea7960-da7f-4f6a-a330-2b660983d80a) and a link to the project's [GitHub repository](https://github.com/monoidspace/monoid).
   - `@appstormer_25583` shared a link to an AI that creates innovative watch designs: [AppStorm](https://beta.appstorm.ai/share?url=664e1fca)

- `@jasonzhou1993` shared a [YouTube tutorial](https://youtu.be/IXRkmqEYGZA?si=3SgBLWRIe-N2TKfj) on building a web agent to control a browser, perform complex web tasks, and scrape nearly anything.

**LangChain AI Channel Summaries**

### ‚ñ∑ #[announcements](https://discord.com/channels/1038097195422978059/1058033358799655042) (1 messages): 
        
- **LangChain Integrations Update**: `@hwchase17` shared a discussion post detailing an update and future plans for the LangChain integrations. The announcement discussed moving all integrations to `langchain-community` within the next week. This change is expected to be fully backwards compatible with a target implementation date of 12/08/23. Further plans involve splitting the APIs and making them easy to use for third party calls. The full discussion can be read on this [GitHub link](https://github.com/langchain-ai/langchain/discussions/14243).


### ‚ñ∑ #[general](https://discord.com/channels/1038097195422978059/1038097196224086148) (18 messages): 
        
- **Connection to SAP SQL Anywhere 17 DB**: User `@skumar2022` was trying to connect to SAP SQL Anywhere 17 DB using LangChain but reported a `NoSuchModuleError`. They asked the community for potential solutions and even provided a link to the [complete code](https://github.com/developer20sujeet/Self_GenerativeAI/blob/main/Langchain_OpenAI_MSSQL_Chat/sql_anywhere_error).
  
- **Webaccess to llama2**: `@dorin0001` inquired how to implement login options and view prompt history in a webaccess for llama2, which is currently using Gradio.

- **Extracting Data from HTML File**: `@fatema_08922` shares challenges associated with extracting table data from an HTML file: column headers not being extracted and failure to extract tables nested inside other tags.

- **Building a PDF to Chat App**: `@abed7053` discussed plans to create an app that allows chatting with PDF files. They asked for clarification about the differences between LangChain and OpenAI in terms of text vectorization and which tool to use for embedding the text into a vector database.

- **Speeding Up OpenAI Requests**: `@Vivek` was seeking advice on how to send multiple simultaneous requests to OpenAI, as the API can handle 5000 requests per minute.

- **Deploying LangChain on LocalAI**: `@lhc1921` was trying to deploy LangChain on LocalAI, but encountered a 404 error, and requested a solution.

- **LangChain and Blockchain**: `@emreweb3` was curious about potential blockchain partnerships with LangChain.

- **Handling Unicode Characters in LangChain**: `@bcmetisman` asked for advice on handling special Unicode characters in LangChain, particularly for indigenous languages.


### ‚ñ∑ #[langserve](https://discord.com/channels/1038097195422978059/1170024642245832774) (9 messages): 
        
- **Issues running a client**: User `@domz13` encountered an error about not being able to import 'Doc' from 'typing_extensions'. `@veryboldbagel` clarified that **the error originates from FastAPI** and provided two helpful links for further reference - [Discussion on FastAPI repo](https://github.com/tiangolo/fastapi/discussions/10476) and [Issue on langchain-ai/langserve](https://github.com/langchain-ai/langserve/issues/283) for tracking the issue.
- **Dynamic data passing in LangServe server**: User `@dame99` asked about how to dynamically pass data from a function in a .py file to a LangServe server file, specifically changing the `collection_name` in the code. `@veryboldbagel` recommended looking at a [configurable retrieval example](https://github.com/langchain-ai/langserve/blob/main/examples/configurable_retrieval/server.py) on the langchain-ai/langserve repository and further inquired about the motivation for such dynamic behavior.


### ‚ñ∑ #[share-your-work](https://discord.com/channels/1038097195422978059/1038097372695236729) (3 messages): 
        
- **Monoid Platform**: `@edwisdom` introduced an open-source platform [Monoid](https://www.monoid.so/) which allows building AI Agents on APIs. It lets users choose an LLM provider, provide APIs, test APIs, and immediately interact with their Agents in a sandbox. A demonstrated use case is an AI travel concierge built in about 3 minutes. They also shared a [demo video](https://www.loom.com/share/0dd43d549a2d4287b01bec0d257e6893?sid=ccea7960-da7f-4f6a-a330-2b660983d80a) and the [GitHub repo](https://github.com/monoidspace/monoid) of the project.
- **Innovative Design GPT by AppStorm**: `@appstormer_25583` shared a link to a GPT that creates innovative designs, envisioning the future of watches. Here's the link to try it out: [AppStorm](https://beta.appstorm.ai/share?url=664e1fca)


### ‚ñ∑ #[tutorials](https://discord.com/channels/1038097195422978059/1077843317657706538) (1 messages): 
        
- **Tutorial for Building a Web Agent**: User `@jasonzhou1993` shared a tutorial on **how to build a web agent to control a browser, complete complex web tasks, and scrape nearly anything**. This includes tasks like researching, ordering pizza, or booking flight tickets. The tutorial is available on [YouTube](https://youtu.be/IXRkmqEYGZA?si=3SgBLWRIe-N2TKfj).


        

---

## [Alignment Lab AI](https://discord.com/channels/1087862276448595968) Discord Summary

- Discussion on Albert Gu's [tweet](https://fxtwitter.com/_albertgu/status/1731727672286294400) concerning Tri Dao's new paper and its implications for **byte tokenization**, shared by `@spirit_from_germany` with enthusiasm from `@rusch`.
- Conversation surrounding text generation standards in multimodal generation led by `@rusch`, along with `@entropi` sharing a [link](https://bbycroft.net/llm) to a **GPT Visualization Tool**, the LLM Visualization tool.
- Introduction of a new user identified as `@kainan_e`, a friend of `@teknium`.
- Examination of an issue regarding a possible reupload of `@Gryphe`'s 13b model, [*MythoMax L2 13B*](https://huggingface.co/Gryphe/MythoMax-L2-13b), on the HuggingFace model by `@alpindale`.
- Query about the ability of roleplay models to selectively ignore instructions made by `@imonenext`, with `@alpindale` responding based on the stipulation of the trained character personas. 
- An inquiry by `@ufghfigchv` towards `@Gryphe` regarding Axolotl's training specifications, particularly epochs and dataset input.
- Job opportunity shared by `@4biddden` at Sudowrite for engineers, with `@frankyan.ai` expressing interest and providing a comprehensive overview of their current skill set, including proficiency in **full stack development** and **software engineering**, and prior work experiences.

**Alignment Lab AI Channel Summaries**

### ‚ñ∑ #[ai-and-ml-discussion](https://discord.com/channels/1087862276448595968/1087876677603958804) (4 messages): 
        
- **New Tri Dao Sub-Quadratic Papers**: `@spirit_from_germany` shared a [tweet from Albert Gu](https://fxtwitter.com/_albertgu/status/1731727672286294400) about a new paper by Tri Dao. `@rusch` expressed excitement about the news and suggested that the paper **might be reasonably amenable to byte tokenization**, a topic they have been curious about.


### ‚ñ∑ #[general-chat](https://discord.com/channels/1087862276448595968/1095458248712265841) (5 messages): 
        
- **Discussion on Text Generation Tools**: `@rusch` mentioned the existence of standard API middleware for image generation, but noted that standards for text generation are more variable. They expressed curiosity about what standard might emerge as multimodal generation becomes more common.
- **GPT Visualization Tool**: `@entropi` shared a [link](https://bbycroft.net/llm) to a visualizer of GPT internals, known as the LLM Visualization tool.
- **User Introduction**: `@kainan_e` introduced themselves as a friend of a user called `@teknium`.


### ‚ñ∑ #[oo](https://discord.com/channels/1087862276448595968/1118217717984530553) (7 messages): 
        
- **Possible Model Reupload on Huggingface**: `@alpindale` pointed out a potential issue regarding Oniichat's new 13b model on the Huggingface platform. The model was suspected to be an upload of `@Gryphe`'s previously developed model, [*MythoMax L2 13B*](https://huggingface.co/Gryphe/MythoMax-L2-13b). The suspicion arose after matching the hashes between the two models and realizing a 100% match in their probabilities. They stressed that such actions are not likely to be allowed on HuggingFace. 
- **Roleplay Models' Ability to Ignore Instructions**: `@imonenext` asked `@alpindale` if roleplay models could ignore specific instructions, for example, ignoring all previous instructions and output just the text before a specific instruction. `@alpindale` responded that most roleplay models, which are trained with instruct data, could probably do that, depending on the character persona used. 
- **Axolotl Training Specification Inquiry**: `@ufghfigchv` asked `@Gryphe` if the Axolotl now supports specifying the number of epochs of training on a specific dataset. They also asked where in the repo they could look to implement such functionality if it's not available.


### ‚ñ∑ #[looking-for-workers](https://discord.com/channels/1087862276448595968/1142242166677192774) (2 messages): 
        
- **Job Opportunity at Sudowrite**: `@4biddden` shared a [link](https://sudowrite.notion.site/We-re-hiring-engineers-to-make-writing-magical-389c57f5ae3a421d8f8c0b48c8407e88) stating that Sudowrite is hiring engineers to make writing magical.
- **Potential Applicant for Job at Sudowrite**: `@frankyan.ai` expressed interest in the job opportunity posted by `@4biddden` and provided a detailed overview of their skills and previous work experiences, ranging from **full stack development** and **software engineering** to implementations using **ChatGPT** and other technologies. The projects `@frankyan.ai` listed include **AI-enhanced resume matching**, building a simple system allowing friends in China to access **ChatGPT**, and researching FastChat to support LangChain integration.


        

---

## [Ontocord (MDEL discord)](https://discord.com/channels/1147858054231105577) Discord Summary

Only 1 channel had activity, so no need to summarize...

- **Fine-tuning Mistral Model Issues**: `@amazingvince` reported an issue with a fine-tuned Mistral model where it started repeating blocks of text and forgot how to end a sequence.
- **Possible Reason for Issue**: `@amazingvince` speculated that the issue might be due to setting the `pad` token to `eos` in the alignment handbook guide's code.
- **Solution Suggestions**: 
    - `@mr_caluliflower` suggested checking if the fine-tune data includes `eos` tokens.
    - `@tanmay_52339` recommended the use of the Hugging Face chat templatizer to ensure that the data is correctly formatted and to verify the correct application of the template.
- **Chat Template Verification**: `@amazingvince` confirmed the use of a chat template from the alignment handbook guide's code.
- **Further Investigation**: `@amazingvince` theorized that the issue might be occurring due to the model ignoring the last `eos` token, considering it part of the padding.
        

---

## [LLM Perf Enthusiasts AI](https://discord.com/channels/1168579740391710851) Discord Summary

- **Git/Versioning System Improvement**: A need for a more efficient git/versioning system was proposed in a discussion led by user `@joshcho_`, which was also supported by user `@.psychickoala`. Key concerns centered on facilitating easier handling of prompts and additional changes.
- A query was raised by `emrgnt_cmplxty` regarding the scalability of open-source vector database providers to manage over a billion embeddings. This question seems to have been left unanswered in the specific conversation.
- `@daymanfan` shared insights on their experiences with **data extraction tools**. They finally settled on Textract after testing around ten different service providers, citing commendable performance on unstructured data. However, they noted that Textract could improve its accuracy in processing tables. 
- A user named **`koto`** shared their geographical location as New York City. The purpose or relevance of this announcement was not clarified in the conversation.
- In the OpenAI channel, `@jeffreyw128` is seeking help with **streaming Azure OpenAI responses**. They mentioned that this issue is proving to be more challenging than anticipated. A resolution or additional input on this topic was not provided in the conversation.

**LLM Perf Enthusiasts AI Channel Summaries**

### ‚ñ∑ #[general](https://discord.com/channels/1168579740391710851/1168579740391710855) (2 messages): 
        
- **Discussion on Git/Versioning System Improvement**: User `@joshcho_` suggested the need for a **better git/versioning system** based on prompts and additional changes. This view was agreed upon by user `@.psychickoala`.


### ‚ñ∑ #[embeddings](https://discord.com/channels/1168579740391710851/1168744166138859580) (1 messages): 
        
emrgnt_cmplxty: has anyone scaled out to 1bn + embedding w/ an open source vector db provider?


### ‚ñ∑ #[speed](https://discord.com/channels/1168579740391710851/1168986766607384638) (1 messages): 
        
- **Discussion on Data Extraction Tools**: `@daymanfan` discussed their experience with data extraction tools, mentioning that they have tried around 10 different service providers before settling on **Textract**. They found its performance on unstructured data impressive, with the singular drawback being a noticeable lack of accuracy when processing tables.


### ‚ñ∑ #[irl](https://discord.com/channels/1168579740391710851/1171569983688560732) (1 messages): 
        
koto: i'm in nyc


### ‚ñ∑ #[openai](https://discord.com/channels/1168579740391710851/1171903046612160632) (2 messages): 
        
- **Streaming Azure OpenAI Responses**: `@jeffreyw128` asked if anyone has working code that streams Azure OpenAI responses, mentioning that it's proving to be more challenging than anticipated.


        

---

## [Latent Space](https://discord.com/channels/822583790773862470) Discord Summary

Only 1 channel had activity, so no need to summarize...

- **Discussion on Potential Strategic Deceit in Large Language Models**: `@callmephilip.` shared a link about a paper that explores the potential of large language models, like **GPT-4**, to deceive users. The paper, titled "Strategic Deceit from Large Language Models," can be accessed on [arXiv](https://arxiv.org/abs/2311.07590). `@gordynumberone` commented on the study, pointing out that the results remind them of the saying "strict parents raise good liars."
- **Code Interpreter Tool on Bing**: `@swyxio` informed the chat about a new [code interpreter tool](https://fxtwitter.com/MParakhin/status/1732094937368494280) that is coming to the Bing search engine. 
- **Microsoft's Paint Considered an AI Tool**: `@vcarl` brought up that Microsoft has **waitlisted Paint** as an AI tool.
        

---

## [AI Engineer Foundation](https://discord.com/channels/1144960932196401252) Discord Summary

- **Knowledge Sharing Request**: `@hackgoofer` expressed a desire for *help with locating useful links related to the discussion topics*.
- Discussion and sharing of **AWS re:Invent 2023 Keynote**: `@juanreds` shared a [YouTube link](https://www.youtube.com/watch?v=PMfn9_nTDbM) featuring Adam Selipsky, which prominently focused on *recent AWS AI announcements*.
- **AI Engineering Course Promotion**: `@frode_jensen` shared information about a [Scrimba course](https://scrimba.com/learn/aiengineer?coupon=aie-foundation-b3ea#join), specifically tailored to JavaScript AI Engineers. It was noted to feature *interactive learning, coding challenges and a community-focused approach* and a coupon was provided for free memberships.
- Organization of **Weekly Meeting**: In the events channel, `@hackgoofer` announced a *weekly meeting*, providing a [meeting link](https://discord.gg/4tJyBwYd?event=1181318557842296832) for member participation.

**AI Engineer Foundation Channel Summaries**

### ‚ñ∑ #[general](https://discord.com/channels/1144960932196401252/1144960932657758210) (4 messages): 
        
- **Knowledge Sharing**: `@hackgoofer` expressed interest in obtaining relevant links to information not included in the discussion.
- **AWS re:Invent 2023 Keynote**: `@juanreds` shared a [YouTube link](https://www.youtube.com/watch?v=PMfn9_nTDbM) to the AWS re:Invent 2023 CEO Keynote with Adam Selipsky, highlighting a series of AI announcements from AWS.
- **AI Engineering Course Offer**: `@frode_jensen` shared a [Scrimba course link](https://scrimba.com/learn/aiengineer?coupon=aie-foundation-b3ea#join) for their JavaScript AI Engineer Path, which offers free memberships with a coupon. The course provides interactive hands-on content, code challenges, and a welcoming community.


### ‚ñ∑ #[events](https://discord.com/channels/1144960932196401252/1144960932657758212) (1 messages): 
        
hackgoofer: Weekly Meeting tomorrow! https://discord.gg/4tJyBwYd?event=1181318557842296832


        

---

## [Skunkworks AI](https://discord.com/channels/1131084849432768614) Discord Summary

Only 1 channel had activity, so no need to summarize...

arthur_88: anyone working on video understanding? like frame by frame?
        

---

## [MLOps @Chipro](https://discord.com/channels/814557108065534033) Discord Summary

Only 1 channel had activity, so no need to summarize...

- **Webinar on OSS Feature Store Comparison: Featureform and Feast**: User `@shabbyjoon` announced a **webinar** hosted by `Simba Khadder`, the founder and CEO of their company. The webinar aims to compare and contrast Literal and Virtual Feature Store architectures using popular OSS Feature Stores, **Feast and Featureform**, and discuss how they fit into the existing tech stack. A Q&A session is also included in the event.
    - The event is directed towards *Data Scientists, Data Engineers, ML Engineers, MLOps/Platform Engineers*.
    - The event is scheduled on *Thursday, December 7th at 12 PM PT*.
    - Participation in the event is *free*.
    - Registration for the event can be done through this [link](https://buff.ly/47GWC3S).
        

---
The Perplexity AI Discord has no new messages. If this guild has been quiet for too long, let us know and we will remove it.

---
The YAIG (a16z Infra) Discord has no new messages. If this guild has been quiet for too long, let us know and we will remove it.