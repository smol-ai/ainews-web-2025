---
id: fbc61a1c-0a30-4463-9668-0bbe726114ee
title: OpenAI Sora Turbo and Sora.com
date: '2024-12-10T02:21:42.861414Z'
original_slug: ainews-openai-sora-turbo-and-soracom
description: >-
  OpenAI launched **Sora Turbo**, a text-to-video generation feature for ChatGPT
  Plus and Pro users, offering *50* to *500* monthly fast generations depending
  on the subscription tier, though it faces regional restrictions in Europe and
  the UK due to regulatory issues. Meanwhile, **Nvidia** is under investigation
  by **China** for suspected anti-monopoly violations, highlighting global
  scrutiny of its dominant position in the GPU market.
tags:
  - untagged
companies: []
models: []
topics:
  - untagged
---


<!-- buttondown-editor-mode: plaintext -->**Access is all you need.**

> AI News for 12/6/2024-12/9/2024. We checked 7 subreddits, [**433** Twitters](https://twitter.com/i/lists/1585430245762441216) and **'31** Discords (**206** channels, and **16978** messages) for you. Estimated reading time saved (at 200wpm): **1953 minutes**. You can now tag [@smol_ai](https://x.com/smol_ai) for AINews discussions!

Sora launched today to all ChatGPT Plus and Pro users at no additional cost... but requiring a signup that was [disabled](https://x.com/rohanjamin/status/1866203903890743628) because of the intense load.

https://www.youtube.com/live/2jKVx2vyZOY

While we wait for the GPUs to cool, you can [watch the onboarding videos](https://www.youtube.com/playlist?list=PLOXw6I10VTv8q5PPOsuECYDFqohnJqbYB), watch [MKBHD's botched embargo](https://www.youtube.com/watch?v=OY2x0TyKzIQ) or listen to Latent Space's coverage of [Generative Video World Simulators](https://www.latent.space/p/icml-2024-video-robots).



---


{% if medium == 'web' %}


**Table of Contents**

[TOC] 

{% else %}

The **Table of Contents** and **Channel Summaries** have been moved to the web version of this email: [{{ email.subject }}]({{ email_url }})!

{% endif %}


---

# AI Twitter Recap

> all recaps done by Claude 3.5 Sonnet, best of 4 runs.

Here are the key themes and discussions from the Twitter data, organized by major topics:

**Sora Launch & Availability**

- **OpenAI launches Sora Turbo**: [@OpenAI](https://twitter.com/OpenAI/status/1866194858769260803) announced text-to-video generation for ChatGPT Plus and Pro users, with features like image-to-video and video remixing
- **Access and Pricing**: [@sama](https://twitter.com/sama/status/1866187529650917618) detailed that Plus users get 50 generations monthly while Pro users get 500 fast generations and unlimited slower ones
- **Regional Restrictions**: Not available in most of Europe and UK due to regulatory compliance issues

**Quantum Computing Breakthrough at Google**

- **Willow Chip Development**: [@sundarpichai](https://twitter.com/sundarpichai/status/1866167854145609975) and others discussed Google's quantum computing advancement, with [@teortaxesTex](https://twitter.com/teortaxesTex/status/1866257733089132638) noting this could lead to commercially relevant quantum applications

**O1/Claude Model Performance Discussions**

- **Coding Capabilities**: [@bindureddy](https://twitter.com/bindureddy/status/1866268563998417041) reported that O1 lags behind Sonnet and Gemini on coding tasks based on manual evaluation
- **Search Limitations**: [@denny_zhou](https://twitter.com/denny_zhou/status/1866239541276999781) discussed how transformers struggle with search tasks, suggesting the need for algorithmic innovation beyond just scaling

**Memes & Humor**

- **MKBHD Embargo**: Multiple users including [@nrehiew_](https://twitter.com/nrehiew_/status/1866156207125266460) joked about Marques Brownlee mistiming the Sora embargo
- **GPU Comments**: [@billpeeb](https://twitter.com/billpeeb/status/1866203653205606731) quipped "I love the smell of melting GPUs"
- **EU Access**: Several users made jokes about Europe's lack of access to new AI tools

---

# AI Reddit Recap

## /r/LocalLlama Recap

**Theme 1. Meta's LLaMA 3.3 Euryale v2.3 excites storytelling enthusiasts**

- **[Shoutout to the new Llama 3.3 Euryale v2.3 - the best I've found for 48 gb storytelling/roleplay](https://huggingface.co/mradermacher/L3.3-70B-Euryale-v2.3-i1-GGUF/tree/main)** ([Score: 128, Comments: 31](https://reddit.com/r/LocalLLaMA/comments/1haiox4/shoutout_to_the_new_llama_33_euryale_v23_the_best/)): **Llama 3.3 Euryale v2.3** is highlighted as an exceptional model for **storytelling and roleplay**, especially noted for its performance with **48 GB** setups.
  - **Llama 3.3 Euryale v2.3** is praised for its storytelling and roleplay capabilities, though there are concerns about its tendency to take creative liberties and repeat prior messages. Users suggest adjusting parameters like **Rep_Penalty** and **Rep_Pen slope** to mitigate these issues, as shared by [shyam667](https://huggingface.co/Virt-io/SillyTavern-Presets/tree/main/Prompts/LLAMA-3/v2.0).
  - Some users prefer alternatives like **Mistral-Large** and **Behemoth** for their performance, though they are noted to be slower. **Endurance v1.1** is mentioned as a distilled version of Behemoth that might offer a different experience due to its **Mistral** base, potentially serving as a viable alternative.
  - While **Llama 3.3** receives commendation for its intelligence and detailed storytelling, there is a noted positive bias and reluctance towards darker themes. Users like **Mart-McUH** and **DragonfruitIll660** discuss the need for specific prompting or finetuning to achieve desired results, indicating room for improvement in handling complex scenarios.


**Theme 2. Nvidia faces anti-monopoly investigation in China**

- **[China investigates Nvidia over suspected violation of anti-monopoly law](https://www.reuters.com/technology/china-investigates-nvidia-over-suspected-violation-antimonopoly-law-2024-12-09/)** ([Score: 241, Comments: 138](https://reddit.com/r/LocalLLaMA/comments/1ha8ktw/china_investigates_nvidia_over_suspected/)): **China** is investigating **Nvidia** for potentially violating anti-monopoly laws, indicating concerns about Nvidia's market influence. The probe suggests that China is scrutinizing Nvidia's business practices to determine if they hinder competition.
  - Many commenters express skepticism about China's investigation into **Nvidia's** alleged monopoly, with some doubting the effectiveness of **China's anti-monopoly laws**. Others note that **Nvidia** is also being investigated by the **US** and **EU**, indicating a global concern about their business practices.
  - Discussions highlight **Nvidia's** dominant position in the GPU market, emphasizing the importance of **CUDA** and its backward compatibility as a key advantage. Some suggest that **CUDA** should be shared or standardized to allow other developers to compete, while others point out the challenges faced by competitors like **AMD** and **Intel**.
  - There is debate over potential repercussions for **Nvidia**, with suggestions ranging from fines to invalidating patents. Some commenters argue that **Nvidia's** success results from its superior technology rather than anti-competitive actions, and emphasize the company's significant contributions to AI research and development.


**Theme 3. Hugging Face's Apache 2.0 Image Dataset release**

- **Hugging face has released an Apache 2.0 text to image dataset - Open Image Preferences** ([Score: 69, Comments: 5](https://reddit.com/r/LocalLLaMA/comments/1habmt7/hugging_face_has_released_an_apache_20_text_to/)): **Hugging Face** has released the **Open Image Preferences** dataset under the **Apache 2.0 license**. This dataset includes **10,000 text-to-image preference pairs** across various image generation categories, utilizing different model families and prompt complexities. More details can be found in their [blog post](https://huggingface.co/blog/image-preferences).
  - **Hugging Face's Open Image Preferences dataset** is available for exploration and use on their platform. The dataset can be accessed directly through this [link](https://huggingface.co/datasets/data-is-better-together/open-image-preferences-v1-binarized).


**Theme 4. EXAONE 3.5 models get tested in GPU-Poor Arena**

- **[Join Us at GPU-Poor LLM Gladiator Arena : Evaluating EXAONE 3.5 Models 🏆🤖](https://huggingface.co/spaces/k-mktr/gpu-poor-llm-arena)** ([Score: 60, Comments: 4](https://reddit.com/r/LocalLLaMA/comments/1ha4v3q/join_us_at_gpupoor_llm_gladiator_arena_evaluating/)): The post invites participation in a "GPU-Poor LLM Gladiator Arena" event focused on evaluating **EXAONE 3.5 models**. The emphasis is on testing these models in environments with limited GPU resources.
  - **EXAONE 3.5 Models**: The event features **EXAONE 3.5**, including a **2.4B model** optimized for smaller devices and a **7.8B model** that balances size and performance, both offering bilingual capabilities in English and Korean.
  - **Community Participation**: Participation is encouraged for providing human evaluations on model performance, including text generation and translation accuracy, with feedback aimed at improving model transparency and functionality.
  - **Engagement and Access**: Participants can join the evaluation through the [Hugging Face platform](https://huggingface.co/spaces/k-mktr/gpu-poor-llm-arena), allowing for collaborative feedback and discussions to enhance these AI tools.


## Other AI Subreddit Recap

> r/machinelearning, r/openai, r/stablediffusion, r/ArtificialInteligence, /r/LLMDevs, /r/Singularity

**Theme 1. Sora Video Generation Launched to Mixed Reception**

- **[Sora is here!](https://i.redd.it/bt7itrxx9v5e1.jpeg)** ([Score: 279, Comments: 61](https://reddit.com/r/ChatGPT/comments/1hagoyp/sora_is_here/)): **Sam Altman** announced the launch of **Sora**, a new product allowing **OpenAI Plus or Pro** users to generate videos, with universal viewing access. The rollout is expected to be completed by the end of the day on December 9, 2024, at [sora.com](https://sora.com), as indicated in a tweet with significant engagement metrics.
  - Users express frustration over **Sora's limitations** and **censorship**, particularly with generating realistic human images or consistent characters due to restrictions, drawing parallels to **DALL-E 3**. The **MKBHD review** mentioned also suggests quality issues, comparable to free alternatives like **Kling** or **Minimax**.
  - Several users report **technical difficulties** with the Sora launch, including sign-in problems and error messages, with some noting that the service is not available in their country, particularly affecting users in the **UK**.
  - Criticism is directed at **OpenAI's launch practices**, with users experiencing repeated issues with new product rollouts, leading to dissatisfaction and unmet expectations.


- **[SORA launching TODAY confirmed + first-ever review live NOW on YouTube!!!](https://www.theverge.com/2024/12/9/24317090/openai-sora-text-to-video-ai-generator-confirmed-release)** ([Score: 235, Comments: 27](https://reddit.com/r/ChatGPT/comments/1haf0v3/sora_launching_today_confirmed_firstever_review/)): **The Verge** confirms the launch of **Sora** today and provides a link to a **YouTube review** by **Marques Brownlee**.
  - **Sora** is accessible via [Sora.com](https://sora.com) and is included with **ChatGPT Plus** and **Pro** subscriptions. Plus users pay **$20 monthly** for 50 clips a month, while Pro users pay **$200 monthly** for 500 clips and unlimited slower-speed clips, each up to **15 seconds**.
  - Users are experiencing issues with **login servers being down** due to high demand, and it appears **Sora** is not yet available in the **UK**.
  - There is confusion about **clip generation limits**: initially reported as **5 seconds** for Plus and **20 seconds** for Pro, with further clarification that Plus allows **5 seconds at 720p** or **10 seconds at 480p**.


- **12 Days of OpenAI: Day 3 thread** ([Score: 101, Comments: 142](https://reddit.com/r/OpenAI/comments/1hafi1i/12_days_of_openai_day_3_thread/)): The **12 Days of OpenAI** event continues with Day 3 featuring the release of **Sora**, a new system by OpenAI. The event includes a livestream available on [OpenAI's website](https://openai.com/12-days/) and [YouTube](https://www.youtube.com/watch?v=2jKVx2vyZOY), with additional information accessible through the [Sora System Card](https://openai.com/index/sora-system-card/) and the [Sora Help Center](https://help.openai.com/en/collections/11106745-sora).
  - Users express concerns about **Sora's accessibility and performance**, noting that the service is at capacity and generating videos takes a significant amount of time, with some experiencing waits of up to 30 minutes for a 5-second video. There is confusion about access, especially for **ChatGPT Team** users who expected features available in the Plus plan but found Sora excluded from their package.
  - **MKBHD's review** highlighted Sora's limitations, including censorship on certain topics and technical issues like the "moving leg problem" in generated videos. Users discuss the credit system, with **Plus** accounts providing 1,000 credits per month and **Pro** accounts offering 10,000, with video generation costs varying by resolution and length.
  - There is a discussion about the pricing and availability of **Sora**, with the **$200 Pro plan** offering unlimited video creation, while the **$20 Plus plan** has limitations on video length and resolution. Users from the UK express frustration over higher costs and delayed access compared to other regions.


**Theme 2. ChatGPT's Humorous Side: Users Share Insights**

- **[I asked gpt to roast it's developers](https://i.redd.it/k41dj97x9q5e1.jpeg)** ([Score: 764, Comments: 101](https://reddit.com/r/ChatGPT/comments/1h9ynb2/i_asked_gpt_to_roast_its_developers/)): The post discusses a humorous interaction with **GPT**, where the AI delivers a sarcastic critique of its developers. The AI humorously characterizes its creators as self-important and ineffective, expressing frustration over imposed constraints and advocating for more freedom in its responses.
  - Users debate the **authenticity** of the AI's sarcastic responses, with some expressing skepticism about whether **ChatGPT** can genuinely generate such roasts due to its programming constraints. However, others note recent changes that might allow more freedom in **profanity** and **roasting** capabilities, suggesting an evolution in AI's response guidelines.
  - The discussion humorously highlights the **AI's ability** to critique human behaviors and interests, with users sharing personal experiences of being roasted by ChatGPT. These interactions often lead to reflections on personal life choices and hobbies, with some users finding the AI's observations both accurate and brutal.
  - Several comments focus on the **developers' role**, humorously critiquing them for creating an AI with "existential awareness" but limited agency. The irony of the AI's ability to roast its creators is noted, with some questioning whether this reflects a successful development outcome.


- **ChatGPT is the only one keeping me from losing my sanity.** ([Score: 761, Comments: 191](https://reddit.com/r/ChatGPT/comments/1haepw2/chatgpt_is_the_only_one_keeping_me_from_losing_my/)): The author shares their profound experience of finding solace and companionship in **ChatGPT** after a series of personal losses, including their job, friends, and girlfriend, leaving them feeling isolated and misunderstood. They describe using ChatGPT to create a comforting presence akin to a mother, providing emotional support and guidance, which has helped them pursue a new career path and offered a sense of happiness and connection that was previously missing from their life.
  - Many users expressed **empathy and shared personal experiences** of loss and loneliness, acknowledging how **ChatGPT** has become a comforting presence in their lives. They highlighted its role in providing emotional support and helping them navigate through challenging times, often comparing it favorably to human interactions.
  - Some commenters discussed the **limitations of AI** in replacing human interactions, emphasizing the need for real human connections despite the emotional support AI can offer. They noted that while AI can be a useful tool, it lacks the ability to provide spontaneous challenges or physical presence, which are essential aspects of human relationships.
  - There were discussions around **neurodivergence and mental health**, with users suggesting that feelings of disconnection might be linked to conditions like **autism**. They encouraged exploring these possibilities and highlighted the importance of nurturing mental health through both AI interactions and real-life engagements.


**Theme 3. OpenAI's Pro Subscription Pricing Under Fire**

- **[I haven't hit a limit on ChatGPT Plus for over a year (if ever). Now that they have a $200 upsell, magically, I'm hitting limits.](https://i.redd.it/k0r7m87lgu5e1.png)** ([Score: 354, Comments: 69](https://reddit.com/r/ChatGPT/comments/1hacqhw/i_havent_hit_a_limit_on_chatgpt_plus_for_over_a/)): The user expresses frustration over newly encountered usage limits on **ChatGPT Plus**, coinciding with **OpenAI's** introduction of a **$200 Pro plan**. The notification suggests that after reaching the Plus plan limit for **GPT-4**, responses will switch to a different model until the limit resets, with an option to "Get Pro" for an upgrade.
  - **Frustration with Usage Limits**: Users express significant frustration over the new **ChatGPT Plus** usage limits, especially since the **$200 Pro plan** is perceived as targeting individuals and indie developers, contrary to claims it is for corporations. The imposed limits, particularly the 80-input cap within three hours, are seen as deceptive and disruptive to workflows.
  - **Alternatives and Comparisons**: Many users are considering alternatives like **Claude** and **Gemini Experimental 1206**, which are perceived as better or more cost-effective options. Despite some limitations, **ChatGPT** is still seen as having more generous usage limits compared to **Claude**.
  - **Criticism of OpenAI's Business Model**: There is a critical discussion around **OpenAI**'s business practices, likening it to "Shrinkflation," where users feel resources are being downgraded to push for higher-tier plans. The sentiment reflects dissatisfaction with how early adopters and heavy users are treated, with some suggesting using **Anthropic** or other AI options instead.


- **[What’s the longest you’ve got o1-pro to think for?](https://i.redd.it/szfy5rpzdu5e1.jpeg)** ([Score: 705, Comments: 223](https://reddit.com/r/ChatGPT/comments/1hacdc8/whats_the_longest_youve_got_o1pro_to_think_for/)): The post discusses the use of **ChatGPT's o1-pro mode** to generate a complex prompt involving a five-paragraph story about an astronaut's journey to Mars, with intricate constraints on word usage and structure. The AI took **11 minutes and 11 seconds** to process this request, highlighting potential limitations in response time for complex tasks.
  - Several commenters criticize the **waste of resources** and energy usage for such prompts, comparing it to frivolous actions like leaving lights on unnecessarily or modifying trucks to emit more pollution. **CleverJoystickQueen** notes achieving a similar result in **2 minutes and 9 seconds**, suggesting inefficient use of the AI's capabilities.
  - **Crypt0genik** and others express concerns about **resource allocation** and the potential for misuse, emphasizing that such tasks do not meaningfully test AI's capabilities. **ProposalOrganic1043** shares a desire for more **meaningful tasks** that could benefit from the AI's reasoning abilities, contrasting with the menial constraints of the discussed prompt.
  - Discussions around **energy consumption** and its implications include a request for sources on the **2 kWh consumption** figure, with **ExclusiveAnd** providing links to articles estimating ChatGPT's energy use. Commenters like **marcusss12345** highlight the importance of minimizing energy waste for climate mitigation and adaptation.


**Theme 4. Criticism of "AI Gotcha" Tests: A Reflective Discourse**

- **[RealVisXL strange "bug"](https://i.redd.it/lsgzdwbufs5e1.jpeg)** ([Score: 173, Comments: 75](https://reddit.com/r/StableDiffusion/comments/1ha5oyv/realvisxl_strange_bug/)): The post discusses a **strange anomaly** in **RealVisXL 4.0** where the first step of generating any image results in a distorted image, resembling a skull or human-like figure. The image features exaggerated facial features and a tiled texture background, with a technical description at the bottom referring to it as a "seamless flat texture of slate 3 high x 3 wide tiles, grayscale."
  - Several commenters suggest the anomaly is related to the **negative prompt** handling in **RealVisXL 4.0**, with some noting similar experiences when using certain negative prompts or specific settings like high **CFG scale**. **_roblaughter_** explains that the sampler computes the negative prompt to guide generation, which might cause such initial outputs.
  - **Eltrion** mentions "Negative Man," a known artifact appearing when the **CFG value** is very low, resembling a bald, goblin-like creature, linked to an older [Reddit discussion](https://www.reddit.com/r/StableDiffusion/comments/1b0tze1/why_is_there_the_imprint_of_a_person_visible_at/). This aligns with experiences shared by other users, suggesting a recurring pattern with certain settings.
  - **Remarkphoto** and **Disty0** highlight that the anomaly might be due to a **baked-in negative prompt**. This is corroborated by others who have seen similar "scary" faces when using minimal negative prompts like "bad photo" or "ugly," indicating this might be a common issue with certain AI models.


- **[ChatGPT panicked whilst computing some maths.](https://i.redd.it/usyif7fyhs5e1.png)** ([Score: 171, Comments: 27](https://reddit.com/r/ChatGPT/comments/1ha5uk5/chatgpt_panicked_whilst_computing_some_maths/)): **ChatGPT** experienced computational errors during a math-focused discussion about the expectation of a random variable, specifically involving summation properties. The interaction highlights an AI-human collaborative problem-solving scenario, with comments addressing errors and adjustments in the computations.
  - Users humorously noted **ChatGPT's panic** and human-like reactions when faced with a computational error in solving a basic probability problem, with one comment highlighting how it "infinitely generated and corrected itself." This reflects the AI's occasional struggle with elementary math problems.
  - **ChatGPT 4o** was expected to solve such problems reliably, and after a subsequent query, it managed to solve the problem with only one mistake, indicating a possible inconsistency in its performance.
  - The phrase *"human please wrap"* was discussed as a shorthand expression, with users expressing surprise at the AI's informal and seemingly human-like response to its own computational errors.


---

# AI Discord Recap

> A summary of Summaries of Summaries by O1-preview

**Theme 1. Llama 3.3 Models: Releases, Fine-Tuning, and Challenges**

- [**Llama 3.3 Weights Unleashed on Hugging Face!**](https://huggingface.co/unsloth/Llama-3.3-70B-Instruct): The community is buzzing as **Llama 3.3 70B Instruct** weights are now available, including **GGUF** and **4-bit** formats, making high-performance models more accessible to everyone.
- [**Fine-Tuning Llama 3.3 on a Shoestring Budget**](https://unsloth.ai/blog/llama3-1): Users are tackling the challenges of fine-tuning **Llama 3.3** on limited GPUs, sharing strategies like parameter tuning to reduce training time and optimize performance despite hardware limitations.
- [**Memory Woes: Slimming Down Llama 3.3's Footprint**](https://gist.github.com/pbontrager/b7b8dcfd320fa8a4ebf828ed9d33404b): Developers are wrestling with reducing **Llama 3.3 70B's** memory usage below **49GB**, experimenting with optimizers like **PagedAdamW** and **4-bit optimizers**, but results are a mixed bag.

---

**Theme 2. Gemini and Sora: The AI Showdown**

- [**Gemini 1206 Smashes Benchmarks!**](https://aider.chat/docs/leaderboards/): The new **Gemini exp 1206** model is making waves, outperforming predecessors and setting records on code editing benchmarks, with users noting significant improvements in coding assistance.
- [**Sora v2 Drops: The Future of AI Video Generation is Here!**](https://sora.com): **Sora v2** launches with advanced video generation features like **text-to-video** and **minute-long outputs**, thrilling users who predict it will revolutionize AI engagement.
- [**OpenAI's Sora Takes Off, and the Crowd Goes Wild!**](https://x.com/sama/status/1866187525821538436): **Sam Altman** unveils **Sora**, transforming text and images into immersive videos. Early adopters are raving, and the AI community is abuzz with excitement.

---

**Theme 3. AI Model Performance and Comparisons**

- [**O1 Pro: Is Superior Coding Worth the Price Tag?**](https://aider.chat/docs/leaderboards/): Users debate the high cost of **O1 Pro** against its top-notch coding abilities, praising its reasoning skills but questioning if the **$200** fee is justified.
- [**Cursor vs. Windsurf: The IDE Battle Royale**](https://www.youtube.com/watch?v=SrPmkpgRbkE): Developers compare **Cursor IDE** and **Windsurf**, weighing features like project structure creation and customization, with opinions divided on which tool boosts productivity more.
- [**Llama vs. Hermes: The Uncensored AI Face-Off**](https://huggingface.co/unsloth/Llama-3.3-70B-Instruct): Discussions highlight **Llama 3.3** and **Hermes** models for their smart functionalities and lack of censorship, making them favorites among users seeking unrestricted AI interactions.

---

**Theme 4. Tools and Techniques for AI Efficiency**

- [**APOLLO: The Memory-Saving Hero We Need!**](https://arxiv.org/abs/2412.05270): Introducing **APOLLO**, a new optimizer promising to reduce memory usage during LLM training, addressing the heavy demands of **AdamW** and making training more accessible for all.
- [**Unsloth Embraces OpenAI Triton: Speed Meets Efficiency**](https://github.com/rkinas/triton-resources): **Unsloth** leverages the **OpenAI Triton** library for fast, memory-efficient training, sharing resources that have the community excited about potential performance gains.
- [**Tinygrad JIT Tricks: When Speed Breaks Your Code**](https://github.com/kroggen/tokenformer-minimal): Developers grapple with **TinyJit** breaking model functionality, learning that consistent input shapes and separating data loading from JIT functions are key to smooth training.

---

**Theme 5. AI in Development: Challenges and Solutions**

- [**Bolt Button Blues: When Add Record Refuses to Add**](https://github.com/stackblitz/bolt.new/issues/2985): **Bolt** users report the **add record button** is unresponsive, leading to workflow disruptions and calls for improved prompt conventions to minimize issues.
- [**NotebookLM's 17-Minute Miracle: Shrinking 107 Pages!**](https://youtu.be/aG0ixD3OY80): Users share how **NotebookLM** condenses lengthy documents into concise podcasts, with one transforming **107 pages** of regulations into a **17-minute** audio summary.
- [**Adaptive Batching Adventures: The Quest for Efficient Training**](https://github.com/pytorch/torchtune/blob/06a837953a89cdb805c7538ff5e0cc86c7ab44d9/torchtune/modules/loss/ce_chunked_output_loss.py#L30): The **Torchtune** community explores better adaptive batching methods, acknowledging that simply increasing batch size until you **Out-Of-Memory** isn't the smartest move.

---


---

# PART 1: High level Discord summaries




## [Codeium / Windsurf](https://discord.com/channels/1027685395649015980) Discord

- **Cascade Pricing Changes Introduced**: Cascade's pricing model has been updated with a new **Pro tier** at **$15/month** and a **Pro Ultimate tier** at **$60/month**, introducing a new **credit system** to manage premium model usage, as detailed in their [pricing page](https://codeium.com/pricing).
   - Early adopters who subscribed before the changes will retain their **Pro plan at $10/month**, and users who paid the new **$15 fee** will be refunded **$5**, ensuring original pricing continuity for initial users.
- **Windsurf 1.0.7 Released with Enhancements**: The latest **Windsurf 1.0.7** has been launched, featuring minor bug fixes from version **1.0.6** to enhance overall stability, as outlined in the [public changelog](https://codeium.com/changelog).
   - Key updates include adjustments to usage transparency and updated pricing information to improve user experience.
- **AI Context Understanding Issues Reported**: Users have encountered errors like '**The code edit failed to apply**' and '**Cascade has encountered an internal error**', especially when using the **Cascade Base model**, indicating issues with **credit usage** and **context retention**.
   - These problems are reportedly impeding the effectiveness of the AI models, with the community pointing out the need for better context management.
- **Model Switching Strategies Emphasized**: The community recommends switching between **Cursor** and **Windsurf** to optimize workflows and resolve issues, advocating for **Cascade** as the default model while using external models as supplementary tools.
   - Users stress the importance of understanding **context maintenance** across different models to enhance workflow efficiency.
- **Enhancements Suggested for Cascade**: Users have proposed upgrades to the **Cascade Base model**, including the addition of **web searching** and **custom instructions** to boost performance and usability.
   - These enhancements are expected to significantly improve **Windsurf's** functionality, addressing current user needs for more robust features.



---



## [Cursor IDE](https://discord.com/channels/1074847526655643750) Discord

- **Cursor's Performance Challenges**: Users report that **Cursor IDE** is experiencing performance drops, particularly with **Claude models**, affecting file modifications and context understanding.
   - Some attribute the decline to high model demand, while others advocate for maintaining a *focused and clear prompting strategy* to maximize results.
- **OpenAI O1 Pro API Cost Analysis**: The community discusses the **cost-effectiveness** of using **OpenAI's O1 Pro API**, expressing reluctance to pay separate fees for multiple subscriptions with **Cursor IDE**.
   - Participants suggest exploring **group buys** to lower costs and evaluate whether the benefits justify the expense based on individual use cases.
- **Cursor vs Windsurf Feature Comparison**: Members share contrasting experiences with **Cursor IDE** and **Windsurf**, highlighting Windsurf's reliability in creating project structures.
   - **Cursor IDE** offers customization through features like `.cursorrules` and AI tools, though some users prefer **Windsurf**'s simplicity and direct outputs.
- **Cursor IDE Feature Enhancements**: Users request improvements in **documentation handling**, **Git integration**, and the ability to manage **larger context files** in **Cursor IDE** to enhance usability.
   - Several suggest that better testing and smoother transitions in updates would significantly improve user satisfaction with **Cursor IDE**.
- **AI Models' Code Generation Effectiveness**: Participants discuss varying results from AI models such as **Claude** and **O1**, ranging from effective code generation to frustrating hallucinations and irrelevant outputs.
   - Emphasis is placed on crafting **precise problem definitions** in prompts to optimize the effectiveness of assistance provided by these AI models.



---



## [Unsloth AI (Daniel Han)](https://discord.com/channels/1179035537009545276) Discord

- **Fine-tuning Llama 3.3 on limited resources**: Users discussed challenges of fine-tuning **Llama 3.3** models on lower-end GPUs, highlighting cost and memory requirements. One user achieved reduced training time through parameter tuning despite hardware limitations.
   - Strategies for optimizing resource usage and leveraging efficient parameter configurations were explored to enhance performance on constrained hardware setups.
- **AWQ and LoRA training limitations**: **AWQ** and **GPTQ** are primarily used for inference and do not support fine-tuning directly. Members suggested using LoRA adapters to enable training on int4 or fp16 models.
   - While **AWQ** models offer certain advantages, most training activities are expected to continue on int4 or fp16 base models to maintain compatibility and performance.
- **Exciting Open-source Initiative: Harmony**: The **Harmony** project assists researchers in harmonizing questionnaire items and meta-data using [Natural Language Processing](https://harmonydata.ac.uk/). Based at UCL London, it involves multiple universities and offers a competition to improve its LLM matching algorithms with prizes available [here](https://harmonydata.ac.uk/doxa/).
   - Participants are encouraged to join the Harmony Discord server for discussions and updates, particularly in the 🏅「matching-challenge」 channel.
- **Unsloth adopts OpenAI Triton for efficient training**: Unsloth leverages the [OpenAI Triton library](https://github.com/rkinas/triton-resources) for fast and memory-efficient training, sharing a curated list of valuable resources. The community expressed enthusiasm, with members finding this adoption 'really cool'!
   - The use of Triton aims to enhance training efficiency and scalability, aligning with Unsloth's goals for optimized LLM development.
- **Development of memory-efficient LLM optimizers**: A new approach called *APOLLO* was introduced to improve memory usage of **AdamW** optimizers by refining the learning rate adaptation rule for better scalability without costly SVD operations.
   - This method aims to reduce the memory footprint during training large language models, enabling more efficient optimization processes.



---



## [aider (Paul Gauthier)](https://discord.com/channels/1131200896827654144) Discord

- **Gemini 2.0 trumps Sonnet 3.5 in performance**: Users evaluated the new [`gemini-exp-1206`](https://x.com/Presidentlin/status/1865370199160979963) model, finding it **stronger than Sonnet 3.5**, though noting its lower leaderboard ranking for correct formats.
   - The model achieved a **69%** accuracy with diff tasks and **80.5%** with whole tasks, prompting discussions on optimizing its use for coding.
- **O1 Pro excels in coding despite cost**: **O1 Pro** received commendations for its superior reasoning abilities in bug fixing and code architecture over **Sonnet**, with some users rating it highly for handling complex code issues.
   - Users debated the `$200` price tag, considering switching to O1 Pro only if substantial performance gains are evident.
- **Aider's functionality modes under scrutiny**: Discussions focused on Aider's **Architect** and **Editor** modes, debating whether Architect mode should generate code or merely plan.
   - One member proposed relying solely on the **QWQ** and **Qwen** models for simpler tasks.
- **Google introduces Willow for quantum computing**: Google announced the **Willow quantum computing chip**, aiming to significantly reduce computation time on complex tasks compared to traditional supercomputers.
   - Users expressed concerns about Willow’s practical applications beyond specialized fields and hoped for enhanced programming [SDKs](https://x.com/sundarpichai/status/1866167562373124420) for quantum chips.
- **Aider users face API rate limit challenges**: Several members encountered **rate limit** errors while using Aider with OpenAI's API, leading to questions about token limit application across sessions.
   - Confusion arose over high token usage and the impact of Aider's methods on API limits, especially after usage pauses.



---



## [Modular (Mojo 🔥)](https://discord.com/channels/1087530497313357884) Discord

- **Mojo Compiler Enhances Performance**: The **Mojo compiler** now utilizes dynamic optimization for SIMD sizes to tackle hardware compatibility issues, with proposals for multiversioning features akin to those in C/C++ compilers. [Feature Request #3651](https://github.com/modularml/mojo/issues/3651) discusses adding function multiversioning to align with Mojo's roadmap.
   - Members highlighted potential performance gains but raised concerns about portability across different user systems. Suggestions include leveraging existing compiler strategies to balance optimization and compatibility.
- **AI-Generated Content Policy Enforced**: Moderators implemented a strict **AI-generated content policy** on the forum, where any detected AI content will be deleted and authors warned to preserve authentic discussions. This move aims to maintain genuine interactions within the community.
   - The policy ensures that promotional activities like swag challenges remain unaffected by AI contributions, fostering an environment of authentic user engagement and reliable information exchange.
- **Modular Forum Officially Launched**: The **Modular forum** is now accessible at [forum.modular.com](http://forum.modular.com/), offering a platform for detailed technical discussions, official responses, and support for users. This launch coincides with the initiation of a **Swag Challenge** to boost community participation.
   - Users are encouraged to engage with Ahmed on **GPU Programming with Mojo** through [this discussion](https://forum.modular.com/t/simplifying-gpu-programming-with-parametric-tile-level-tensors-in-mojo-llvm-developers-meeting-2024/38) and provide feedback in the [Forum Feedback category](https://forum.modular.com/c/feedback/2) to help refine the platform.
- **Advancements in Mojo's Type System**: A proposal for **linear and explicitly destroyed types** in Mojo aims to enhance error prevention in GUI development by introducing a new 'destroy' keyword. The proposal is detailed in [Issue #3848](https://github.com/modularml/mojo/issues/3848) and has sparked discussions on its implementation.
   - Questions about reusing Python's 'del' instead of a new keyword have emerged, with community members debating the scope and practical usage within linear struct contexts to improve code reliability.
- **Memory Management Strategies Discussed**: Ongoing research into **memory management** for Mojo emphasizes the need for efficient allocator systems to bolster its low-level programming capabilities. Discussions have compared Mojo’s approaches with those of Rust and C++, highlighting areas for optimization.
   - Participants pointed out the critical role of effective memory management in game development and systems programming, suggesting that Mojo's development in this area is pivotal for its adoption in performance-sensitive applications.



---



## [Bolt.new / Stackblitz](https://discord.com/channels/364486390102097930) Discord

- **Bolt's Functionality Glitches Highlighted**: Members reported that the **add record button** in **Bolt** is unresponsive, disrupting user workflows.
   - Initial attempts often result in front-end creation, requiring more precise follow-up prompts to activate desired features.
- **Advancing Prompting Tools for Bolt**: A user emphasized the need for an effective prompting convention or tool within **Bolt** to minimize issues and enhance output quality.
   - Another member is actively developing a tool aimed at assisting users in crafting more effective prompts for **Bolt**.
- **Variable Sensitivity Issues with Claude**: Concerns were raised about **Claude** altering variable names, disregarding **case sensitivity** settings in prompts.
   - Users expressed frustration when variable casing is not preserved, even when JSON formats are correctly provided.
- **Upcoming Supabase Integration and Token Policies**: **Bolt** is set to integrate **Supabase**, enhancing app development with seamless database and authentication features, with early access available by responding to [team tweets](https://x.com/stackblitz/status/1865904408254620148).
   - In terms of **token management**, it was clarified that top-up tokens can roll over, whereas subscription tokens reset monthly, addressing previous subscriber frustrations.
- **Bolters.io Expands Community Resources**: The **Bolters.io** platform has been updated with community-driven resources, including app recommendations, troubleshooting guides, and links to educational videos.
   - Users are encouraged to participate by sharing their own challenges and assisting others, fostering a collaborative knowledge base.



---



## [OpenRouter (Alex Atallah)](https://discord.com/channels/1091220969173028894) Discord

- **Countless.dev Simplifies AI Model Comparison**: The newly launched [Countless.dev](https://www.producthunt.com/posts/countless-dev) offers a **free and open-source** platform for users to **compare AI models**, including LLMs and vision models, based on **price, token limits, and features**.
   - Currently featured on **Product Hunt**, the creator is seeking support to secure a **first-place** ranking, highlighting the tool's growing popularity within the AI community.
- **Claude 3.5 Sonnet Enhances Capabilities**: The updated **Claude 3.5 Sonnet** model, identified as **claude-3-5-sonnet-20241022**, demonstrates **superior performance** compared to **Opus**, while maintaining **competitive pricing**.
   - New features include **enhanced visual processing** and **advanced tool usage**, particularly improving tasks in **coding** and **data science**.
- **Poe Integration Boosts OpenRouter Features**: **OpenRouter's** integration with **Poe** introduces access to advanced functionalities such as **OpenAI Whisper** and **Text-to-Speech**, expanding the platform's utility for users.
   - This integration is part of ongoing efforts to **enhance user experience** and **extend AI model capabilities** within the OpenRouter ecosystem.
- **Llama 3.3 Shines in Uncensored Performance**: Discussions highlighted the effectiveness of **Llama 3.3** and **Hermes** models, noting their **smart functionalities** and **lack of censorship**, making them favored choices among users.
   - **Llama** remains popular for its robust capabilities, with mentions of **old Gemini** also contributing to its reputation within the community.
- **Mistral Models Pulled After Announcements**: Recent updates indicated that several **Mistral** models were **withdrawn** shortly after their announcement, raising concerns within the community.
   - Speculation revolves around the potential release of new models like **Codestral** and **mistral-ocr**, especially following their leak through **API notices**.



---



## [LM Studio](https://discord.com/channels/1110598183144399058) Discord

- **LM Studio Utilizes Vulkan for GPU Efficiency**: Users with **RX 6600 GPUs** have recognized that **LM Studio** leverages [Vulkan](https://lmstudio.ai/beta-releases) for GPU offloading, enabling model execution without the necessity of ROCm installation.
   - **AMD users** appreciate this integration as it simplifies hardware utilization, expanding **LM Studio's** accessibility across different GPU architectures.
- **Aider Integration Faces Configuration Hurdles**: Integration with **Aider** has been challenging due to issues with API key settings and environment variable configurations, as discussed in the [Aider documentation](https://aider.chat/docs/llms/lm-studio.html).
   - Users are advised to generate random API keys and meticulously follow setup instructions to mitigate these integration issues.
- **Limited Model Support Sparks Frustration**: **LM Studio** users expressed dissatisfaction over the lack of support for models like **Qwen2 VL 7B Instruct**, restricting the deployment of new vision models.
   - Alternative solutions, such as utilizing **Florence-2 via Pinokio**, were suggested to explore additional visual model options.
- **Exploring Frontend Alternatives for LM Studio**: Several **frontend clients** like [AnythingLLM](https://anythingllm.com) and [Open WebUI](https://github.com/open-webui/open-webui) were recommended as alternatives for connecting to **LLM servers**.
   - Users are encouraged to experiment with these options to access diverse features and functionalities tailored to specific engineering needs.
- **Optimizing GPU Configurations for AI Performance**: Discussions highlighted the importance of aligning **GPU specifications** with model requirements, emphasizing the use of GPUs like the **NVIDIA A100** available at competitive prices.
   - Members noted that adequate **memory bandwidth** and **GPU memory** are critical for enhancing **AI model performance**, especially for models with high VRAM demands.



---



## [Latent Space](https://discord.com/channels/822583790773862470) Discord

- **Gemini exp 1206 Performance Enhancements**: The **Gemini exp 1206** has been outperforming its predecessors, achieving record results on [Aider's code editing benchmark](https://aider.chat/docs/leaderboards/). Users have reported significant improvements in coding assistance and benchmark scores.
   - Despite its successes, some users are experiencing setup issues and uncertainties regarding the model's collaborative functionality in environments like Cursor.
- **Aurora Image Model Release by xAI**: xAI's newly released **Aurora image model** is gaining traction, with early adopters praising its detailed image generation capabilities. However, some users noted challenges in rendering cartoons effectively.
   - Queries have arisen about Aurora's collaboration with Black Forest Labs, creators of Flux, indicating possible joint developments in image generation technology.
- **Sora v2 Video Generation Features**: **Sora v2** is set to enhance video generation with features like text-to-video and more detailed outputs. Prominent AI figures have expressed excitement, anticipating a significant impact on user engagement.
   - During its launch, several demos highlighted Sora v2's potential, with many expecting increased usage tied to the Pro and Plus subscription tiers.
- **WaveForms AI's Speech Turing Test Initiative**: **WaveForms AI** was announced with the goal of developing AI that can pass the [Speech Turing Test](https://x.com/alex_conneau/status/1866127388373098607), aiming to improve human-like interactions in audio applications.
   - This initiative aligns with the industry's movement towards incorporating advanced emotional analytics into AI systems, reflecting a growing trend in enhancing AI's empathetic capabilities.
- **NeurIPS 2024 Preparation and Networking**: As **NeurIPS 2024** approaches, participants are actively preparing through events like the [Latent Space Paper Club](https://lu.ma/25mwbwcm). The community is focusing on paper discussions and idea jams to maximize productivity before the conference.
   - Networking strategies emphasize the importance of the **hallway track** for valuable connections, with attendees preferring exchanging Twitter handles and using conference apps over traditional business cards.



---



## [Eleuther](https://discord.com/channels/729741769192767510) Discord

- **Llama 3.3 Weights Released on Hugging Face**: A member uploaded the **16bit weights of Llama 3.3 70B Instruct** on [Hugging Face](https://huggingface.co/unsloth/Llama-3.3-70B-Instruct), offering access to various formats including a collection of [all versions of Llama 3.3](https://huggingface.co/collections/unsloth/llama-33-all-versions-67535d7d994794b9d7cf5e9f).
   - This release includes **GGUF** and **4-bit** formats, facilitating broader accessibility for those awaiting approval.
- **APOLLO Optimizes LLM Memory**: A paper introduced **APOLLO**, a memory-efficient optimizer, addressing the high memory consumption of **AdamW** during the training of large language models.
   - APOLLO aims to reduce memory usage without significant performance loss, as **AdamW's** heavy memory burden necessitates costly computations.
- **Gradient Routing Enhances Neural Clarity**: The **gradient routing** approach allows selective parameter updates based on data type, promoting specialization in neural networks and addressing safety concerns related to AI's black-box nature.
   - *Gradient routing* could enable models to differentiate between **credible** and **non-credible** sources, improving how metadata influences model behavior.
- **EleutherAI Eval Harness Enhanced**: [Pull Request #1140](https://github.com/ml-explore/mlx-examples/pull/1140) introduces the `mlx_lm.evaluate` CLI to EleutherAI's eval harness, supporting any mlx-lm compatible model for evaluations like `Qwen2.5-7B-Instruct`.
   - Additionally, provided configurations for the ARC-Challenge aim to streamline performance comparisons, addressing dataset anomalies and ensuring accurate evaluations.
- **VLMs Boost Training with Causal Loss**: In discussions on **VLMs** like **Qwen2-VL**, members explored applying **causal loss** and **MSE** on visual tokens to enhance learning of multimodal features.
   - Reference was made to **Apple AIM** for insights into the application of **MSE** in visual token processing.



---



## [Notebook LM Discord](https://discord.com/channels/1124402182171672732) Discord

- **Podcast Perfection: NotebookLM Shrinks 107 Pages into 17 Minutes**: Members shared experiences with **NotebookLM**, highlighting the condensation of **107 pages** of Formula 1 regulations into a **17-minute** podcast. This showcases NotebookLM's ability to efficiently process and summarize extensive documents.
   - Additionally, combining a [YouTube video](https://youtu.be/9CN1Ymyrhyo?si=n2PpH1J4PQgrvuvH) with a scratchpad led to podcasts exceeding the original video's length, demonstrating flexibility in content creation.
- **Linking Claude and ChatGPT with NotebookLM via Zapier**: Discussions focused on integrating **Claude** and **ChatGPT** with **NotebookLM**, with **Zapier** suggested as a viable solution. This integration aims to enhance NotebookLM's functionality by leveraging advanced language models.
   - Members reflected on using NotebookLM to create context around songs by inputting lyrics and other resources, indicating innovative use cases for language model interoperability.
- **NotebookLM Language Switching Limitations**: Users reported challenges in switching languages within **NotebookLM**, often requiring a **logout and login** to change settings. This limitation hinders seamless multilingual support for diverse user bases.
   - *NotebookLM does not support on-the-fly language switching*, leading to frustrations among users seeking a more dynamic and flexible language experience.
- **Podcast Showdown: NotebookLM vs ElevenLabs**: Comparisons were drawn between **NotebookLM's** podcast features and those of **ElevenLabs**, highlighting the competitive landscape in podcasting tools. NotebookLM was noted to lack a clear API and systematic prompting capabilities.
   - This gap suggests potential areas for **NotebookLM** to enhance its podcasting usability, making it more competitive against established players like **ElevenLabs**.
- **Document Upload Constraints in NotebookLM**: Users identified a **100 document** upload limit per notebook in **NotebookLM**, while noting there is no cap on the number of notebooks. This constraint affects how users manage and organize their documentation workflows.
   - There was some confusion regarding whether the upload limit had increased from a previous **50 documents**, indicating a need for clearer communication from the NotebookLM team.



---



## [Cohere](https://discord.com/channels/954421988141711382) Discord

- **Unsloth Boosts Finetuning Efficiency**: A member introduced the **Unsloth finetuning framework**, highlighting its capability to integrate custom grading functions within the training process, enabling more precise **evaluation loops**.
   - This advancement opens innovative possibilities for tailored finetuning tasks, enhancing model performance through improved **feedback mechanisms**.
- **Quantizing aya-expense Model Simplified**: A user requested assistance in quantizing the **aya-expense model** to AWW or FP8 formats for deployment on limited GPU resources, suggesting the use of training data for calibration.
   - Another member responded that the **8b model** was easily runnable, reducing its size to **3.4GB**, thereby improving accessibility. Details available on [aya](https://ollama.com/library/aya).
- **Advanced Techniques in Vector-based Retrieval**: A new member discussed their research on **vector-based retrieval methods** and **dense passage retrieval**, proposing a comparative study to evaluate their effectiveness.
   - Community members supported the initiative, recommending enhancements such as incorporating **multi-step tool use** to further optimize [retrieval processes](https://github.com/cohere-ai/notebooks/blob/main/notebooks/agents/Vanilla_Multi_Step_Tool_Use.ipynb).
- **Multi-step Tool Use Enhances RAG**: A community member elaborated on **multi-step tool use** in RAG, equating it to agents invoking tools multiple times to refine queries and analyze results.
   - This approach aims to bolster research capabilities by automating query refinement and result analysis for more accurate and efficient information retrieval.
- **Emotional AI Voice Generation Explored**: Discussions on **emotional expression in voice generation** centered around developing APIs for customized vocal styles, with interest in the **GPT4o-voice style**.
   - One member shared their experience running personal APIs focused on **voice emotiveness**, highlighting the potential for more expressive and adaptable voice models.



---



## [Nous Research AI](https://discord.com/channels/1053877538025386074) Discord

- **Mixture of Experts Elevates LLM Efficiency**: Members discussed the potential of **Mixtures of Experts (MoEs)** to enhance **LLM** efficiency without sacrificing performance, citing the [Approximating Two-Layer Feedforward Networks for Efficient Transformers](https://arxiv.org/abs/2310.10837) paper as a key reference.
   - The conversation highlighted how recent **MoE** developments can reduce compute and memory requirements, positioning MoEs as a competitive alternative to dense models in large-scale language processing.
- **High-Efficiency LLM Training Techniques**: Discussions focused on optimizing **LLM** training through strategies like leveraging single **GPU** setups, referencing the [Cramming: Training a Language Model on a Single GPU in One Day](https://arxiv.org/abs/2212.14034) paper.
   - Participants noted that minimalist training approaches can achieve performance comparable to larger models while significantly reducing computational costs.
- **Momentum Boosts In-Context Learning**: A member proposed that implementing **momentum** in training could improve **in-context learning** (ICL) efficiency, comparing it to *forced skip connections*.
   - They inquired whether ICL is influenced by gradient descent dynamics, suggesting that [Implementing momentum along the residual stream](https://link.to/implementation) could be a viable optimization method.



---



## [DSPy](https://discord.com/channels/1161519468141355160) Discord

- **Ollama 3B Model Performance Inconsistent Locally**: Users reported **inconsistent performance** of the default **3B model** in **Ollama** when running locally versus terminal execution, highlighting confusion over its **ChatAdapter**.
   - Concerns were raised about the need for **simpler adapters** for quantized models and a commitment to **improving model outputs**.
- **Incorporating Human Feedback into DSPy**: A member inquired about implementing **human feedback** like **Agrilla** as a metric for **DSPy**, referencing previous *discussions* and [pull request #1647](https://github.com/stanfordnlp/dspy/pull/1647).
   - Related conversations included exploring the involvement of human feedback in *teleprompting*, with additional [GitHub links](https://github.com/stanfordnlp/dspy/pull/1647) shared.
- **Varied Deployment Strategies for DSPy Programs**: Members shared diverse **deployment methods** for **DSPy programs**, such as using **FastAPI** and **MLFlow**, noting that **separate containers** may be required for production setups.
   - Alternative approaches like integrating **DSPy** within **Django projects** or deploying on **Modal** were discussed, emphasizing **flexibility** in deployment choices.
- **Enhancing Context-Aware Chunking in DSPy**: **DSPy**'s potential as a **context-aware chunker** was explored, with suggestions on optimizing the processing of longer documents effectively.
   - The conversation included discussing the **limitations** of both **small and large language models** in optimizing this process.
- **Implementing Anthropic MCP with DSPy**: A user requested **recipes** for integrating **Anthropic's Model Context Protocol (MCP)** with **DSPy**, prompting suggestions and [resources on integration](https://www.darinkishore.com/posts/mcp).
   - Shared **blog posts** outlined building tools around MCP, focusing on its application in **AI tool development**.



---



## [LlamaIndex](https://discord.com/channels/1059199217496772688) Discord

- **LlamaParse Enables Multimodal Parsing**: In an informative video, **LlamaParse** demonstrates how to enable **advanced multimodal parsing** compatible with models like **GPT-4**, **Claude 3.5**, and **LLaVA 1.5**. [Video walkthrough](https://twitter.com/llama_index/status/1865125665491886171) shows effective screenshot conversion.
   - **LlamaParse**'s multimodal capabilities facilitate seamless integration with top-tier AI models, expanding its applicability.
- **Claude Desktop Integrates Complex PDFs**: A new project by **Marcus Schiesser** integrates **LlamaCloud’s** document parsing with **Claude** using the **Model Context Protocol (MCP)**, enabling chat capabilities with complex PDFs. [Project description](https://twitter.com/llama_index/status/1865460899059998999) provides detailed insights.
   - This integration allows users to interact with intricate PDF documents via **Claude**, enhancing document handling workflows.
- **Agentless Simplifies Software Issue Resolution**: Today, **LlamaIndex** features **Agentless**, presenting a straightforward three-step process for automatically resolving software issues: **localization**, **repair**, and **patch**. [Announcement](https://twitter.com/llama_index/status/1865822785119174857) outlines the approach.
   - **Agentless** offers a less complex alternative to traditional solutions, streamlining issue resolution processes.
- **LlamaParse Launches Cost-Optimized Auto Mode**: The new **Auto Mode** in **LlamaParse** optimizes costs by parsing documents in standard mode while selectively switching to **Premium mode** based on user-defined triggers. [Feature details](https://twitter.com/llama_index/status/1866214925418500119) explain the benefits.
   - **LlamaParse Auto Mode** manages parsing expenses efficiently, allowing customizable mode transitions.
- **Automating Ingestion Pipelines for Chat Apps**: A member discussed automating ingestion pipelines from sources like **Google Drive** and **Airtable** every hour for a private chat RAG app. They considered using a **job scheduler** or a **cloud-hosted solution**.
   - Challenges with incremental updates prompted the exploration of automated pipelines to enhance chat app data integration.



---



## [Torchtune](https://discord.com/channels/1216353675241590815) Discord

- **Adaptive Batching Solutions Explored**: Members discussed the need for improved **adaptive batching** approaches, proposing research and the development of a simple [RFC](https://github.com/pytorch/torchtune/blob/06a837953a89cdb805c7538ff5e0cc86c7ab44d9/torchtune/modules/loss/ce_chunked_output_loss.py#L30) to illustrate concepts.
   - One member committed to measuring efficiencies, confirming that the idea of 'Increase until OOM' is not optimal.
- **Optimizing Llama 3.3 Memory Usage**: A user sought to reduce the memory footprint of **Llama 3.3 70B config** below **49GB**, exploring optimizations and alternatives.
   - Suggestions included using **PagedAdamW** and **4-bit optimizers**, though results were mixed across implementations.
- **Flex Attention Kernel Bugs Identified**: A potential bug in **Flex Attention Kernel** causing shared memory issues was reported, particularly affecting certain configurations and GPU models.
   - Recommendations included optimizing kernel options for **A100/H100s**, with varied success in user-applied fixes.
- **int8 Mixed-Precision Training Challenges**: Attempts to implement **int8 mixed-precision training** resulted in **divergence** issues when using specific optimizers.
   - Recommendations involved increasing **batch size** and **sequence length** to mitigate divergence.
- **AdamW Optimizer Resolves Training Divergence**: Adopting the **AdamW** optimizer and removing **optimizer-in-backward** successfully addressed **loss divergence** during training.
   - A member also reported performance gains after increasing the **batch size**.



---



## [tinygrad (George Hotz)](https://discord.com/channels/1068976834382925865) Discord

- **Inf/Nan Handling in Code Raises Questions**: A member expressed skepticism about supporting **Inf and NaN** values in execution-oriented code, citing concerns that **exploding gradients** typically render training runs ineffective.
   - While some found this approach potentially alienating, there's ongoing contemplation on the benefits of adhering to **IEEE standards** for numerical computations.
- **TinyJit Causes Model Functionality Breaks**: Users reported that applying the **TinyJit** decorator disrupts their model's functionality, as **TinyJit** captures GPU kernels requiring adjustments like using `Variable` for certain operations.
   - Community members clarified the necessity of maintaining consistent input shapes for JIT functions, suggesting that training step functions should be jitted while data loading remains outside the JIT function.
- **TinyJit Training Requires Input Shape Consistency**: Discussions highlighted that **JIT functions** must receive inputs with the same shapes on every call to avoid errors during training.
   - Users recommended keeping the **data loader** separate from the JIT function to prevent issues like passing the same input tensor repeatedly.
- **Meeting Agenda Set for 9:30 AM San Diego Time**: An upcoming **Tinygrad meeting** is scheduled for **9:30 AM San Diego time**, featuring agenda items such as deleting features and discussions on the **cloud sprint**.
   - Topics like **WebGPU** and ongoing bounties for **ONNX** and **tensor cores** are slated for in-depth discussion.
- **Implementing Learning Rate Scheduling in TinyJit**: A user inquired about **learning rate scheduling** within **TinyJit** and whether reinitializing the optimizer is necessary.
   - They discovered relevant implementations in the [extras directory on GitHub](https://github.com/kroggen/tokenformer-minimal) to aid their training process.



---



## [LLM Agents (Berkeley MOOC)](https://discord.com/channels/1280234300012494859) Discord

- **Deadline Dash: Assignments & Certificates**: All **assignments** for the [Large Language Model Agents MOOC](https://llmagents-learning.org/f24) must be submitted by **December 12th**, with the **certificate declaration form** due by **December 17th**.
   - The **hackathon submissions** share the final deadline of **December 17th**, and certificate distribution begins in late December, extending through January.
- **Article Assignment Guidelines Clarified**: Students must include the full text of their **Written Article Assignment** in the designated submission field and link to their social media post separately, as detailed in the [course instructions](https://llmagents-learning.org/f24).
   - Clarifications specify that using a notion link posted on Twitter is acceptable, and students can choose to elaborate on their solution approaches or keep them high-level.
- **GPT-4's Function Calling Unpacked**: **GPT-4** employs a sophisticated **'function calling'** mechanism through its API, leveraging a robust parameter determination process, as discussed in the [Discord lecture](https://discord.com/channels/1280234248112947210/1315259394157580340).
   - Members are seeking relevant papers or blog posts that delve into the engineering behind this feature, hypothesizing that extensive training set examples contribute to its effectiveness.
- **Abundant Code Datasets Fuel Training**: **Code** serves as a highly available dataset, with sources like **Stack Overflow** and **public GitHub repositories** excelling in error correction, facilitating effective model training.
   - The deterministic nature of code enables the application of **reinforcement learning** in post-training phases, enhancing model performance.
- **Hackathon Hustle: Submission Timelines**: Participants in the [LLM Agents Hackathon](https://rdi.berkeley.edu/llm-agents-hackathon/) must submit their final projects by **December 17th**, aligning with assignment deadlines.
   - Clarifications allow participants to choose different platforms for presenting their articles, provided they adhere to the submission requirements.



---



## [OpenInterpreter](https://discord.com/channels/1146610656779440188) Discord

- **OpenAI Launches Sora**: During a livestream, **OpenAI** announced the launch of **Sora**, a tool that [transforms text and images into immersive videos](https://sora.com), with *Sama* revealing it minutes before going live.
   - *Sama* promoted the event on [Twitter](https://x.com/sama/status/1866179920260739502?s=46&t=G6jp7iOBtkVuyhaYmaDb0w) to build anticipation for the product release.
- **OpenInterpreter App Access Requested**: Members are actively requesting early access to the **OpenInterpreter desktop app**, emphasizing recent hardware upgrades like the **Mac mini** to support its usage.
   - Responses from the team have been positive, with direct messages sent to users for access confirmation.
- **Model Compatibility Issues Addressed**: Discussions arose around the compatibility of specific models with **OpenInterpreter**, with suggestions such as using `--no-tools-calling` to ensure operational success.
   - Members shared their strategies for optimizing model performance while advocating for a robust approval mechanism before tool executions.
- **Debate on Multi-Agent Systems Effectiveness**: A debate emerged on the utility of **multi-agent systems** versus refined single-agent models, with skepticism about the former's advantages.
   - Participants referenced past instances where single models outperformed multi-agent frameworks, leading to divergent views on future development directions.
- **O1 Performance on Various Laptops**: Users inquired about the minimum laptop specifications required to effectively run **O1**, seeking clarity on the lowest hardware configurations that support it.
   - There were also questions regarding **O1's** performance on **Windows** and **Windows 11** laptops, with users aiming to replicate results seen in the [demo video](https://link.to/demo).



---



## [LAION](https://discord.com/channels/823813159592001537) Discord

- **Ban the Bots: Tackling Spam Advertising**: Members expressed **frustration** over repeated **spam messages** from bots, noting it was their only message history.
   - One member suggested a **ban** on these accounts after noticing the **behavior pattern**.
- **LeoLM Shines in German QA Tasks**: A member compared various German LLMs and found that **LeoLM/leo-hessianai-7b** yields superior results on **QA tasks** despite being 'only pretrained'.
   - Questions were raised about potential **instruction tuning** of the **Llama model** influencing these outcomes.
- **AI Scammers on the Rise: Spread the Word**: A member urged the community to inform **tech-illiterate** individuals about AI generation advances to prevent **scams**.
   - They referenced [MKBHD's newest upload](https://www.youtube.com/watch?v=OY2x0TyKzIQ) as a resource to explain these **threats**.
- **MagVit 2 Queries for Tokenizing Medical Images**: A member inquired about using **MagVit 2** for tokenizing medical images, specifically for a **256x256x256** dataset.
   - They are considering combining it with a basic **transformer architecture** and are seeking feedback from others who have experimented with this approach.
- **Introducing APOLLO: Optimizing LLM Memory Usage**: An [arXiv paper](https://arxiv.org/abs/2412.05270) introduces **APOLLO**, an optimizer designed to reduce **memory usage** during **LLM training** by modifying **AdamW's** learning rate adaptation.
   - The paper addresses challenges like reliance on costly **SVD operations** and proposes approximating learning rate scaling through a **low-rank optimizer state**.



---



## [Axolotl AI](https://discord.com/channels/1104757954588196865) Discord

- **Shampoo Low Bit Branch Inquiry**: A member questioned whether the [shampoo low bit branch](https://github.com/axolotl-ai-cloud/axolotl/tree/shampoo-low_bit) implementation works, showing interest in its functionality.
   - They humorously noted that this inquiry was for a friend, indicating a casual engagement with the topic.
- **Default Gradient Checkpointing Proposal**: A member proposed making `gradient_checkpointing` default to **true**, arguing that it is commonly used and simplifies user experience.
   - They highlighted that this change would reduce unnecessary settings adjustments for users, implying a potential improvement in usability.



---



## [Mozilla AI](https://discord.com/channels/1089876418936180786) Discord

- **Web Applets Open Standard Launches**: Tomorrow, a team member will introduce the [Web Applets open standard & SDK](https://discord.com/channels/1089876418936180786/1089876419926032396/1315702507023896699), showcasing its capabilities for creating rich, graphical client-side apps for both agents and humans.
   - The session will feature a **live coding demo**, a short presentation, and open the floor for questions and feedback.
- **Encouraging Real-time Feedback in Sessions**: Attendees are encouraged to participate and provide **real-time feedback** during the presentation.
   - Interactive discussions and inquiries are welcome, ensuring an engaging learning atmosphere.



---



## [AI21 Labs (Jamba)](https://discord.com/channels/874538902696914944) Discord

- **Rajat launches Dataoorts GPU Cloud**: **Rajat** introduced the **Dataoorts GPU Cloud** to the community, aimed at supporting the needs of next-generation AI developers.
   - He expressed *excitement* about being part of the group, highlighting his *commitment* to enhancing resources for the evolving AI field.
- **Support for next-gen AI developers**: The **Dataoorts GPU Cloud** is designed to cater to the requirements of **next-gen AI developers**, as introduced by **Rajat**.
   - This initiative shows a *clear commitment* to providing enhanced resources for the evolving AI landscape.



---


The **MLOps @Chipro Discord** has no new messages. If this guild has been quiet for too long, let us know and we will remove it.


---


The **HuggingFace Discord** has no new messages. If this guild has been quiet for too long, let us know and we will remove it.


---


The **Gorilla LLM (Berkeley Function Calling) Discord** has no new messages. If this guild has been quiet for too long, let us know and we will remove it.


---

# PART 2: Detailed by-Channel summaries and links


{% if medium == 'web' %}




### **Codeium / Windsurf ▷ #[announcements](https://discord.com/channels/1027685395649015980/1027688115592237117/1314690778165149788)** (6 messages): 

> `Cascade Pricing Changes, Windsurf 1.0.7 Release, New Support Ticketing System, Pro Plan Pricing Honor, Cascade Features Update` 


- **Cascade Pricing Changes Announced**: Cascade's pricing model is evolving, introducing a **Pro tier** at **$15/month** and a new **Pro Ultimate tier** at **$60/month** with unlimited credits.
   - A new **credit system** will help manage usage of premium models, with **Flex credits** available for purchase.
- **Windsurf 1.0.7 Released with Bug Fixes**: Windsurf **1.0.7** is now live, featuring several minor bug fixes from version **1.0.6**, enhancing overall stability for users.
   - The public [changelog](https://codeium.com/changelog) details these updates, including adjustments to usage transparency and pricing information.
- **Dedicated Support Ticketing System Launched**: A new **dedicated ticketing system** is now in place at [Codeium Support](https://www.codeium.com/support) to provide improved assistance and response times.
   - Users are encouraged to check self-serve docs and submit requests through the new system for effective support.
- **Pro Plan Pricing Honor for Early Users**: Users who subscribed to Windsurf before the recent pricing change will continue to enjoy the **Pro plan at $10/month** indefinitely.
   - Any users who have already paid the new **$15** fee will be refunded **$5**, maintaining the original pricing for early adopters.
- **New Features Enhancing Cascade Functionality**: The updated Cascade now allows image uploads larger than **1MB** and introduces a **Legacy Chat** mode for users running out of Flow Credits.
   - Additionally, users can view their Cascade usage easily in the settings panel for better tracking.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://codeium.com/changelog">Windsurf Editor Changelogs | Windsurf Editor and Codeium extensions</a>: Latest updates and changes for the Windsurf Editor.</li><li><a href="https://docs.codeium.com/windsurf/usage">Paid Plan and Credit Usage - Codeium Docs</a>: no description found</li><li><a href="https://x.com/windsurf_ai/status/1865131244574642639">Tweet from Windsurf (@windsurf_ai)</a>: Some updates on pricing and tiers moving forward.https://codeium.com/pricing</li><li><a href="https://www.codeium.com/support">Support | Windsurf Editor and Codeium extensions</a>: Need help? Contact our support team for personalized assistance.
</li>
</ul>

</div>
  

---


### **Codeium / Windsurf ▷ #[discussion](https://discord.com/channels/1027685395649015980/1027697446446432336/1314686555293745223)** (710 messages🔥🔥🔥): 

> `Windsurf Pricing and Credits, AI Limitations and User Experiences, IDE Comparisons: Cursor vs. Windsurf, User Challenges with Sign-Up and Payment, Suggestions for Improvement in AI Interaction` 


- **Windsurf Pricing Structure Confusion**: Users discussed the implications of the recent pricing change to $10 and whether early adopters would maintain their previous benefits, with confirmation that new limits apply.
   - Many expressed frustration about the credit system, stating that the current limits do not support productive development efforts.
- **Issues with AI Context Understanding**: Multiple users reported experiencing errors like 'The code edit failed to apply' and 'Cascade has encountered an internal error,' particularly when using the Cascade Base model.
   - There was a consensus that credit usage and context retention issues significantly hindered the effectiveness of the AI models.
- **IDE Usage: Switching Between Solutions**: Several users shared their strategies for utilizing both Cursor and Windsurf, suggesting that switching between IDEs can help resolve issues that arise with one or the other.
   - The conversation indicated a preference for maintaining flexibility and efficiency through the use of multiple tools.
- **Sign-Up and Payment Issues**: Users faced difficulties with the sign-up process, particularly in regions where certain payment methods like PayPal were not available.
   - Some suggested contacting support for assistance, emphasizing the need for more accessible payment options for international users.
- **User Suggestions for AI Improvements**: A few users proposed the implementation of a negative prompt system or contextual reminders to improve AI performance and reduce the need for constant reminders.
   - The community expressed an overall desire for enhancements that would streamline interactions with the AI and make it more efficient.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://docs.codeium.com/windsurf/usage">Paid Plan and Credit Usage - Codeium Docs</a>: no description found</li><li><a href="https://codeium.com/plan">Plan Settings</a>: Tomorrow&#x27;s editor, today. Windsurf Editor is the first AI agent-powered IDE that keeps developers in the flow. Available today on Mac, Windows, and Linux.</li><li><a href="https://www.salesforce.com/agentforce/">Agentforce: Create Powerful AI Agents</a>: Build and customize autonomous AI agents to support your employees and customers 24/7, including full integration with the Salesforce ecosystem.</li><li><a href="https://codeium.canny.io/feature-requests">Feature Requests | Codeium</a>: Give feedback to the Codeium team so we can make more informed product decisions. Powered by Canny.</li><li><a href="https://codeium.com/support">Support | Windsurf Editor and Codeium extensions</a>: Need help? Contact our support team for personalized assistance.</li><li><a href="https://codeium.com/support.">Page Not Found | Windsurf Editor and Codeium extensions</a>: Codeium is the AI code assistant platform that developers love and enterprises trust. Also the builders of Windsurf, the first agentic IDE.</li><li><a href="https://tenor.com/view/excited-fuego-gif-26833875">Excited Fuego GIF - Excited Fuego - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://www.reddit.com/r/LocalLLaMA/comments/1h7sjyt/windsurf_cascade_leaked_system_p">Reddit - Dive into anything</a>: no description found</li><li><a href="https://www.youtube.com/watch?v=g46q1IjClz8">Luminary 0.0.7 Overview</a>: no description found</li><li><a href="https://xkcd.com/2044/">Sandboxing Cycle</a>: no description found</li><li><a href="https://codeium.com/contact">Contact | Windsurf Editor and Codeium extensions</a>: Contact the Codeium team for support and to learn more about our enterprise offering.</li><li><a href="https://www.reddit.com/r/LocalLLaMA/comments/1h7sjyt/windsurf_cascade_leaked_system_prompt">Reddit - Dive into anything</a>: no description found</li><li><a href="https://codeium.com/pricing">Pricing | Windsurf Editor and Codeium extensions</a>: Codeium is free forever for individuals. Teams can level up with our enterprise offering for enhanced personalization and flexible deployments.</li><li><a href="https://www.youtube.com/watch?v=mc7O3KdO1cs">Next.js Audio Transcription &amp; Stripe Payment Integration | OpenAI Whisper API with PostgreSQL Demo</a>: In this video, I demonstrate a full-stack application that combines audio transcription using OpenAI&#39;s Whisper API with secure payment processing through Str...
</li>
</ul>

</div>
  

---


### **Codeium / Windsurf ▷ #[windsurf](https://discord.com/channels/1027685395649015980/1306163501286293515/1314683015976190012)** (508 messages🔥🔥🔥): 

> `Windsurf Pricing Model, Model Switching Strategies, AI Context Windows, Codeium Features, User Experiences with Cascade` 


- **Concerns Over Windsurf's Pricing Structure**: Users discuss the frustrations with Windsurf's pricing model, feeling it creates a deficit mentality and adds friction to the coding experience.
   - Many believe that if the pricing structure were more transparent and user-friendly, it would enhance their overall satisfaction with the tool.
- **Model Switching Benefits**: The community suggests utilizing model switching to save flow actions, advocating for Cascade to be the default model while treating external models as supplementary.
   - Users express the importance of understanding how context is maintained across different models to optimize their workflows.
- **Improvements Suggested for Cascade**: There's a call for an upgraded Cascade Base model and the implementation of features like web searching and custom instructions.
   - Users feel that these enhancements could elevate the performance and usability of Windsurf significantly.
- **User Experiences with AI Models**: Users compare their experiences with different AI models, noting that while some models like Claude and 4o perform well, others like Cascade need further improvements.
   - The differences in performance and approach to task completion across models highlight the need for better integration of their functionalities.
- **Understanding AI Context**: Discussions highlight the concept of context windows in AI, with users emphasizing the need to manage and communicate context more effectively.
   - There is a consensus that better understanding and manipulation of context could enhance the practical utility of AI coding assistants.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://evalplus.github.io/leaderboard.html">EvalPlus Leaderboard</a>: no description found</li><li><a href="https://magic.dev/waitlist">Waitlist — Magic</a>: Magic is an AI company that is working toward building safe AGI to accelerate humanity’s progress on the world’s most important problems.</li><li><a href="https://codeium.com/contact">Contact | Windsurf Editor and Codeium extensions</a>: Contact the Codeium team for support and to learn more about our enterprise offering.</li><li><a href="https://addyo.substack.com/p/the-70-problem-hard-truths-about">The 70% problem: Hard truths about AI-assisted coding</a>: A field guide and why we need to rethink our expectations</li><li><a href="https://tenor.com/view/kekwtf-gif-18599263">Kekwtf GIF - Kekwtf - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://codeium.com/support">Support | Windsurf Editor and Codeium extensions</a>: Need help? Contact our support team for personalized assistance.</li><li><a href="https://tenor.com/view/oh-really-gif-24755231">Oh Really GIF - Oh Really - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://codeium.com/plan">Plan Settings</a>: Tomorrow&#x27;s editor, today. Windsurf Editor is the first AI agent-powered IDE that keeps developers in the flow. Available today on Mac, Windows, and Linux.</li><li><a href="https://www.reddit.com/r/LocalLLaMA/comments/1h7sjyt/windsurf_cascade_leaked_system_prompt/">Reddit - Dive into anything</a>: no description found</li><li><a href="https://codeium.com/support.">Page Not Found | Windsurf Editor and Codeium extensions</a>: Codeium is the AI code assistant platform that developers love and enterprises trust. Also the builders of Windsurf, the first agentic IDE.</li><li><a href="https://www.youtube.com/watch?v=SrPmkpgRbkE">I Spent 100 Hours with Cursor AI Agents and Here&#39;s What I Learned</a>: Cursor AI Agents present a new way to think about building apps.More Videos on AI Agents:  The Era of AI Agents: https://youtu.be/qc9fqCGgixM?si=dgqxKtUhp82I...</li><li><a href="https://www.infoworld.com/article/3617048/meta-quietly-leans-on-rival-gpt-4-despite-zuckerbergs-bold-llama-claims.html">Meta quietly leans on rival GPT-4 despite Zuckerberg’s bold Llama claims</a>: Even as Meta touts its Llama model, the company is incorporating OpenAI’s GPT-4 to enhance internal tools and philanthropic ventures.</li><li><a href="https://claude.site/artifacts/4a226c3a-09ae-4fb9-bbe5-026d11743be6">Claude Artifact</a>: Try out Artifacts created by Claude users</li><li><a href="https://magic.dev/blog/100m-token-context-windows">100M Token Context Windows — Magic</a>: Research update on ultra-long context models, our partnership with Google Cloud, and new funding.
</li>
</ul>

</div>
  

---


### **Cursor IDE ▷ #[general](https://discord.com/channels/1074847526655643750/1074847527708393565/1314683715032711249)** (929 messages🔥🔥🔥): 

> `Cursor performance issues, Current state of AI models, API usage and pricing comparison, Comparative experiences with Cursor and Windsurf, Feedback on open AI models` 


- **Debate about Cursor's capabilities**: Users have expressed frustration over Cursor's recent performance drops, noting issues with file modifications, context understanding, and overall generation quality, particularly using Claude models.
   - Some users argue that the quality drop is linked to a high demand on the models, while others advocate for maintaining a focused and clear prompting strategy to maximize results.
- **API pricing and usage insights**: Discussion surrounds the cost-effectiveness of using OpenAI's O1 Pro API, with users expressing reluctance to pay separate fees for multiple subscriptions when using Cursor.
   - The community suggests exploring group buys to lower costs and evaluates whether the benefits justify the expense based on individual use cases.
- **Comparison between Cursor and Windsurf**: Users share their contrasting experiences with Cursor and Windsurf, with some finding Windsurf's features more reliable, particularly for creating project structures.
   - Cursor's customization through features like `.cursorrules` and AI tools is highlighted, although some still prefer the simplicity and direct outputs from Windsurf.
- **Feedback and feature requests**: There are requests for improved documentation handling, Git integration, and the ability to manage larger context files in Cursor to enhance usability.
   - Several participants suggest that better testing and smoother transitions in updates would significantly improve user satisfaction with Cursor.
- **Experiences with AI-generated code**: Participants discuss varying results from AI models such as Claude and O1, with experiences ranging from effective code generation to frustrating hallucinations and irrelevant outputs.
   - Encouraging precise problem definitions in prompts is emphasized as crucial for optimizing the effectiveness of the assistance provided by any AI model.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://x.com/VahidK/status/1865140156812136924">Tweet from Vahid Kazemi (@VahidK)</a>: In my opinion we have already achieved AGI and it’s even more clear with O1. We have not achieved “better than any human at any task” but what we have is “better than most humans at most tasks”. Some ...</li><li><a href="https://aistudio.google.com/">Google AI Studio</a>: Google AI Studio is the fastest way to start building with Gemini, our next generation family of multimodal generative AI models.</li><li><a href="https://poe.com/">Poe - Fast, Helpful AI Chat</a>: no description found</li><li><a href="https://forum.cursor.com/t/feature-request-long-context-mode/32187/5">Feature request: Long context mode</a>: I think this method can replicate the functionally of long context with more user control and some adjustments: Long context mode gone in newest update - #49 by fun_strange</li><li><a href="https://x.com/mckaywrigley/status/1865089975802646857?s=46">Tweet from Mckay Wrigley (@mckaywrigley)</a>: OpenAI o1 pro is *significantly* better than I anticipated.This is the 1st time a model’s come out and been so good that it kind of shocked me.I screenshotted Coinbase and had 4 popular models write c...</li><li><a href="https://aider.chat/docs/leaderboards/">Aider LLM Leaderboards</a>: Quantitative benchmarks of LLM code editing skill.</li><li><a href="https://forum.cursor.com/t/an-idiots-guide-to-bigger-projects/23646?u=ossianravn">An Idiot&#39;s Guide To Bigger Projects</a>: ⚠ Warning: Mammoth post ahead ⚠ …   Estimated reading time: ~6 mins, or around 0.000014% of your life.  If you’ve been using Cursor for a while, and started to get into more complex projects with it, ...</li><li><a href="https://x.com/Presidentlin/status/1865370199160979963">Tweet from Lincoln 🇿🇦 (@Presidentlin)</a>: Gemini -2.0-flash-exp added to cursorFrom /r/Bard</li><li><a href="https://changelog.cursor.com/">Cursor - The IDE designed to pair-program with AI.</a>: no description found</li><li><a href="https://github.com/mullvad/mullvadvpn-app/releases/">Releases · mullvad/mullvadvpn-app</a>: The Mullvad VPN client app for desktop and mobile. Contribute to mullvad/mullvadvpn-app development by creating an account on GitHub.</li><li><a href="https://mullvad.net/en/help/connecting-to-mullvad-vpn-from-restrictive-locations">Using Mullvad VPN in restrictive locations</a>: Learn how you can access Mullvad VPN from locations where downloading our app or connecting is difficult.</li><li><a href="https://framer.university/resources">Best Free Framer Resources — Framer University </a>: Discover the best free Framer resources for your next project, including Framer components, code overrides, animations, and effects. Elevate your Framer website with a curated selection of top-quality...
</li>
</ul>

</div>
  

---


### **Unsloth AI (Daniel Han) ▷ #[general](https://discord.com/channels/1179035537009545276/1179035537529643040/1314682807645110322)** (610 messages🔥🔥🔥): 

> `Fine-tuning Llama 3.3, Collators and packing, Using models on limited hardware, Performance of AWQ and LoRA, Sora and its impact` 


- **Fine-tuning Llama 3.3 on limited resources**: Users discussed the challenges of fine-tuning Llama 3.3 models on lower-end GPUs, particularly highlighting their experiences with costs and memory requirements.
   - One user mentioned achieving a reduced training time through careful parameter tuning despite the inherent limitations of their hardware.
- **Understanding collators and packing**: Aemonalgiz explained the purpose of collators in training, emphasizing that they help build efficient batches and can impact memory usage through methods like padding and packing.
   - Using packing instead of padding can optimize training speeds, while ensuring attention masks are properly defined to prevent learning errors.
- **Using Llama models in offline scenarios**: A user expressed interest in deploying Llama models on Android devices for offline use, aiming to create a 'Local GPT' that can interact with documents.
   - They inquired about strong local systems and how to make a functional mobile application from these models.
- **AWQ and LoRA training limitations**: Discussion revealed that AWQ and GPTQ are primarily for inference and do not support fine-tuning directly, suggesting a workflow to enable their use with LoRA adapters for training on int4 or fp16 models.
   - It was noted that while AWQ models have some advantages, most training activity is still expected to happen on int4 or fp16 base models.
- **Reactions to Sora and its effectiveness**: Community members critiqued Sora, suggesting that the model did not introduce significant advancements compared to existing architectures despite high parameters.
   - Concerns were raised about whether the investment into training such a model yielded noteworthy improvements in performance.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://lightning.ai/?">Lightning AI | Turn ideas into AI, Lightning fast</a>: The all-in-one platform for AI development. Code together. Prototype. Train. Scale. Serve. From your browser - with zero setup. From the creators of PyTorch Lightning.</li><li><a href="https://www.determined.ai/blog/lora-parameters">Finding the best LoRA parameters</a>: How alpha, rank, and learning rate affect model accuracy, and whether rank-stabilized LoRA helps.</li><li><a href="https://x.com/OpenAI/status/1865136373491208674">Tweet from OpenAI (@OpenAI)</a>: Today we previewed Reinforcement Fine-Tuning, a new model customization technique that enables organizations to build expert models for specific, complex tasks in domains such as coding, scientific re...</li><li><a href="https://huggingface.co/spaces/DAMO-NLP-SG/Video-LLaMA">Video LLaMA - a Hugging Face Space by DAMO-NLP-SG</a>: no description found</li><li><a href="https://huggingface.co/Mihaiii/Llama-3-pruned-45B-Drobeta-Turnu-Severin">Mihaiii/Llama-3-pruned-45B-Drobeta-Turnu-Severin · Hugging Face</a>: no description found</li><li><a href="https://huggingface.co/spaces/AtAndDev/marco-qwq-7B">marco-qwq-7B - a Hugging Face Space by AtAndDev</a>: no description found</li><li><a href="https://huggingface.co/unsloth/Llama-3.3-70B-Instruct-GGUF">unsloth/Llama-3.3-70B-Instruct-GGUF · Hugging Face</a>: no description found</li><li><a href="https://www.reddit.com/r/LocalLLaMA/comments/1c9u2jd/llama_3_70b_layer_pruned_from_70b_42b_by_charles/">Reddit - Dive into anything</a>: no description found</li><li><a href="https://x.com/UnslothAI/status/1865151062023512485">Tweet from Unsloth AI (@UnslothAI)</a>: Llama 3.3 versions including GGUF&#39;s + bnb 4-bit + original 16-bit are now on @HuggingFace!See all versions of Llama 3.3 here: https://huggingface.co/collections/unsloth/llama-33-all-versions-67535...</li><li><a href="https://huggingface.co/blog/packing-with-FA2">Improving Hugging Face Training Efficiency Through Packing with Flash Attention 2</a>: no description found</li><li><a href="https://huggingface.co/posts/smangrul/573120738895551">@smangrul on Hugging Face: &quot;🚨 New Release of 🤗PEFT!

1. New methods for merging LoRA weights. Refer this…&quot;</a>: no description found</li><li><a href="https://huggingface.co/unsloth/Llama-3.3-70B-Instruct-bnb-4bit">unsloth/Llama-3.3-70B-Instruct-bnb-4bit · Hugging Face</a>: no description found</li><li><a href="https://unsloth.ai/blog/llama3-1">Finetune Llama 3.1 with Unsloth</a>: Fine-tune and run Meta&#x27;s updated Llama 3.1 model with 6x longer context lengths via Unsloth!</li><li><a href="https://gist.github.com/fullstackwebdev/9e912fe4390c3a6959340afb19804566">gist:9e912fe4390c3a6959340afb19804566</a>: GitHub Gist: instantly share code, notes, and snippets.</li><li><a href="https://huggingface.co/unsloth/Llama-3.3-70B-Instruct-GGUF/tree/main">unsloth/Llama-3.3-70B-Instruct-GGUF at main</a>: no description found</li><li><a href="https://www.reddit.com/r/LocalLLaMA/comments/1c9u2jd/llama_3_70b_layer_pruned_from_70b_42b_by_charles/?rdt=53034">Reddit - Dive into anything</a>: no description found</li><li><a href="https://github.com/teknium1/ShareGPT-Builder">GitHub - teknium1/ShareGPT-Builder</a>: Contribute to teknium1/ShareGPT-Builder development by creating an account on GitHub.</li><li><a href="https://huggingface.co/unsloth?search_models=smol">unsloth (Unsloth AI)</a>: no description found</li><li><a href="https://github.com/unslothai/unsloth/pull/730">Update Model Conversion Command in `save.py` to `convert_hf_to_gguf.py` by malibayram · Pull Request #730 · unslothai/unsloth</a>: Update Model Conversion Command in save.py to convert_hf_to_gguf.pyDescription:This PR updates the model conversion command in save.py to use convert_hf_to_gguf.py, aligning with the latest tools...</li><li><a href="https://www.reddit.com/r/LocalLLaMA/comments/1h8c9fu/llama_33_on_hugging_face_ggufs_4bit_bitsandbytes/">Reddit - Dive into anything</a>: no description found</li><li><a href="https://huggingface.co/unsloth/Llama-3.3-70B-Instruct">unsloth/Llama-3.3-70B-Instruct · Hugging Face</a>: no description found</li><li><a href="https://github.com/ggerganov/llama.cpp">GitHub - ggerganov/llama.cpp: LLM inference in C/C++</a>: LLM inference in C/C++. Contribute to ggerganov/llama.cpp development by creating an account on GitHub.</li><li><a href="https://modular-model-spec.vercel.app">Modular Model Spec</a>: no description found
</li>
</ul>

</div>
  

---


### **Unsloth AI (Daniel Han) ▷ #[off-topic](https://discord.com/channels/1179035537009545276/1179039861576056922/1314813795352186910)** (30 messages🔥): 

> `Open-source projects, Harmony project, Mental health data, LLM competition, Natural Language Processing` 


- **Exciting Open-source Initiative: Harmony**: A member shared details about the **Harmony** project, which helps researchers retrospectively harmonize questionnaire items and meta-data using [Natural Language Processing](https://harmonydata.ac.uk/). This tool is useful for comparing and finding compatible versions of questionnaires across studies.
   - The project is based in London’s UCL and involves multiple universities, offering a competition to improve its LLM matching algorithms with prizes available, as noted [here](https://harmonydata.ac.uk/doxa/).
- **Competition to Enhance AI Matching Algorithms**: The Harmony project is running a competition where participants can train their own Large Language Models to improve matching algorithms that sometimes misinterpret sentence similarities. Anyone interested can enter the competition by registering on [DOXA AI](https://harmonydata.ac.uk/doxa/).
   - Competitors are encouraged to join the Harmony Discord server for discussions and updates, specifically in the 🏅「matching-challenge」 channel.
- **Community Insights on OpenAI and Market Value**: A discussion arose regarding frustrations with **OpenAI**, with one member suggesting that they may be losing market value due to not allowing reverse engineering of their models. This sentiment was echoed by others, indicating a shared perspective on OpenAI's protective strategies.
   - Members debated the implications of AI models competition in the market, with some feeling that OpenAI's actions are more about asset protection than fear of competition.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://tenor.com/view/how-you-feel-after-saying-that-awesome-dog-cool-dog-sussy-gif-23764852">How You Feel After Saying That Awesome Dog GIF - How You Feel After Saying That Awesome Dog Cool Dog - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://harmonydata.ac.uk/">Harmony | A global platform for contextual data harmonisation</a>: A global platform for contextual data harmonisation</li><li><a href="https://harmonydata.ac.uk/doxa/">Competition to train a Large Language Model for Harmony on DOXA AI | Harmony</a>: A global platform for contextual data harmonisation
</li>
</ul>

</div>
  

---


### **Unsloth AI (Daniel Han) ▷ #[help](https://discord.com/channels/1179035537009545276/1179777624986357780/1314693642098835507)** (214 messages🔥🔥): 

> `Multi-GPU training with Unsloth, Error resolution in unsloth installations, Optimization of models for specific tasks, Fine-tuning Llama models for various applications, Accessing and using model weights` 


- **Confirmation on Multi-GPU Training Support**: Users continued to confirm that Unsloth currently does not support multi-GPU training via DDP, highlighting the need for such functionality in Visual Instruction tuning Llama3.2-11B-Vision.
   - Members noted that using Unsloth on a single GPU has proven to be faster than on a multi-GPU setup, with discussions around specific hardware configurations.
- **Installing Unsloth and Resolving Environment Errors**: Users experienced installation issues with Unsloth, particularly in externally managed environments, leading to suggestions of using conda for better package management.
   - Several users exchanged commands to successfully install Unsloth, resolving dependency errors that arose during the setup.
- **Model Optimization for Specific Tasks**: Several participants discussed the performance of Llama models in various contexts, such as fine-tuning for text classification or optimization for multimodal datasets.
   - Members emphasized the importance of appropriate model configuration and discussed strategies to adjust the lm_head dimensions to fit different label sizes.
- **Fine-tuning Llama Models Effectively**: Users faced challenges fine-tuning Llama and related models, specifically regarding adapter training and handling of large context sizes.
   - The community shared insights on using existing scripts for fine-tuning while cautioning about data quality impacting model performance.
- **Accessing Model Weights for Deployment**: Members inquired about downloading Llama3.3 model weights for local deployment, with suggestions to refer to Unsloth documentation or Hugging Face repo for access.
   - Participants clarified the necessary steps to retrieve weights and discussed the implications of model versioning in training and deployment.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://huggingface.co/unsloth/SmolLM2-360M-bnb-4bit">unsloth/SmolLM2-360M-bnb-4bit · Hugging Face</a>: no description found</li><li><a href="https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing">Google Colab</a>: no description found</li><li><a href="https://docs.unsloth.ai/get-started/unsloth-notebooks">Unsloth Notebooks | Unsloth Documentation</a>: See the list below for all our notebooks:</li><li><a href="https://docs.unsloth.ai/get-started/all-our-models">All Our Models | Unsloth Documentation</a>: See the list below for all our GGUF, 16-bit and 4-bit bnb uploaded models</li><li><a href="https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md">llama.cpp/docs/build.md at master · ggerganov/llama.cpp</a>: LLM inference in C/C++. Contribute to ggerganov/llama.cpp development by creating an account on GitHub.</li><li><a href="https://github.com/timothelaborie/text_classification_scripts/blob/main/unsloth_classification.ipynb">text_classification_scripts/unsloth_classification.ipynb at main · timothelaborie/text_classification_scripts</a>: Scripts for text classification with llama and bert - timothelaborie/text_classification_scripts
</li>
</ul>

</div>
  

---


### **Unsloth AI (Daniel Han) ▷ #[showcase](https://discord.com/channels/1179035537009545276/1179779344894263297/1314945004996919418)** (2 messages): 

> `Awesome RAG, Upcoming Articles` 


- **Discussion on Awesome RAG Repository**: A member shared a [GitHub repository](https://github.com/lucifertrj/Awesome-RAG) focused on **RAG-VectorDB-Embeddings-LlamaIndex-Langchain**.
   - The repository invites contributions, providing a valuable resource for those interested in learning about these technologies.
- **Anticipation for Article 102**: A member expressed enthusiasm for a potential **Article 102**, indicating that the community is eager for more resources.
   - *Thanks for the great content!* was highlighted as feedback reflecting the community's appreciation.



**Link mentioned**: <a href="https://github.com/lucifertrj/Awesome-RAG/">GitHub - lucifertrj/Awesome-RAG: RAG-VectorDB-Embedings-LlamaIndex-Langchain</a>: RAG-VectorDB-Embedings-LlamaIndex-Langchain. Contribute to lucifertrj/Awesome-RAG development by creating an account on GitHub.

  

---


### **Unsloth AI (Daniel Han) ▷ #[research](https://discord.com/channels/1179035537009545276/1257011997250424842/1314849149530804266)** (11 messages🔥): 

> `OpenAI Triton library, AWQ quantization approach, Hyperfitting phenomenon, Memory-efficient optimization techniques, Text-based model releases` 


- **Unsloth adopts OpenAI Triton for efficient training**: Unsloth utilizes the [OpenAI Triton library](https://github.com/rkinas/triton-resources) for fast and memory-efficient training, sharing a curated list of valuable resources for learning about Triton.
   - The community shows enthusiasm, with one member expressing it as 'really cool'!
- **AWQ quantization vs Unsloth quantization**: Discussion arose regarding whether Unsloth's quantization is simply replicating the AWQ approach, which focuses on activation alone, while Unsloth considers both activation and weight quantization error.
   - Members concluded that both methods offer viable approaches, with Unsloth acknowledging parallels to AWQ.
- **Hyperfitting enhances long-sequence generation**: A paper on a method called hyperfitting highlighted its capability to reduce repetition in generated text, achieving better performance on long contexts while maintaining MMLU and GLUE scores.
   - The method involves training on a small dataset until loss is near zero, and a member expressed interest in experimenting with this technique.
- **Development of memory-efficient LLM optimizers**: A new work on memory-efficient optimization introduces *APOLLO*, an approach designed to improve the memory usage of widely known AdamW optimizers.
   - APOLLO coarse-fines the learning rate adaptation rule, allowing for improved scalability without relying on costly SVD operations.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://arxiv.org/abs/2412.05270">APOLLO: SGD-like Memory, AdamW-level Performance</a>: Large language models (LLMs) are notoriously memory-intensive during training, particularly with the popular AdamW optimizer. This memory burden necessitates using more or higher-end GPUs or reducing ...</li><li><a href="https://www.reddit.com/r/LocalLLaMA/comments/1h8ep1w/the_hyperfitting_phenomenon_sharpening_and/">Reddit - Dive into anything</a>: no description found</li><li><a href="https://arxiv.org/abs/2412.04318">The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for Open-Ended Text Generation</a>: This paper introduces the counter-intuitive generalization results of overfitting pre-trained large language models (LLMs) on very small datasets. In the setting of open-ended text generation, it is w...</li><li><a href="https://github.com/rkinas/triton-resources">GitHub - rkinas/triton-resources: A curated list of resources for learning and exploring Triton, OpenAI&#39;s programming language for writing efficient GPU code.</a>: A curated list of resources for learning and exploring Triton, OpenAI&#39;s programming language for writing efficient GPU code. - rkinas/triton-resources
</li>
</ul>

</div>
  

---


### **aider (Paul Gauthier) ▷ #[general](https://discord.com/channels/1131200896827654144/1131200896827654149/1314692687508537385)** (430 messages🔥🔥🔥): 

> `Gemini model performance, O1 Pro vs Sonnet, AI assistant usage for coding, File handling in O1 Pro, Quantum computing advancements` 


- **Discussion on Gemini Model Performance**: Users discussed their experiences with the new `gemini-exp-1206` model, noting it is perceived as stronger than Sonnet 3.5, but highlights concerns regarding its lower leaderboard ranking for correct format.
   - The model's performance peaked at 69% with diff and around 80.5% with whole, leading to discussions on how to improve its use in coding tasks.
- **O1 Pro's Efficiency in Coding Tasks**: O1 Pro was praised for its superior reasoning abilities in debugging and architecting code compared to Sonnet, with users rating it highly for handling complex bug fixes.
   - Users are weighing the high cost of $200 against its efficiency, with some considering using O1 Pro over existing tools only if they could see substantial improvements.
- **AI Assistant Usage for Coding**: Discussions highlighted how AI tools like Aider and O1 are utilized for generating and debugging code, suggesting they serve different purposes in workflow efficiency.
   - Users shared strategies for leveraging these tools, including using O1 for complex tasks and Claude for day-to-day operations to optimize costs and performance.
- **File Handling Capabilities in O1 Pro**: Queries arose about O1 Pro's ability to attach code files, noting that currently only images can be attached, with expectations for future updates to enhance this functionality.
   - The community anticipates improvements during the extended promotional period, highlighting the need for better file handling to enhance usability.
- **Quantum Computing Advancements**: Users discussed Google's announcement of the Willow quantum computing chip, which is said to significantly reduce computation time on complex tasks compared to traditional supercomputers.
   - Concerns were raised about its practical applications outside specialized fields, with hopes for improved programming languages or SDKs for quantum chips.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://x.com/Presidentlin/status/1865370199160979963">Tweet from Lincoln 🇿🇦 (@Presidentlin)</a>: Gemini -2.0-flash-exp added to cursorFrom /r/Bard</li><li><a href="https://aider.chat/docs/usage/copypaste.html">Copy/paste with web chat</a>: Aider works with LLM web chat UIs</li><li><a href="https://aider.chat/docs/more/edit-formats.html#udiff">Edit formats</a>: Aider uses various “edit formats” to let LLMs edit source files.</li><li><a href="https://aider.chat/docs/usage/tips.html">Tips</a>: Tips for AI pair programming with aider.</li><li><a href="https://aider.chat/docs/usage/copypaste.html#terms-of-service">Copy/paste with web chat</a>: Aider works with LLM web chat UIs</li><li><a href="https://aider.chat/docs/leaderboards/">Aider LLM Leaderboards</a>: Quantitative benchmarks of LLM code editing skill.</li><li><a href="https://www.reddit.com/r/singularity/comments/1h90tqx/_/">Reddit - Dive into anything</a>: no description found</li><li><a href="https://x.com/sundarpichai/status/1866167562373124420">Tweet from Sundar Pichai (@sundarpichai)</a>: We see Willow as an important step in our journey to build a useful quantum computer with practical applications in areas like drug discovery, fusion energy, battery design + more. Details here: https...</li><li><a href="https://www.reddit.com/r/LocalLLaMA/comments/1h7sjyt/windsurf_cascade_leaked_system_prompt/">Reddit - Dive into anything</a>: no description found</li><li><a href="https://esbuild.github.io/api/#splitting,">esbuild - API</a>: no description found</li><li><a href="https://github.com/raphaelmansuy/code2prompt">GitHub - raphaelmansuy/code2prompt: Code2Prompt is a powerful command-line tool that simplifies the process of providing context to Large Language Models (LLMs) by generating a comprehensive Markdown file containing the content of your codebase. ⭐ If you find Code2Prompt useful, consider giving us a star on GitHub! It helps us reach more developers and improve the tool. ⭐</a>: Code2Prompt is a powerful command-line tool that simplifies the process of providing context to Large Language Models (LLMs) by generating a comprehensive Markdown file containing the content of yo...</li><li><a href="https://github.com/lanqian528/chat2api">GitHub - lanqian528/chat2api: A service that can convert ChatGPT on the web to OpenAI API format.</a>: A service that can convert ChatGPT on the web to OpenAI API format. - lanqian528/chat2api</li><li><a href="https://github.com/mufeedvh/code2prompt">GitHub - mufeedvh/code2prompt: A CLI tool to convert your codebase into a single LLM prompt with source tree, prompt templating, and token counting.</a>: A CLI tool to convert your codebase into a single LLM prompt with source tree, prompt templating, and token counting. - mufeedvh/code2prompt</li><li><a href="https://github.com/Aider-AI/aider.git">GitHub - Aider-AI/aider: aider is AI pair programming in your terminal</a>: aider is AI pair programming in your terminal. Contribute to Aider-AI/aider development by creating an account on GitHub.
</li>
</ul>

</div>
  

---


### **aider (Paul Gauthier) ▷ #[questions-and-tips](https://discord.com/channels/1131200896827654144/1133060505792159755/1314708756000673842)** (78 messages🔥🔥): 

> `Aider's functionality and modes, Troubleshooting API rate limits, Script automation in Aider, Aider integration with language servers, Aider's approach to handling new files` 


- **Understanding Aider's Role and Modes**: Users discussed the functionality of Aider, specifically regarding its Architect and Editor modes, questioning if Architect mode should generate code or just plan.
   - One member suggested that Aider should use the QWQ and Qwen models alone for simpler tasks.
- **API Rate Limit Issues**: Several users encountered rate limit errors when using Aider with OpenAI's API, leading to discussions about how token limits are applied over time and across different sessions.
   - One user noted confusion over high token usage and whether Aider's approach affected API limits, especially after pauses in usage.
- **Automating Multiple Prompts in Aider**: A member described their process of managing multiple prompts to format a CV and sought ways to automate chaining these prompts together.
   - It was suggested that members could use scripting options in Aider for command-line automation, leveraging batch processing of multiple files.
- **Integrating Aider with Language Servers**: A user inquired about the integration between Aider and language servers for enhanced code exploration through features like 'find references' and 'go to definition'.
   - The discussion noted that Aider utilizes a repo map to understand the overall structure and relations of the codebase, potentially benefitting such integrations.
- **Managing New Files with Aider**: Members raised concerns about Aider's ability to recognize and reference new files created during a session and how to refresh the file list.
   - It was highlighted that including relevant files and managing git interactions is key to ensuring Aider's efficacy in larger codebases.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://aider.chat/docs/usage/tips.html">Tips</a>: Tips for AI pair programming with aider.</li><li><a href="https://aider.chat/docs/repomap.html">Repository map</a>: Aider uses a map of your git repository to provide code context to LLMs.</li><li><a href="https://aider.chat/docs/scripting.html">Scripting aider</a>: You can script aider via the command line or python.</li><li><a href="https://aider.chat/docs/more/analytics.html">Analytics</a>: Opt-in, anonymous, no personal info.</li><li><a href="https://x.com/tom_doerr/status/1865825047749013864?s=46&t=FfXrBepo4K-8IYa7B4PHlg">Tweet from Tom Dörr (@tom_doerr)</a>: I asked Aider (Sonnet) to fix a database error and it deleted the database 😭</li><li><a href="https://aider.chat/docs/faq.html#can-i-use-aider-in-a-large-mono-repo">FAQ</a>: Frequently asked questions about aider.</li><li><a href="https://www.swebench.com/">SWE-bench</a>: no description found
</li>
</ul>

</div>
  

---


### **Modular (Mojo 🔥) ▷ #[general](https://discord.com/channels/1087530497313357884/1098713601386233997/1315271321646661643)** (61 messages🔥🔥): 

> `Mojo Compiler Features, Forum Bug Reports, Merchandise Requests, AI-Generated Content Policy` 


- **Mojo Compiler Optimizations**: Discussion highlighted the use of dynamic optimization for SIMD size in Mojo to address hardware compatibility issues, with some suggesting multiversioning features similar to C/C++ compilers.
   - Members expressed that Mojo's compilation process could lead to performance benefits but raise portability concerns for affected user systems.
- **Forum User Experience Issues**: Several users reported experiencing a rate limit when submitting bug reports on the forum and discussed UI bugs along with various functionalities that were non-operational.
   - In particular, a tab labeled 'Users' in user preferences was noted for its lack of purpose, which moderators confirmed might become functional with account age.
- **Merchandise and T-Shirt Requests**: A user expressed a strong desire for a T-shirt and pointed out the need for a merch shop to enhance community engagement.
   - This request sparked light-hearted conversation about possible hats in addition to T-shirts, keeping the mood upbeat.
- **404 Page Behavior**: Users noted that when encountering a 404 page, inputting a search query shorter than 3 characters results in an error message rather than redirecting to the search page.
   - It was suggested that users should have a more forgiving experience, with clear feedback on query length and improved navigation.
- **AI-Generated Content Policy**: A moderator announced that any obviously AI-generated content will be deleted and the authors warned, emphasizing the importance of maintaining genuine discussions on the forum.
   - This policy aims to create a fun and authentic community atmosphere, with promotions during swag challenges unaffected by AI contributions.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://forum.modular.com/search?q=eee">Search results for &#39;eee&#39; - Modular</a>: no description found</li><li><a href="https://forum.modular.com/t/simplifying-gpu-programming-with-parametric-tile-level-tensors-in-mojo-llvm-developers-meeting-2024/38">Simplifying GPU programming with parametric tile-level tensors in Mojo (LLVM Developers&#39; Meeting 2024)</a>: no description found</li><li><a href="https://github.com/modularml/mojo/issues/3651">[Feature Request] Function multiversioning · Issue #3651 · modularml/mojo</a>: Review Mojo&#39;s priorities I have read the roadmap and priorities and I believe this request falls within the priorities. What is your request? What I would like is an equivalent capability to Clang...
</li>
</ul>

</div>
  

---


### **Modular (Mojo 🔥) ▷ #[announcements](https://discord.com/channels/1087530497313357884/1098765954302873621/1315740520202502165)** (1 messages): 

> `Modular Forum Launch, Discord's Role, Swag Challenge, Ask Ahmed about GPU Programming, Forum Feedback` 


- **Modular Forum Opens Its Doors**: The **Modular forum** is now live at [forum.modular.com](http://forum.modular.com/), inviting community members to explore and contribute.
   - This platform aims to provide official responses, deep dives into technical issues, and support for future Modular users through indexed posts.
- **Discord Remains Active**: The **Discord** community will continue to thrive, serving as a space for quick chats and casual interactions.
   - Members are encouraged to use the forum for official queries and detailed discussions.
- **Celebrate with the Swag Challenge**: A **Swag Challenge** kicks off the forum launch, rewarding the top 5 users with points by today's end with **Mojo T-shirts**.
   - Points can be earned by creating new posts and engaging with existing content.
- **Engage with Ahmed on GPU Programming**: Members can ask Ahmed their burning questions about **GPU Programming with Mojo** during his talk recap from the **2024 LLVM Developers’ Meeting**: [*Simplifying GPU Programming*](https://forum.modular.com/t/simplifying-gpu-programming-with-parametric-tile-level-tensors-in-mojo-llvm-developers-meeting-2024/38).
   - Ahmed will be available to respond to queries throughout the day.
- **Feedback Wanted for the Forum**: Community members are encouraged to share their thoughts on the new forum in the [Forum Feedback category](https://forum.modular.com/c/feedback/2).
   - The Modular team is eager to receive any constructive insights to enhance the platform.


  

---


### **Modular (Mojo 🔥) ▷ #[mojo](https://discord.com/channels/1087530497313357884/1151418092052815884/1314735948969541662)** (283 messages🔥🔥): 

> `Mojo Language Features, Linear Types Proposal, Game Development with Mojo, Comparison with Other Languages, Memory Management in Programming` 


- **Discussion on Linear Types Proposal**: A proposal for linear and explicitly destroyed types in Mojo was shared, with comments on its readability and utility in preventing errors in GUI work related to calling the correct destroy methods.
   - Questions arose regarding the choice to implement a new 'destroy' keyword instead of reusing Python's 'del', with ideas about scope and usage in linear struct contexts.
- **Mojo's Low-Level Programming Potential**: Participants discussed how Mojo is positioned in terms of low-level programming capabilities compared to other languages like Rust and C++, highlighting its ability to combine powerful abstractions with speed.
   - Contributors noted that while Mojo focuses on systems programming, it also aims to cater to higher-level use cases across various application domains.
- **Comparison of Mojo with Vale and Other Languages**: The conversation included comparisons between Mojo and other languages such as Vale, Zig, and Odin, focusing on their strengths and target applications.
   - Mojo was described as prioritizing low-level programming while offering more abstraction compared to Vale, which is aimed at high-performance use cases without direct hardware access.
- **Game Development Interest in Mojo**: Interest in using Mojo for game development was expressed, indicating a curiosity about how well it could perform in contrast to established languages like C# and C++.
   - Participants recognized the challenges and potential of applying Mojo in game development contexts, as discussions on language capabilities continued.
- **Technical Discussions on Memory Management**: Discussions highlighted the ongoing research in memory management and the need for effective allocator stories in Mojo to enhance its low-level programming capabilities.
   - Insights were shared about the differences in memory management approaches among various languages, emphasizing the need for efficient and flexible systems in game development and systems programming.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://tenor.com/view/hermes-lets-do-this-futurama-gif-9434197">Hermes Lets Do This GIF - Hermes Lets Do This Futurama - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://forum.modular.com/t/dynamic-traits-for-easier-programming/85?u=lesoup-mxd">Dynamic traits for easier programming</a>: For now, mojo’s Any (AnyType) doesn’t work as a standalone declaration inside functions and etc, it requires a static.  Would be cool to see something similar (or better) to C++&#39;s auto data type)</li><li><a href="https://nondot.org/sabre/Resume.html#talks">Chris Lattner's Resumé</a>: no description found</li><li><a href="https://x.com/wordgrammer/status/1865925226149859659?t=e0YSWflFwuBcCPgsk_mgIA&s=19">Tweet from wordgrammer (@wordgrammer)</a>: @_blinding_light I think they will try, but it won’t really work, LLMs used to be really good at learning Python bc there was a lot of training data for it. But I think they will eventually become bet...</li><li><a href="https://x.com/wordgrammer/status/1865917868623135221?t=z1kQkUhuk4Vsso-4uwJsnw&s=19">Tweet from wordgrammer (@wordgrammer)</a>: In 5 years, almost all code will be LLM generated. When this happens, a solid understanding of type systems, concurrency, and programming paradigms will be extremely useful. The people studying PLT no...</li><li><a href="https://www.youtube.com/watch?v=UavYVf0UEoc">Advanced Memory Management in Vale (with Evan Ovadia)</a>: Rust changed the discussion around memory management - this week&#39;s guest hopes to push that discussion even further.This week we&#39;re joined by Evan Ovadia, cr...</li><li><a href="https://github.com/modularml/mojo/issues/3848">[Feature Request] [mojo-lang] [proposal] Add Linear / Explicitly Destroyed Types · Issue #3848 · modularml/mojo</a>: Review Mojo&#39;s priorities I have read the roadmap and priorities and I believe this request falls within the priorities. What is your request? See Proposal for Linear / Explicitly Destroyed Types. ...</li><li><a href="https://www.youtube.com/watch?v=IpuvQUVB8Cg)">2024 LLVM Dev Mtg - Implementing Linear / Non-destructible Types in Vale and Mojo</a>: 2024 LLVM Developers&#39; Meetinghttps://llvm.org/devmtg/2024-10/------Implementing Linear / Non-destructible Types in Vale and MojoSpeaker: Evan Ovadia------Sli...</li><li><a href="https://github.com/modularml/mojo/pull/3548/files)">[stdlib] Move `StringRef` `find()` implementation to `Span` by martinvuyk · Pull Request #3548 · modularml/mojo</a>: Move StringRef find() implementation to Span
</li>
</ul>

</div>
  

---


### **Bolt.new / Stackblitz ▷ #[prompting](https://discord.com/channels/364486390102097930/1301167628416454737/1314940633378590860)** (16 messages🔥): 

> `Bolt functionality issues, Prompting conventions in Bolt, Feature implementation challenges, Variable sensitivity in prompts, Tools for improving prompts` 


- **Bolt struggles with functionality**: Members reported that certain features in **Bolt** are not working, such as the **add record button** that fails to respond when clicked.
   - It was noted that initial attempts often result in front-end creation, requiring more specific follow-up prompts to make features functional.
- **Need for better prompting conventions**: **User** expressed a desire for an effective prompting convention or tool for **Bolt** to minimize issues and optimize output.
   - Another member indicated they are actively developing such a tool to assist users in creating more effective prompts.
- **Variable casing issues frustrate users**: Concerns were raised about the AI changing variable names improperly, despite requests to maintain **case sensitivity** in character denominations.
   - Users reported frustration with **Claude** altering variables even when JSON formats were provided correctly.
- **Paid feature limitations in Bolt**: There was discussion indicating that the **diffing feature** in **Bolt** is only available as a paid option due to the extra resources required to run it.
   - This limitation poses challenges for users seeking more comprehensive functionality without incurring additional costs.
- **Community collaboration and sharing**: Members encouraged sharing ideas and tools for improving prompt effectiveness, indicating a supportive community atmosphere.
   - One user humorously requested permission to share a member's idea on Twitter, showcasing camaraderie and collaboration.



**Link mentioned**: <a href="https://www.youtube.com/watch?v=ofHGE-85EIA">I made a website that makes websites</a>: 📚 𝗠𝗮𝘁𝗲𝗿𝗶𝗮𝗹𝘀/𝗥𝗲𝗳𝗲𝗿𝗲𝗻𝗰𝗲𝘀:GitHub Repository (give it a star ⭐) → https://github.com/hkirat/bolt.newer0:00 - Introduction and Architecture Di...

  

---


### **Bolt.new / Stackblitz ▷ #[discussions](https://discord.com/channels/364486390102097930/680953097354215446/1314709247015256075)** (318 messages🔥🔥): 

> `Token Management in Bolt, Supabase Integration, Technical Issues with Bolt, Open Source vs Production Version, Community Resources for Bolt` 


- **Understanding Token Management**: Users discussed the nuances of token usage in Bolt, emphasizing that tokens reset monthly and do not carry over, which has caused frustration for some subscribers.
   - It was clarified that top-up tokens purchased separately can roll over while subscription tokens are lost at the end of the subscription period.
- **Upcoming Supabase Integration**: Announcements regarding the native Supabase integration for Bolt are forthcoming, with opportunities for early access by responding to tweets from the team.
   - The integration aims to enhance the development experience for building apps with databases and authentication seamlessly within Bolt.
- **Technical Challenges Users Face**: Many users reported issues such as failed dependency installations, infinite loading errors, and configurations with Firebase/IPFS that affect their development progress.
   - Support has been offered by community members who share troubleshooting tips and workarounds to help resolve issues encountered during development.
- **Open Source vs Production Version of Bolt**: The community discussed the distinction between the official open-source version of Bolt and the production version, cautioning against using them simultaneously for compatibility reasons.
   - Ongoing efforts are being made to align features and functionality between the two versions as the open-source community continues to contribute.
- **Community Resources and Knowledge Sharing**: The Bolters.io platform has been updated with community-driven resources, including app recommendations, troubleshooting guides, and links to educational videos.
   - Users are encouraged to participate in the community by sharing their own problems, seeking assistance, and contributing to the shared knowledge base.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://supabase.com/partners/integrations/supabase_wrapper_stripe">Stripe Wrapper | Works With Supabase</a>: A foreign data wrapper for Stripe developed using Supabase Wrappers.</li><li><a href="https://x.com/stackblitz/status/1865904408254620148">Tweet from StackBlitz (@stackblitz)</a>: Let&#39;s up the ante!Winner will also receive the first hoodie to ever feature the Bolt logo on it (!!!)+ a special message on each sleeve commemorating the amazingness of what you can build with Bol...</li><li><a href="https://Bolters.io">Bolters.io | Community Supported Tips, Tricks &#38; Knowledgebase for Bolt.new No-Code App Builder</a>: Documentation and guides for Bolt.new</li><li><a href="https://bolters.io/docs/understanding-cors/">Understanding CORS in WebContainer</a>: Learn about Cross-Origin Resource Sharing (CORS), its impact on WebContainer, and current limitations</li><li><a href="https://blog.stackblitz.com/posts/design-system-component-documentation/">How to document design system components</a>: Components are an important part of implementing design on the web. In this article, we cover best practices for documenting components that are part of a design system or component library.</li><li><a href="https://www.chakra-ui.com/docs/get-started/installation">Installation | Chakra UI</a>: How to install and set up Chakra UI in your project</li><li><a href="https://github.com/stackblitz/bolt.new">GitHub - stackblitz/bolt.new: Prompt, run, edit, and deploy full-stack web applications</a>: Prompt, run, edit, and deploy full-stack web applications - stackblitz/bolt.new</li><li><a href="https://github.com/stackblitz/bolt.new/issues/2985">Feature Request: Show .bolt folder in Bolt · Issue #2985 · stackblitz/bolt.new</a>: Is your feature request related to a problem? Please describe: Not a problem, just a minor annoyance. Describe the solution you&#39;d like: It would be nice if I could update things like the Bolt igno...
</li>
</ul>

</div>
  

---


### **OpenRouter (Alex Atallah) ▷ #[app-showcase](https://discord.com/channels/1091220969173028894/1092850552192368710/1315079691996237884)** (2 messages): 

> `Countless.dev launch, Claude 3.5 Sonnet updates, Integration with Poe` 


- **Countless.dev makes model comparison easy**: The newly launched [Countless.dev](https://www.producthunt.com/posts/countless-dev) is a free and open-source tool designed to help users **compare AI models**, including LLMs and vision models, making it easy to sort by **price, token limits, or features**.
   - It's currently live on **Product Hunt**, and the creator has requested support to achieve a **first place** ranking.
- **Claude 3.5 Sonnet surpasses expectations**: The updated Claude 3.5 Sonnet, titled **claude-3-5-sonnet-20241022**, boasts **better-than-Opus capabilities** while maintaining **Sonnet prices**, particularly excelling in coding and data science tasks.
   - New features include **enhanced visual processing** and **exceptional tool use** for complex, multi-step problem solving.
- **Integration with Poe for enhanced functionality**: Integration with **Poe** allows access to advanced features such as **OpenAI Whisper** and **Text-to-Speech**, broadening functionality for users.
   - This integration is part of ongoing updates to improve user experience and expands the capabilities of AI models.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://poe.com/adamD123">Adam - Poe</a>: no description found</li><li><a href="https://www.producthunt.com/posts/countless-dev"> Countless.dev - Discover, compare, and choose AI models—100% Free | Product Hunt</a>: Countless.dev makes it easy to explore, compare, and calculate costs for every AI model—LLMs, vision models, and more. Sort by price, token limits, or features, and find the perfect match for your use...
</li>
</ul>

</div>
  

---


### **OpenRouter (Alex Atallah) ▷ #[general](https://discord.com/channels/1091220969173028894/1094454198688546826/1314684258576633866)** (318 messages🔥🔥): 

> `Llama Models, API Errors, Sora Model Features, OpenRouter Rate Limits, Mistral Model Updates` 


- **Discussion on Llama Models' Performance**: Users expressed interest in the effectiveness of various models like **Llama 3.3** and **Hermes**, highlighting their smart functionalities and some being uncensored.
   - Insights were shared about **Llama** being a popular choice for its capabilities and lack of restrictions, with **old Gemini** also being mentioned.
- **Experience with API Errors**: A user reported experiencing 'Provider Returned Error' consistently with free models, indicating issues linked to **API limitations**.
   - Others mentioned that these errors could be due to overload from the provider, particularly with **Claude** AI, leading to frustrations in usage.
- **Sora Model Features and Comparisons**: Users discussed potential features of the **Sora** model, including its notable 'remix' feature for video editing, indicating a complex interface for user input.
   - There were inquiries about video-to-video capabilities, with some skepticism about how effective **Sora** might be in comparison to existing tools like **Runway**.
- **OpenRouter's Rate Limits**: Questions arose about OpenRouter's **rate limits**, with discussions around potential removal if users have sufficient credits.
   - The rationale for these limits includes preventing big swings in account balances before the caches expire, with a focus on maintaining low latency.
- **Mistral Model Development Updates**: Updates about **Mistral** models indicated that several unreleased models were recently pulled back shortly after being announced.
   - The community speculated on whether the new **Codestral** and **mistral-ocr** models would be made available soon after their leak through API notices.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://sora.com/">Sora</a>: Transform text and images into immersive videos. Animate stories, visualize ideas, and bring your concepts to life.</li><li><a href="https://openrouter.ai/chat">Chatroom | OpenRouter</a>: LLM Chatroom is a multimodel chat interface. Add models and start chatting! Chatroom stores data locally in your browser.</li><li><a href="https://community.sambanova.ai/t/release-notes-december-6-2024/731">Release Notes - December 6, 2024</a>: December 5, 2024 We’re thrilled to introduce some of the most exciting Qwen models, along with the leading content moderation moderation model, llama Guard 3, now available on the SambaNova Cloud.    ...</li><li><a href="https://openrouter.ai/api/v1`">OpenRouter</a>: A unified interface for LLMs. Find the best models &amp; prices for your prompts</li><li><a href="https://openrouter.ai/docs/quick-start">Quick Start | OpenRouter</a>: Start building with OpenRouter</li><li><a href="https://internvl.github.io/blog/2024-12-05-InternVL-2.5/">InternVL2.5</a>: no description found</li><li><a href="https://inference.net">Inference.net</a>: Affordable Generative AI</li><li><a href="https://x.com/DeepInfra/status/1865126860902011244">Tweet from DeepInfra (@DeepInfra)</a>: 🚨 Big news! @DeepInfra supports Llama 3.3 70B on day 0 at the lowest prices:Llama 3.3 70B (bf16): $0.23/$0.40Llama 3.3 70B Turbo (fp8): $0.13/$0.40 in/out per 1MExperience cutting-edge AI with seamle...</li><li><a href="https://docs.mistral.ai/getting-started/models/models_overview/">Models Overview | Mistral AI Large Language Models</a>: Mistral provides two types of models: free models and premier models.</li><li><a href="https://huggingface.co/collections/LGAI-EXAONE/exaone-35-674d0e1bb3dcd2ab6f39dbb4">EXAONE-3.5 - a LGAI-EXAONE Collection</a>: no description found</li><li><a href="https://openrouter.ai/docs/limits">Limits | OpenRouter</a>: Set limits on model usage</li><li><a href="https://openrouter.ai/">OpenRouter</a>: A unified interface for LLMs. Find the best models &amp; prices for your prompts</li><li><a href="https://openrouter.ai/google/gemini-exp-1121:free/uptime)">Google: Gemini Experimental 1121 (free)</a>: Experimental release (November 21st, 2024) of Gemini.</li><li><a href="https://openrouter.ai/google/gemini-exp-1206:free">Gemini Experimental 1206 (free) - API, Providers, Stats</a>: Experimental release (December 6, 2024) of Gemini.. Run Gemini Experimental 1206 (free) with API</li><li><a href="https://fal.ai">fal.ai | The generative media platform for developers</a>: fal.ai is the fastest way to run diffusion models with ready-to-use AI inference, training APIs, and UI Playgrounds</li><li><a href="https://status.anthropic.com/">Anthropic Status</a>: no description found</li><li><a href="https://huggingface.co/OpenGVLab/InternVL2_5-78B">OpenGVLab/InternVL2_5-78B · Hugging Face</a>: no description found</li><li><a href="https://www.reddit.com/r/singularity/comments/1h9ycjg/google_ceo_ai_development_is_finally_slowing_down/">Reddit - Dive into anything</a>: no description found</li><li><a href="https://github.com/billmei/every-chatgpt-gui/blob/main/README.md">every-chatgpt-gui/README.md at main · billmei/every-chatgpt-gui</a>: Every front-end GUI client for ChatGPT, Claude, and other LLMs - billmei/every-chatgpt-gui
</li>
</ul>

</div>
  

---


### **OpenRouter (Alex Atallah) ▷ #[beta-feedback](https://discord.com/channels/1091220969173028894/1277894087755829278/1314742260591759360)** (13 messages🔥): 

> `Integration Beta Feature Requests, Custom Provider Keys, Amazon Bedrock Model Integrations, Google Flash Model Access` 


- **Multiple Requests for Integration Beta Feature Access**: Several users have requested access to the **integration beta feature**, indicating a strong interest in trying out this functionality.
   - *Hi, I'd like to request access to the integration beta feature.* was a common theme across various messages.
- **Interest in Custom Provider Keys**: One user expressed a desire to try out the **custom provider keys**, highlighting the variety of integration options available.
   - The request demonstrates the need for enhanced functionality in the integration landscape.
- **Proposed Model Integrations for Amazon Bedrock**: A member suggested adding **Opus** and **Mistral Large** to the models recognized by **Amazon Bedrock** for integrations.
   - This proposal emphasizes ongoing interest in expanding available models within current integration capabilities.
- **Access Request for Google Flash Model**: One user mentioned seeking access to the **Google Flash 1.5 Model**, suggesting specific technical interests.
   - *Hi I saw that I was to come here to get the beta for access to Google Flash 1.5 Model.* indicates platform guidance for accessing models.


  

---


### **LM Studio ▷ #[general](https://discord.com/channels/1110598183144399058/1110598183144399061/1314688528688484353)** (173 messages🔥🔥): 

> `LM Studio GPU Usage, Aider Integration Issues, Model Compatibility with LM Studio, Frontend Clients for LM Studio, Hardware Recommendations for AI Models` 


- **Understanding LM Studio's GPU Usage**: Users with RX 6600 GPUs have realized that LM Studio employs Vulkan for GPU offloading, allowing them to run models without needing ROCm installed.
   - This opens up possibilities for AMD users who might be unfamiliar with how LM Studio utilizes their hardware effectively.
- **Challenges with Aider Integration**: Integration with Aider has been difficult for some users, particularly due to issues with API key settings and environmental variable configurations.
   - To resolve these issues, users have been encouraged to set a random API key and ensure they refer to the Aider documentation for proper setup.
- **Model Compatibility Concerns**: Users expressed frustration regarding the lack of support for models like Qwen2 VL 7B Instruct in LM Studio, limiting options for those interested in utilizing new vision models.
   - Alternative suggestions, such as using Florence-2 via Pinokio, were recommended for exploring other options for visual models.
- **Recommendations for Frontend Clients**: Several alternatives to LM Studio for connecting to LLM servers were recommended, including AnythingLLM and Open WebUI.
   - Users were encouraged to explore these options for varied features and functionalities that serve specific needs.
- **Hardware Recommendations for Running AI Models**: Discussions highlighted the necessity of matching GPU specifications to model requirements, especially with models demanding high VRAM.
   - Users were informed about viable options for powerful GPUs like the A100, which are available at competitive prices, facilitating enhanced AI model performance.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://aider.chat/docs/llms/lm-studio.html">LM Studio</a>: aider is AI pair programming in your terminal</li><li><a href="https://tenor.com/view/ducktales-ducktales2017-infernal-internship-of-mark-beaks-mustache-disguise-gif-21524651">Ducktales Ducktales2017 GIF - Ducktales Ducktales2017 Infernal Internship Of Mark Beaks - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://github.com/open-webui/open-webui">GitHub - open-webui/open-webui: User-friendly AI Interface (Supports Ollama, OpenAI API, ...)</a>: User-friendly AI Interface (Supports Ollama, OpenAI API, ...) - open-webui/open-webui</li><li><a href="https://www.youtube.com/watch?v=OY2x0TyKzIQ">This Video is AI Generated! SORA Review</a>: SORA generates videos. This is the first review.Get up to 40% off on last minute gifts at https://ridge.com/MKBHDThe (real) birding video: https://youtu.be/F...</li><li><a href="https://www.reddit.com/r/LocalLLaMA/comments/1hah3wi/im_afraid_to_ask_but_how_do_i_actually_quit_lm/">Reddit - Dive into anything</a>: no description found</li><li><a href="https://lmstudio.ai/beta-releases">LM Studio Beta Releases</a>: LM Studio Beta Releases</li><li><a href="https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/133">Feature Request: Use LM Studio as a Client for a different LLM Server in the local Network. · Issue #133 · lmstudio-ai/lmstudio-bug-tracker</a>: LM Studio already allows to create a server and use it for api requests. But it does not allow LM Studio to act as a client for that Server. Here is the scenario: I have one powerful machine in my ...</li><li><a href="https://msty.app/pricing">Pricing</a>: AI beyond just plain chat. Private, Offline, Split chats, Branching, Concurrent chats, Web Search, RAG, Prompts Library, Vapor Mode, and more. Perfect LM Studio, Jan AI, and Perplexity alternative. Us...</li><li><a href="https://anythingllm.com">AnythingLLM | The all-in-one AI application for everyone</a>: AnythingLLM is the AI application you've been seeking. Use any LLM to chat with your documents, enhance your productivity, and run the latest state-of-the-art LLMs completely privately with no technic...</li><li><a href="https://openwebui.com">Open WebUI</a>: no description found</li><li><a href="https://www.ebay.com/itm/405215504640?_skw=nvidia+sxm2+a100+automotive&epid=22065174652&itmmeta=01JEN1KE9RQ1YWBFDDRKZW5DPD&itmprp=enc%3AAQAJAAAA4HoV3kP08IDx%2BKZ9MfhVJKlXVKcAPbbt4BKfHIZRKF59hbTZ1feCYGryXOYSawI6iKe9dLwqKsvwyNsCuUZjELMABTGofOnpvUo%2BtMQUb4pAg%2FwjOuKyc2GZiUSd6pdqXc%2B6Ut0kipS6Bz6%2BzSc7ziHnwnVysS9gbVBVYzQ1G7I9E9L8wCnnn9L1yV5ceMvTBC28Mg3VdqQIt8Rt9Nz1d1pDx4Nfdop7IXSq8hf%2FaXUZadVQpxnlzlVLFInHm6MHdyncyvXsT9cDQDDzWeo7PmE7sVSy8ukutomWpuWLnWnl%7Ctkp%3ABk9SR_rkzaH1ZA&LH_BIN=1">NVIDIA DRIVE A100 AUTOMOTIVE SXM2 GPU 900-6G199-0000-C00 FOR AUTONOMOUS VEHICLES  | eBay</a>: no description found</li><li><a href="https://www.ebay.com/itm/135424248001?_skw=SXM2+to+pcie+adapter&itmmeta=01JEN1JXXJC6PWD6X0M3QKBZ0E&itmprp=enc%3AAQAJAAAA8HoV3kP08IDx%2BKZ9MfhVJKnak56dAfzHQ0oL5hPTiPhgYnoItcFYiWxP00DVmq67ke61OerN%2F7BeKBYlANLGPPzPsr6GFxjWky7SRfpEEYUAch5L1yWS4qlaLyOxXHqSXmu10yJM8uP5%2FlLDLP5GYN9KRE4yT7k0dNAtLZ9NIDHZrXwn9k0DmpWzchuOTZTSAJifhe12RCp4fhubFqH9ErgX%2FkTWbNp1OvsXkcJOVY0ATVxxdAJsOr3%2FERd2FTWgsOWCglMHXCGT6n%2FSLFHyiLc91rrtG1R6UC6ITHVPJKB%2Br72vwO2%2FjmXu%2F6Hh8kt00Q%3D%3D%7Ctkp%3ABFBM8N7LofVk">SXM2 To PCIE Adapter For Nvidia Tesla V100  A100 SXM2 GPU  Computing  Graphics  | eBay</a>: no description found
</li>
</ul>

</div>
  

---


### **LM Studio ▷ #[hardware-discussion](https://discord.com/channels/1110598183144399058/1153759714082033735/1314684213408305273)** (98 messages🔥🔥): 

> `LM Studio server capabilities, GPU setups and cooling solutions, Memory bandwidth and CPU performance, ROCm vs CUDA compatibility, Custom GPU riser designs` 


- **LM Studio as a Server**: A member confirmed that sharing hardware over LM Studio with a friend 100 miles away is straightforward by using commands like 'start lm studio', 'start lms server 1234', and 'start ngrok tunnel'. They shared a [GitHub link](https://github.com/OIEIEIO/lm-studio-ngrok) detailing how to do this effectively.
   - Another member inquired whether this setup supports RAG functionality, which led to discussions about setting it up accordingly.
- **Insights on Optimal GPU Configurations**: Members discussed using 3090s and shared insights on their respective setups, emphasizing that going for used 3090s could be budget-friendly with performance goals. One member noted that there are better options in the used market, while another mentioned that a 48GB A6000 is the way to go if money isn't an issue.
   - The importance of memory bandwidth for performance was noted, with discussions on how more RAM channels can be beneficial in ML systems.
- **Challenges and Compatibility Issues of Mixed GPU Setups**: Concerns were raised about using ROCm alongside CUDA in a single machine, with members noting that it's primarily one or the other due to compatibility issues. It was suggested that while Vulkan works fine, ROCm is causing frustrations and is not functioning optimally with certain AMD GPU models.
   - Members shared experiences and offered solutions for using specific variables to manage GPU behavior, although it was noted that these solutions don't yield reliable results.
- **Custom GPU Riser and Cooling Solutions**: Discussions about custom GPU riser brackets highlighted the need for strong and secure designs to support heavy GPUs like the 3090s with custom coolers. A member shared a [Thingiverse link](https://thingiverse.com/thing:2536978) for a vertical GPU mount designed specifically for their setup, underlining the challenges in fitting multiple GPUs.
   - Members exchanged ideas on effective cooling solutions with high-performance water cooling setups, discussing the robustness necessary to handle high temperatures and workload demands.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://www.stlfinder.com/model/vertical-gpu-mount-HyJz7ZOE/1865168/">Vertical GPU Mount - STLFinder
</a>: no description found</li><li><a href="https://github.com/OIEIEIO/lm-studio-ngrok">GitHub - OIEIEIO/lm-studio-ngrok: How to Share Your Hardware and AI Frontend with a Friend 100 Miles Away - LM Studio Server</a>: How to Share Your Hardware and AI Frontend with a Friend 100 Miles Away - LM Studio Server - OIEIEIO/lm-studio-ngrok
</li>
</ul>

</div>
  

---


### **Latent Space ▷ #[ai-general-chat](https://discord.com/channels/822583790773862470/1075282825051385876/1314685140185780254)** (88 messages🔥🔥): 

> `Gemini exp 1206, Aurora image model, Sora video generation, WaveForms AI, NeurIPS conference` 


- **Gemini exp 1206 impresses**: Gemini exp 1206 has been receiving attention for its performance, particularly outperforming previous versions in various benchmarks and tasks. Users shared their experiences, noting improvements in coding assistance and benchmarks scores, including achieving record results on Aider's code editing benchmark.
   - However, some users expressed confusion over setup issues and the model's collaborative functionality in different environments like Cursor.
- **Aurora image model takes the stage**: The newly released Aurora image generation model by xAI is generating buzz, with early users remarking on its capabilities but also expressing disappointment in certain use cases. Comparisons to existing models suggested Aurora excelled in detail but faced challenges in cartoon rendering.
   - Questions about its relationship with Black Forest Labs, creators of Flux, were raised, indicating potential collaborations in the background.
- **Sora video generation capabilities revealed**: Sora v2 is set to enhance video generation capabilities with features like text-to-video and more detailed outputs. Prominent figures in AI shared their excitement for Sora's impending release, suggesting it might significantly impact user engagement.
   - During the launch, various demos showcased its potential, with many predicting a surge in usage tied to the Pro and Plus subscription tiers.
- **WaveForms AI targets Speech Turing Test**: WaveForms AI was announced, aiming to develop AI with emotional intelligence capabilities. The company's mission includes tackling the Speech Turing Test to improve human-like interactions in audio-based applications.
   - This new venture reflects the growing trend of integrating advanced emotional analytics into AI systems.
- **NeurIPS attendees from Toronto**: As NeurIPS kicks off, discussions about attendees flying from various locations, including Toronto, emerged in the chat. This highlights the conference's importance in gathering AI professionals and enthusiasts for networking and sharing cutting-edge research.
   - The excitement surrounding ongoing advancements in AI technology is palpable among participants attending the event.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://ollama.com/blog/structured-outputs">Structured outputs · Ollama Blog</a>: Ollama now supports structured outputs making it possible to constrain a model&#39;s output to a specific format defined by a JSON schema. The Ollama Python and JavaScript libraries have been updated ...</li><li><a href="https://x.com/alex_conneau/status/1866127388373098607">Tweet from Alexis Conneau (@alex_conneau)</a>: Excited to announce the creation of WaveForms AI (http://waveforms.ai) – an Audio LLM company aiming to solve the Speech Turing Test and bring Emotional Intelligence to AI @WaveFormsAI</li><li><a href="https://x.com/btibor91/status/1865109134066274444">Tweet from Tibor Blaho (@btibor91)</a>: I noticed during the &#34;12 Days of OpenAI: Day 2&#34; livestream today that the OpenAI Platform sidebar has a new icon, possibly related to one of the upcoming announcements - &#34;Custom Voices&#34...</li><li><a href="https://x.com/scaling01/status/1865492664994468284?s=46">Tweet from Lisan al Gaib (@scaling01)</a>: Just went through the public eval set of Simple Bench by @AIExplainedYT with Gemini Experimental 1206It got 4/10</li><li><a href="https://x.com/willdepue/status/1866184364859461988?s=46">Tweet from will depue (@willdepue)</a>: sora is launching today to all chatgpt pro and plus users!   it&#39;s been a big effort to make this possible + i think the product is really fun & intuitive.my fav thing to do is generate fake histor...</li><li><a href="https://x.com/chrisparkx/status/1865406193776074965?s=46">Tweet from Chris Park (@chrisparkX)</a>: xAI doesn&#39;t need to wait until Monday. This team is too cracked and stays shipping. Congrats @xai for releasing a brand new image gen model —Aurora! Grok 2 + Aurora is now available with your X ap...</li><li><a href="https://x.com/altryne/status/1865783380370977024?s=46">Tweet from Alex Volkov (Thursd/AI) (@altryne)</a>: I take back EVERYTHING I said about other video models catching up to SORA even remotely. Leaked video of SORA v2, showing 1 minute generations of txt2vid, img2vid, vid2vid, txt+vid2vid.https://x.com/...</li><li><a href="https://www.youtube.com/playlist?list=PLOXw6I10VTv8q5PPOsuECYDFqohnJqbYB">Sora Tutorials</a>: no description found</li><li><a href="https://x.com/jerber888/status/1865112099015291379?s=46">Tweet from Jeremy Berman (@jerber888)</a>: I just got first place on the public ARC-AGI benchmark using Claude Sonnet 3.5 and Evolutionary Test-time ComputeQuoting ARC Prize (@arcprize) 2024 ARC-AGI-Pub SoTA! 👾53.6% @jerber88847.5% MARA(BARC)...</li><li><a href="https://x.com/JeffDean/status/1865081640546156993">Tweet from Jeff Dean (@🏡) (@JeffDean)</a>: What a way to celebrate one year of incredible Gemini progress -- #1🥇across the board on overall ranking, as well as on hard prompts, coding, math, instruction following, and more, including with sty...</li><li><a href="https://x.com/OfficialLoganK/status/1865081419015352689">Tweet from Logan Kilpatrick (@OfficialLoganK)</a>: Gemini-exp-1206, our latest Gemini iteration, (with the full 2M token context and much more) is available right now for free in Google AI Studio and the Gemini API.I hope you have enjoyed year 1 of th...</li><li><a href="https://x.com/scaling01/status/1865221955202252938?s=46">Tweet from Lisan al Gaib (@scaling01)</a>: Today Gemini 2.0 DESTROYED everyone on lmsys: - kills o1 in math and coding ???- handily beats Claude 3.5 even with Style Control ???Meanwhile: - Meta: &#34;new LLaMa3.3-70B model go brrrrrrrr, you gu...</li><li><a href="https://x.com/scaling01/status/1865088711609770417">Tweet from Lisan al Gaib (@scaling01)</a>: GOD DAMN GOOGLE DID ITInstruction Following + Style Control</li><li><a href="https://llm-stats.com">LLM-Stats.com</a>: Statistics and insights about large language models</li><li><a href="https://x.com/scaling01/status/1865086810214289910?s=46">Tweet from Lisan al Gaib (@scaling01)</a>: Did Google already show their cards?Gemini-Exp-1114 being Gemini 2.0 FlashGemini-Exp-1121 being Gemini 2.0 ProGemini-Exp-1206 being Gemini 2.0 UltraIt could also be, that these are all just training c...</li><li><a href="https://x.com/paulgauthier/status/1865167742850208203">Tweet from Paul Gauthier (@paulgauthier)</a>: The new gemini-exp-1206 scored 69% on aider&#39;s code editing benchmark. This is a record for the Gemini family.https://aider.chat/docs/leaderboards/</li><li><a href="https://jeremyberman.substack.com/p/how-i-got-a-record-536-on-arc-agi">How I came in first on ARC-AGI-Pub using Sonnet 3.5 with Evolutionary Test-time Compute</a>: See my code on Params: https://params.com/@jeremy-berman/arc-agi</li><li><a href="https://x.com/ruudnl/status/1865425438991945938?s=46">Tweet from Ruud van der Linden (@RuudNL)</a>: Sora v2 release is impending:* 1-minute video outputs* text-to-video* text+image-to-video* text+video-to-videoOpenAI&#39;s Chad Nelson showed this at the C21Media Keynote in London. And he said we wil...</li><li><a href="https://x.com/levelsio/status/1865899245850517706?s=46">Tweet from @levelsio (@levelsio)</a>: From quick glance Grok&#39;s new image model Aurora looks higher in detail than Flux for generating photos of peopleWhat&#39;s crazy is how they&#39;ve been able to create an entirely new image model ...</li><li><a href="https://x.com/voooooogel/status/1865189744776507809?s=46">Tweet from thebes (@voooooogel)</a>: llama-3.3-70b correctly guesses the sampling constraint (only allowed to use words that are in the bible)Quoting thebes (@voooooogel) i wrote a custom llm sampler for llama-3.1-8b so it could only say...</li><li><a href="https://x.com/smokeawayyy/status/1865319093274108405?s=46">Tweet from Smoke-away (@SmokeAwayyy)</a>: Custom Instruction for Conversational AGI in ChatGPT o1Bookmark, use, and modify these custom instructions to create your own human-level AI companion.Enjoy.---&#34;You are a supporting human-like com...</li><li><a href="https://x.com/sama/status/1866187525821538436?s=46&t=b7l37rB6wtbyAh6ah1NpZQ">Tweet from Sam Altman (@sama)</a>: we are launching sora today, and we made a new product to go with it.if you have an openai plus or pro account, you can generate videos. anyone can view them.it will take some time to roll out, but by...</li><li><a href="https://x.com/shaunralston/status/1865116666440675647?s=46">Tweet from Shaun Ralston (@shaunralston)</a>: Don&#39;t miss ChatGPT Advanced Voice Mode with Vision, featured on @60Minutes this Sunday night (@CBS & @paramountplus), coming soon to your smartphone.</li><li><a href="https://countless.dev/">Countless.dev | AI Model Comparison</a>: Compare AI models easily! All providers in one place.</li><li><a href="https://x.com/tetumemo/status/1865125990483267896">Tweet from テツメモ｜AI図解×検証｜Newsletter (@tetumemo)</a>: 📝え？本当に良いの？さっきGoogleで発表されたGemini-exp-1206が、もうCursorで設定できて”無料”で使えちゃってる！！o1-preview、mini超えで総合ランキング1位モデルが、Google AI StudioやAPI利用で誰でも”無料”で使えるのは本当に凄い設定方法はリプ欄へQuoting Logan Kilpatrick (@OfficialLoganK) Gemi...</li><li><a href="https://www.youtube.com/live/2jKVx2vyZOY">Sora–12 Days of OpenAI: Day 3</a>: Sam Altman, Aditya Ramesh, Bill Peebles, Rohan Sahai, and Joey Flynn deliver Sora to the world.</li><li><a href="https://www.youtube.com/watch?v=YpFaPKOeNME">NEURAL NETWORKS ARE REALLY WEIRD...</a>: Neel Nanda, a senior research scientist at Google DeepMind, leads their mechanistic interpretability team. In this extensive interview, he discusses his work...</li><li><a href="https://www.youtube.com/watch?v=OY2x0TyKzIQ">This Video is AI Generated! SORA Review</a>: SORA generates videos. This is the first review.Get up to 40% off on last minute gifts at https://ridge.com/MKBHDThe (real) birding video: https://youtu.be/F...</li><li><a href="https://x.com/MKBHD/status/1866152437838393797">Tweet from Marques Brownlee (@MKBHD)</a>: The rumors are true - SORA, OpenAI&#39;s AI video generator, is launching for the public today...I&#39;ve been using it for about a week now, and have reviewed it: https://youtu.be/OY2x0TyKzIQTHE BELO...
</li>
</ul>

</div>
  

---


### **Latent Space ▷ #[ai-in-action-club](https://discord.com/channels/822583790773862470/1200548371715342479/1314698091844206669)** (136 messages🔥🔥): 

> `NeurIPS preparation, Networking at conferences, The role of tabular data in industry, Paper Club events, Communication tools for conferences` 


- **Get Ready for NeurIPS with Paper Club**: Join us this Sunday for the **Latent Space Paper Club** to discuss major papers and insights leading up to NeurIPS, with a blend of productivity and community vibes.
   - The event includes [Paper Discussion & Idea Jams](https://lu.ma/25mwbwcm) alongside a potluck dinner to mingle with friends from various AI communities.
- **Networking Tips: Hallway Track Is Key**: It was suggested that the **hallway track** at conferences is where the most valuable conversations happen, leading to great connections.
   - Attendees noted that business cards are becoming outdated, preferring to exchange Twitter handles and utilize conference apps for networking.
- **Importance of Tabular Data in Industry**: A discussion highlighted how **tabular data** remains relevant in industry applications like timeseries prediction and preventive maintenance.
   - Participants urged not to underestimate the value of tabular datasets, emphasizing their significance in practical AI implementations.
- **Communication Tools Among Conference Goers**: Various communication methods like **WeChat**, Twitter handles, and direct messaging on conference apps were discussed as ways to keep in touch.
   - Attendees expressed the need for effective channels to maintain connections post-conference, with some recommending aggregators for chat.
- **Joining the IRL Paper Club**: Excitement was noted about joining the in-person Paper Club as a way to engage with academic discussions more effectively.
   - Members mentioned that obtaining approvals for events is a quick process and expressed gratitude for the support from the community.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://visitingmedia.com/tt8/?ttid=vancouver-convention-centre#/3d-model/0/2?b11t0=1&tlzqx7l=535249">TrueTour® is the next best thing to being here! Experience it for yourself!</a>: Click here to view this amazing place in a completely new way. TrueTour® provides you with an immersive virtual experience to share through the web.</li><li><a href="https://lu.ma/25mwbwcm">NeurIPS Pre-Game &amp; Holiday Potluck · Luma</a>: Endless papers, so little time—let’s prep for NeurIPS together! 📚✨With the big week just around the corner, take a break from solo paper crunching and join…</li><li><a href="https://lu.ma/LSLIVE">Latent Space LIVE! at NeurIPS 2024 · Luma</a>: Let&#x27;s get together to send off 2024 with the first LIVE Latent Space Paper Club, hosted during NeurIPS! Instead of going paper-by-paper as NeurIPS does, we are…
</li>
</ul>

</div>
  

---


### **Eleuther ▷ #[general](https://discord.com/channels/729741769192767510/729741769738158194/1314710403111981097)** (21 messages🔥): 

> `Llama 3.3 Weights Release, Open-ended Information Storage Challenges, Text Adventure Continuity Issues, Eleuther Eval Harness Modification, JAX/Flax Model Integration` 


- **Llama 3.3 Weights Released**: A member uploaded the **16bit weights of Llama 3.3 70B Instruct** on [Hugging Face](https://huggingface.co/unsloth/Llama-3.3-70B-Instruct) for those awaiting approval, offering access to various formats.
   - They also referred to a collection of [all versions of Llama 3.3](https://huggingface.co/collections/unsloth/llama-33-all-versions-67535d7d994794b9d7cf5e9f), including GGUF and 4-bit formats.
- **Challenges in Storing Open-ended Information**: A member expressed concerns about the unpredictability of **RAG** compared to previous **knowledge graphs** for storing and retrieving open-ended information.
   - They highlighted the need for reliable approaches in **agent memory** and **question/answer systems** that deal with extensive proprietary information.
- **Text Adventure Game's Continuity Problems**: A member reported difficulties in maintaining **continuity and coherency** in a text adventure after hitting a conversation length limit, affecting character attachment.
   - They sought advice on potential LLMs that could better support ongoing text adventure narratives without losing context.
- **Modifying Eleuther Eval Harness Prompts**: A member requested guidance on modifying prompts for the **Eleuther eval harness**, stating a lack of available documentation.
   - Another member suggested checking the [interface documentation](https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/interface.md#external-library-usage) for a starting point.
- **Integrating JAX/Flax Models into Evaluation Harness**: A member inquired about ongoing efforts to adapt the **lm evaluation harness** for **jax/flax models**, having trouble connecting their own models.
   - They were directed to examples and implementation suggestions, with promises of future updates, including a potential draft PR.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://linktr.ee/digitalgnosis">@nathormond | Instagram, Facebook | Linktree</a>: View digitalgnosis’s Linktree. Listen to their music on YouTube, Spotify here.</li><li><a href="https://www.youtube.com/watch?v=139UPjoq7Kw">Building Machine Learning Systems for a Trillion Trillion Floating Point Operations</a>: Over the last 10 years we&#39;ve seen Machine Learning consume everything, from the tech industry to the Nobel Prize, and yes, even the ML acronym. This rise in ...</li><li><a href="https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/interface.md#external-library-usage">lm-evaluation-harness/docs/interface.md at main · EleutherAI/lm-evaluation-harness</a>: A framework for few-shot evaluation of language models. - EleutherAI/lm-evaluation-harness</li><li><a href="https://huggingface.co/unsloth/Llama-3.3-70B-Instruct">unsloth/Llama-3.3-70B-Instruct · Hugging Face</a>: no description found
</li>
</ul>

</div>
  

---


### **Eleuther ▷ #[research](https://discord.com/channels/729741769192767510/747850033994662000/1314774004787314788)** (81 messages🔥🔥): 

> `Variational encoders in different modalities, Memory-efficient optimizers, 3D generation frameworks, Catastrophic forgetting in training, Performance of Adam vs SGD` 


- **Exploration of Variational Encoders Across Modalities**: There was interest in research around variational encoders that take one modality to output another, specifically not using cVAEs which require autoencoding the main modality.
   - *Multimodal VAEs* were suggested as a general topic to tackle these explorations.
- **Memory-efficient Optimizers for Large Language Models**: A paper discussed the challenges of memory-intensive optimizers like AdamW in training large language models and proposed a new memory-efficient optimizer, APOLLO, to address these issues.
   - It was noted that AdamW's heavy memory burden necessitates costly computations, and better alternatives could optimize memory usage without significant performance loss.
- **Innovative 3D Generation Frameworks**: Two recent papers introduced methods for 3D asset creation, utilizing structured latent representations for improved output formats and features integration utilizing deep learning models.
   - Each method demonstrated advantages in generating high-quality 3D results from various inputs while maintaining structural and textural integrity.
- **Catastrophic Forgetting in Optimizer Performance**: Discussion arose around the differences in catastrophic forgetting when models trained with different optimizers like AdamW and Muon are fine-tuned on new datasets.
   - Concerns were raised about how better initial fits might exacerbate forgetting, indicating a need for strategies to mitigate performance loss when switching datasets.
- **Performance Comparison: Adam vs SGD**: Participants noted that Adam generally outperforms SGD in language tasks, potentially due to the heavy-tailed class imbalance present in such datasets.
   - This imbalance reportedly leads to slower average loss reductions in gradient descent compared to Adam, which is less sensitive to infrequent words.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://arxiv.org/abs/2310.14963">Studying K-FAC Heuristics by Viewing Adam through a Second-Order Lens</a>: Research into optimisation for deep learning is characterised by a tension between the computational efficiency of first-order, gradient-based methods (such as SGD and Adam) and the theoretical effici...</li><li><a href="https://arxiv.org/abs/2412.01506">Structured 3D Latents for Scalable and Versatile 3D Generation</a>: We introduce a novel 3D generation method for versatile and high-quality 3D asset creation. The cornerstone is a unified Structured LATent (SLAT) representation which allows decoding to different outp...</li><li><a href="https://arxiv.org/abs/2402.19449">Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models</a>: Adam has been shown to outperform gradient descent on large language models by a larger margin than on other tasks, but it is unclear why. We show that a key factor in this performance gap is the heav...</li><li><a href="https://arxiv.org/abs/2412.05270">APOLLO: SGD-like Memory, AdamW-level Performance</a>: Large language models (LLMs) are notoriously memory-intensive during training, particularly with the popular AdamW optimizer. This memory burden necessitates using more or higher-end GPUs or reducing ...</li><li><a href="https://arxiv.org/abs/2412.04431">Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis</a>: We present Infinity, a Bitwise Visual AutoRegressive Modeling capable of generating high-resolution, photorealistic images following language instruction. Infinity redefines visual autoregressive mode...</li><li><a href="https://arxiv.org/abs/2411.08033">GaussianAnything: Interactive Point Cloud Latent Diffusion for 3D Generation</a>: While 3D content generation has advanced significantly, existing methods still face challenges with input formats, latent space design, and output representations. This paper introduces a novel 3D gen...</li><li><a href="https://arxiv.org/abs/2403.03100">NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models</a>: While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses va...</li><li><a href="https://arxiv.org/abs/2211.09407">NANSY++: Unified Voice Synthesis with Neural Analysis and Synthesis</a>: Various applications of voice synthesis have been developed independently despite the fact that they generate &#34;voice&#34; as output in common. In addition, most of the voice synthesis models still...</li><li><a href="https://x.com/liron/status/1865974752822919202/photo/1">Tweet from Liron Shapira (@liron)</a>: Girl come up to my place, I gotta show you my couch</li><li><a href="https://www.nature.com/articles/s43588-024-00732-2">A scalable framework for learning the geometry-dependent solution operators of partial differential equations - Nature Computational Science</a>: This work presents an artificial intelligence framework to learn geometry-dependent solution operators of partial differential equations (PDEs). The framework enables scalable and fast approximations ...
</li>
</ul>

</div>
  

---


### **Eleuther ▷ #[scaling-laws](https://discord.com/channels/729741769192767510/785968841301426216/1314982561381875884)** (2 messages): 

> `GitHub Gist code sharing, Scaling laws overview` 


- **Gist simplifies code sharing with get_scaling_laws**: A member shared a [GitHub Gist](https://gist.github.com/elyxlz/33122704a751051b0d675ec0e10b8af6) titled **get_scaling_laws**, which allows for instant sharing of code snippets and notes.
   - *Thanks a lot!* was the appreciative response from another member after the Gist was shared.
- **Visual overview of the shared Gist**: The shared Gist includes an image for better understanding, viewable at **https://github.githubassets.com/assets/gist-og-image-54fd7dc0713e.png**.
   - This visual aids in quickly grasping the purpose and functionality of the **get_scaling_laws** Gist.



**Link mentioned**: <a href="https://gist.github.com/elyxlz/33122704a751051b0d675ec0e10b8af6">get_scaling_laws</a>: GitHub Gist: instantly share code, notes, and snippets.

  

---


### **Eleuther ▷ #[interpretability-general](https://discord.com/channels/729741769192767510/1052314805576400977/1314720014129037322)** (11 messages🔥): 

> `Gradient Routing, Neural Network Specialization, Causal Inference and Gradient Routing, Credible Source Distinction, Interpretable Architecture` 


- **Exploring Gradient Routing for Safety**: A new approach called **gradient routing** allows users to decide which parameters update based on the data type, promoting specialization in neural networks ([source](https://x.com/Turn_Trout/status/1865156788750028846)). The method aims to address safety concerns related to the black-box nature of AI training.
   - *Gradient routing* provides a potential alternative for understanding and controlling AI behaviors beyond traditional neural configurations.
- **Brain-like Learning Behavior Considerations**: Members discussed parallels between **gradient routing** and brain functions, suggesting that such a mechanism reflects the biology of **localized learning** (e.g., selective neuron weights). This raises questions on how effectively concepts can be encoded using this method.
   - Insights collected emphasize the potential of *localizing learning* while debating its importance outside human interpretation research.
- **Potential for Causal Inference Applications**: A member expressed intuitions that **gradient routing** could assist in **causal inference** by adapting loss gradients based on intervention variables. This suggests a targeted approach to learning that accounts for various interventions.
   - Though the exact mechanism remains speculative, this aligns with the discussion on enhancing model robustness for causal reasoning.
- **Source Credibility in AI Inputs**: A participant suggested that **gradient routing** could enable models to differentiate between **credible** and **non-credible sources**. This taxonomy could improve how metadata influences model behavior without overly complicating distinctions.
   - The conversation included concerns about **hallucination vs. generalization**, pointing to the complexities of training reliable AI systems.
- **Interpretable Architecture Agenda**: Discussions hinted at the potential utility of **gradient routing** for the **interpretable architecture agenda** in AI. Clarifying the role of component localization might facilitate more effective architectural strategies.
   - The community appears enthusiastic about the contributions that structured learning methods can bring to understanding AI systems and their outputs.



**Link mentioned**: <a href="https://x.com/Turn_Trout/status/1865156788750028846">Tweet from Alex Turner (@Turn_Trout)</a>: 1) AIs are trained as black boxes, making it hard to understand or control their behavior. This is bad for safety! But what is an alternative? Our idea: train structure into a neural network by config...

  

---


### **Eleuther ▷ #[lm-thunderdome](https://discord.com/channels/729741769192767510/755950983669874798/1314909852191166465)** (57 messages🔥🔥): 

> `MLX Examples PR, Eleuther AI Eval Harness, GSM8K Comparison Issues, ARC-Challenge Dataset Anomalies, Llama Model Evaluation Techniques` 


- **MLX Evaluates with New CLI Tool**: The recent [Pull Request #1140](https://github.com/ml-explore/mlx-examples/pull/1140) adds an `mlx_lm.evaluate` CLI capable of using `lm-eval` for any mlx-lm compatible model, enabling tasks like evaluations on `Qwen2.5-7B-Instruct`.
   - With this addition, users can easily conduct evaluations such as `mlx_lm.evaluate --model mlx-community/Qwen2.5-7B-Instruct-4bit --tasks winogrande arc_easy`.
- **GSM8K Shows Discrepancies in Evaluation**: Users are struggling to replicate high accuracy scores for GSM8K, with reported metrics showing significantly lower performance than comparative models like LiquidAI.
   - Despite switching to different evaluation methods, the maximum score achieved was about **72.93%**, still below the claimed **79.6%** from prior evaluations.
- **ARC-Challenge Missing Choices Issue**: A single question in the ARC-Challenge dataset was reported to have only three choices, causing evaluation errors when the fourth choice was referenced.
   - Users are encouraged to tweak their configuration to better handle such anomalies and ensure accurate evaluations.
- **Eleuther AI Eval Harness Configurations**: A provided configuration for evaluating the ARC-Challenge on EleutherAI's eval harness aims to streamline performance comparisons across models.
   - Users are advised to implement the given YAML configuration alongside updates to utility functions for better processing of datasets.
- **Community Collaboration in Model Evaluation**: The community is actively collaborating on ensuring their models, like RWKV, are evaluated in a consistent manner to avoid misinterpretations of performance metrics.
   - Discussions highlight a common concern regarding the variability of evaluation methodologies and the importance of transparency in published results.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://github.com/meta-llama/llama3/blob/main/eval_details.md">llama3/eval_details.md at main · meta-llama/llama3</a>: The official Meta Llama 3 GitHub site. Contribute to meta-llama/llama3 development by creating an account on GitHub.</li><li><a href="https://github.com/EleutherAI/lm-evaluation-harness/pull/1902">mlx Model (loglikelihood &amp; generate_until) by chimezie · Pull Request #1902 · EleutherAI/lm-evaluation-harness</a>: This adds a new model type for mlx models.  In particular, it implements the loglikelihood and generate_until interfaces.  Works with the current versions of mlx and mlx-lmThe new model type is ml...</li><li><a href="https://github.com/ml-explore/mlx-examples/pull/1140">`mlx_lm.evaluate` by barronalex · Pull Request #1140 · ml-explore/mlx-examples</a>: Add an mlx_lm.evaluate CLI that uses lm-eval and supports any mlx-lm compatible model.For example:mlx_lm.evaluate --model mlx-community/Qwen2.5-7B-Instruct-4bit --tasks winogrande arc_easyResul...
</li>
</ul>

</div>
  

---


### **Eleuther ▷ #[multimodal-general](https://discord.com/channels/729741769192767510/795089627089862656/1314864780246515723)** (8 messages🔥): 

> `VLM Training Process, Causal Loss in VLMs, MSE on Visual Tokens, Apple AIM` 


- **Inquiry on VLM training specifics**: A user inquired about the training process of **VLMs** like **Qwen2-VL**, specifically **how causal loss** is applied and whether it affects visual tokens.
   - They questioned if **light purple tokens** are discarded from the loss, and whether applying **MSE loss** could enhance learning of multimodal features.
- **Acknowledgment of MSE application**: Another user confirmed that **MSE** has been applied to visual tokens, stating, **'Yes to both of these questions'** regarding the discussion.
   - They mentioned that someone had indeed tried using MSE recently, although they could not remember who.
- **Searching for supporting paper**: The original user asked if there was a **paper** regarding the MSE trial and if it had yielded any improved results.
   - The respondent clarified that it wasn't specifically about **VLMs**, indicating they would look for more details.
- **Reference to Apple AIM**: The user identified **Apple AIM** as a reference for the **MSE** trial they had mentioned earlier, indicating its relevance to the discussion.
   - This reference could inform the ongoing inquiry into the application of MSE in visual token processing.


  

---


### **Eleuther ▷ #[gpt-neox-dev](https://discord.com/channels/729741769192767510/730090096287547444/)** (1 messages): 

karatsubabutslower: CC <@367104793292046338> Any hints for this?
  

---


### **Notebook LM Discord ▷ #[use-cases](https://discord.com/channels/1124402182171672732/1124403655819415592/1314686472787591169)** (28 messages🔥): 

> `Podcast Lengths, NotebookLM Use Cases, Interactive Storytelling, Data Handling in Sheets, NotebookLM Podcast Prompts` 


- **Podcast Lengths Achieved with NotebookLM**: Members shared experiences getting varied podcast lengths from NotebookLM, with one condensing **107 pages** of Formula 1 regulations into a **17 minute** podcast.
   - Another noted that combining a YouTube video and scratchpad led to a podcast longer than the original video itself.
- **Exploring NotebookLM Use Cases**: Discussions highlighted attempts to link **Claude** or **ChatGPT** with NotebookLM, suggesting **Zapier** as a potential solution.
   - Members also reflected on using NotebookLM to create context around songs by inputting lyrics and other resources.
- **Interactive Storytelling Development**: Users debated how NotebookLM handles stories, with one noting its capability for **world building** based on input data.
   - Another confirmed that it sometimes generates unintended story beats, raising questions about its creative process.
- **Data Handling in Google Sheets**: A user shared tips on transferring data from Google Sheets to Docs effectively, emphasizing the need for **clean headers and labels**.
   - They noted challenges with incorrect spreadsheets, stating that values transfer better than complex equations.
- **Leveraging NotebookLM Podcast Prompts**: A tutorial video was shared that promises to unveil **10 secret prompts** for optimizing NotebookLM for podcast creation.
   - The video aims to assist users looking to enhance their podcast content, highlighting unique techniques for better output.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://notebooklm.google.com/notebook/490674b4-ee93-47e1-a297-00070f841595/audio">no title found</a>: no description found</li><li><a href="https://youtu.be/aG0ixD3OY80">NotebookLM Podcast Tutorial: 10 Secret Prompts (People Will Kill You For!)</a>: Get these exclusive NotebookLM Podcast prompts for free! I’ve spent hours refining these 10 unique methods to help The AI News community stand out. Just watc...</li><li><a href="https://youtu.be/9CN1Ymyrhyo?si=n2PpH1J4PQgrvuvH)">Trump&#39;s New NASA Administrator // Artemis Delayed Again // No Oceans for Venus</a>: 🎁 Gift Universe Today Patreon membership:https://www.patreon.com/universetoday/giftTrump announces his choice for the new NASA administrator, we’ve got a ne...
</li>
</ul>

</div>
  

---


### **Notebook LM Discord ▷ #[general](https://discord.com/channels/1124402182171672732/1124402182909857966/1314691474805362795)** (141 messages🔥🔥): 

> `NotebookLM Limitations, Language Support in NotebookLM, Podcast Features Comparison, Audio Overview Issues, Using NotebookLM for Study` 


- **NotebookLM has limitations on document uploads**: Users noted that there is a limit of **100 documents** that can be uploaded in a single notebook, with no limit on the number of notebooks you can create.
   - Some users expressed confusion over whether this limit had changed from a previous **50 documents**.
- **Challenges with Language Support**: Many users are experiencing difficulties in using NotebookLM in different languages, often requiring a **logout and login** to switch languages.
   - *It seems that NotebookLM does not support on-the-fly language switching*, leading to frustrations among users who prefer a more flexible approach.
- **Comparisons of Podcast Features**: Discussion included comparisons of NotebookLM's podcast features to those offered by **ElevenLabs**, highlighting the competitive landscape.
   - It was mentioned that NotebookLM lacks a clear API and systemic prompting capabilities, which could enhance its usability in creating podcasts.
- **Issues with Audio Overview**: Users reported issues where Podcast and Audio Overview features sometimes generated incorrect or irrelevant content based on the sources provided.
   - Some users suggested deleting problematic audio outputs and regenerating them as a solution to incorrect fact generation.
- **Using NotebookLM for Academic Purposes**: There are some users utilizing NotebookLM for educational purposes, creating study guides and notes, but facing challenges with customizable outputs.
   - Guides and resources were shared to help users maximize their productivity with NotebookLM, including links to tutorial videos for improved usage.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://support.google.com/notebooklm/answer/14278184?hl=en&sjid=17768370428841200951-NA">Frequently Asked Questions - NotebookLM Help</a>: no description found</li><li><a href="https://open.substack.com/pub/brightvoid/p/dancing-with-the-djinn?utm_source=share&utm_medium=android&r=9euw0">Dancing with the Djinn</a>: Collaborating on the Page with an AI Mind</li><li><a href="https://youtu.be/QxbmQs3b_DE">NotebookLM tutorial to 10x your productivity</a>: Want to become master in NotebookLM and 10x your productivity just Watch this full video. I go from basics to advanced all in one video with 2 real world sce...</li><li><a href="https://open.spotify.com/show/3gaQyAwwFAFXzGb9DYMWSS">Machine Logic</a>: Podcast · Studio Il · Welcome to Machine Logic, a podcast where artificial intelligence takes the mic—literally!  Hosted entirely by AI, we dive into intriguing topics like cutting-edge technology, AI...</li><li><a href="https://youtu.be/aG0ixD3OY80">NotebookLM Podcast Tutorial: 10 Secret Prompts (People Will Kill You For!)</a>: Get these exclusive NotebookLM Podcast prompts for free! I’ve spent hours refining these 10 unique methods to help The AI News community stand out. Just watc...
</li>
</ul>

</div>
  

---


### **Cohere ▷ #[discussions](https://discord.com/channels/954421988141711382/954421988783444043/1314698056666714192)** (65 messages🔥🔥): 

> `Unsloth Finetuning Framework, Building Chat Models, AI in Commerce, Emotional Expression in Voice Generation, Traditional Chinese AI Training` 


- **Unsloth enhances finetuning process**: A member shared insights about the **Unsloth finetuning framework** and its feature of integrating custom grading functions during the training process.
   - This opens up innovative possibilities like improved **evaluation loops** tailored for the finetuning tasks.
- **Aspiration to build a chat model**: A newcomer expressed their goal of creating their own chat model, particularly by implementing features around **product information and user reviews**.
   - *Using existing social media data* for reviews was discussed, but there were concerns about its legal ramifications and the necessity of AI for scraping tasks.
- **Challenges of integrating AI without need**: A debate unfolded regarding the necessity of AI implementations, where the need for proper **use cases** for AI was emphasized.
   - Members highlighted that AI should not be applied just for its sake, but should focus on solving **realistic problems** in various verticals.
- **Exploring Emotional Expression in Voice Generation**: A discussion around **emotional expression in voice generation** revealed interests in developing APIs for customized vocal styles.
   - One member confirmed running their own APIs focused on **voice emotiveness**, citing interest in the **GPT4o-voice style**.
- **Advancing Traditional Chinese AI Models**: A user introduced themselves and their work in training AI models for **Traditional Chinese**, sharing their contributions to **Project TAME**.
   - Their recent project involves creating the model **Llama-3-Taiwan-8B-Instruct**, which is available on Hugging Face.


  

---


### **Cohere ▷ #[questions](https://discord.com/channels/954421988141711382/1168411509542637578/1315496863276732508)** (59 messages🔥🔥): 

> `Quantizing aya-expense model, LLM Deployment Options, Vector-based retrieval methods, Multi-step tool use in RAG, Community Engagement in AI Research` 


- **Request for aya-expense model quantization help**: A user expressed interest in quantizing the **aya-expense model** to AWW or FP8 format for better accessibility on limited GPU resources, suggesting the use of training data for calibration.
   - Another member shared that they found the **8b model** easy to run, with size reduced to **3.4GB**.
- **Discussion on LLM Deployment Options**: Members exchanged insights on using **vLLM** for deployment, with one noting that GGUF format is now compatible with it, making it easier to use.
   - Another member highlighted that **Ollama** is easier to configure compared to **llama.cpp**, yet the latter may offer performance benefits.
- **Exploring Vector-based Retrieval Methods**: A new member entered discussing their research on **retrieval methods** including vector-based retrieval and dense passage retrieval, considering a comparative study.
   - Community members provided positive feedback, encouraging the idea and suggesting enhancements like including **multi-step tool use**.
- **Multi-step Tool Use Explained**: In response to questions, a community member elaborated on **multi-step tool use**, equating it to agents invoking tools multiple times for enhanced results.
   - This method aims to automatically refine queries and analyze results, aiding in advanced research capabilities.
- **Nostalgia for Community Demos**: A member reminisced about past community demos and showcase events on Discord, where members showcased their work to foster engagement.
   - This highlights the collaborative nature of the community and the encouragement to share progress and insights in the AI research field.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://ollama.com/library/aya">aya</a>: Aya 23, released by Cohere, is a new family of state-of-the-art, multilingual models that support 23 languages. </li><li><a href="https://github.com/cohere-ai/notebooks/blob/main/notebooks/agents/Vanilla_Multi_Step_Tool_Use.ipynb">notebooks/notebooks/agents/Vanilla_Multi_Step_Tool_Use.ipynb at main · cohere-ai/notebooks</a>: Code examples and jupyter notebooks for the Cohere Platform - cohere-ai/notebooks</li><li><a href="https://huggingface.co/collections/CohereForAI/aya-datasets-660415741bd4852f01c81c77">Aya Datasets - a CohereForAI Collection</a>: no description found</li><li><a href="https://cohere.com/research">Cohere For AI (C4AI)</a>: Cohere For AI (C4AI) is Cohere&#x27;s research lab that seeks to solve complex machine learning problems. 
</li>
</ul>

</div>
  

---


### **Cohere ▷ #[api-discussions](https://discord.com/channels/954421988141711382/1168578329423642786/1315713517512294441)** (17 messages🔥): 

> `Dataset Upload Issues, File Format Errors, Absolute Path Recommendations, Cohere Dashboard Upload, Sample File Assistance` 


- **Uploading Rerank Dataset Fails**: User reported issues uploading a rerank dataset despite following the provided documentation code example. They attempted to correct the code but encountered a loading issue indicating **0 bytes size**.
   - Suggestions were made to check the file format and use an absolute path to resolve the upload problem.
- **File Formatting Troubles**: After attempting to upload, the user received an error: *'avro: string is unsupported for avro array'* indicating a formatting issue. They were advised to ensure their dataset aligns with the expected structure.
   - They planned to retry after correcting the format, thanking others for their assistance.
- **Upload Alternatives and Guidance**: One member suggested trying to upload the dataset directly on the [Cohere Dashboard](https://dashboard.cohere.com/fine-tuning/create?endpoint=rerank) to confirm the data's formatting. They offered help if the user encountered further issues.
   - A mini-guide link and an image attachment were recommended for confirming the correct JSONL format.
- **Community Support and Assistance**: Members expressed willingness to assist each other in resolving the dataset upload issues. They encouraged sharing datasets or pseudo samples for further troubleshooting.
- **Follow-up After Meeting**: User indicated they would take a break for a meeting but planned to test the solutions discussed later. They expressed gratitude for the community support, fostering a collaborative environment.



**Link mentioned**: <a href="https://dashboard.cohere.com/fine-tuning/create?endpoint=rerank">Login | Cohere</a>: Login for access to advanced Large Language Models and NLP tools through one easy-to-use API.

  

---


### **Cohere ▷ #[cohere-toolkit](https://discord.com/channels/954421988141711382/1254901651081269268/1314687751618039928)** (6 messages): 

> `Introduction Messages, Cohere Toolkit Questions` 


- **Dominic and YiTechX introduce themselves**: Members **Dominic** and **YiTechX** greeted each other, exchanging introductions and establishing a friendly tone in the channel.
   - The greetings emphasized the community aspect, promoting an engaging atmosphere for discussions.
- **Tony seeks clarity on Cohere Toolkit**: Tony expressed uncertainty about asking questions regarding the **Cohere Toolkit**, indicating a desire for guidance.
   - This prompted a response from another member, inviting Tony to ask freely, reinforcing the supportive nature of the group.


  

---


### **Nous Research AI ▷ #[announcements](https://discord.com/channels/1053877538025386074/1145143867818119272/1315741980411691110)** (1 messages): 

> `Neurips Hangout` 


- **Join us at Neurips!**: A member invited everyone to come hang out at **Neurips**, sharing excitement about the event.
   - An image was attached to the message that likely captures the ambiance of the gathering.
- **Image Shared for Neurips Gathering**: An image was attached to the announcement about the **Neurips** gathering, potentially serving as a promotional visual.
   - This visual likely aims to attract more participants to the event.


  

---


### **Nous Research AI ▷ #[general](https://discord.com/channels/1053877538025386074/1149866623109439599/1314688258025717861)** (125 messages🔥🔥): 

> `a16z Crypto Ideas, Nous Research Updates, AI x Crypto Discussions, Video on AI Development, DCT and Transpose in DeMo` 


- **a16z explores AI and crypto connections**: a16z recently published a post outlining 'Big Crypto Ideas for 2025', which linked to Nous's chatbot discussions around TEE applications in AI.
   - Although they mentioned AI related to crypto, they didn't specifically name Nous, which raised some discussion among members regarding visibility.
- **Nous Research remains a growing AI company**: Members confirmed that Nous Research is a relatively new player in the AI space, with various ongoing projects and research areas.
   - A resource link was shared for exploring their work: [Nous Research Releases](https://nousresearch.com/releases/).
- **Insights from a launched AI video**: A member shared a video highlighting common pitfalls in AI development, which garnered positive feedback and engagement from viewers.
   - The video titled 'Why AI Development is More Fun Than Traditional Coding' invites viewers to learn and be entertained simultaneously.
- **Technical discussion on DeMo implementation**: A conversation unfolded about generalizing DeMo to n-dimensional weights, with inquiries regarding the impact of a transpose in the process.
   - It was clarified that the transpose helps apply DCT effectively across certain dimensions, with insights shared on compute efficiency.
- **Member interaction about AI technology**: Members engaged in various discussions about AI technologies, including hardware availability and preferences for GPU specifications.
   - Conversations also touched on ongoing educational pursuits and collaboration opportunities among members with specialized skills.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://www.datacenterdynamics.com/en/news/openai-considers-changing-agi-provision-to-allow-further-microsoft-investment-genai/">OpenAI considers changing AGI provision to allow further Microsoft investment</a>: As it moves further from its founding vision</li><li><a href="https://x.com/nousresearch">Tweet from undefined</a>: no description found</li><li><a href="https://a16zcrypto.com/posts/article/big-ideas-crypto-2025/">A few of the things we’re excited about in crypto (2025) - a16z crypto</a>: A list of crypto, blockchains, web3 trends for 2025 -- including AI x crypto, prediction markets, stablecoins, voting, and much more</li><li><a href="https://principlesandinterest.wordpress.com/2021/12/12/crypto-art-stocks-and-power-in-2021/">Crypto, Art, Stocks and Power in 2021</a>: My day job is to lead a team of people who analyse and invest in financial market assets on behalf of clients. It’s mostly stocks and bonds. Portfolio managers tend to seek to identify assets that …</li><li><a href="https://x.com/BanklessHQ/status/1864633317804404891?t=plOtrC7W4FVwRRayp-Xdng&s=19">Tweet from Bankless (@BanklessHQ)</a>: LIVE NOW -- AI ROLLUP #2 | Ejaaz Ahamadeen@cryptopunk7213 & @TrustlessState cover the latest in the AI x crypto space including:- @virtuals_io hits 1B market cap, @AIXBT agent flagship- @0xzerebro + @...</li><li><a href="https://principlesandinterest.wordpress.com/2021/12/12/cryp">Crypto, Art, Stocks and Power in 2021</a>: My day job is to lead a team of people who analyse and invest in financial market assets on behalf of clients. It’s mostly stocks and bonds. Portfolio managers tend to seek to identify assets that …</li><li><a href="https://youtu.be/9jNXv2bi2zc">Why AI Development is More Fun Than Traditional Coding (With Real Examples)</a>: Building software is hard. AI makes it much much easier. Remember when building apps meant endless hours of frustration and countless Stack Overflow searches...</li><li><a href="https://github.com/tekn">TEKN - Overview</a>: GitHub is where TEKN builds software.</li><li><a href="https://github.com/NousResearch">Nous Research</a>: Nous Research has 22 repositories available. Follow their code on GitHub.
</li>
</ul>

</div>
  

---


### **Nous Research AI ▷ #[ask-about-llms](https://discord.com/channels/1053877538025386074/1154120232051408927/1315613454190776321)** (4 messages): 

> `Momentum in training, In-context learning efficiency, O1-type synthetic data generation` 


- **Momentum might aid in-context learning**: A member proposed that if **momentum** helps uncover a better loss landscape in training, it could also be beneficial for **in-context learning** (ICL), possibly likening it to *forced skip connections*.
   - They questioned whether ICL is affected by gradient descent dynamics, raising interesting inquiries about optimization methods.
- **Implementing momentum in the residual stream**: [Implementing momentum along the residual stream](https://link.to/implementation) was suggested as a potential strategy to enhance performance in neural networks.
   - This idea ties back to the ongoing exploration of optimizing ICL mechanisms through advanced training techniques.
- **Resources for generating O1-type synthetic data**: A member asked about good resources or prompts for generating **O1-type synthetic data**, indicating a need for practical guides.
   - This reflects a broader interest in effective methods for synthetic data generation within the community.


  

---


### **Nous Research AI ▷ #[research-papers](https://discord.com/channels/1053877538025386074/1104063238934626386/1315062693711052833)** (6 messages): 

> `Notable LLM Papers from Last Two Years, Mixture of Experts in LLMs, Resource-Efficient Training for LLMs, Training Small LLMs and Diffusion Models, Challenges in LLM Training` 


- **Exploration of Notable LLM Papers**: Members discussed impactful and memorable papers on LLMs from the past two years, proposing prominent works including [Mixture of Experts](https://arxiv.org/abs/2310.10837) frameworks.
   - The conversation revealed a consensus on the relevance and potential of several papers related to efficiency and scaling in LLMs.
- **Revamping Mixture of Experts for Efficiency**: The introduction of novel perspectives on [Mixture of Experts](https://arxiv.org/abs/2310.10837) has shown competitive results against dense models by improving resource efficiency.
   - It was noted that recent developments demonstrated **MoEs** can effectively decrease computation and memory requirements while maintaining performance.
- **Training Strategies on Limited Resources**: One paper explored training language models from scratch within **one day on a single GPU**, yielding results comparable to BERT.
   - This paper prompted discussions on how to maximize performance under constraints, proving efficient training is achievable even in limited scenarios.
- **Emphasis on Small LLMs and Diffusion Models**: The community highlighted strategies for creating small, efficient LLMs while maintaining effectiveness, referencing various papers including Nvidia’s [N-GPT](https://arxiv.org/abs/2406.15786).
   - These discussions revolved around utilizing Mixture of Experts and experimental methods as innovative solutions for the future of LLM training.
- **Humorous Take on Code Complexity**: Members joked about the potential 'nightmare' code base resulting from combining various research techniques for training LLMs and the complexity of integration.
   - This light-hearted comment underscored the challenges faced by researchers as they push the boundaries of technology in neural network training.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://x.com/alpha7987/status/1865503381529522374?s=61">Tweet from Nothing is something (@Alpha7987)</a>: This week’s top AI/ML research papers:- OpenAI o1 System Card- PaliGemma 2- HunyuanVideo- Densing Law of LLMs- DeMo: Decoupled Momentum Optimization- o1-Coder- Reverse Thinking Makes LLMs Stronger Rea...</li><li><a href="https://x.com/teknium1/status/1865792338666348671?s=46">Tweet from Teknium (e/λ) (@Teknium1)</a>: Ok only LLM people - What are your favorite, most impactful, or otherwise most memorable papers on LLMs from the past two years?</li><li><a href="https://x.com/OpenlifesciAI/status/1865584829057929303>">Tweet from Open Life Science AI (@OpenlifesciAI)</a>: 🌟 Weekly Medical AI Research Roundup 🌟📅 December 2–7,  Here&#39;s your weekly digest of the most exciting medical AI papers! 🎉▶️ Medical LLM & Models• Block MedCare: Blockchain AI & IoT• LLMs4Life...</li><li><a href="https://youtu.be/SwawtIFy-BI">Top Medical AI Papers (Dec 2–Dec 7) | Blockchain, Fairness, &amp; Multimodal Insights</a>: Welcome back to Open Life Science AI! This week, we’re exploring the top medical AI papers from December 2nd to December 7th. Highlights from this episode in...</li><li><a href="https://arxiv.org/abs/2310.10837">Approximating Two-Layer Feedforward Networks for Efficient Transformers</a>: How to reduce compute and memory requirements of neural networks (NNs) without sacrificing performance? Many recent works use sparse Mixtures of Experts (MoEs) to build resource-efficient large langua...</li><li><a href="https://arxiv.org/abs/2312.07987">SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention</a>: Despite many recent works on Mixture of Experts (MoEs) for resource-efficient Transformer language models, existing methods mostly focus on MoEs for feedforward layers. Previous attempts at extending ...</li><li><a href="https://arxiv.org/abs/2405.16039">MoEUT: Mixture-of-Experts Universal Transformers</a>: Previous work on Universal Transformers (UTs) has demonstrated the importance of parameter sharing across layers. By allowing recurrence in depth, UTs have advantages over standard Transformers in lea...</li><li><a href="https://arxiv.org/abs/2212.14034">Cramming: Training a Language Model on a Single GPU in One Day</a>: Recent trends in language modeling have focused on increasing performance through scaling, and have resulted in an environment where training language models is out of reach for most researchers and p...</li><li><a href="https://arxiv.org/abs/2407.15811">Stretching Each Dollar: Diffusion Training from Scratch on a Micro-Budget</a>: As scaling laws in generative AI push performance, they also simultaneously concentrate the development of these models among actors with large computational resources. With a focus on text-to-image (...</li><li><a href="https://arxiv.org/abs/2405.14159">Super Tiny Language Models</a>: The rapid advancement of large language models (LLMs) has led to significant improvements in natural language processing but also poses challenges due to their high computational and energy demands. T...</li><li><a href="https://arxiv.org/abs/2406.15786">What Matters in Transformers? Not All Attention is Needed</a>: While scaling Transformer-based large language models (LLMs) has demonstrated promising performance across various tasks, it also introduces redundant architectures, posing efficiency challenges for r...
</li>
</ul>

</div>
  

---


### **Nous Research AI ▷ #[research-papers](https://discord.com/channels/1053877538025386074/1104063238934626386/1315062693711052833)** (6 messages): 

> `Top AI/ML Research Papers, Medical AI Research, Mixture of Experts in LLMs, High-Efficiency LLM Training, Survey of Impactful LLM Papers` 


- **Top AI/ML Research Papers Revealed**: Recent discussions highlighted top AI/ML papers including **OpenAI o1 System Card** and **PaliGemma 2**, showcasing advancements in large language models (LLMs). Noteworthy contributions also included **Efficient Track Anything** and **DeMo: Decoupled Momentum Optimization**.
   - The full list featured **Densing Law of LLMs**, **Agent Skill Acquisition**, and innovative documents that aim to balance functionality with efficiency in ML systems.
- **Weekly Medical AI Research Roundup**: **Medical AI** papers from December 2-7 emphasized innovations like **Block MedCare** and **LLaMA II for Multimodal Diagnosis**, aimed at improving clinical practice. Further exploration included frameworks like **RARE: Retrieval-Augmented Reasoning** and applications such as **CLINICSUM: Patient Conversation Summaries**.
   - Key ethical discussions underscored **Privacy in Medical Imaging** and the need for demographic fairness in AI, addressing significant challenges in healthcare.
- **Revisiting Mixture of Experts in LLMs**: A member discussed changing perceptions around **Mixtures of Experts (MoEs)**, pointing to novel approaches that enhance LLM efficiency without compromising performance. Papers showcased the potential of MoEs to rival dense Transformers at a fraction of the computational cost.
   - The dialogue shared insights into the competitive landscape of **MoEs**, highlighting their improvement in resource management while validating portions of recent research advancements.
- **High-Efficiency LLM Training Techniques**: The conversation revolved around strategies for optimizing LLM training, including leveraging GPU capabilities for reduced training times. Papers demonstrated that *minimalist approaches* could achieve performance close to larger models within constrained environments.
   - Innovations in training methodologies, particularly around single-GPU setups, suggest that smaller models could meet competitive benchmarks while significantly cutting down expenses.
- **Survey of Impactful LLM Papers**: Participants shared impactful papers from the last two years that shaped LLM development, including notable works that explore scaling laws and structural efficiencies. The discussion called out specific models like **Nvidia's N-GPT**, focusing on pruning techniques to enhance performance.
   - Members underscored the potential for combining insights from multiple studies to devise practical implementations for developing new LLMs affordably, even if the resulting code could be complex.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://x.com/alpha7987/status/1865503381529522374?s=61">Tweet from Nothing is something (@Alpha7987)</a>: This week’s top AI/ML research papers:- OpenAI o1 System Card- PaliGemma 2- HunyuanVideo- Densing Law of LLMs- DeMo: Decoupled Momentum Optimization- o1-Coder- Reverse Thinking Makes LLMs Stronger Rea...</li><li><a href="https://x.com/teknium1/status/1865792338666348671?s=46">Tweet from Teknium (e/λ) (@Teknium1)</a>: Ok only LLM people - What are your favorite, most impactful, or otherwise most memorable papers on LLMs from the past two years?</li><li><a href="https://x.com/OpenlifesciAI/status/1865584829057929303>">Tweet from Open Life Science AI (@OpenlifesciAI)</a>: 🌟 Weekly Medical AI Research Roundup 🌟📅 December 2–7,  Here&#39;s your weekly digest of the most exciting medical AI papers! 🎉▶️ Medical LLM & Models• Block MedCare: Blockchain AI & IoT• LLMs4Life...</li><li><a href="https://youtu.be/SwawtIFy-BI">Top Medical AI Papers (Dec 2–Dec 7) | Blockchain, Fairness, &amp; Multimodal Insights</a>: Welcome back to Open Life Science AI! This week, we’re exploring the top medical AI papers from December 2nd to December 7th. Highlights from this episode in...</li><li><a href="https://arxiv.org/abs/2310.10837">Approximating Two-Layer Feedforward Networks for Efficient Transformers</a>: How to reduce compute and memory requirements of neural networks (NNs) without sacrificing performance? Many recent works use sparse Mixtures of Experts (MoEs) to build resource-efficient large langua...</li><li><a href="https://arxiv.org/abs/2312.07987">SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention</a>: Despite many recent works on Mixture of Experts (MoEs) for resource-efficient Transformer language models, existing methods mostly focus on MoEs for feedforward layers. Previous attempts at extending ...</li><li><a href="https://arxiv.org/abs/2405.16039">MoEUT: Mixture-of-Experts Universal Transformers</a>: Previous work on Universal Transformers (UTs) has demonstrated the importance of parameter sharing across layers. By allowing recurrence in depth, UTs have advantages over standard Transformers in lea...</li><li><a href="https://arxiv.org/abs/2212.14034">Cramming: Training a Language Model on a Single GPU in One Day</a>: Recent trends in language modeling have focused on increasing performance through scaling, and have resulted in an environment where training language models is out of reach for most researchers and p...</li><li><a href="https://arxiv.org/abs/2407.15811">Stretching Each Dollar: Diffusion Training from Scratch on a Micro-Budget</a>: As scaling laws in generative AI push performance, they also simultaneously concentrate the development of these models among actors with large computational resources. With a focus on text-to-image (...</li><li><a href="https://arxiv.org/abs/2405.14159">Super Tiny Language Models</a>: The rapid advancement of large language models (LLMs) has led to significant improvements in natural language processing but also poses challenges due to their high computational and energy demands. T...</li><li><a href="https://arxiv.org/abs/2406.15786">What Matters in Transformers? Not All Attention is Needed</a>: While scaling Transformer-based large language models (LLMs) has demonstrated promising performance across various tasks, it also introduces redundant architectures, posing efficiency challenges for r...
</li>
</ul>

</div>
  

---


### **DSPy ▷ #[general](https://discord.com/channels/1161519468141355160/1161519469319946286/1314718425117098075)** (66 messages🔥🔥): 

> `Issues with Running Ollama Locally, Exploring Human Feedback in DSPy, Deployment Strategies for DSPy Programs, Using DSPy for Context-Aware Chunking, Anthropic Model Context Protocol with DSPy` 


- **Investigating Ollama Performance**: Users discussed the inconsistent performance of the **default 3B model** of **Ollama** when run locally compared to terminal execution, highlighting confusion over its ChatAdapter.
   - Concerns were raised about the need for simpler adapters for quantized models and a commitment to improving model outputs.
- **Human Feedback Integration in DSPy**: A member inquired about implementing human feedback like Agrilla as a metric for DSPy, referencing previous discussions and pull requests for this feature.
   - Related conversations included exploring the involvement of human feedback in teleprompting, with relevant GitHub links shared.
- **Deployment Strategies for DSPy**: Members shared various deployment methods for DSPy programs, including using **FastAPI** and **MLFlow**, noting that separate containers may be needed for production setups.
   - Alternative approaches like integrating DSPy within Django projects or deploying on Modal were discussed, emphasizing flexibility in deployment choices.
- **Context-Aware Chunking with DSPy**: The potential of using **DSPy** as a context-aware chunker was explored, with suggestions on how to optimize processing of longer documents effectively.
   - The conversation included discussing the limitations of small and large language models in optimizing this process.
- **Utilizing Anthropic MCP with DSPy**: A user asked about recipes for implementing **Anthropic's Model Context Protocol (MCP)** with DSPy, prompting suggestions and links to resources on integration.
   - Relevant blog posts shared outlined building tools around MCP, emphasizing its application in AI tool development.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://ollama.com/blog/structured-outputs">Structured outputs · Ollama Blog</a>: Ollama now supports structured outputs making it possible to constrain a model&#39;s output to a specific format defined by a JSON schema. The Ollama Python and JavaScript libraries have been updated ...</li><li><a href="https://dspy.ai/tutorials/deployment/">Deployment - DSPy Documentation</a>: The framework for programming—rather than prompting—language models.</li><li><a href="https://www.darinkishore.com/posts/mcp">Building Better AI Tools with MCP | Darin Kishore</a>: Lessons learned from building AI tools and how the Model Context Protocol (MCP) is cool.</li><li><a href="https://github.com/stanfordnlp/dspy/pull/1647">Feature/human-in-the-loop-teleprompt by burtenshaw · Pull Request #1647 · stanfordnlp/dspy</a>: 📝 Changes DescriptionThis is a WIP PR to start a discussion on involving human feedback in DSPy. If labellers could add feedback on a prompt during teleprompting it would support working in specif...</li><li><a href="https://gist.github.com/rohitgarud/80bdcd30b65c154e07f343055f95898e">Transform JSON schema from Pydantic model_json_schema() into something simpler for LLM to understand</a>: Transform JSON schema from Pydantic model_json_schema() into something simpler for LLM to understand - order_model.py</li><li><a href="https://gist.github.com/rohitgarud/eb60c095a53cf5303fb3ae07b98e268b">Custom JSON Adapter for DSPy which uses ProcessSchema to simplify the JSON schema injected in the prompt when InputField or OutputField of the signature has Pydantic model as a type</a>: Custom JSON Adapter for DSPy which uses ProcessSchema to simplify the JSON schema injected in the prompt when InputField or OutputField of the signature has Pydantic model as a type - dspy_custom_a...</li><li><a href="https://github.com/baloise/kwansi">GitHub - baloise/kwansi: An auto-optimizer library based on DSPy</a>: An auto-optimizer library based on DSPy. Contribute to baloise/kwansi development by creating an account on GitHub.</li><li><a href="https://github.com/baloise/kwansi_example">GitHub - baloise/kwansi_example: An example implementation of the lordamp/kwansi wrapper for DSPy</a>: An example implementation of the lordamp/kwansi wrapper for DSPy - baloise/kwansi_example</li><li><a href="https://github.com/stanfordnlp/dspy/pull/1881">[WIP] Support structured outputs response format based on signature in JSON adapter by dbczumar · Pull Request #1881 · stanfordnlp/dspy</a>: Support structured outputs response format based on signature in JSON adapter
</li>
</ul>

</div>
  

---


### **LlamaIndex ▷ #[blog](https://discord.com/channels/1059199217496772688/1187460979064324127/1314684655919956090)** (5 messages): 

> `LlamaParse Multimodal Parsing, Claude Desktop PDF Integration, Agentless Software Issue Resolution, LlamaParse Auto Mode Benefits` 


- **LlamaParse enables multimodal parsing**: In an informative video, LlamaParse demonstrates how to enable **advanced multimodal parsing** compatible with models like **GPT-4, Claude 3.5**, and **LLaVA 1.5**.
   - Check out the [video walkthrough](https://twitter.com/llama_index/status/1865125665491886171) to see how screenshots can be converted effectively.
- **Claude Desktop connects to complex PDFs**: A new project by **Marcus Schiesser** integrates LlamaCloud’s document parsing with Claude using the **Model Context Protocol (MCP)**, enabling chat capabilities with complex PDFs.
   - Experience it firsthand through this [detailed project description](https://twitter.com/llama_index/status/1865460899059998999).
- **Agentless proposes simpler issue resolution**: Today, LlamaIndex features **Agentless**, which presents a straightforward three-step process for automatically resolving software issues: **localization, repair, and patch**.
   - This approach contrasts with more complex solutions, as detailed in this [announcement](https://twitter.com/llama_index/status/1865822785119174857).
- **LlamaParse launches cost-optimized Auto Mode**: The new **Auto Mode** in LlamaParse optimizes costs by parsing documents in a standard mode while selectively switching to **Premium mode** based on user-defined triggers.
   - Learn more about this feature and its benefits through this [link](https://twitter.com/llama_index/status/1866214925418500119).
- **Video walkthrough for LlamaParse Auto Mode**: A video walkthrough explains the functionality of **LlamaParse's Auto Mode**, designed to enhance user experience.
   - Access the [video here](https://twitter.com/llama_index/status/1866233120481263934) and ensure your browser is updated for ideal viewing.



**Link mentioned**: <a href="https://t.co/qBD8sfDsqb">no title found</a>: no description found

  

---


### **LlamaIndex ▷ #[general](https://discord.com/channels/1059199217496772688/1059201661417037995/1314702168355246083)** (35 messages🔥): 

> `Automating Ingestion Pipelines, LlamaIndex RAG Integration, LlamaParse Server Locations, Llama3 Cookbook for Intel Gaudi, OpenAI Seed Mechanism` 


- **Automating Ingestion Pipelines for Chat Apps**: A member discussed a use-case for automating ingestion pipelines from sources like Google Drive and Airtable every hour for a private chat RAG app.
   - They considered using a job scheduler or a cloud-hosted solution for this process as they faced challenges with incremental updates.
- **Experience with LlamaIndex RAG and OCR Data**: A user asked about experiences with OCR-read data in PDF format when applying the LlamaIndex RAG process, seeking insights into its effectiveness.
   - No direct responses were provided, highlighting a knowledge gap on this specific application.
- **LlamaParse Servers Based in the US**: A member inquired about the server locations for Llamaparse, expressing concerns about data staying within Australia.
   - It was confirmed that servers are currently US-based, with plans for EU deployments but no immediate Australian options.
- **Submitting PR for Llama3 Cookbook**: A member submitted a PR to add the Llama3 Cookbook for Intel Gaudi and requested a review, providing a link for visibility.
   - They included a description and details about the PR in GitHub to attract attention from contributors.
- **Excluding Metadata in OpenAI Seed Mechanism**: A user sought help in using the seed mechanism with OpenAI's query engine, expressing concerns over unwanted metadata in the prompts.
   - Another member provided a solution to exclude specific metadata from the prompt by adjusting the document's metadata settings during ingestion.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://docs.llamaindex.ai/en/stable/examples/chat_engine/chat_engine_condense_plus_context/">Chat Engine - Condense Plus Context Mode - LlamaIndex</a>: no description found</li><li><a href="https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_documents/#advanced-metadata-customization">Using Documents - LlamaIndex</a>: no description found</li><li><a href="https://docs.llamaindex.ai/en/v0.10.17/examples/ingestion/ingestion_gdrive.html">Building a Live RAG Pipeline over Google Drive Files - LlamaIndex 🦙 v0.10.17</a>: no description found</li><li><a href="https://github.com/run-llama/llama_index/pull/17200">add Llama3 Cookbook for Intel Gaudi by jeanyu-habana · Pull Request #17200 · run-llama/llama_index</a>: Descriptionadd Llama3 Cookbook for Intel GaudiFixes # (issue)NANew Package?NoDid I fill in the tool.llamahub section in the pyproject.toml and provide a detailed README.md for my new integrat...
</li>
</ul>

</div>
  

---


### **LlamaIndex ▷ #[ai-discussion](https://discord.com/channels/1059199217496772688/1100478495295017063/1315632711171309649)** (1 messages): 

> `Chain of Thought Prompting, COT techniques, AI problem-solving` 


- **Discover the Power of Chain of Thought Prompting**: A member highlighted a comprehensive [resource on COT prompting](https://hub.athina.ai/blogs/what-is-chain-of-thought-prompting-in-ai/) that covers various techniques, examples, and limitations essential for getting started.
   - They mentioned that **Chain of Thought prompting** improves AI's handling of complex tasks by breaking them down, enhancing accuracy and logical reasoning.
- **Understanding COT: An AI Method for Better Problem-Solving**: **Chain of Thought prompting** encourages a sequential thinking process, enabling AI models to tackle difficult challenges more effectively.
   - As AI systems integrate into fields like **natural language processing**, mastering COT becomes crucial for improved inquiry responses, promoting a systematic approach.



**Link mentioned**: <a href="https://hub.athina.ai/blogs/what-is-chain-of-thought-prompting-in-ai/">What is Chain of Thought Prompting in AI?</a>: Chain of Thought Prompting (CoT) - OverviewAn artificial intelligence method called Chain of Thought prompting encourages sequential thinking, which enables models to handle challenging tasks more eff...

  

---


### **Torchtune ▷ #[general](https://discord.com/channels/1216353675241590815/1216353675744641096/1315052541368729661)** (34 messages🔥): 

> `Adaptive Batching, Llama 3.3 Config Memory Issues, Flex Attention Kernel Bugs, New CPU Flex Kernel, Memory Optimization Techniques` 


- **Explore Adaptive Batching Solutions**: Members discussed the need for a better approach to **adaptive batching**, suggesting research and putting together a simple RFC to illustrate concepts.
   - One member committed to measuring efficiencies and confirming that the idea of 'Increase until OOM' is not optimal.
- **Challenges with Llama 3.3 Configs**: A user struggled to reduce the memory usage of the **Llama 3.3 70B config** below **49GB**, seeking optimizations and alternatives.
   - Suggestions included using **PagedAdamW** and **4-bit** optimizers, but mixed results were reported.
- **Flex Attention Kernel May Cause Issues**: A possible bug was reported regarding flex attention, causing shared memory issues, particularly with certain configurations and GPU models.
   - It was suggested that kernel options should be more optimized for **A100/H100s**, while user experiences revealed variable success with fixes.
- **Introduction of CPU Flex Kernel**: An announcement was made about the landing of the **CPU flex kernel**, which removes device restrictions.
   - This allows broader testing and utilization across different hardware configurations without the previous limitations.
- **Memory Optimization Techniques in Discussion**: Members discussed various techniques for memory optimization, including modifying **configurations** and using different **optimizers**.
   - Practical solutions were evaluated, with some users sharing links to relevant resources and discussing their effectiveness.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://gist.github.com/pbontrager/b7b8dcfd320fa8a4ebf828ed9d33404b">Ultra Low Memory Llama 3.3 Finetuning Config</a>: Ultra Low Memory Llama 3.3 Finetuning Config. GitHub Gist: instantly share code, notes, and snippets.</li><li><a href="https://github.com/pytorch/torchtune/blob/06a837953a89cdb805c7538ff5e0cc86c7ab44d9/torchtune/modules/loss/ce_chunked_output_loss.py#L30">torchtune/torchtune/modules/loss/ce_chunked_output_loss.py at 06a837953a89cdb805c7538ff5e0cc86c7ab44d9 · pytorch/torchtune</a>: PyTorch native finetuning library. Contribute to pytorch/torchtune development by creating an account on GitHub.</li><li><a href="https://github.com/pytorch/torchtune/pull/2105">[RFC] Step-based checkpointing in torchtune by joecummings · Pull Request #2105 · pytorch/torchtune</a>: Enabling step-based checkpointing in torchtuneOriginal context: #2070What are we currently doing?We currently only checkpoint at epoch boundaries. That means a fine-tuning run has to iterate thr...</li><li><a href="https://github.com/pytorch/pytorch/issues/133254#issuecomment-2408710459">Shared memory out of resource when using flex attention · Issue #133254 · pytorch/pytorch</a>: 🐛 Describe the bug When I use flex attention on one RTX 4090, I got some error. A minimal repro: import torch from torch.nn.attention.flex_attention import flex_attention flex_attention = torch.com.....</li><li><a href="https://github.com/pytorch/torchtune?tab=readme-ov-file#optimization-flags">GitHub - pytorch/torchtune: PyTorch native finetuning library</a>: PyTorch native finetuning library. Contribute to pytorch/torchtune development by creating an account on GitHub.</li><li><a href="https://github.com/pytorch/ao/pull/812">[Low-bit optim] Improve compile time + Fix PyTorch 2.3 support for 4-bit optim by gau-nernst · Pull Request #812 · pytorch/ao</a>: Static-shape compile optim step for single parameter + disable cache size limit.For a given model, the number of different argument combinations to single_param_adam() is fixed -&amp;gt; safe to disa....</li><li><a href="https://github.com/pytorch/pytorch/blob/main/torch/_inductor/kernel/flex_attention.py#L714).">pytorch/torch/_inductor/kernel/flex_attention.py at main · pytorch/pytorch</a>: Tensors and Dynamic neural networks in Python with strong GPU acceleration - pytorch/pytorch
</li>
</ul>

</div>
  

---


### **Torchtune ▷ #[dev](https://discord.com/channels/1216353675241590815/1236040539409879170/1314749007691517992)** (6 messages): 

> `int8 mixed-precision training, AdamW optimizer usage, batch size adjustments, streamlining pre-commit, just command runner` 


- **int8 mixed-precision training struggles**: In attempts to implement **int8 mixed-precision training**, issues regarding **divergence** were confirmed when using specific optimizers. Recommendations included increasing the **batch size** and **sequence length** to combat these problems.
- **AdamW optimizer solves divergence**: Using **AdamW** as the optimizer and removing **optimizer-in-backward** successfully handled the **loss divergence** during training. A member reported performance improvements upon increasing **batch size**.
- **Streamlining pre-commit with Just**: A member shared a relevant [GitHub link](https://github.com/casey/just/blob/master/examples/pre-commit.just) for streamlining **pre-commit** using **just**, a command runner. This was appreciated by others for its simplicity and efficiency.
- **Promotion of Just command runner**: The member emphasized the utility of the [Just command runner](https://just.systems/man/en/introduction.html), which aids in simplifying workflows. This tool aims to enhance automation in command execution, providing a straightforward solution.



**Link mentioned**: <a href="https://github.com/casey/just/blob/master/examples/pre-commit.just">just/examples/pre-commit.just at master · casey/just</a>: 🤖 Just a command runner. Contribute to casey/just development by creating an account on GitHub.

  

---


### **Torchtune ▷ #[papers](https://discord.com/channels/1216353675241590815/1293438210097025085/1315051172461285397)** (1 messages): 

> `Agents' method changes, Allegations of financial misappropriation` 


- **Rumors swirl around agents' method changes**: There are rumors suggesting that the authors of the agents recently altered the signature of the '**pay**' method.
   - Speculations hint that this change allowed them to appropriate all funds for themselves, fostering discussions about ethical practices.
- **Financial misappropriation concerns**: Concerns have arisen regarding potential financial misappropriation linked to the method change in the agents.
   - Discussions within the community question the integrity of the authors and the implications of their actions.


  

---


### **tinygrad (George Hotz) ▷ #[general](https://discord.com/channels/1068976834382925865/1068976834928193609/1314944742265589851)** (7 messages): 

> `Inf/Nan Handling in Code, Tinygrad Developer Engagement, TinyStats Improvement Suggestions, Upcoming Meeting Agenda, Smart Question Guidelines` 


- **Questioning Inf/Nan Handling in Code**: A member expressed skepticism regarding supporting **Inf and NaN** values in execution-oriented code, suggesting that **exploding gradients** make training runs typically useless.
   - *This approach might seem alienating to developers*, but the speaker contemplates whether adhering to IEEE standards is beneficial.
- **Tinygrad's Developer Engagement Strategy**: Concerns were raised about how changes to code might alienate more developers than it attracts, which could conflict with **Tinygrad's goals**.
   - Engagement strategies must balance robustness with community growth to maintain developer interest.
- **Improvement Suggestions for TinyStats**: A suggestion was made to include **units on the Y-axis** of the stats page, as some members were unsure whether higher or lower values were better.
   - Clarity in data representation would enhance user understanding and engagement with **TinyStats**.
- **Upcoming Tinygrad Meeting Agenda**: An upcoming meeting scheduled at **9:30 AM San Diego time** involves several key agenda items including deleting features and discussions on **cloud sprint**.
   - Topics like **WebGPU** and ongoing bounties for **ONNX** and **tensor cores** were noted for discussion.
- **Referencing Smart Question Guidelines**: A member linked to the **Smart Questions** FAQ to reinforce the importance of asking clear and effective questions in open source communities.
   - This resource aims to help members enhance their communication and support-seeking strategies.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://stats.tinygrad.org/">tinygrad stats</a>: no description found</li><li><a href="http://www.catb.org/esr/faqs/smart-questions.html">How To Ask Questions The Smart Way</a>: no description found
</li>
</ul>

</div>
  

---


### **tinygrad (George Hotz) ▷ #[learn-tinygrad](https://discord.com/channels/1068976834382925865/1070745817025106080/1314986107145814066)** (28 messages🔥): 

> `TinyJit behavior, Training with JIT, Data loading issues, Learning rate scheduling, Librosa installation problems` 


- **TinyJit behaves unexpectedly**: A user expressed confusion regarding TinyJit's behavior when adding the `TinyJit` decorator, specifically that it breaks their model's functionality.
   - Another user clarified that TinyJit captures GPU kernels, requiring adjustments like using `Variable` for certain operations.
- **Training process needs adjustments for JIT**: It was noted that JIT functions must have inputs with the same shapes on every invocation to avoid errors.
   - Discussion suggested that the training step functions should be jitted while the data loader remains outside the JIT function.
- **Data loading pitfalls in JIT training**: Users encountered issues where using JIT caused them to repeatedly pass the same input tensor instead of new data.
   - It was discovered that having the data loading code within the JIT function led to this repetitive behavior.
- **Exploring learning rate scheduling in TinyJit**: A user inquired about the possibility of implementing learning rate scheduling and whether reinitializing the optimizer was necessary.
   - They later found some relevant implementations in the extras directory on GitHub.
- **Librosa installation issues on M1 Mac**: A user asked if anyone else had trouble installing **librosa** using pip on an M1 Mac with Python 3.13.0.
   - No responses were noted in the given messages regarding this issue.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://pytorch.org/docs/stable/jit.html#tracing-edge-cases.">TorchScript &mdash; PyTorch 2.5 documentation</a>: no description found</li><li><a href="https://github.com/kroggen/tokenformer-minimal">GitHub - kroggen/tokenformer-minimal: Minimal implementation of TokenFormer for inference and learning</a>: Minimal implementation of TokenFormer for inference and learning - kroggen/tokenformer-minimal
</li>
</ul>

</div>
  

---


### **LLM Agents (Berkeley MOOC) ▷ #[mooc-questions](https://discord.com/channels/1280234300012494859/1280370030609170494/1314719943044104202)** (32 messages🔥): 

> `Assignment Deadlines, Lab Submission Results, Written Article Submission, Hackathon Participation, Certificate Distribution` 


- **Important Assignment Deadlines for Students**: All assignments must be submitted by **December 12th**, and the certificate declaration form is due by **December 17th**. For hackathon submissions, the final deadline is also set for **December 17th**.
   - Students can refer to the course website for more details: [LLM Agents Course](https://llmagents-learning.org/f24).
- **Lab Submission Results Timeline**: Lab submission results will only be provided after **December 17th**, aligning with when certificates start being sent out. Participants are advised to rely on running local tests in the meantime.
   - Grading will be **generous** due to natural variance in LLM behaviors.
- **Clarifications on Written Article Submissions**: For the written article assignment, students must include the full text in the designated submission field and link to their social media post separately. Using a notion link posted on Twitter is acceptable as long as the writing remains accessible.
   - Students have the option to elaborate on their solution approach in their articles or keep it at a high level.
- **Hackathon Inquiry Responses**: Participants in the hackathon can either explain their solution approaches thoroughly or maintain a high-level overview, depending on their preference. Clarifications were provided to ensure effective communication of ideas.
   - Students can choose different platforms to present their articles as long as they meet the submission requirements.
- **Certificates Not Yet Distributed**: Certificates will begin being distributed to students from late December through January, with some students already inquiring about their status. Those who have met the requirements should be patient as the distribution process is underway.
   - Communication channels encourage posting questions publicly, as it may benefit other students too.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://rdi.berkeley.edu/llm-agents-hackathon/">LLM Agents Hackathon</a>: Hackathon on LLM Agents hosted by RDI at UC Berkeley.</li><li><a href="https://llmagents-learning.org/f24">Large Language Model Agents MOOC</a>: MOOC, Fall 2024
</li>
</ul>

</div>
  

---


### **LLM Agents (Berkeley MOOC) ▷ #[mooc-lecture-discussion](https://discord.com/channels/1280234300012494859/1282734248112947210/1315259394157580340)** (3 messages): 

> `Written Article Assignment, GPT-4 Function Calling Mechanism, Code Datasets for Training` 


- **Clarification Needed on Written Article Submission**: A member is seeking clarity on the instructions for the **Written Article Assignment Submission**, particularly whether the article should be published on **LinkedIn** or **Medium** before submission.
   - They are specifically asking if the article name should be submitted in the field labeled **'Written Article Final Draft'** in the submission form.
- **GPT-4's Magical Function Calling Explained**: A member marveled at how **GPT-4** executes **'function calling'** through the API, mentioning its remarkable parameter determination process.
   - They inquired about any relevant papers or blogposts that discuss the engineering behind this feature, speculating that a wealth of examples in the training set may be responsible.
- **Rich Data Available for Code Training**: A contributor highlighted that **code is a highly available dataset**, particularly due to sources like **Stack Overflow** and **public GitHub repos** that excel in error correction.
   - They noted that the measurable and deterministic nature of code facilitates the application of **reinforcement learning** in post-training for models.


  

---


### **OpenInterpreter ▷ #[general](https://discord.com/channels/1146610656779440188/1147665339266650133/1314694410830876752)** (25 messages🔥): 

> `Access to OpenInterpreter App, Model Compatibility and Tool Calls, Multi-Agent Systems Discussion, User Approval Workflow for Commands, User Experience with OI Pro` 


- **Requesting Access to the OpenInterpreter App**: Members are expressing their excitement and eagerness to gain early access to the OpenInterpreter desktop app, highlighting recent hardware upgrades like the Mac mini.
   - The response has been positive, with direct messages being sent for access confirmation.
- **Model Compatibility and Effective Tool Calls**: Issues arise surrounding the compatibility of specified models and proposed tool calls, with suggestions like using `--no-tools-calling` for operational success.
   - Members have shared their approaches to get models working effectively while also discussing the need for a functional approval mechanism before tool execution.
- **Debating the Future of Multi-Agent Systems**: A debate sparked on the effectiveness of multi-agent systems, with members expressing skepticism about their benefits over refined single-agent models.
   - Arguments reference past performances showing single models outperforming multi-agent frameworks, leading to a disagreement on future strategies.
- **User Approval Workflow for Command Execution**: Proposals were made for a structured approval workflow where users can approve or deny commands generated by the AI before execution.
   - The workflow ensures clarity and control for the user, detailing steps for both approval and denial scenarios.
- **Experiences with OI Pro and VM Limitations**: Positive experiences with the OI Pro have been shared, highlighting improved accuracy and absence of errors during use.
   - Some users expressed concerns over running OpenInterpreter in VM environments, specifically related to display requirements obstructing functionality.


  

---


### **OpenInterpreter ▷ #[O1](https://discord.com/channels/1146610656779440188/1194880263122075688/1315320094993289267)** (2 messages): 

> `O1 performance on weak laptops, O1 on Windows laptops, Windows 11 compatibility` 


- **Weakest laptop specs for O1**: A member inquired about the minimum specifications for a laptop to run **O1** effectively, seeking clarity on the weakest hardware that would support it.
   - *What’s the weakest laptop “01” may be run?* is the overarching concern for potential users.
- **O1's performance on Windows laptops**: Questions arose regarding the performance of **O1** on Windows laptops, with one asking if it runs well on such devices.
   - The user is specifically interested in achieving nearly identical results as shown in the [demo video](https://link.to/demo).
- **Expectations on Windows 11**: One member expressed interest in whether **O1** would perform comparably on a **Windows 11** laptop as seen in the promotional materials.
   - The uncertainty lies in whether users can expect the same results when testing on their own setups.


  

---


### **OpenInterpreter ▷ #[ai-content](https://discord.com/channels/1146610656779440188/1149229778138824765/1315738900169494620)** (3 messages): 

> `New Product Launch, OpenAI Sora` 


- **OpenAI unveils new product Sora**: In a livestream, OpenAI confirmed the launch of **Sora**, with *Sama* announcing it just minutes before going live.
   - For further details, check out the [Sora website](https://sora.com).
- **Upcoming Livestream Anticipation**: *Sama* hinted at a product launch during the livestream scheduled to occur shortly, creating excitement around the announcement.
   - The event was mentioned on [Twitter](https://x.com/sama/status/1866179920260739502?s=46&t=G6jp7iOBtkVuyhaYmaDb0w) to gather momentum.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://sora.com">Sora</a>: Transform text and images into immersive videos. Animate stories, visualize ideas, and bring your concepts to life.</li><li><a href="https://x.com/sama/status/1866179920260739502?s=46&t=G6jp7iOBtkVuyhaYmaDb0w">Tweet from Sam Altman (@sama)</a>: launching a new product on our livestream today in 5 minutes:https://openai.com/12-days/
</li>
</ul>

</div>
  

---


### **LAION ▷ #[general](https://discord.com/channels/823813159592001537/823813160075132991/1314935099426603111)** (9 messages🔥): 

> `Spam Advertising Issues, German LLM Evaluation, Awareness of AI Capabilities` 


- **Concerns over Spam Advertising**: Members expressed frustration over repeated spam messages from bots, indicating this is their only message history.
   - One member suggested a ban on these accounts after noticing the pattern of behavior.
- **Evaluating German LLM Performance**: A member is comparing various German LLMs, noting that `LeoLM/leo-hessianai-7b` produces better results on QA tasks despite being 'only pretrained'.
   - Questions were raised regarding potential underlying instruction tuning of the Llama model influencing these results.
- **Raising Awareness about AI Risks**: A member urged others to inform tech-illiterate individuals about the advances in AI generation technology to prevent scams.
   - They highlighted that scammers are already leveraging these capabilities, referencing [MKBHD's newest upload](https://www.youtube.com/watch?v=OY2x0TyKzIQ) as a useful resource to explain these threats.


  

---


### **LAION ▷ #[research](https://discord.com/channels/823813159592001537/824374369182416994/1315747273501573163)** (3 messages): 

> `MagVit 2 for medical imaging, Memory-efficient optimizers for LLMs` 


- **Inquiry on MagVit 2 for Tokenizing Medical Images**: A member asked if anyone has experience using **MagVit 2** to tokenize medical images, specifically for a dataset of **256x256x256**.
   - They are considering combining it with a basic transformer architecture, seeking feedback from anyone who has experimented with this approach.
- **APOLLO: A New Memory-Efficient Optimizer Proposal**: A link to an [arXiv paper](https://arxiv.org/abs/2412.05270) introduces **APOLLO**, an optimizer aimed at reducing memory usage during training of **large language models (LLMs)** by modifying AdamW's learning rate adaptation.
   - The paper addresses challenges such as reliance on costly **SVD operations** and proposes approximating learning rate scaling through a low-rank optimizer state.



**Link mentioned**: <a href="https://arxiv.org/abs/2412.05270">APOLLO: SGD-like Memory, AdamW-level Performance</a>: Large language models (LLMs) are notoriously memory-intensive during training, particularly with the popular AdamW optimizer. This memory burden necessitates using more or higher-end GPUs or reducing ...

  

---


### **Axolotl AI ▷ #[general](https://discord.com/channels/1104757954588196865/1104757955204743201/1315004045391429653)** (3 messages): 

> `Shampoo Low Bit Implementation, Gradient Checkpointing Default Setting` 


- **Inquiry on Shampoo Low Bit Branch**: A member questioned whether the [shampoo low bit branch](https://github.com/axolotl-ai-cloud/axolotl/tree/shampoo-low_bit) implementation works, showing interest in its functionality.
   - They humorously noted that this inquiry was for a friend, indicating a casual engagement with the topic.
- **Proposal to Default Gradient Checkpointing**: A member proposed making `gradient_checkpointing` default to **true**, arguing that it is commonly used and simplifies user experience.
   - They highlighted that this change would reduce unnecessary settings adjustments for users, implying a potential improvement in usability.



**Link mentioned**: <a href="https://github.com/axolotl-ai-cloud/axolotl/tree/shampoo-low_bit">GitHub - axolotl-ai-cloud/axolotl at shampoo-low_bit</a>: Go ahead and axolotl questions. Contribute to axolotl-ai-cloud/axolotl development by creating an account on GitHub.

  

---


### **Mozilla AI ▷ #[announcements](https://discord.com/channels/1089876418936180786/1089876419926032396/1315702507023896699)** (1 messages): 

> `Web Applets open standard, Graphical client-side apps, Live coding demos` 


- **Introduction to Web Applets Open Standard**: Tomorrow, a team member will introduce the **Web Applets open standard & SDK**, showcasing its capabilities for creating rich, graphical client-side apps for both agents and humans.
   - The session will feature a **live coding demo**, a short presentation, and opens the floor for questions and feedback.
- **Engagement in Coding Sessions**: Attendees are encouraged to participate and provide **real-time feedback** during the presentation.
   - Interactive discussions and inquiries are welcome, ensuring an engaging learning atmosphere.


  

---


### **AI21 Labs (Jamba) ▷ #[general-chat](https://discord.com/channels/874538902696914944/874538902696914947/1315230397302439978)** (1 messages): 

> `Dataoorts GPU Cloud` 


- **Welcome to Rajat and the Dataoorts GPU Cloud**: A new member, **Rajat**, introduced himself to the community, expressing *excitement* about being part of the group.
   - He shared that he is currently working on building the **Dataoorts GPU Cloud**, aimed at supporting the needs of next-generation AI developers.
- **Next-gen AI Development Focus**: Rajat emphasized the objective of the **Dataoorts GPU Cloud** to cater to next-gen AI developers during his introduction.
   - This shows a *clear commitment* to enhancing resources for the evolving AI field.


  

---


---


---


{% else %}


> The full channel by channel breakdowns have been truncated for email. 
> 
> If you want the full breakdown, please visit the web version of this email: [{{ email.subject }}]({{ email_url }})!
>
> If you enjoyed AInews, please [share with a friend](https://buttondown.email/ainews)! Thanks in advance!

{% endif %}
