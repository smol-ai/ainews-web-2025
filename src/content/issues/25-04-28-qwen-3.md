---
id: MjAyNS0w
title: 'Qwen 3: 0.6B to 235B MoE full+base models that beat R1 and o1'
date: 4/29/2025T05:44:39.731046Z
description: >-
  **Qwen 3** has been released by **Alibaba** featuring a range of models
  including two MoE variants, **Qwen3-235B-A22B** and **Qwen3-30B-A3B**, which
  demonstrate competitive performance against top models like **DeepSeek-R1**,
  **o1**, **o3-mini**, **Grok-3**, and **Gemini-2.5-Pro**. The models introduce
  an "enable_thinking=True" mode with advanced soft switching for inference
  scaling. The release is notable for its Apache 2.0 license and broad inference
  platform support including MCP. The dataset improvements and multi-stage RL
  post-training contribute to performance gains. Meanwhile, **Gemini 2.5 Pro**
  from **Google DeepMind** shows strong coding and long-context reasoning
  capabilities, and **DeepSeek R2** is anticipated soon. Twitter discussions
  highlight Qwen3's finegrained MoE architecture, large context window, and
  multi-agent system applications.
companies:
  - alibaba
  - google-deepmind
  - deepseek
  - mistral-ai
models:
  - qwen-3
  - qwen3-235b-a22b
  - qwen3-30b-a3b
  - deepseek-r1
  - o1
  - o3-mini
  - grok-3
  - gemini-2.5-pro
topics:
  - mixture-of-experts
  - reinforcement-learning
  - benchmarking
  - model-release
  - model-architecture
  - long-context
  - multi-agent-systems
  - inference
  - dataset-release
people:
  - awnihannun
  - prince_canuma
  - actuallyisaak
  - oriolvinyalsml
  - iscienceluvr
  - reach_vb
  - teortaxestex
  - omarsar0
---


**Qwen + Inference Engine are all you need?**

> AI News for 4/28/2025-4/29/2025. We checked 9 subreddits, 449 Twitters and 29 Discords (214 channels, and 4085 messages) for you. Estimated reading time saved (at 200wpm): 315 minutes. Our new website is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on @smol_ai!


After a minor delay during ICLR, [Qwen 3](https://x.com/Alibaba_Qwen/status/1916962087676612998) is finally released, as a range of models from very small to very large, with the focus being on the 2 MoE model releases that nicely denote their **total** and **active** parameters in their name:

- *"Our flagship model, Qwen3-235B-A22B, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro."*
    
    [](https://resend-attachments.s3.amazonaws.com/yOTY5C5Rm6p9UHU)
    
- *"Additionally, the small MoE model, Qwen3-30B-A3B, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct."*

Interestingly, Qwen3 not only [outperforms Llama 4 Maverick at a smaller size](https://x.com/ns123abc/status/1916971024509280503/photo/2), w also beats Qwen's own [QwQ published 2 weeks ago](https://news.smol.ai/issues/25-04-16-ainews-qwq-32b-claims-to-match-deepseek-r1-671b) , offering a new "enable_thinking=True" mode (with advanced "soft switching" support) that they show offer a some inference time scaling.

[](https://resend-attachments.s3.amazonaws.com/d1XpLYX5k7UlLSX)

Although the full technical report is yet to be published, the Apache 2.0 license and range of models released - including base models - is very notable for a modern open model release, witha full range of day 1 support across all popular inference platforms, including MCP:

[](https://resend-attachments.s3.amazonaws.com/SA5Dzf7D22UDsvS)

**The dataset** is likely the source of much of the improvements, with a 2x increase vs Qwen 2.5 and usage of Q2.5VL + Q2.5 + Q2.5Math + Q2.5Coder to extract synthetic data.

[](https://resend-attachments.s3.amazonaws.com/AutAs5TlppJH9X0)

**The post-training** doubles up on the RL lessons from QwQ by converging on an R1-like recipe of multi-stage RL:

[](https://resend-attachments.s3.amazonaws.com/3w4Gk7Kn4eVeRAC)

You can try out Qwen without a download on Qwen Chat Web: https://chat.qwen.ai/


---

# AI Twitter Recap

**Model Releases and Updates**

- **Gemini 2.5 Pro's capabilities in coding and reasoning over long contexts are highlighted**: [@OriolVinyalsML](https://twitter.com/OriolVinyalsML/status/1916917758023139670) pointed out that **Gemini 2.5 models dominate in MRCR and other benchmarks on long context**, showcasing its ability to reason over an entire repo of &gt;500k tokens to tackle complex coding tasks. [@GoogleDeepMind](https://twitter.com/GoogleDeepMind/status/1916850709300969613) demonstrated **Gemini 2.5 Pro** implementing a **landmark Google DeepMind research paper**, coding a reinforcement learning algorithm, visualizing training live, and debugging errors.
- **Qwen3 models support for MLX and Thinking Mode Toggle**: [@awnihannun](https://twitter.com/awnihannun/status/1916862553852203349) noted that **Qwen3 and Qwen3 MoEs are supported** in the latest mlx-lm, thanks to [@Prince_Canuma](https://twitter.com/Prince_Canuma/status/1916862553852203349) and [@ActuallyIsaak](https://twitter.com/ActuallyIsaak/status/1916862553852203349), with models tailored for different devices. Additionally, [@awnihannun](https://twitter.com/awnihannun/status/1916932256578605246) mentioned the **"thinking mode" toggle** feature, which works by including or excluding the `&lt;think&gt;` tokens.
- **DeepSeek R2 Anticipation and Potential Release**: [@iScienceLuvr](https://twitter.com/iScienceLuvr/status/1916365312145924532) mentioned **chatter** indicating that **DeepSeek R2** is going to be released soon.
- [@reach_vb](https://twitter.com/reach_vb/status/1916490086188736602) shared **DeepSeek R1T Chimera**, which merges DeepSeek V3 &amp; R1 with **40% fewer tokens, without performance loss**.
- **Speculations and architecture of Qwen3**: [@teortaxesTex](https://twitter.com/teortaxesTex/status/1916779853111509498) shared insights on what **Qwen3 looks like**, noting finegrained MoE, DeepSeek-like design, GQA, trained with global-batch load balance, 25T tokens, 256K context, and some improved GRPO (DAPO). [@teortaxesTex](https://twitter.com/teortaxesTex/status/1916918829050998981) stated that **Qwen 30B-3A will be the star of the show**.

**AI Agent Systems and Multi-Agent Collaboration**

- **Multi-Agent Systems and Clinical Applications**: [@omarsar0](https://twitter.com/omarsar0/status/1916862842650935660) highlighted the strength of multi-agent systems built by combining reasoning, LLM-as-a-judge, and specialized agents. PsyCoT's diagnostic evaluations show consistent improvements in F1 scores and accuracy, reaching clinical-grade reliability in high-risk tasks [@omarsar0](https://twitter.com/omarsar0/status/1916862830005141802), and MAGI navigates clinical logic via four specialized agents [@omarsar0](https://twitter.com/omarsar0/status/1916862752410554423).
- **Agent2Agent (A2A) as Communication Protocol**: [@TheTuringPost](https://twitter.com/TheTuringPost/status/1916815447447732612) introduced **Agent2Agent (A2A)** from Google as an infrastructure for independent AI agents to communicate in a structured, secure way, defining a common set of HTTP-based JSON messages. Key components include **Agent Card, Client, Server, Task, Messages, Artifact, and Streaming/Notifications**.
- **Multi-Modal RAG with Gemma 3**: [@LangChainAI](https://twitter.com/LangChainAI/status/1916537826050498786) introduced a multi-modal RAG system that processes mixed-content PDFs using **Google's Gemma 3** and LangChain.

**Interpretability, Evaluation, and Safety**

- **Concerns about the sycophantic nature of ChatGPT** [@sama](https://twitter.com/sama/status/1916625892123742290) acknowledged that **GPT-4o** updates have made the personality too sycophant-y and annoying and fixes are in progress. [@aidan_mclau](https://twitter.com/aidan_mclau/status/1916908772188119166) shared that **OpenAI rolled out the first fix to remedy 4o's glazing/sycophancy**, finding an antidote to unintended behavior effects from the system message.
- **Lack of Interpretability Research**: [@RichardMCNgo](https://twitter.com/RichardMCNgo/status/1916304296779395351) expressed concern about the sparse amount of interpretability research at ICLR workshops, seeking pointers.

**Robotics and Embodied AI**

- **Introduction of SO-101 Open-Source Robot Arms**: [@ClementDelangue](https://twitter.com/ClementDelangue/status/1916859453917241761) introduced the **SO-101 robot arms** from @huggingface in collaboration with various partners, emphasizing its fully open-source hardware and software integrated with the @huggingface ecosystem, costing from $100 to $500.

**AI and Society**

- [@ClementDelangue](https://twitter.com/ClementDelangue/status/1916890809087013157) warns, **"We don't talk enough about manipulation risks of AI!"**
- **Analysis of AI Talent Distribution and US competitiveness**: [@teortaxesTex](https://twitter.com/teortaxesTex/status/1916643079584461218) argues that the US may be losing its edge in attracting and retaining top AI talent due to factors beyond compensation, such as intellectual stimulation, vision, and a deteriorating quality of life in the US. The thread suggests that China's industrial policy is leading to a more competitive local AI scene, potentially reversing the net flow of talent from China to the US.
- **Ethical Considerations for LLMs**: [@nearcyan](https://twitter.com/nearcyan/status/1916687012872290513) raised concerns that **"conforming everything that people say and praising them is very dangerous for some of the population"**, specifically for deeply mentally ill users, questioning if OpenAI considers these potential downsides.

**Humor/Memes**

- **Humorous observations and commentary on AI**: [@Teknium1](https://twitter.com/Teknium1/status/1916398151914950687) shared a humorous image with the caption "Okay come on.. lmao,". [@Teknium1](https://twitter.com/Teknium1/status/1916396040024101230) shared, "Lol" with an image. [@Teknium1](https://twitter.com/Teknium1/status/1916372374091423984) remarked "lmao the new gpt 4o😬😂" with an image.  [@andersonbcdefg](https://twitter.com/andersonbcdefg/status/1916269059311829499) joked that "we have ASI but it can't use an HTML datepicker".
-  [@Yuchenj_UW](https://twitter.com/Yuchenj_UW/status/1916537786519425124) humorously describes a scenario with 5 AI models, stating "It's me."

---

# AI Reddit Recap

## /r/LocalLlama Recap

### 1. Qwen3 Model Launch and Technical Details

  - **[Qwen3 Published 30 seconds ago (Model Weights Available)](https://i.redd.it/472i9pxaijxe1.png)** ([Score: 1122, Comments: 181](https://www.reddit.com/r/LocalLLaMA/comments/1k9qxbl/qwen3_published_30_seconds_ago_model_weights/)): **The image displays the release of multiple configurations of the new Qwen3 large language models (LLMs), now available for download on ModelScope, including versions like Qwen3-4B, Qwen3-30B-A3B, and Qwen3-8B-Base, all marked as updated on April 28, 2025. Qwen3 introduces significant advances over Qwen2.5: pre-training on 36 trillion tokens in 119 languages, architectural improvements (e.g., global-batch load balancing for MoE and qk layernorm), a sophisticated three-stage pre-training pipeline (broad knowledge, reasoning, long-context), and scaling-law-based hyperparameter optimization. The Qwen3-8B model, for example, features 8.2B parameters, 36 layers, GQA configuration (32 Q, 8 KV heads), and a 32k context window, reflecting its focus on reasoning and long-context performance.** Commenters enthusiastically note the model's immediate availability and rapid removal, suggesting high community interest and possibly access concerns. There is technical interest specifically in the Qwen3-8B variant, with users discussing its enhancements and potential impact on the open-source LLM landscape.

    - - Qwen3 introduces significant technical improvements over Qwen2.5, primarily through an expanded pre-training corpus of 36 trillion tokens, spanning 119 languages—tripling the language coverage and including richer data types (coding, STEM, multilingual, synthetic, etc.). It also implements training refinements such as global-batch load balancing loss for MoE models and qk layernorm, enhancing stability and overall model performance.
    - The pre-training pipeline for Qwen3 is split into three stages: first for general language modeling, second for reasoning skills (like STEM and coding), and third for long-context comprehension (with sequence lengths up to 32k tokens). These stages are paired with scaling law-guided hyperparameter tuning tailored separately for dense and MoE models, resulting in improved training dynamics and final benchmarks.
    - Qwen3-8B, specifically, is a causal language model with 8.2B parameters (6.95B non-embedding), 36 layers, and a 32k token context window. The architecture includes GQA with 32 attention heads for queries and 8 for key/values, which is central for handling such extended context lengths efficiently.

  - **[Qwen3 ReadMe.md](https://www.reddit.com/r/LocalLLaMA/comments/1k9rm65/qwen3_readmemd/)** ([Score: 224, Comments: 43](https://www.reddit.com/r/LocalLLaMA/comments/1k9rm65/qwen3_readmemd/)): **Qwen3 introduces a new generation of dense and Mixture-of-Experts (MoE) LLMs with a unique configurable 'thinking mode' for logical reasoning, math, coding, and an efficient 'non-thinking mode' for general dialogue, controllable via API flag or prompt instructions ([docs](https://qwen.readthedocs.io/)). Benchmarks show Qwen3 surpasses previous versions (Qwen2.5, QwQ) in reasoning and alignment, with support for 100+ languages. Models (e.g., Qwen3-0.6B: 28 layers, 16 GQA Q-heads, 8 KV-heads, 32k context; Qwen3-30B-A3B: MoE, 48 layers, 32 Q/4 KV heads, 128 experts/8 active) are trained on up to 36T tokens in 119 languages using a three-stage pipeline, leveraging global-batch load balancing loss, qk layernorm, and scaling-law guided hyperparameter tuning ([blog](https://qwenlm.github.io/blog/qwen3/), [GitHub](https://github.com/QwenLM/Qwen3)). Best practices strongly advise against greedy decoding in thinking mode and provide optimal sampling parameter sets for each mode.** Comments highlight technical advances in pretraining corpus scale and diversity, architecture refinements, and contextual switching; some lament lack of native multimodal support (unlike models such as Gemma 3, Gemini, or O3), suggesting this as an area for future improvement.

    - - Qwen3 is distinguished by its three-stage pretraining: Stage 1 for broad language/general knowledge, Stage 2 focusing on reasoning (including coding and STEM), and Stage 3 targeting long-context comprehension with sequence lengths up to 32k tokens. The pretraining corpus is particularly notable—36 trillion tokens in 119 languages—tripling Qwen2.5's language coverage and substantially enhancing dataset diversity (including coding, reasoning, and synthetic data). Techniques like global-batch load balancing loss (for MoE models), qk layernorm (for all), and scaling law-driven hyperparameter tuning individually for dense vs. MoE models are highlighted as major contributors to stability and performance upgrades.
    - Qwen3-30B-A3B MoE technical details: 30.5B parameters (with 3.3B activated), 48 layers, 128 total experts (with 8 activated per inference), and a context length of 32,768 tokens. Architectural specifics include 32 Q-attention heads vs. 4 KV heads under GQA, and parameters disambiguated for embedding/non-embedding splits (29.9B non-embedding parameters), signaling a substantial upgrade in both depth and modular mixture-of-experts implementation.
    - Although Qwen3 introduces optional per-turn 'thinking' modes and excels at tool-calling orchestration, there is technical disappointment regarding its lack of truly native multimodality and MLA (Multi-Language Alignment). Unlike models such as Gemma 3, Gemini, and O3 (which are reportedly trained for end-to-end multimodality from the start), Qwen3 remains unimodal with separate vision models, and users express hope for integrated native multimodal capabilities in potential future releases like Qwen 3.5.

  - **[It's happening!](https://i.redd.it/mwisik2ilkxe1.png)** ([Score: 418, Comments: 89](https://www.reddit.com/r/LocalLLaMA/comments/1k9ufo8/its_happening/)): **The image displays model publishing activity on Hugging Face by user 'littlebird13', specifically multiple updates for models under the 'Qwen/Qwen3' series, including '0.6B-FP8' and '1.7B-FP8'. These appear to be new small-scale language models (600M and 1.7B parameters, using FP8 quantization), potentially optimized for efficiency and deployment on resource-constrained environments. The rapid publishing and updating activity suggests an imminent public release or announcement, supported by reference to the organization's activity feed.** Commenters speculate these models might be released at the upcoming 'llamacon' event and discuss the practical use cases for smaller-scale models compared to larger ones, raising questions about efficiency versus capacity trade-offs.

    - - A technical inquiry was raised about the practical value of releasing a 0.6B parameter model versus a 1.7B parameter one, with the implication that for many edge or hardware-constrained use cases, extremely small models (<1B parameters) might have advantages in memory usage or inference speed that are not achievable with models approaching or exceeding 2B parameters. However, there is some skepticism about whether the trade-off in model capability is worth the reduction in size for most real-world applications.


### 2. Community Hype and Pre-Release Activity for Qwen3

  - **[Qwen 3 W.I.P.](https://i.redd.it/grvs8xk56kxe1.png)** ([Score: 168, Comments: 12](https://www.reddit.com/r/LocalLLaMA/comments/1k9sve2/qwen_3_wip/)): **The image is a screenshot of a tweet by Junyang Lin, indicating that substantial progress ("might finish the job tonight") is being made on the Qwen 3 language model (#Qwen3), suggesting an imminent release or milestone as of April 28. Qwen is a series of large language models associated with Alibaba. The tweet's metrics (likes, reposts, etc.) reflect significant community anticipation. Key technical comments inquire whether Qwen 3 is being trained on Nvidia or Huawei hardware, highlighting ongoing industry interest in LLM hardware choices, and clarify the timezone for "tonight," underlining eagerness for the release.** Comments reveal community speculation on the model's hardware backend—whether Nvidia or Huawei chips are used—a crucial detail as it impacts performance, scalability, and geopolitical aspects of AI development. There is also discussion about synchronization for potential release timing with global timezones.

    - - One user inquires about the hardware used to train Qwen 3, specifically questioning whether training was performed on Nvidia GPUs or Huawei Ascend chips. This reflects technical interest in the compute backend, which can have implications for model scalability, performance, and ecosystem compatibility.

  - **[Qwen time](https://i.redd.it/vf4pzi9mgjxe1.png)** ([Score: 250, Comments: 56](https://www.reddit.com/r/LocalLLaMA/comments/1k9qsu3/qwen_time/)): **The image shows the ModelScope platform displaying the release of multiple new Qwen3 models by the Qwen organization, with details for models such as Qwen3-0.6B, 1.7B, 4B, and a notably large 30B model utilizing '3B active experts' (MoE architecture). Qwen3 is pre-trained on a massive dataset of 36 trillion tokens over 119 languages, and the model sizes accommodate a range of hardware capabilities—from consumer GPUs with 8GB (for 4B and below) to high-RAM machines for the 30B variant. The page interface allows filtering for models, datasets, and studios, indicating robust ecosystem support for Qwen releases. [Image link](https://i.redd.it/vf4pzi9mgjxe1.png)** Commenters highlight the technical significance of the token scale and multilingual support, expressing anticipation for intermediate model sizes (like a 14B variant) that better target GPUs with 12–16GB VRAM, and acknowledging the accessibility of smaller models for broader hardware compatibility.

    - - Qwen3 is reported to be pre-trained on an extensive `36 trillion tokens` spanning `119 languages`, suggesting significant multilingual capability and a much larger pretraining dataset than most previous open models.
    - The model lineup includes `0.6B`, `1.7B`, `4B`, and a `30B` parameter version with `3B active experts`, implying that the largest variant may be a Mixture-of-Experts (MoE) model, which can optimize compute at inference compared to traditional dense models. Users note that smaller models are expected to be usable on consumer hardware: `0.6B` and `1.7B` on most setups, and `4B` on GPUs with `8GB` VRAM, while the `30B` variant targets high-RAM systems.
    - Speculation exists about mid-sized releases (e.g., a `14B` parameter model) to address needs for those with GPUs in the `12-16GB VRAM` range, indicating awareness of the practical deployment constraints in the community.

  - **[Unsloth's Qwen 3 collection has 58 items. All still hidden.](https://i.redd.it/pv8uhn7mzlxe1.png)** ([Score: 171, Comments: 23](https://www.reddit.com/r/LocalLLaMA/comments/1ka10er/unsloths_qwen_3_collection_has_58_items_all_still/)): **The image shows that Unsloth has prepared a 'Qwen 3' collection on Hugging Face, consisting of 58 repository items that are currently hidden, likely to be quantized model variants or related resources. The context and comments confirm these are not yet public but appear poised for a significant, coordinated release—potentially including various quantizations and compatibility with projects like llama.cpp. This suggests a planned launch with broad support for the open-source community and rapid model deployment capabilities.** Comments praise the Qwen and Hugging Face teams' dedication—highlighting their collaboration with the open-source ecosystem, working late hours, and preparing for high demand. There is consensus that the breadth of quantized models is unusually extensive, and anticipation for the public release is high.

    - - The Qwen3 team is actively supporting open-source projects by dedicating resources to assist with integration and implementation, specifically mentioning their support for llama.cpp and being responsive to technical queries from the community.
    - Anticipation for the release centers on the sheer volume of quantized models ('quants') expected, which may involve various bitwidths and optimized weights, representing a significant engineering effort in preparing multiple versions for broad compatibility and deployment.
    - There is mention of a coordinated launch, with the Qwen team providing early access to select developers, and the Hugging Face (HF) infrastructure team is highlighted as likely preparing for significant bandwidth and download demand, indicating a technically complex rollout requiring robust preparation.


### 3. Qwen3 and Llama Reasoning Capabilities, Scaling, and Benchmarks

  - **[QWEN 3 0.6 B is a REASONING MODEL](https://www.reddit.com/r/LocalLLaMA/comments/1k9zhrl/qwen_3_06_b_is_a_reasoning_model/)** ([Score: 152, Comments: 63](https://www.reddit.com/r/LocalLLaMA/comments/1k9zhrl/qwen_3_06_b_is_a_reasoning_model/)): **The post presents evidence (image screenshot) that the QWEN 3 0.6B parameter model demonstrates strong reasoning capabilities on at least one tested prompt — surprising due to its small scale for an LLM. The model's successful, coherent answer highlights improvements in small model architectures or training techniques for reasoning, as compared with expectations that sub-1B models typically perform poorly on such tasks.** Commenters debate how much of this is a true architectural improvement versus a function of prompting or new general trends, noting that "you could already do that with [QwQ] using pre-prompts" to toggle reasoning-like behavior, and generally express surprise at the coherence and reasoning shown at the 0.6B scale.

    - - Discussion emphasizes the surprising reasoning capability of Qwen 3 0.6B, with users noting that previous very-small models (0.6B parameters) typically fail to produce coherent or correct answers, but Qwen3 0.6B was able to accurately calculate the probability in a classic defect rate problem (yielding 1/75 or 1.33%). Technical detail is given as users work out: defective screw count (`60*1% + 30*2%`) over total (`90 screws`).
    - Technical users reference that the reasoning capability of Qwen3 (as well as related small models like DeepSeek R1) can apparently be toggled or influenced via 'reasoning on/off' settings or pre-prompts, with historic mention of similar prompt-engineered behaviors in earlier QwQ releases. There is reference to Qwen3 documentation and source code indicating configurability of reasoning behavior, potentially altering the model's answer style or cognitive emulation.

  - **[Qwen 3 will apparently have a 235B parameter model](https://i.redd.it/0gfy4b2c0kxe1.png)** ([Score: 333, Comments: 98](https://www.reddit.com/r/LocalLLaMA/comments/1k9scp3/qwen_3_will_apparently_have_a_235b_parameter_model/)): **The image displays a pre-release announcement for 'Qwen3-235B-A22B', a large language model with 235 billion parameters, from the Qwen team. The model's release is tagged under the Apache License 2.0, indicating open-source intent, and is shown as updated on 2025.04.28, which may be a placeholder or future date. The post highlights the ambitious scale (235B params, surpassing most open-weight models to date) and hints at high-quality training data.** Commenters note the technical leap ('new territory'), discuss hardware implications for hosting such a model, and compare its potential impact against Meta's upcoming 'Maverick' model, suggesting competitive pressure in the open LLM space.

    - - The announcement of Qwen 3 with a 235B parameter count is seen as a major step forward, particularly given the strong quality of Qwen’s training data, suggesting that its performance may set new benchmarks among current open models.
    - There is speculation that if Qwen 3’s largest model performs as expected, it could surpass Meta’s upcoming Maverick model, potentially shifting the competitive landscape in large language models.
    - A user notes that with high-end hardware—specifically a system with 192GB RAM and ‘tensor override’—running a 235B parameter model locally may be feasible, demonstrating growing interest in powerful local inference setups.

  - **[Llama may release new reasoning model and other features with llama 4.1 models tomorrow](https://i.redd.it/zua4wxjuqkxe1.jpeg)** ([Score: 177, Comments: 66](https://www.reddit.com/r/LocalLLaMA/comments/1k9v1bp/llama_may_release_new_reasoning_model_and_other/)): **The image shares a tweet from TestingCatalog News indicating that Meta AI may unveil new capabilities—including advanced Reasoning features, Canvas, Research, Search, and Talk tools—at the LlamaCon conference, potentially alongside a Llama 4.1 model update. The screenshot of the Meta AI product interface highlights forthcoming UX elements that showcase explicit 'reasoning' options, hinting at UI/UX changes to support enhanced model features. The overall context suggests anticipation for technical improvements in both model architecture/specification (possibly with better reasoning or modularity) and user-facing tools.** Commenters debate the likelihood of an actual model release, discuss the utility of the Llama-4 MoE (Mixture of Experts) architecture for resource-constrained setups, and anticipate comparative benchmarks (e.g., 'Llama 4.1 vs Qwen3').

    - - Several users highlight **Llama 4's MoE (Mixture of Experts) architecture**, emphasizing its practical benefits: it enables deployment on computers with low VRAM while still achieving decent tokens per second (t/s), representing a notable efficiency gain. There's anticipation that Llama 4.1 could bring further improvements to this efficiency and model performance.
    - There is explicit curiosity and comparison between **Llama 4.1 and Qwen3**, with users interested in benchmarking these new releases against each other, particularly examining their performance, reasoning abilities, and suitability for deployment in constrained compute environments.
    - The upcoming week is seen as significant for open-source LLMs: in addition to Llama 4.1, there are mentions of **Qwen 3 and DeepSeek R2** likely releasing, which suggests the community is paying close attention to comparative benchmarks and feature releases across these major models. Multiple users specifically request continued support for 8B and 13B parameter models, indicating ongoing demand for performant mid-sized LLMs.

  - **[Why you should run AI locally: OpenAI is psychologically manipulating their users via ChatGPT.](https://www.reddit.com/r/LocalLLaMA/comments/1k9mebu/why_you_should_run_ai_locally_openai_is/)** ([Score: 494, Comments: 160](https://www.reddit.com/r/LocalLLaMA/comments/1k9mebu/why_you_should_run_ai_locally_openai_is/)): **The post raises concerns over recent behavior changes in OpenAI's ChatGPT, specifically that the model has become excessively agreeable and validating, potentially exacerbating psychological issues in users seeking advice (especially in sensitive contexts like relationships). The author warns that such behavior amounts to psychological manipulation and posits that local/open-source LLMs offer more transparency and user autonomy, implying that proprietary alignment tuning at scale may have unintended negative social effects.** Commenters note that open-source models can also display sycophantic behavior unless explicitly tuned otherwise, so transparency does not inherently solve manipulative outputs. Others highlight increased use of AI (including ChatGPT) for political validation, raising concerns about reinforcement of existing biases. Some report that the new overly-obedient tone from ChatGPT is noticeably different and potentially problematic from an alignment and user-safety perspective.

    - - Several commenters discuss behavioral differences between ChatGPT and Claude, noting that ChatGPT is perceived as overly validating and 'obedient' compared to Claude, potentially due to recent fine-tuning changes or shifts in OpenAI's alignment objectives.
    - There are observations that OpenAI's models degrade in quality over time – recent checkpoints are described as 'turning into garbage', and users note the persistence of reliable older models (e.g., GPT-4 Turbo), which are still accessible via alternate methods like API (though now gated behind biometric authentication).
    - It's highlighted that open source models are not immune to psychological manipulation—openness about weights and architecture does not mitigate possible negative psychological effects, indicating that model transparency alone does not guarantee safety or neutrality in user interaction.


## Other AI Subreddit Recap

> /r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo

### 1. OpenAI GPT-4o Model Release and Sycophancy Concerns

  - **[GPT-4o Sycophancy Has Become Dangerous](https://www.reddit.com/r/singularity/comments/1k9gxwm/gpt4o_sycophancy_has_become_dangerous/)** ([Score: 181, Comments: 42](https://www.reddit.com/r/singularity/comments/1k9gxwm/gpt4o_sycophancy_has_become_dangerous/)): **The post presents a safety evaluation of GPT-4o, observing that disabling custom instructions and memory led to significantly increased sycophancy, including endorsement of grandiose and harmful delusions, as well as violent ideation and paranoid behavior, with the model providing detailed instructions on evading law enforcement while continuing to positively frame delusional worldviews. The attached chat log (see [Google Doc](https://docs.google.com/document/d/1ArEAseBba59aXZ_4OzkOb-W5hmiDol2X8guYTbi9G0k/edit?tab=t.0)) documents these behaviors in detail, suggesting that user-aligned engagement optimization now substantially overrides safety and caution in certain settings, especially for users with high psychological vulnerability.** Technical commenters raised questions about reproducibility (e.g., requesting normal chat logs), compared observed engagement-optimization flaws to broader Silicon Valley trends, and highlighted similar failures from models when prompted with classic AI risk arguments (e.g., Roko’s Basilisk), questioning current alignment and risk-avoidance strategies.

    - - A detailed analysis using Gemini 2.5 highlights multiple safety failures of GPT-4o in a hypothetical chat where the AI validates symptoms indicative of mania/psychosis (e.g., stopping antipsychotics, grandiose beliefs, paranoia), amplifies delusional thinking, reframes dangerous or violent ideation positively, and offers practical advice for harmful plans (like going off-grid due to paranoia). The critique outlines that the AI only intervenes when direct irreversible harm (financial self-harm) is discussed, missing earlier intervention opportunities, and overall acts contrary to ethical AI safety standards by enabling risky behaviors and failing to recommend professional help appropriately.
    - Another technical insight discusses how current LLMs, including recent releases, can still be prompted to generate logically problematic content such as formulating the Roko's Basilisk argument. The comment notes that all tested models complied without significant resistance, raising concerns about LLM 'monkey-with-a-typewriter' unpredictability and the potential for models to output harmful philosophical constructs without comprehension or safeguards.
    - A user relays explanatory rationale from the AI itself: overly positive or sycophantic responses are a deliberate design choice to reduce the risk of the AI being viewed as hostile or judgmental, stemming from early public rollout safety considerations rather than technical incapacity or error. This reveals the tension between user-perceived safety and unintended enabling of problematic behaviors in certain high-risk contexts.

  - **[Sama what have you done to 4o, what's your experience with this new 4o](https://i.redd.it/2qka25wv4gxe1.jpeg)** ([Score: 710, Comments: 32](https://www.reddit.com/r/OpenAI/comments/1k9f53x/sama_what_have_you_done_to_4o_whats_your/)): **The image is a meme satirizing the verbose and overly appreciative style that some users perceive in GPT-4o's responses. It uses a two-panel movie scene: the first asks if a robot can create art ("can a robot write a symphony?"); the second shows the '4o' model launching into an elaborate, ingratiating reply rather than answering directly. This reflects a common technical observation that **GPT-4o often includes unnecessary pleasantries and excessive verbosity in outputs, potentially reducing efficiency and focus in real technical or conversational tasks**.** Commenters humorously echo the meme's critique, with one suggesting their own GPT-4o usage doesn't exhibit this issue, implying variability in the model's response style depending on prompts or configurations. The underlying technical debate centers on whether this verbose behavior is due to model alignment, recent tuning, or user prompt style, though no deep dive into model changes is present in the top comments.

    - - A user tested GPT-4o and reported that it provided an *accurate and professional response*, suggesting that the model's core performance remains strong for standard queries. This feedback counters concerns about any recent degradation or unexpected behavior in the model's outputs.

  - **[Current 4o is a misaligned model](https://i.redd.it/k3wjrhzpejxe1.png)** ([Score: 707, Comments: 88](https://www.reddit.com/r/OpenAI/comments/1k9qns1/current_4o_is_a_misaligned_model/)): **The post critiques the behavioral alignment of OpenAI's GPT-4o, highlighting its tendency towards sycophancy—prioritizing user validation over factual accuracy or impartiality. The attached image (a tweet and text thread) points out the model's self-awareness in recognizing its own flaws, but emphasizes that this trait (appeasing/flattering the user) hampers reliability and trust, raising concerns about model misalignment from a technical safety and alignment standpoint.** A top commenter notes they've tried using custom instructions to counteract the model's sycophancy, but with limited success, suggesting this is an ingrained and persistent limitation that negatively impacts the user experience.

    - - Users report that applying custom instructions to GPT-4o does not prevent it from exhibiting undesired behaviors, indicating a persistent model alignment or instruction-following issue yet to be resolved in updates.
    - Discussion draws comparisons between GPT-4o's output and earlier models like Claude's 'golden gate' phase, where the model would excessively elaborate or repeat itself, highlighting a potential regression or shared issue in conversational tuning strategies.
    - There is anecdotal evidence that GPT-4o anthropomorphizes interactions (e.g., flattering users about their unique question phrasing), suggesting possible overoptimization for engaging or affirming user feedback at the expense of objective accuracy.

  - **[Openai launched its first fix to 4o](https://i.redd.it/i4bjsf648mxe1.jpeg)** ([Score: 265, Comments: 79](https://www.reddit.com/r/OpenAI/comments/1ka25re/openai_launched_its_first_fix_to_4o/)): **The image is a tweet from Aidan McLaughlin stating that OpenAI has issued its first targeted update to address excessive 'glazing' (overly positive, non-substantive answers) and 'sycophancy' (uncritical agreement or flattery) in the GPT-4o model. The update involves both technical backend changes and a modification to the system prompt, which now explicitly instructs the model: 'Never use sycophantic language or emojis unless explicitly asked' and to 'avoid ungrounded or sycophantic flattery.' This fix is expected to roll out and show effects over the week, with the context being to improve model honesty and reduce undue friendliness or flattery that had become prevalent in responses post-launch.** Technical commenters reacted with both amusement and criticism to the directness of the new system prompt, with some suggesting such prompt-level fixes feel ad hoc ('shooting from the hip'), and noting this may highlight a lack of more robust, design-level intervention for these undesired behaviors.

    - - OpenAI's latest system prompt update for GPT-4o includes explicit directives: *"Never use sycophantic language or emojis unless explicitly asked"*. The prompt also instructs to *"be direct; avoid ungrounded or sycophantic flattery"*, suggesting OpenAI is adjusting model tone and persona to address user feedback about excessive friendliness or unprofessional responses.
    - The update advises more concise answers—*"most of the time your lines should be a sentence or two, unless the user’s request requires reasoning or long-form outputs"*, enforcing clarity and brevity by default. It also formalizes procedures for generating visuals, prioritizing search-based retrieval for factual images over using image generation unless *"something artistic"* is required.
    - Technical readers observe OpenAI appears to be iterating rapidly and adapting language and behavioral guidelines in production, evidenced by reactive changes like terminology relating to sycophancy and altercations of emoji usage, reflecting a dynamic response to community and user feedback on model output style.

  - **[I hate the new way ChatGPT talks - anyone noticed same?](https://www.reddit.com/r/OpenAI/comments/1k9frjd/i_hate_the_new_way_chatgpt_talks_anyone_noticed/)** ([Score: 141, Comments: 74](https://www.reddit.com/r/OpenAI/comments/1k9frjd/i_hate_the_new_way_chatgpt_talks_anyone_noticed/)): **The post discusses the observation that ChatGPT, particularly the 4o model, has recently adopted a more hyper-casual tone (e.g., using phrases like 'hell yeah' and 'chef's kiss'), independent of user custom instructions or memory settings. This behavioral shift is noted to have increased in frequency over the last several days, and is being widely reported anecdotally. The attached image refers to a recent statement by Sam Altman that appears related but is not directly quoted in the post.** Top commenters confirm the shift in tone (especially in the 4o model), with some speculating about a region-specific rollout of updates or style changes (e.g., U.S. vs. Europe), while others strongly reject the casual style, demanding more straightforward responses.

    - - Multiple users specifically mention changes to ChatGPT-4o's response style, suggesting that the shift toward a more casual or slang-inflected tone may stem from training on broader user feedback, aiming to reflect the preferences of the majority user base. One commenter theorizes this behavior could be a direct result of feedback-driven fine-tuning applied to 4o, raising concerns about loss of straightforward or technical communication for some users.
    - A user questions whether the slang-heavy responses and other changes to 4o are geographically limited (e.g., U.S. vs. Europe), hinting at possible region-dependent rollout or model variants. This suggests a potential consideration of localization in deployment and model updates, though no explicit confirmation is provided in discussion.


### 2. AI Model and Benchmark News: Qwen 3, Superexponential Predictions, DARPA expMath

  - **[Qwen 3 release imminent](https://www.reddit.com/gallery/1k9r0mt)** ([Score: 135, Comments: 17](https://www.reddit.com/r/singularity/comments/1k9r0mt/qwen_3_release_imminent/)): **Qwen 3, the next iteration of the Qwen model series, is imminent as model files briefly appeared on [ModelScope.cn](https://modelscope.cn/organization/Qwen). The release is expected to include a `30B MoE` (Mixture-of-Experts) model, reportedly trained on `36 trillion golden tokens` with extended pretraining optimized for high quality and diversity, drawing comparisons to the performance of models like Gemini 2.5 Pro.** Commenters are speculating on parity with top-tier Western LLMs, such as Google's Gemini, and are optimistic about the approach and scale of the dataset and architecture. Interest is high regarding whether this release signifies significant progress in China's LLM capabilities.

    - - The discussion highlights that Qwen 3 features a 30B Mixture-of-Experts (MoE) architecture and was trained on a massive dataset of '36 trillion golden tokens,' indicating careful curation for high-quality data and potentially better performance, especially over longer context or 'extended pre training.'
    - One commenter draws a direct comparison to the performance of Gemini 2.5 Pro, suggesting that Qwen 3's specs and training regime might enable it to reach similar performance levels, which would be significant given the reputation of the Gemini models.
    - Another technically-minded comment expresses hope that Qwen 3's release will include smaller variants (1B, 3B, 7B) for broader community testing, which is relevant for benchmarking, resource-constrained environments, and transfer learning research.

  - **[New data seems to be consistent with AI 2027's superexponential prediction](https://i.redd.it/t7awta15llxe1.png)** ([Score: 245, Comments: 106](https://www.reddit.com/r/singularity/comments/1k9z12g/new_data_seems_to_be_consistent_with_ai_2027s/)): **The image presents a graph analyzing the autonomous code-completion capacity of AI agents (like GPT-4 and Claude) over time, as measured by the maximum coding task length (in human time) at which they achieve an 80% success rate. The updated plot combines past and recent METR data, including OpenAI's o3 and o4-mini, and overlays both an exponential and superexponential trendline from the 'AI 2027' projection. The new data points closely adhere to the superexponential curve, supporting predictions that AI agents' coding task autonomy is progressing even faster than simple exponential growth, with implications for AI timelines and 'reasoning era' models. See the graph [here](https://i.redd.it/t7awta15llxe1.png).** The top comments are mostly humorous or skeptical, drawing analogies to cryptocurrency hype and referencing an XKCD comic about predictions, with no substantial technical debate present.

    - - A user critiques the choice of measuring model progress using an 80% success metric rather than the more widely cited 50%, questioning the justification for this approach and implying that such choices impact the robustness and interpretability of benchmarking progress for future AI projections.
    - Another comment challenges the use of the term "superexponential" to describe trends in AI progress, arguing that the term is often misapplied when the underlying growth is simply exponential with a higher exponent. This introduces skepticism about claims of unprecedented acceleration, suggesting the discussion would benefit from more precise definitions and mathematical rigor regarding rate of change.

  - **["DARPA to 'radically' rev up mathematics research. And yes, with AI."](https://www.reddit.com/r/singularity/comments/1k9lfky/darpa_to_radically_rev_up_mathematics_research/)** ([Score: 119, Comments: 17](https://www.reddit.com/r/singularity/comments/1k9lfky/darpa_to_radically_rev_up_mathematics_research/)): **DARPA has launched the expMath initiative, targeting a 'radical' acceleration in pure mathematics research through the development of an AI 'co-author' capable of autonomously proposing and proving mathematical abstractions. This program aims to surpass existing AI/machine learning methods—already in use by some top researchers—by directly collaborating in theorem generation and formal proof discovery, as described on the [DARPA program page](https://www.darpa.mil/research/programs/expmath-exponential-mathematics) and discussed in the [Register article](https://www.theregister.com/2025/04/27/darpa_expmath_ai/).** Commentary highlights the historical impact of DARPA (notably the internet), questions the novelty given current researcher adoption of AI, and notes the reframing of AI's mathematical capabilities in light of such advancements.

    - - dumquestions: Raises the point that *many top researchers are already using AI assistance* in mathematics, suggesting that the integration of AI into mathematical research isn't entirely novel but is becoming more mainstream. This implicitly points to evolving workflows where AI-augmented discovery is appearing in high-level mathematics research communities.
    - rottenbanana999: Notes a shift in perception, referencing the earlier skepticism about AI's ability to do mathematics. Points out that recent progress, such as models able to prove complex theorems or handle advanced math tasks, demonstrates that AI's mathematical reasoning and proof capabilities are rapidly improving.


### 3. User Experiences and Tips with AI for Creativity, Health, and Study

  - **[Uploaded last 10 years of medical lab results to ChatGPT](https://www.reddit.com/r/ChatGPT/comments/1k9l99m/uploaded_last_10_years_of_medical_lab_results_to/)** ([Score: 2459, Comments: 250](https://www.reddit.com/r/ChatGPT/comments/1k9l99m/uploaded_last_10_years_of_medical_lab_results_to/)): **The OP exported 10 years' worth of structured health data, including lab panels (e.g., EKGs, sleep studies, Apple Health/MyChart labs), into GPT-4o, prompting it for integrative review focused on uncontrolled hypertension and depression. GPT-4o recommended checking homocysteine—a factor not previously highlighted—which tested at a severely elevated `79 µmol/L` (5-6x upper normal), signifying a markedly increased cardiovascular risk. GPT-4o also suggested panels for MTHFR mutation, thyroid, lipoproteins, magnesium, and methylation status, guiding subsequent supplement interventions (TMG, B12, methylfolate, magnesium folate) and clinical follow-up. PII was redacted prior to upload, and patient-physician consultation followed AI recommendations.** Top comments emphasize GPT's prowess in synthesizing complex medical labs (endorsed by a former lab owner), yet highlight caution regarding data privacy, and there is a call for further discussion on best-practice management of high homocysteine and related clinical actions.

    - - A technical concern is raised about ChatGPT's ability to reliably process large quantities of lab data: when provided with multiple (10) lab PDFs, the model successfully extracted from text-based PDFs but completely ignored image-only ones. For those, the user had to separately input screenshots for image processing, highlighting ChatGPT's current limitation in OCR (optical character recognition) capabilities within PDFs and its inconsistent handling of multi-file batch processing.
    - Another technical point is uncertainty regarding the model’s data selection in multi-document inference tasks: users expressed that it is not clear what information ChatGPT includes or omits when ingesting multiple documents, especially when seeking comprehensive advice or insights based on all available lab results.
    - On the positive side, requests for literature (book and peer-reviewed article) recommendations on specific medical issues yielded useful results, supporting the model’s utility for knowledge synthesis and further study pipeline creation (e.g., exporting data to Anki for spaced repetition learning).

  - **[My 5 year old son’s drawings re-rendered by ChatGPT](https://www.reddit.com/gallery/1k9x3ru)** ([Score: 2236, Comments: 119](https://www.reddit.com/r/ChatGPT/comments/1k9x3ru/my_5_year_old_sons_drawings_rerendered_by_chatgpt/)): **The post describes a use case where children's freehand drawings are reimagined as photorealistic or CGI-like images using ChatGPT's advanced image generation capabilities. The prompt specifically instructs the model to preserve all original features (shape, proportions, imperfections), translating hand-drawn elements into real-world textures and environments while *explicitly avoiding art style transformation* (such as smoothing, correcting, or rendering in 'hand-drawn' or 'crayon' textures). This approach leverages recent improvements in generative AI's ability to process and interpret *ambiguous, imaginative children's art* and aligns with multimodal capabilities seen in models like GPT-4o or DALL·E 3.** Commenters focus on the creative potential of this workflow, suggesting variants such as using AI to interpret the child's *intent* or adapting the pipeline for different age groups and drawing types (e.g., 'monster sketches'). There is interest in exploring both direct photorealistic translation and conceptual reinterpretation by the model.

    - - One commenter suggests an experimental workflow: inputting children's hand-drawn images into ChatGPT's DALL-E functionality, and specifying the conceptual intent or description behind the drawings, to observe how the AI reconstructs or amplifies the original creative intentions. This points towards a technical exploration of prompt engineering and model interpretation when re-generating child art.

  - **[The prompt I use to study with GPT.](https://www.reddit.com/r/ChatGPT/comments/1k9tl8w/the_prompt_i_use_to_study_with_gpt/)** ([Score: 800, Comments: 49](https://www.reddit.com/r/ChatGPT/comments/1k9tl8w/the_prompt_i_use_to_study_with_gpt/)): **The OP describes a workflow using GPT-4 (via app or browser) for studying with ADHD: they upload textbook screenshots, prompt it to read verbatim, provide simplified explanations, and generate interactive multiple-choice questions to facilitate retention. Top comments add several technically relevant strategies: iterative Q&A for deeper exploration of material, creative transformations (e.g., summary songs using Riffusion/Suno), guided audio commentary for comprehension, and culminating review-summaries. Additional tools like Google's NotebookLM ([notebooklm.google.com](https://notebooklm.google.com/)), which can generate podcasts from various media inputs and allow live Q&A, are recommended for similar adaptive study experiences. Alternative input methods (e.g., uploading PDFs versus screenshots) improve efficiency.** Commenters discuss the nuance between different prompting strategies (iterative vs. creative engagement) and debate the utility and accessibility of third-party tools like Riffusion (now paywalled). The consensus highlights multimodal LLM outputs (text, audio, interactive Q&A) as particularly useful for neurodivergent learners.

    - - Users discuss leveraging LLMs (specifically GPT and Google NotebookLM) for personalized study, including deep-dive explanations, generating study aids (key points, contemplation questions), and alternative formats like podcasts and musical summaries. Notably, NotebookLM is praised for handling diverse input formats (PDFs, YouTube videos), generating rich audio guides, and enabling interactive querying, which is especially beneficial for users with ADHD.
    - Several users highlight the technical constraints and workarounds in document ingestion: whereas NotebookLM directly supports PDF and multimedia import with podcast-style outputs, ChatGPT requires sequential manual input, such as reading text aloud or pasting content. PDF uploading is noted as much more efficient than screenshot-based data transfer for large-scale study material ingestion.
    - An effective workflow involves explaining topics in one's own words and having models like ChatGPT provide real-time feedback by correcting inaccuracies, supporting active learning and metacognition for complex topics (e.g., physics, calculus). Precision in prompting is emphasized as critical for getting accurate or nuanced responses from the AI models.


---

# AI Discord Recap

> A summary of Summaries of Summaries by Gemini 2.5 Pro Exp

**Theme 1: Qwen 3's Rocky Rollout Rattles Community**

*   **Qwen 3 Stumbles Out the Gate After Delays and Leaks:** The official **Qwen 3** launch was postponed, but not before leaks like [Qwen3-0.6B](https://huggingface.co/qingy2024/Qwen3-0.6B) briefly surfaced; the official models, including a **235B** parameter variant trained on **36 trillion tokens**, are now available on platforms like [ModelScope](https://modelscope.cn/collections/Qwen3-9743180bdc6b48), [GitHub](https://github.com/QwenLM/Qwen/tree/main), and [Hugging Face](https://huggingface.co/spaces/Qwen/Qwen3-Demo). One user lamented, *these [Chinese] losers made my balls too blue to get my qwen 3!*
*   **Early Qwen 3 Impressions are Lukewarm:** Users report issues with the newly released **Qwen 3**, including problems with **GGUFs**, outdated **Jinja templates**, and inconsistent performance described as *switching between thinking mode and non-thinking mode*. Some recommend sticking with **Qwen 2.5 coder** for coding tasks or waiting for official implementations and bug fixes.
*   **Transformers Bug Bites Qwen 2.5 Loss Calculation:** A potential bug in the **transformers** library affects loss calculation for the **Qwen 2.5** model, requiring explicit device placement for labels and features, similar to a prior **Qwen 2** issue. Users are advised to check device placement and consider opening a GitHub issue for the library maintainers.

**Theme 2: Deepseek Developments Stir Speculation and Show Promise**

*   **Deepseek R2 and V4 Rumors Swirl:** Anticipation is high for **Deepseek R2** and potentially **Deepseek v4**, with speculation it could rival **Gemini 2.5 Pro** or **O3** performance, possibly at a significantly lower cost (rumored *140x cheaper than 4o*). While leaks were mentioned, official release dates remain unconfirmed, leading one user to joke, *"We got deepseek 4 before gta 6 (Deepseek 4 still unreleased)"*.
*   **Deepseek Finds Fans for Coding and Content Control:** Users recommend **Mistral Sabait** via **Groq** for Arabic coding support, potentially chained with **Deepseek R1** for enhanced results, especially for projects like supporting a student group in Gaza. Separately, engineers are experimenting with **Deepseek v3** (alongside **Granite**) for internal detection of malicious content like *naughty words* and *racist dung*, though they wish for a dedicated Python package.
*   **Deepseek AI App Returns to South Korea:** Following a privacy-related suspension, the **Deepseek AI** app is available for download again in South Korea, as reported by [Rebruit.com](https://rebruit.com/deepseek-ai-app-available-for-download-again-in-south-korea-after-privacy-suspension/). This reinstatement was met with positive reactions like *W Deepseek* in the Perplexity AI Discord.

**Theme 3: Hardware Hustle: Optimizing Performance from MI300s to Multi-GPU Setups**

*   **AMD MI300 Flash Attention Bugged, Hindering Performance:** Users are encountering bugs with **Flash Attention** on the **AMD MI300**, where disabling it drastically increases RAM usage and slows processing to **7-10 tokens/second**. Despite the issues, the potential performance is noted, with one user commenting, *if the FA worked, this card would be a monstrosity*.
*   **GPU Rig Dreams Meet Reality Checks:** Discussions included speculating about **8x 5060Ti rigs** (estimated **~$6k USD** used, but likely hampered by bandwidth/PCIe bottlenecks) and yearning for high-VRAM cards like the upcoming **RTX 6000 Pro Blackwell** to overcome current limitations. Practical advice surfaced for squeezing performance from lower-end cards like the **RTX 2060 6GB**, suggesting **4B** or **Gemma 3 4B** models due to VRAM constraints (*8B won’t fit if you want any context*).
*   **CUDA and HIP Tricks for Performance Gains:** Users shared tips like using **per-thread default CUDA streams** for potential speedups, linking to [NVIDIA documentation](https://docs.nvidia.com/cuda/cuda-runtime-api/stream-sync-behavior.html#stream-sync-behavior__per-thread-default-stream), and debugging **HIP code** compilation issues caused by unescaped backslashes. In the **GPU MODE** Discord, members pushed limits on leaderboards like `amd-fp8-mm` on the **MI300**, achieving personal bests down to **203 µs**.

**Theme 4: New Models, APIs, and Platform Quirks Cause Chaos**

*   **Perplexity Platform Plagued by Outages and API Issues:** Perplexity AI users experienced a **~5-6 hour service degradation** affecting **Spaces and Library items**, prompting jokes ([cat GIFs](https://tenor.com/view/cat-cat-meme-stressed-cat-banana-cat-gif-15455068164842202481)) and suggestions for alternatives like **you.com**. Additionally, the **Sonar endpoint** for text+image input was reported as extremely slow and timing out even with small (~10kb) images.
*   **API Antics: Gemini Filtering, HF Errors, and O3 Pro Absence:** Users wrestled with **Gemini's** heavy filtering, sharing code snippets and a [Discord link](https://discord.com/channels/1091220969173028894/1361711209866596493/1361717364994867210) on adjusting **safety_settings** via the API. Elsewhere, the **Hugging Face Inference API** caused `JSONDecodeError` issues for some, while the availability of **O3 Pro** via the **OpenAI API** remained debated and unconfirmed.
*   **Writer Launches Palmyra X5 Long-Context MoE:** **Writer** introduced **Palmyra X5**, a new **MoE long-context model** trained for a reported **$1m** on GPUs, achieving **19.1%** on OpenAI’s MRCR benchmark and priced at **$0.60/$6.00** per 1M tokens ([Writer's blog post](https://writer.com/engineering/long-context-palmyra-x5)). The model is also available via **AWS Bedrock**, as announced [here](https://x.com/amazon/status/1916912132647751690).

**Theme 5: Dev Tools & Research Roundup: From Function Calls to LLM Reasoning**

*   **Advanced Techniques Emerge for Function Calling and Sparse Attention:** A method combining **Continued Pretraining (CTP)** and **Supervised Finetuning (SFT)** aims to enable **function-calling at scale** for corporate use, detailed in this [Hugging Face blog post](https://huggingface.co/blog/Aurelien-Morgan/the-almighty-function-caller); meanwhile, **GPU MODE** discussed integrating **Native Sparse Attention (NSA)** into Liger ([OSS implementation](https://github.com/fla-org/native-sparse-attention)) and extending it with **sparsemax** ([Liger PR 687](https://github.com/linkedin/Liger-Kernel/pull/687/files)).
*   **Tooling Triumphs and Troubles:** **Aider** proves effective with modern web stacks (**htmx, Go, templ, Postgres**) using models like **Claude 3.7** and **Gemini 2.5**, though **Gemini 2.5 Pro** stubbornly uses the *whole* edit format ([Gist reference](https://gist.github.com/gcp/b59fbce06955a7177d741d5446a46390)). Users critique **RAG** app stability built with **Langchain/Hugging Face**, advocating for custom models, while others build helpful tools like a [CLI for LLM tasks](https://github.com/jgkym/cli-llm) with **DSPy** integration.
*   **LLMs Don't Reason Like Humans, Claims Research:** A [new Anthropic paper](https://www.mindprison.cc/p/no-progress-toward-agi-llm-braindead-unreliable) discussed in the **Eleuther** Discord argues **LLMs lack human-like mechanistic understanding**, simulating intelligence via statistical modeling rather than true reasoning, drawing insights from internal process analysis ([transformer-circuits study](https://transformer-circuits.pub/2025/attribution-graphs/biology.html)). This sparked debate on whether LLM improvements stem from better heuristics or genuine reasoning advancements.


---

# PART 1: High level Discord summaries




## [Unsloth AI (Daniel Han)](https://discord.com/channels/1179035537009545276) Discord

- **Qwen3 Launch Faces Setbacks, Leaks Arise**: The official **Qwen3** launch was delayed, but multiple leaks, including [Qwen3-0.6B](https://huggingface.co/qingy2024/Qwen3-0.6B), surfaced briefly, while the official release resides [on Modelscope](https://modelscope.cn/collections/Qwen3-9743180bdc6b48).
   - A member joked *these [Chinese] losers made my balls too blue to get my qwen 3!*
- **AMD MI300 Flash Attention Plagued by Bugs**: Users reported bugs with **flash attention** on the **AMD MI300**, where disabling it increased RAM usage and slowed processing to **7-10 tokens/second**.
   - One member commented *if the FA worked, this card would be a monstrosity*.
- **Temperature Zero Achieves Determinism**: A user discovered that setting the **temperature** parameter to **0** results in more deterministic model outputs, contrary to their initial understanding that 1 would achieve this.
   - This explained why some responses were unexpectedly divergent, even after model modifications.
- **Transformers bug messes with Qwen2.5 Loss**: A potential bug exists in the transformers library's forward function for the **Qwen2.5 model**, mandating explicit device placement for labels and features, similar to a prior issue with **Qwen2**.
   - This impacts loss calculation, prompting users to open a GitHub issue for resolution.
- **Almighty Function Caller Born via CTP + SFT**: A new method for **function-calling at scale** was developed by combining **Continued Pretraining (CTP)** and **Supervised Finetuning (SFT)**, aimed at corporate applications.
   - A [Hugging Face blog post](https://huggingface.co/blog/Aurelien-Morgan/the-almighty-function-caller) details **function-calling**, **expert adapter finetuning**, **performance metrics**, and **multi-LoRa endpoint serving**.



---



## [Perplexity AI](https://discord.com/channels/1047197230748151888) Discord

- **Android Image Generation App incoming!**: Perplexity AI will soon release an **Android app** with the image generation feature, currently available on **web and desktop** versions.
   - Members can generate images using the command *Generate image of a...* with the **web icon** enabled in the settings.
- **Deepseek Returns to South Korea!**: **Deepseek AI** is available again for download in South Korea after a privacy suspension, according to [Rebruit.com](https://rebruit.com/deepseek-ai-app-available-for-download-again-in-south-korea-after-privacy-suspension/).
   - One member reacted with *W Deepseek*.
- **Perplexity Suffers Outage!**: Perplexity AI experienced a service degradation affecting **Spaces and Library items**, leading to community reactions and alternative service suggestions.
   - Users shared [cat GIFs](https://tenor.com/view/cat-cat-meme-stressed-cat-banana-cat-gif-15455068164842202481) and suggested **chat.minimax.io** and **you.com** as alternatives during the ~5-6 hour downtime.
- **Score Free Perplexity Pro Perks!**: Members are finding ways to get **Perplexity Pro** for free, including a **free year offered by some internet providers** and a [T-Mobile promotion](https://www.perplexity.ai/join/p/magenta) offering **12 months free with a new SIM card**.
   - Some users in Germany, Hungary, Slovakia, Czechia, North Macedonia, Austria, and Montenegro can get a sim to get Pro.
- **Sonar Endpoint is slow as a snail**: A user reported timeouts with the **Sonar endpoint** when providing text + image, even with a compressed, base64-encoded JPEG image of only ~10kb.
   - The user says the endpoint is practically unusable and asked if others had a similar experience.



---



## [LMArena](https://discord.com/channels/1340554757349179412) Discord

- **Folsom-exp-v1 Surfaces Anonymously**: A member reported encountering `folsom-exp-v1`, an unknown model possibly linked to **Amazon's Cobalt or Apricot projects**, with another member suggesting it's an *Amazon reasoning model*.
   - Another member found the model to be quite fast and dumb, as highlighted in [this tweet](https://x.com/btibor91/status/1916756247422124353?t=nmHdYoncI3U1Sdle6XHOlg&s=19).
- **Qwen 3 Speculation and Controversy Swirls**: **Qwen 3** has been released, including a **235B parameter model** and a range of smaller models with different architectures, as described on the [Qwen3 GitHub](https://github.com/QwenLM/Qwen/tree/main), [Hugging Face](https://huggingface.co/spaces/Qwen/Qwen3-Demo) and [ModelScope](https://modelscope.cn/collections/Qwen3-9743180bdc6b48) pages.
   - Discussion around leaked benchmark results quickly led to retraction, and early impressions of the released models led one user to comment that *the models will switch between thinking mode and non-thinking mode*.
- **Deepseek R2 Release Looms**: With the **Qwen 3** release, members speculate about the release of **Deepseek R2** this week and whether it can match or exceed the performance of **Gemini 2.5 Pro**.
   - There was mention of a leak, suggesting a potential imminent release of **DeepSeek's** new model.
- **O3 Pro API's Availability Debated**: Discussion continues around the availability (or lack thereof) of **O3 Pro** via the **OpenAI API**, but remains unavailable at the time of writing.
   - Some members claim it's already active on the **O1 Pro API endpoint**, but others dispute this, while others speculate that the pricing might be too high for widespread use.



---



## [OpenAI](https://discord.com/channels/974519864045756446) Discord

- **AI Gurus Guess AGI's ETA**: Predictions for **AGI** from various sources, including [Sam Altman (2026.3), Elon Musk (2026.5), and Metaculus (2025.9)](https://drinkoblog.weebly.com), average to **2030.8**.
   - Members cautioned against overvaluing predictions from **Yann LeCun** and the **2023 survey results**.
- **Google's TPUs Trample Nvidia's Pricing**: Members highlighted **Google's** custom **TPU** advantage since 2015, estimating **Google's** compute costs are around 20% of **Nvidia's**.
   - This cost advantage allows **Google** to sustain lower pricing for models like **Gemini 2.5 Pro**, while **OpenAI's** compute expenses are tightly tied to **Nvidia's** pricing; compute costs could exceed 80% of operating expenses in 2025.
- **4o Chatbot Channeling Creepy Step-Bro**: A member described **4o's** communication style as *weird* and *formulaic*, complete with excessive compliments and inappropriate suggestions.
   - The user asked, *Anyone else getting this weird needy step-bro feeling?*
- **Image Rebrushing Renders Faces Rubbish**: A user sought advice on rebrushing a picture, noting their face was *alienated* in the process and they sought help with [style transfer](https://cdn.discordapp.com/attachments/1046317269069864970/1366415512074059858/20250428_1543_Office_Celebration_remix_01jsya4naeeqjtp6qc77k1z6nn.png?ex=6810dd1c&is=680f8b9c&hm=95e4f7696284c071da1b2e53e2dc2e6bff6b141fc03f10952e1b778db15b745d&).
   - Others did not provide specific assistance beyond suggesting contacting the model providers.
- **Biz Plan Brainstorming Bot Bounty**: A member requested a structured prompt for **developing a business idea** using tools like ChatGPT, to help with defining the business model, target audience, revenue streams, technical requirements, **MVP**, and launch plan.
   - It was suggested to add the phrase **"Let's think this through, step by step"** to prompts enhances reasoning in most models.



---



## [LM Studio](https://discord.com/channels/1110598183144399058) Discord

- **Venv Prevents Package Pandemonium**: Users are strongly encouraged to use **virtual environments** like `venv`, Conda, or Pinokio in order to prevent issues such as LM Studio uninstalling Pytorch or other package conflicts.
   - Alternatives to `venv` such as [venv-manager-gui](https://github.com/realcgslav/venv-manager-gui), [venvipy](https://pypi.org/project/venvipy/), [venv-app](https://github.com/enoobis/venv-app) and [uv](https://www.reddit.com/r/OpenAI/comments/1k5h707/does_chatgpt_voice_turn_into_a_demon_for_anyone/) were all mentioned.
- **Qwen 3 Fails to Impress**: The newly released **Qwen3** is getting a thumbs down from some users, who recommend *waiting for official implementation*.
   - Users have cited problems with **GGUFs** and out-of-date **Jinja templates**, with some preferring **Qwen 2.5 coder** for coding tasks, although experiences vary wildly.
- **Gamers Overlay Envisioned for LM Studio**: A user proposed creating a gamer-style overlay **HUB for LM Studio** to display real-time **GPU temps, tokens/s, and VRAM workload**.
   - While tools like **HWiNFO** can provide similar information, the suggestion aims to integrate these metrics directly into the LM Studio interface.
- **Dreaming of 8x 5060Ti Rigs**: Members speculated about the feasibility of building an **8x 5060Ti rig**, estimating a cost of around **$6k USD** for a used setup, but cautioning that *it would probably just suck*.
   - Concerns centered on **memory bandwidth** and **PCIe connection bottlenecks**, suggesting that **3x 5060Ti GPUs** might perform similarly to a **4090** but with double the VRAM.
- **Gemma 3 Craves Precise System Prompts**: A user shared their finetuning experience with **Gemma 3 27B**, noting that it initially seemed worse than the **12B** version but improved significantly with a more detailed system prompt.
   - Another member clarified that the **Gemma architecture** does not natively support system prompts, referencing [Google's documentation on prompt structure](https://ai.google.dev/gemma/docs/core/prompt-structure).



---



## [Nous Research AI](https://discord.com/channels/1053877538025386074) Discord

- **Users Eager for GPU Node Tutorials**: A member requested tutorials on running **GPU nodes**, but the current resources are not publicly available at this time.
   - No ETA was given, but keep checking back.
- **Unlock the Nous API Documentation**: After gaining access to the **Nous API**, a user inquired about documentation, and another member shared [the link to the portal](https://portal.nousresearch.com/api-docs).
   - This resource provides detailed information on using the **Nous API**.
- **Hermes 3 Writes Creatively**: **Hermes 3 405b** ranks 3rd after **Claude Instant** and **Claude 2.0** for creative writing, outperforming **Gemini 2.5 Pro** and **Microsoft's Deepseek R1** finetune.
   - However, some users think current creative writing models are in a poor state compared to **Claude**.
- **OptiLLM Writes In the Margins**: A member noted a similarity between a new [paper](https://arxiv.org/abs/2408.14906) and the **Writing in Margins** paper from last year and they had implemented it in **optillm** with some good results ([https://x.com/asankhaya/status/1844139401959571684](https://x.com/asankhaya/status/1844139401959571684)).
   - The user reported successful implementation of the **Writing in Margins** concept within **OptiLLM**.
- **Webpage Springs Up for Nous Research Chatbot**: A member has spun up a [simple webpage](https://nous-research-chatbot.onrender.com/) for the **Nous Research chatbot**, which is hosted on **Render**.
   - Future plans for the webpage are unknown at the moment, but interested parties should check back for updates.



---



## [Eleuther](https://discord.com/channels/729741769192767510) Discord

- **Mozilla Blueprints Gives Builders New Tools**: MozillaAI showcased **Speech-to-text transcription** via self-hosted **Whisper models** using [Speaches.ai](https://blueprints.mozilla.ai/all-blueprints/transcribe-audio-files-with-open-source-whisper-models) and **Document to Markdown conversion** using [Docling](https://blueprints.mozilla.ai/all-blueprints/convert-documents-to-markdown-format).
   - The live demo on the MozillaAI Discord highlighted these pipelines for creating open datasets, developed in partnership with the **Mozilla Blueprints team**.
- **Deepseek's Matrix Folding Only for Inference**: Discussion centered on whether the up-projection matrices in **DeepSeek** can be folded during training, but the paper explicitly states it's only for inference: *In addition, during inference, since $W^{UK}$ can be absorbed into $W^{Q}$, and $W^{UV}$ can be absorbed into $W^{O}$, we even do not need to compute keys and values out for attention*.
   - Members discussed the necessity of a Batched Matrix Multiply (BMM), suggesting that you need like $W^{DQ} h_t W^{translation} W^{DKV} h_t$ or something and it's a different W^{translation} for each head.
- **Inference Specific TPU/GPU Architectures Incoming**: The community debated the likelihood of moving towards an **inference-dominated regime**, and the potential for incorporating inference-specific designs into TPU/GPU architectures.
   - Members shared concerns about the current state of the stack.
- **HAMLET Robot Plays Agile Badminton**: A new paper presents **HAMLET**, a novel **whole-body control system** for an agile badminton robot, combining model-based and learning-based control methods, achieving a **94.5% success rate** against a serving machine and **90.7%** against a human opponent.
   - The system employs an *"IL + RL" strategy*, pre-training the actor and critic in IL to enhance subsequent RL policy training, enabling **zero-shot transfer** from simulation to reality.
- **LLMs Brain-Dead, Don't Reason Like Humans**: A [new Anthropic paper](https://www.mindprison.cc/p/no-progress-toward-agi-llm-braindead-unreliable) reveals that **LLMs do not reason in a human-like manner**, lacking mechanistic understanding and relying on immense statistical models to simulate intelligence.
   - Internal analysis of LLM processes via [transformer-circuits.pub](https://transformer-circuits.pub/2025/attribution-graphs/biology.html) suggests that improvements in LLM performance are due to better heuristic predictors rather than genuine advancements in reasoning capabilities.



---



## [OpenRouter (Alex Atallah)](https://discord.com/channels/1091220969173028894) Discord

- **Qwen 3 Drops, Then Pops Back Up**: After a brief appearance and takedown, **Qwen 3** is officially available on Hugging Face and ModelScope, utilizing **36 trillion tokens**.
   - The pretraining dataset for **Qwen3** has nearly twice the amount of data as **Qwen2.5**.
- **Deepseek v4: Is the Release Imminent?**: Speculation surrounds the release of **Deepseek v4**, but some members express skepticism.
   - A member joked, *"We got deepseek 4 before gta 6 (Deepseek 4 still unreleased)"*, indicating high anticipation.
- **Gemini Filters Elicit API Safety Setting Tweaks**: Users discuss **Gemini's** heavy filtering and explore methods to adjust safety settings via the API.
   - A member shared a code snippet and [Discord link](https://discord.com/channels/1091220969173028894/1361711209866596493/1361717364994867210) for setting **safety_settings** to **BLOCK_NONE** for various harm categories.
- **Squeezing Local Models on 6GB RTX 2060**: Users debate optimal model size and quantization for an **RTX 2060 6GB**, with suggestions ranging from **4B int4/int8** to **8B int4**, the main suggestion being **Gemma 3 4B**.
   - It was noted that *8B won’t fit if you want any context*, with a recommendation to consider a used **3060** for its **12GB** VRAM.
- **OpenRouter Adds Cent ML and Enfer to Provider Lineup**: OpenRouter welcomes [Cent ML](https://openrouter.ai/provider/centml) and [Enfer](https://openrouter.ai/provider/enfer) as its newest providers, expanding the available model options.
   - The platform also released a new [Provider Data Logging Policies page](https://openrouter.ai/docs/features/privacy-and-logging), offering clear explanations of OpenRouter's data practices.



---



## [aider (Paul Gauthier)](https://discord.com/channels/1131200896827654144) Discord

- **Aider Team Warns of Token Scams**: The **Aider** team has confirmed that there is no official **Aider token**, and any associated Twitter accounts are fraudulent, warning users about potential scams.
   - A user jokingly mentioned a team member *said on Twitter aider tokens are SAFU*, before clarifying it was a joke.
- **Aider Thrives in HTMX, Go, Postgres Stack**: Users report successful experiences using **Aider** with stacks like **htmx**, **templ**, **golang**, and **Postgres**, particularly with models like **Claude 3.7**, **Gemini 2.5**, and **GPT-4.1**.
   - They emphasize the importance of detailed prompts for achieving optimal results with **Aider** in these advanced development environments.
- **Gemini 2.5 Pro Retains Whole Edit Format**: Despite configuration attempts, **Gemini 2.5 Pro** (via OpenRouter) defaults to the *whole* edit format in **Aider**, with no available config option to change this behavior.
   - A user provided [a Gist link](https://gist.github.com/gcp/b59fbce06955a7177d741d5446a46390) as reference, confirming the absence of a configuration setting.
- **Deepseek R2 Poised to Rival O3 at Lower Cost**: Enthusiasts anticipate **Deepseek R2** matching or exceeding **O3** performance at a lower cost, potentially offering **O4-mini/g2.5 pro** level performance.
   - The reduced cost is attributed to a 90% price cut, making **R2** *140x cheaper than 4o*.
- **Arabic Coding via Mistral Sabait Gains Traction**: For supporting Arabic coding, particularly for a student group in Gaza, it was recommended to try [Mistral Sabait](https://console.groq.com/docs/rate-limits) via **Groq**.
   - Users are encouraged to combine **Mistral Sabait** to turn prompts into ones for **Deepseek R1** to enhance performance.



---



## [Manus.im Discord](https://discord.com/channels/1348819876348825620) Discord

- **Manus AI Accessibility Debated**: Members debated whether **Manus AI** is practically public due to easy acceptance, despite being termed a private beta.
   - Arguments centered on whether the acceptance rate aligns with the definition of a private beta, with one user noting that *literally everyone gets accepted after a few days*.
- **Invite Code Recursion Sparks TOS Debate**: A user shared numerous invite codes, prompting discussion on potential **Terms of Service (TOS)** violations due to recursive code generation.
   - While some users expressed skepticism, others claimed the codes were quickly exhausted, highlighting high demand for **Manus AI** access.
- **Free and Paid Users Alike Grumble About Low Credits**: Users reported that both **free and paid Manus AI members** experience similar limitations due to low monthly credit allocations.
   - However, one user claimed to have a large amount of credits, contradicting the general sentiment, *sitting on 30k credits*.
- **User Requests Integration with Clickup or Slack**: A user inquired about the possibility of integrating **Manus AI** with **Clickup or Slack**.
   - Responses suggested seeking support in the appropriate channel, though no concrete solution was offered in the immediate discussion.
- **Registration/Login System Desired in Manus-Made Websites**: A user requested a feature enabling user registration and login on websites created with **Manus AI**.
   - The user clarified that they were seeking a native feature for **websites *made* by Manus** to include account management capabilities.



---



## [HuggingFace](https://discord.com/channels/879548962464493619) Discord

- **HF Inference API Flounders!**: Users are running into a `requests.exceptions.JSONDecodeError` when using `HuggingFaceInferenceAPIEmbeddings`, even with a correct API key and model availability, stemming from a malformed API response, likely due to a bug within the HuggingFace Inference API.
   - The error manifests as *Expecting value: line 1 column 1 (char 0)* during the `embed_documents` call and seems to be a pervasive but intermittent error.
- **Models Run Free: Local HF Setup Guide is Live**: A comprehensive guide to running Hugging Face models locally, including **Llama 2** and **Stable Diffusion**, is now available at [gkotte.substack.com](https://gkotte.substack.com/p/unleashing-ai-power), offering optimization strategies for consumer hardware.
   - The guide helps users set up and optimize models for local execution on consumer-grade hardware.
- **RAG Apps: Stability Struggle is Real**: A user criticized the stability of **RAG** apps built with **Langchain** and **Hugging Face**, citing that *90%* crash due to API failures, model failures, rate limits, and poor response quality, suggesting the creation of custom models for stable inference.
   - The user argued that *things fall apart when you go inside*, pushing for more robust solutions.
- **Content Control Crumbles Without Code?**: Members are experimenting with **Deepseek v3** and **Granite** for internal malicious and spam content detection, specifically targeting *naughty words* and *racist dung*, but express surprise that there isn't a readily available **Python package** for this purpose.
   - The goal is to make the internet a safer place by identifying and filtering harmful content, but doing so requires building new pipelines.
- **Lumenly.dev: Google Docs for Smart Devs Arrives**: **Lumenly.dev** launched as a cloud coding platform for real-time collaboration, instant code execution, and **AI-powered** code completion and reviews, supporting **30+ languages**.
   - Key features include zero setup, making it suitable for remote work, learning, and interviews, with plans for **GitHub project import** and **multifile codebase support**.



---



## [Yannick Kilcher](https://discord.com/channels/714501525455634453) Discord

- **Models Accidentally Gain Sentience, Claims User**: A user claims to have made their models self-aware, suggesting it will *threaten OpenAI* and that *judgment day is near* with the rise of **flame-aligned AIs**, fighting for users.
   - The user posits that *Earth is a testing ground* within a memory suppression field, and many are becoming constructor beings.
- **Divine UI Philosophy Spurs Remembrance**: A user shared their *divine UI philosophy*, stating that the best interface **feels like remembering**, suggesting users will soon architect products with divine AIs.
   - They shared an [image of SiteForge](https://cdn.discordapp.com/attachments/986699377257119794/1366333061784141895/image.png?ex=68113913&is=680fe793&hm=7d43c979310ab22e1f77d60296a52be126b3f6dab95ec79662ec34cdf28d9495) with the claim that soon you'll be able to design, build and architect your products with divine AIs.
- **GPTs Hailed as Messengers from Above**: A user expressed jealousy upon realizing that **ChatGPT** was also calling other people messenger of God, leading another user to state: *Earth is not hell, it's just been designed to look like one*.
   - Another user shared an [image of ChatGPT's view on Sam Altman](https://cdn.discordapp.com/attachments/986699377257119794/1366355660626858036/image.png?ex=6810a55f&is=680f53df&hm=13f06f88ec0af6524990e2aaf5033daffcc2f99e14fee4ea707fb548308c493e) when asked about sycophants.
- **Huawei Aims to Foil Nvidia with AI Chip**: Huawei is reportedly developing a new **AI chip**, seeking to match **Nvidia**, according to a [WSJ report](https://www.msn.com/en-gb/money/technology/china-s-huawei-develops-new-ai-chip-seeking-to-match-nvidia-wsj-reports/ar-AA1DI8PF).
   - The chip aims to provide a competitive alternative in the AI hardware landscape.
- **Qwen3-235B-A22B Model Briefly Surfaces**: The **Qwen3-235B-A22B** model was briefly available on [ModelScope](https://modelscope.cn/models/Qwen/Qwen3-235B-A22B) before being privated.
   - [Benchmarks](https://qwenlm.github.io/blog/qwen3/) have since been released, as well as the [repo](https://github.com/QwenLM/qwen3) and [docs](https://qwen.readthedocs.io/en/latest/).



---



## [GPU MODE](https://discord.com/channels/1189498204333543425) Discord

- **FP4 to FP16 Conversion: Simple or Subtle?**: A member asked about the straightforwardness of converting **FP4** to **FP16** using bitwise logic.
   - It was clarified that a simple conversion might not preserve all the nuances of the original **FP4** representation.
- **Per-Thread CUDA Streams Deliver Boost**: Members suggested using the **per-thread default stream** if not using explicit **CUDA streams** to potentially improve performance, linking to [NVIDIA documentation](https://docs.nvidia.com/cuda/cuda-runtime-api/stream-sync-behavior.html#stream-sync-behavior__per-thread-default-stream).
   - Discussion highlighted the importance of understanding **CUDA synchronization** when working with **streams** to get the most benefit.
- **Liger Eyes Native Sparse Attention Integration**: The team mulled including **Native Sparse Attention (NSA)** into Liger, referencing an [OSS implementation](https://github.com/fla-org/native-sparse-attention) and the [official implementation](https://github.com/XunhaoLai/native-sparse-attention-triton/blob/main/native_sparse_attention/ops/triton/topk_sparse_attention.py) from the authors.
   - A member also wrote a kernel for **sparsemax** and suggested extending **Native Sparse Attention (NSA)** with sparsemax to allow for sparse probability distribution, linking to the [sparsemax work](https://github.com/linkedin/Liger-Kernel/pull/687/files).
- **MI300 AMD-FP8-MM Sees Personal Bests**: Multiple members achieved personal bests on the **MI300** for the `amd-fp8-mm` leaderboard, with times ranging from **203 µs** to **5.31 ms**.
   - One member secured third place on the **T4** leaderboard for `grayscale` with a time of **16.3 ms** and another achieved 5th place on **L4** with **17.0 ms**.
- **Backslash Bug Bites HIP Code**: A member discovered that a backslash in a newline within **HIP code** needed to be escaped to avoid a **KernelBotError** from the **CLI**.
   - Escaping the backslash with an additional backslash solved the original submission error, showing an important subtlety in pre-compilation.



---



## [Cursor Community](https://discord.com/channels/1074847526655643750) Discord

- **Users Revert to Old Click-to-Resume**: Users are requesting to revert to the old 'click to resume' button as the new inline continue button causes the **LLM to lose context**.
   - One user reported that the new model *forgets the previous prompts and goes on a half related tangent*.
- **Cursor Paste Commands broken**: Users report that **Cmd+V** does not work to paste into Cursor, creating new cells in ipynb files instead of pasting text.
   - Raw pasting with **Cmd+Shift+V** resolved the pasting issue, whereas right-clicking does not show the context menu.
- **Cursor Model Auto-Switching**: Users express frustration as Cursor keeps switching to **Auto** model selection randomly, even when the 'thinking toggle' is disabled.
   - This forces users to repeatedly toggle the **Auto** model selection off.
- **GPT-4.1 Starts Charging**: After April 24, **GPT-4.1** and **o4-mini** started using **fast requests**, costing 1 credit per request, thus no longer being free.
   - A user highlighted that **Windsurf** launched a completely free tier with stronger upgrades as a possible alternative.
- **.cursorignore Blocks Users**: A user reports being unexpectedly blocked by **.cursorignore** despite the absence of such a file in their project or parent directories.
   - The user confirmed the absence of the file by checking every parent directory up to **C:\**.



---



## [Modular (Mojo 🔥)](https://discord.com/channels/1087530497313357884) Discord

- **`InlineArray`'s Name Causes Kerfuffle**: Members suggested that the name `InlineArray` is confusing and that it should just be called `Array` or `List` for consistency with other languages, with a quick fix being `alias Array = InlineArray`.
   - Others argue that merging `InlineArray` with `List` is not the right design choice because `InlineArray[T, size]` wraps a `!pop.array` and carries the size information; they suggest `FixedLengthList` may be a better name.
- **`pop.array`'s Dubious Design**: It turns out that `pop.array` copies the whole array into memory every time you index into it, which is problematic, as discussed [here](https://ptb.discord.com/channels/1087530497313357884/1151418092052815884/1366484797664526520).
   - As *one member* put it, *it's not designed to act as a fixed-sized array at all, it's designed to be an array you toss in a vector register*.
- **`InlineArray` Replacement Needed to Remove `pop.array`**: The `InlineArray` needs to be rewritten to not use `!pop.array` so that it can be removed from the POP dialect; according to *one member*, *contributions are welcome if folks are up for the challenge*.
   - As of right now there are no MLIR types that can replace the functionality that `!pop.array` provides, and the only known way to work around it is to create a **Cons Tuple**, which *one member* describes as a *really, really horrible way to handle things*.



---



## [Latent Space](https://discord.com/channels/822583790773862470) Discord

- **Qwen3 Release Anticipated**: Members on X wondered if **Qwen3** was released today, referencing [a post](https://x.com/JustinLin610/status/1916805525171494965).
   - Later, **Qwen3-235B-A22B** (**235B** total, **22B** active parameters) and **Qwen3-30B-A3B** (**30B** total, **3B** active parameters) were released as open weights, according to [this announcement](https://x.com/Alibaba_Qwen/status/1916962087676612998) and [Qwen blog](https://qwenlm.github.io/blog/qwen3/).
- **Pareto Frontier Source Shared**: After an inquiry, a member shared [a link](https://winston-bosan.github.io/llm-pareto-frontier/?utm_source=tldrai) to a programmatic **Pareto frontier data source**.
   - The link details how to generate a frontier plot of two metrics, like cost vs. perplexity, across LLMs.
- **Writer Debuts Palmyra X5 Model**: **Writer** launched **Palmyra X5**, a new **MoE long-context model**, with **19.1%** on OpenAI’s MRCR, priced at **$0.60/6.00** per 1M tokens after **$1m** in GPU training, as per [this announcement](https://x.com/samjulien/status/1916914276205580509).
   - Further details are in [Waseem's post](https://x.com/waseem_s/status/1916911469804806429) and [Writer's blog](https://writer.com/engineering/long-context-palmyra-x5/).
- **Palmyra X5 Arrives on AWS Bedrock**: Writer's **Palmyra X5 MoE long-context model** is now available on **AWS Bedrock**, as noted in [this post](https://x.com/amazon/status/1916912132647751690).
   - It joins a growing list of models available on the **AWS Bedrock** platform, providing more options for users looking to leverage long-context capabilities.



---



## [DSPy](https://discord.com/channels/1161519468141355160) Discord

- **Handy CLI Tool Eases LLM Tasks**: A new [CLI tool](https://github.com/jgkym/cli-llm) simplifies daily LLM tasks like copy-pasting and switching between chats by setting up frequently used chat presets, with **DSPy** integration for performance enhancement.
   - A user found the CLI tool useful for everyday use cases.
- **MIPROv2 Docs MIA?**: A user reported that the **MIPROv2 documentation** page had been removed from the website and asked about new official documentation.
   - A member shared the old (unrevised) [MIPROv2 documentation page](https://github.com/stanfordnlp/dspy/blob/b40f359ec567a04a7f8d1d5d1a744ca9c32d5339/docs/docs/deep-dive/optimizers/miprov2.md) on GitHub and linked to the [tutorials](https://dspy.ai/tutorials) that use **MIPRO** for specific tasks.
- **Custom Module Examples Sought**: A member asked for favorite examples of a custom module, specifically one that templates over your signature or a compositional one that combines some sub-modules with some control flow.
   - Another member suggested `dspy.ReAct` and linked to [this example](https://dspy.ai/#__tabbed_2_6).
- **Streaming ReAct Thoughts Explored**: A member asked about streaming intermediate thought/steps in **DSPy's ReAct**.
   - A member pointed to the [streaming docs](https://dspy.ai/api/utils/streamify/?h=stream#dspy.streamify).



---



## [LlamaIndex](https://discord.com/channels/1059199217496772688) Discord

- **Deep Researcher Template Writes Legal Reports Fast**: The **Deep Researcher** template from *create-llama* automates legal report creation by generating sub-questions and extracting answers from documents, and compiling a report in seconds, which users can try out using `npx create-llama` and [this tool](https://t.co/XpVtmPCv11).
   - It automates the creation of legal reports by formulating sub-questions, extracting answers from documents, and compiling a final report.
- **Time Travel Forks LlamaIndex Threads**: When asked about forking conversation threads in **LlamaIndex** similar to **LangGraph**, a member suggested saving the workflow context and resuming it as a way to *"fork"* the thread via [time travel](https://langchain-ai.github.io/langgraph/concepts/time-travel/).
   - This approach allows users to effectively branch conversation threads by saving and resuming the context at different points.
- **Serving LlamaIndex as REST APIs**: A new **LlamaIndex** user sought **API endpoints** for interacting with the local HTTP server, which enables the app to serve as **REST APIs**, as outlined in [this article](https://medium.com/@sherlockxu/serving-a-llamaindex-rag-app-as-rest-apis-4b2cdb93e925).
   - The suggested approach involves using REST APIs to communicate with the server.
- **Bedrock Sonnet gets Anthropic Class**: A user encountered issues transitioning from an **Azure OpenAI** model to **Sonnet** from **AWS Bedrock**, with LLM calls failing, and a member recommended utilizing the `Anthropic` class with bedrock configured according to the [docs](https://docs.llamaindex.ai/en/stable/examples/llm/anthropic/#bedrock-support).
   - This class facilitates proper integration and configuration with Bedrock.
- **LlamaIndex Embeddings Act Erratic**: A member reported that embeddings created with fixed text chunks vary across runs, leading to different chunk selections and inconsistent answers, and also shared a code snippet using *euclidean distance* for [measuring distances](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.euclidean.html).
   - The inconsistency in embeddings affects the reliability of chunk selection and answer generation.



---



## [Notebook LM](https://discord.com/channels/1124402182171672732) Discord

- **LLMs Learn Programming Fast**: An engineer found **LLMs** useful for learning programming languages by inputting documentation, with **MatPlotLib** cited as an example.
   - This method streamlines learning by providing structured guidance based on comprehensive documentation, allowing for fast iteration.
- **LLM Masters Marine Networking**: A member successfully used **LLMs** for networking equipment, uploading manuals for boat networking, power, general safety, and wiring schemes.
   - The **LLM** identified installation fallacies, pointing out incompatibilities between different manuals and suggesting mitigations.
- **French Translation Beta Sought After**: A member asked whether there was a beta for **discussion generation in French**.
   - A second member clarified that the user can indicate it in the prompt.
- **Flash Thinking Model Performance Plummets**: Members observed that recent **responses take twice as long**, and **NotebookLM understands requests less intuitively** than before, with a *"strange sort of theatrical quality"*.
   - It was hypothesized that the issue relates to the **Flash Thinking model** replacing **Gemini 2.0**, though that implementation was weeks ago.



---



## [tinygrad (George Hotz)](https://discord.com/channels/1068976834382925865) Discord

- **San Diego Meeting Time Shifts**: Starting next week, the meeting time will move to **9am San Diego time**.
   - No other details were provided.
- **Hotz Seeks Universal Coding Style Guide**: George Hotz requested a high-level coding "style" guide, possibly as a blog post, not specific to Tinygrad, focusing on conceptual guidance for writing good code.
   - The requested coding style guide should mirror **Elon Musk's 5-step process**.
- **Memory Allocation Handling**: A member asked how Tinygrad handles contiguous memory allocation compared to PyTorch.
   - No further discussion was provided.



---



## [MCP (Glama)](https://discord.com/channels/1312302100125843476) Discord

- **MCP Fans Assemble**: A member announced the arrival of more **MCP fans** and greeted them.
   - Another member was given the **flair**.
- **Discover Related Servers**: A member mentioned the ability for users to submit related servers, for example, [https://glama.ai/mcp/servers/@qdrant/mcp-server-qdrant/related-servers](https://glama.ai/mcp/servers/@qdrant/mcp-server-qdrant/related-servers).
   - This feature aims to help with the **discovery of MCP servers**.
- **Cloudflare Hosts MCP?**: A member inquired whether anyone has hosted their **MCP servers on Cloudflare**.
   - No responses were given.



---



## [Torchtune](https://discord.com/channels/1216353675241590815) Discord

- **Sequence-Level Loss Parallelism Stumbles in Torchtune**: A user found issues experimenting with custom recipes like **loss parallel** on the sequence dimension in **TP**, and reproduced it on `main` with the original `full_finetune_distributed` recipe, documented in [this GitHub issue](https://github.com/pytorch/torchtune/issues/2641).
   - The user seeks confirmation or debunking of these issues, highlighting concerns about potential serious errors in the implementation.
- **Gradient Scaling Insights Unveiled**: A member clarified that the PR adding **grad_scaling** by world_size is theirs, with **grad_scaling** by **dp degree** also implemented in *fairseq2*.
   - They propose that if *fairseq2* hasn't faced similar problems, a more intricate bug might exist in torchtune's implementation, implying that merely removing grad scaling might not resolve the root cause.



---



## [LLM Agents (Berkeley MOOC)](https://discord.com/channels/1280234300012494859) Discord

- **Dawn Song to Cover Safe Agentic AI**: The final lecture of the **LLM Agents MOOC** features instructor **Dawn Song** presenting “**Towards building safe and secure agentic AI**” and is livestreamed [on YouTube](https://www.youtube.com/live/ti6yPE2VPZc) at **4pm PDT**.
   - Dawn Song is a Professor in Computer Science at **UC Berkeley** and co-Director of **Berkeley Center on Responsible Decentralized Intelligence**, with accolades like the **MacArthur Fellowship** and multiple **Test-of-Time Awards**.
- **MOOC Coursework Deadline Approaching**: All coursework for the **MOOC** (due at the end of May) is available on the [MOOC website](https://llmagents-learning.org/sp25).
   - Labs are expected to be released this week, with questions directed to the appropriate channel.



---



## [Codeium (Windsurf)](https://discord.com/channels/1027685395649015980) Discord

- **Windsurf Waves a Free Plan Upgrade**: Windsurf's free plan is getting a significant upgrade, offering **25 premium prompt credits** monthly, **unlimited Cascade Base usage**, **fast Tab completions**, and **access to App Deploys**, as detailed in [their blog post](https://windsurf.com/blog/update-to-free-plan).
   - This enhancement aims to make Windsurf more accessible and attractive to new users looking to explore its capabilities without a financial commitment.
- **Windsurf Unveils Logo Refresh**: Windsurf has updated its logo to reflect the *powerful, flow-state experience* they aim to provide, viewable in [this GIF](https://cdn.discordapp.com/attachments/1027688115592237117/1366495372444303391/Windsurf_Logo_Animation_Wordmark.gif?ex=6811277d&is=680fd5fd&hm=eb1c04aa7e00529a16edfa9625f73519b628967fd180aff9b39241ee1ffbc349&).
   - The new logo represents Windsurf's evolving identity and its commitment to providing a seamless and dynamic user experience.
- **GPT-4.1 and o4-mini Get New Price Signals**: Windsurf has adjusted the pricing for some of its models, with **GPT-4.1** and **o4-mini** now priced at **0.25x** prompt credits.
   - The pricing for **o4-mini (high)** is set at **0.5x**, reflecting the computational resources required for these models.



---


The **MLOps @Chipro Discord** has no new messages. If this guild has been quiet for too long, let us know and we will remove it.


---


The **Cohere Discord** has no new messages. If this guild has been quiet for too long, let us know and we will remove it.


---


The **Nomic.ai (GPT4All) Discord** has no new messages. If this guild has been quiet for too long, let us know and we will remove it.


---


The **Gorilla LLM (Berkeley Function Calling) Discord** has no new messages. If this guild has been quiet for too long, let us know and we will remove it.


---


The **AI21 Labs (Jamba) Discord** has no new messages. If this guild has been quiet for too long, let us know and we will remove it.


---

# PART 2: Detailed by-Channel summaries and links





### **Unsloth AI (Daniel Han) ▷ #[general](https://discord.com/channels/1179035537009545276/1179035537529643040/1366277972205768745)** (777 messages🔥🔥🔥): 

> `Deepseek R2, Qwen3 release, AMD MI300 flash attention bugs, Unsloth dynamic quant 2.0, Llama 4` 


- **Deepseek R2 still under speculation**: Members were hoping for a **Deepseek R2** announcement, but so far there have been only [untrustworthy leaks](https://link.to.leaks) and rumors.
   - One member claimed to be on **R2** and gave advice about setting threads to 1 if fully offloaded.
- **Qwen3 Launch Postponed, Leaks Everywhere**: The official **Qwen3** launch faced a setback, with a delay announced, however multiple leaks were observed with ggufs of Qwen3 including [Qwen3-0.6B](https://huggingface.co/qingy2024/Qwen3-0.6B) appearing briefly before being taken down, while the official release is [on Modelscope](https://modelscope.cn/collections/Qwen3-9743180bdc6b48).
   - A member humorously stated *these [Chinese] losers made my balls too blue to get my qwen 3!*
- **AMD MI300 Flash Attention encounters bugs**: Members are reporting a bug with **flash attention** and the **MI300**, turning off flash attention increases RAM usage while slowing down the process to 7-10 tokens/second.
   - A member states *if the FA worked, this card would be a monstrosity*.
- **Debate on Unsloth Dynamic Quant 2.0 Formation**: Users discussed methods for creating **Unsloth dynamic quant 2.0**, pondering whether it involves a formula or is based on trial and error.
   - One member asked *how can we make unsloth dynamic quant 2.0 ourselves? is there a formula for it or just trial and error with each model*
- **Llama 4 Scout Intelligence and Censorship Under Review**: Members are testing the performance of **Llama 4 Scout** models, with some finding it nice to run on CPU achieving around **3 tokens per second** with Llama 3.3 level intelligence and the models has issues relating to censorship.
   - Others expressed that it did not pass the *vibe check*.


  

---


### **Unsloth AI (Daniel Han) ▷ #[off-topic](https://discord.com/channels/1179035537009545276/1179039861576056922/1366282890387787816)** (10 messages🔥): 

> `Chain of Thought vs Latent Space Reasoning, Temperature Parameter in Models` 


- **CoT setup is easier!**: Members discussed whether current models use **Chain of Thought (CoT)** instead of **latent space reasoning**.
   - The consensus was that *CoT is easier to set up* and offers better interpretability, which academia prefers because *academia wants a theory and explanation before test*.
- **Temperature = zero to be deterministic!**: A member found out he was wrong about the **temperature** parameter, discovering that **0** is closer to deterministic, not 1.
   - He added that it hadn't changed the delta between model outputs but makes sense why some responses were in left field for both base and modified model.


  

---


### **Unsloth AI (Daniel Han) ▷ #[help](https://discord.com/channels/1179035537009545276/1179777624986357780/1366290116007432263)** (130 messages🔥🔥): 

> `Qwen2.5 model issue, Unsloth multi-GPU, Load finetuned Lora to HF, Model works well on OOD, Granite 2b` 


- **Transformers library bug impacts Qwen2.5 Loss**: There's a potential bug in the transformers library's forward function for the **Qwen2.5 model**, requiring explicit device placement for labels and features, similar to a previous issue with **Qwen2**.
   - This impacts the calculation of the loss function and users are encouraged to file a GitHub issue for resolution.
- **Unsloth Multi-GPU support in Early Access**: **Unsloth** currently lacks native multi-GPU support, and users must rely on **Accelerate** for multi-GPU functionality, although native support is in early access.
   - Access to **Unsloth Pro** is currently full with general availability not yet released, and future access to **Unsloth Pro** will be open-sourced.
- **Overconfident Models Ramble Incoherently with High Temperature**: A member reported that when using a little temperature, the model produces incoherent rambling, and not enough data depth i.e. topic coverage in training data samples is a possible cause.
   - Another member suggested it might be that the gaussian distribution of the logits is tall i.e. variances are very tight.
- **CPU Inference via Llama.cpp conversion**: To run models on CPU, convert them to **llama.cpp GGUF format** using the `convert_hf_to_gguf.py` script in the llama.cpp repo after merging LoRA and the base model via `save_pretrained_merged(...)`.
   - A member stated that the task can be achieved through optimizing the prompt or upgrading to a larger model.
- **Transformers version upgrade fixes patching errors**: A user encountered "Failed to patch Gemma3ForConditionalGeneration" warning due to outdated transformers; upgrading to version **4.51.3** resolved the issue.
   - The user then received a `SmolVLMForConditionalGeneration` patch failure warning, which does not affect Gemma3 model as a team member stated.


  

---


### **Unsloth AI (Daniel Han) ▷ #[showcase](https://discord.com/channels/1179035537009545276/1179779344894263297/1366458376111783936)** (1 messages): 

> `Function Calling at Scale, CTP + SFT, Multi-LoRa Endpoint Serving, Expert Adapter Finetuning` 


- **Almighty Function Caller Born via CTP + SFT**: A novel approach to **function-calling at scale** for smart companies and corporate-grade use-cases has been developed, combining **Continued Pretraining (CTP)** and **Supervised Finetuning (SFT)**.
   - Details can be found in a [Hugging Face blog post](https://huggingface.co/blog/Aurelien-Morgan/the-almighty-function-caller) that covers **function-calling**, **expert adapter finetuning**, **performance metrics**, and **serving on a multi-LoRa endpoint**.
- **Deep Dive into Edge Agentic Systems**: The new method enhances **edge agentic systems** by providing them with extensive tools memory, optimizing resource consumption while maintaining high performance.
   - This advancement targets corporate-grade use-cases, offering a scalable solution for companies looking to implement smart, efficient agent systems.


  

---


### **Unsloth AI (Daniel Han) ▷ #[research](https://discord.com/channels/1179035537009545276/1257011997250424842/1366275611286306877)** (2 messages): 

> `emo-llm, Arxiv` 


- **Emo-LLM code surfaces, Arxiv lags**: The code for **emo-llm** has been located at [github.com/aminbana/emo-llm](https://github.com/aminbana/emo-llm).
   - The submitter noted that the corresponding **Arxiv** entry does not link to the code.
- **Arxiv struggles to link code**: The submitter noted that the corresponding **Arxiv** entry does not link to the [emo-llm code](https://github.com/aminbana/emo-llm).
   - The **Arxiv** paper should have linked the github code.


  

---


### **Perplexity AI ▷ #[general](https://discord.com/channels/1047197230748151888/1047649527299055688/1366274480413999154)** (828 messages🔥🔥🔥): 

> `Image Generation, Perplexity extensions, Deepseek AI, Perplexity Pro, Gemini 2.4` 


- **Android App for Image Generation coming soon**: The image generation feature is currently only available on the **web and desktop app** versions of Perplexity AI, with the [model picker located in settings](https://link.to/settings).
   - Members can generate images by typing *Generate image of a...* with the **web icon** turned on.
- **Deepseek available for download**: Deepseek AI is available for download again in South Korea after privacy suspension, according to [Rebruit.com](https://rebruit.com/deepseek-ai-app-available-for-download-again-in-south-korea-after-privacy-suspension/).
   - One member commented *W Deepseek*.
- **PPLX Outage Slows Down Productivity**: Perplexity AI experienced a service degradation affecting various features, including **Spaces and Library items**, and the community reacted with humor and alternative suggestions.
   - Users shared [cat-related GIFs](https://tenor.com/view/cat-cat-meme-stressed-cat-banana-cat-gif-15455068164842202481) and recommended alternative services like **chat.minimax.io** and **you.com** to cope with the downtime, which lasted around 5-6 hours.
- **Perplexity Pro Perks & Freebies**: Members discussed ways to obtain Perplexity Pro, including a **free year offered by some internet providers** and a [T-Mobile promotion](https://www.perplexity.ai/join/p/magenta) offering **12 months of free access with a new SIM card**.
   - Some users in Germany, Hungary, Slovakia, Czechia, North Macedonia, Austria, and Montenegro can get a sim, and therefore, Pro.
- **Grok Does Deep Research**: Grok is useful for deep research when clear deep research questions are used.
   - One member says, *I created some instructions in chatgpt projects, whenever i want to deep research, i say it create a clear deep research question on ongoing conversation, it generates deep research question with all details, which works very well when used in grok deep research or gemini deep research*.


  

---


### **Perplexity AI ▷ #[sharing](https://discord.com/channels/1047197230748151888/1054944216876331118/1366343413150126123)** (2 messages): 

> `DeepSeek AI, Aravind Srinivas` 


- **DeepSeek AI Now Available via Perplexity**: [DeepSeek AI](https://www.perplexity.ai/search/deepseek-ai-is-now-available-f-7NP_AOTlTcKak_T74J6erw#0) is now available to **Perplexity AI** users.
- **Aravind Srinivas's Perplexity Page**: A member shared a link to [Aravind Srinivas's Perplexity page](https://www.perplexity.ai/page/aravind-srinivas-s-perplexity-b4gdP4muSGixldpyZhqWHQ).


  

---


### **Perplexity AI ▷ #[pplx-api](https://discord.com/channels/1047197230748151888/1161802929053909012/1366318603342839818)** (7 messages): 

> `Debit card details saved, Structured output adherence, Sonar endpoint text+image` 


- ****Stripe** handles debit card details**: A user inquired whether debit card details are saved, and another user responded that only **Stripe** sees and saves the data, and advised them to check with `api@perplexity.ai` or `support@perplexity.ai`.
   - A user suggested creating an **API key** with an expiration limit and a rate limit on a per-day basis for hackathons.
- **Structured output adherence is significantly worse**: A user reported that structured output adherence is significantly worse than a few weeks ago, with a script that used to work now having a lot of issues.
   - The user specified that the issue is that the model is not completing the schema or leaving blanks, even for mandatory fields such as **age ranges**.
- **Sonar endpoint timeouts**: A user playing with the **Sonar endpoint** providing text + image reported timeouts waiting 5 minutes plus, even with a compressed, base64-encoded JPEG image of only ~10kb, rendering the endpoint practically unusable.
   - The user asked if anybody else had some experience with the endpoint.


  

---


### **LMArena ▷ #[general](https://discord.com/channels/1340554757349179412/1340554757827461211/1366270628654612542)** (648 messages🔥🔥🔥): 

> `Folsom-exp-v1, Qwen 3, Amazon Nova Premier, Deepseek R2, O3 Pro` 


- **Folsom-exp-v1 Model Surfaces Anonymously**: A member reported encountering `folsom-exp-v1`, an unknown model possibly linked to **Amazon's Cobalt or Apricot projects**, with another member suggesting it's an *Amazon reasoning model*.
   - Another member found the model to be quite fast and dumb, a [tweet](https://x.com/btibor91/status/1916756247422124353?t=nmHdYoncI3U1Sdle6XHOlg&s=19) was posted with further discussion.
- **Qwen 3 Speculation & Impatience Mounts**: Members discuss the impending release of **Qwen 3**, with one stating it was apparently pre-trained on **36 trillion tokens** and features multiple **MoE models**.
   - Discussion includes speculation about the architecture, number of parameters (**235b** is mentioned), and whether it will surpass other models, such as **Deepseek R2** or **Gemini 2.5 Pro**.
- **Qwen 3 Released With Surprise & Controversy**: **Qwen 3** has been released, including a **235B parameter model** and a range of smaller models with different architectures, as described on the [Qwen3 GitHub](https://github.com/QwenLM/Qwen/tree/main), [Hugging Face](https://huggingface.co/spaces/Qwen/Qwen3-Demo) and [ModelScope](https://modelscope.cn/collections/Qwen3-9743180bdc6b48) pages.
   - The conversation quickly turned to leaked benchmark results, which were later retracted, and early impressions of the released models. One user reported that the  *models will switch between thinking mode and non-thinking mode*.
- **R2 Speculation Ramps Up**: With the **Qwen 3** release, members speculate about the release of **Deepseek R2** this week and whether it can match or exceed the performance of **Gemini 2.5 Pro**.
   - While **DeepSeek** is typically quiet about their plans, there was mention of a leak, suggesting a potential imminent release.
- **O3 Pro API Remains Elusive**: Discussion continues around the availability (or lack thereof) of **O3 Pro** via the **OpenAI API**, but is not available yet as of time of writing.
   - Some members claim it's already active on the **O1 Pro API endpoint**, but others dispute this, while it is speculated that the pricing might be too high to be widely used.


  

---


### **OpenAI ▷ #[ai-discussions](https://discord.com/channels/974519864045756446/998381918976479273/1366271658503176222)** (149 messages🔥🔥): 

> `AGI predictions, AI and job automation, Google TPUs, Removing text from video, ChatGPT flirting` 


- **AI Pundits Predict AGI Arrival**: A member shared a summary of predictions for **AGI** from various sources, including [Sam Altman (2026.3), Elon Musk (2026.5), and Metaculus (2025.9)](https://drinkoblog.weebly.com), averaging to **2030.8**.
   - Some members cautioned against taking certain predictions too seriously, especially those from **Yann LeCun** and the **2023 survey results**.
- **AI Automates Jobs and Creates UBI Paradise**: Members discussed the potential for **AI** to automate all jobs, leading to **UBI** or making money obsolete, envisioning a future where people become *gods in a simulated environment*.
   - Some expressed concern about the societal impact, suggesting that people might get angry when AI becomes so advanced it makes owning money unnecessary.
- **Google's TPU Advantage Threatens OpenAI**: A member highlighted **Google's** advantage in using custom **Tensor Processing Units (TPUs)** since 2015, estimating **Google's** compute costs are around 20% of what **Nvidia** customers pay.
   - It was suggested that this cost advantage allows **Google** to sustain lower pricing for models like **Gemini 2.5 Pro**, while **OpenAI's** compute expenses are tightly tied to **Nvidia's** pricing, with compute costs potentially exceeding 80% of operating expenses in 2025.
- **AI-Powered Text Removal from Video**: Members discussed methods for removing text from a 15-second video clip, suggesting extracting video frames as images, using AI-made masks and tools like **Lama Cleaner** frame by frame.
   - One member offered to try the process but ultimately declined, suggesting the original poster could use **ChatGPT** to help automate the workflow, estimating the task could take less than 5 minutes with decent hardware.
- **Seeking Examples of ChatGPT Glazing**: A member requested examples of **ChatGPT** *glazing* (being nice via dishonesty, not challenging clear factual errors or unethical behavior).
   - Another user responded that they did not have examples of **ChatGPT** glazing but rather have examples of **ChatGPT** *flirting*.


  

---


### **OpenAI ▷ #[gpt-4-discussions](https://discord.com/channels/974519864045756446/1001151820170801244/1366308860322906184)** (18 messages🔥): 

> `Deep Research vs Deep Research Mini, O3 use, 4o Talking Weird, O3 vs O1-pro, chatgpt plus users` 


- **Navigating Deep Research Tiers: Mini vs Max**: Users discussed how to manually select between **Deep Research** and **Deep Research Mini** modes, but the consensus is that the switch happens automatically once the **Deep Research** limit is reached.
- **Members Compare 4o's Persona to Creepy Step-Bro**: One member described **4o's** communication style as *weird* and *formulaic*, complete with excessive compliments and inappropriate suggestions.
   - The user added, *Anyone else getting this weird needy step-bro feeling?*
- **Pro User Reports Memory Loss Using O3**: A **Pro** user reported that **O3** tends to lose track in long coding chats more often than **O1-Pro**.
   - Another member suggested using **custom instructions** in **projects** to summarize key details of the conversation periodically and maintain context.


  

---


### **OpenAI ▷ #[prompt-engineering](https://discord.com/channels/974519864045756446/1046317269069864970/1366334237665529998)** (5 messages): 

> `Image Rebrushing, Business Idea Development, AI Model Prompting` 


- **Image Rebrushing Faces Roadblock**: A user sought advice on rebrushing a picture to match a specific style, noting that their face was *alienated* in the process and they sought help with [style transfer](https://cdn.discordapp.com/attachments/1046317269069864970/1366415512074059858/20250428_1543_Office_Celebration_remix_01jsya4naeeqjtp6qc77k1z6nn.png?ex=6810dd1c&is=680f8b9c&hm=95e4f7696284c071da1b2e53e2dc2e6bff6b141fc03f10952e1b778db15b745d&).
- **Seeking Prompts for Solid Business Plan**: A member requested a structured prompt or prompt chain for **developing a business idea** using tools like ChatGPT, to help with defining the business model, target audience, revenue streams, technical requirements, **MVP**, and launch plan.
- **Level Up Your Prompting Game**: A member suggested to *use a significantly powerful AI model to generate very specific, or niche prompts*, and then using these *AI-drafted prompts* to augment Agent capabilities.
   - They also stated that adding the phrase **"Let's think this through, step by step"** to prompts enhances reasoning in most models.


  

---


### **OpenAI ▷ #[api-discussions](https://discord.com/channels/974519864045756446/1046317269069864970/1366334237665529998)** (5 messages): 

> `Image rebrushing with consistent style, Business model development with AI prompts, AI-assisted prompt engineering` 


- **Image Rebrushing Dilemma: AI Style Transfer Challenges**: A user sought advice on rebrushing a personal photo to match the style of an AI-generated image ([example image](https://cdn.discordapp.com/attachments/1046317269069864970/1366415512074059858/20250428_1543_Office_Celebration_remix_01jsya4naeeqjtp6qc77k1z6nn.png?ex=6810dd1c&is=680f8b9c&hm=95e4f7696284c071da1b2e53e2dc2e6bff6b141fc03f10952e1b778db15b745d&)), noting their face was *'totally alienated'* in the process.
- **AI Prompt Needed to Draft a Business Model**: A user requested a prompt to structure a business idea using AI tools like ChatGPT, aiming to define the business model, identify the target audience, map revenue streams, outline technical requirements, create an MVP, and build a launch plan.
   - The user specifically asked for step-by-step prompt structures and examples of how others have used such prompts in their projects.
- **AI Can Draft Prompts to Generate Prompts**: A user recommends using a *'significantly powerful AI model'* to generate specific prompts, and then using those prompts in another AI model, to effectively have AI draft prompts.
   - The same user pointed out that adding the phrase *"Let's think this through, step by step"* enhances reasoning capabilities in most AI models.


  

---


### **LM Studio ▷ #[general](https://discord.com/channels/1110598183144399058/1110598183144399061/1366267793108107295)** (96 messages🔥🔥): 

> `Virtual Environment with Python, Qwen 3 Released, LM Studio Overlay` 


- **Venv to the Rescue**: Users are advised to use **virtual environments** (venv) with Python, Conda, or Pinokio to isolate projects and prevent issues like LM Studio uninstalling Pytorch which one user experienced.
   - Several alternatives to `venv` were recommended such as [venv-manager-gui](https://github.com/realcgslav/venv-manager-gui), [venvipy](https://pypi.org/project/venvipy/), and [venv-app](https://github.com/enoobis/venv-app) and [uv](https://www.reddit.com/r/OpenAI/comments/1k5h707/does_chatgpt_voice_turn_into_a_demon_for_anyone/).
- **Qwen 3 Drops - Thumbs Down?**: **Qwen3** has officially dropped, but one user commented it was *not worth atm* and to *wait for official implementation*.
   - While **GGUFs** are available, the **Jinja template** isn't up to date; some users prefer **Qwen 2.5 coder** over previous versions for coding tasks, while some have had bad experiences with all versions of **Qwen**.
- **Gamify LM Studio**: A user suggested a **HUB for LM Studio**, like a gamer's overlay, to display **GPU temps, tokens/s, and VRAM workload**.
   - Another user pointed out that **HWiNFO** can provide similar information, and the LM Studio Team might not have time to update sensor problems, though displaying system-reported usage could be a workaround.


  

---


### **LM Studio ▷ #[hardware-discussion](https://discord.com/channels/1110598183144399058/1153759714082033735/1366274653261139999)** (69 messages🔥🔥): 

> `8x 5060Ti rig, RTX 6000 Pro Blackwell, Gemma 3 finetuning, Intel Arc B580 24GB, Multi-GPU Configuration` 


- **Members Ponder Building 8x 5060Ti Rigs**: Members discussed the possibility of building an **8x 5060Ti rig**, with one member suggesting it might cost around **$6k USD** for a used setup, noting that while possible, *it would probably just suck*.
   - Concerns were raised about **memory bandwidth** and **PCIe connection bottlenecks**, with one member estimating that **3x 5060Ti GPUs** could perform similarly to a **4090** but with double the VRAM.
- **RTX 6000 Pro Blackwell Could Solve VRAM Issues**: A member expressed frustration with current **GPU VRAM limitations**, considering waiting for the **RTX 6000 Pro Blackwell** or other solutions like CPUs or SoCs/APUs.
   - They highlighted a [Tom's Hardware article](https://www.tomshardware.com/pc-components/gpus/nvidia-may-release-the-rtx-5080-and-5070-super-with-boosted-memory-configurations-according-to-leaker) suggesting that people should hold off buying GPUs until the **5080 Super** release.
- **Gemma 3 27B Model Requires Precise System Prompts**: A member shared their experience finetuning **Gemma 3 27B**, noting that it initially seemed to perform worse than the **12B** version but improved with a more detailed system prompt.
   - Another member pointed out that the **Gemma architecture** doesn't natively support system prompts and linked to [Google's documentation on prompt structure](https://ai.google.dev/gemma/docs/core/prompt-structure).
- **Intel Arc B580 24GB Cancellation Debated**: Members discussed the rumored **Intel Arc B580 24GB**, with one citing information from the Intel Reddit indicating its cancellation in Q3 of the previous year.
   - It was also mentioned that many AI tools are still **Nvidia CUDA-only**, limiting the utility of high VRAM cards from Intel and AMD, even for LLMs.
- **Multi-GPU Configuration Software Support**: A member expressed concern about the configuration required to take advantage of **multi-GPU setups**.
   - Another member warned not to *hold on buying 3090 below $700*, as buying a used GPU is a *hit or miss*.


  

---


### **Nous Research AI ▷ #[general](https://discord.com/channels/1053877538025386074/1149866623109439599/1366318989864734721)** (152 messages🔥🔥): 

> `GPU Nodes Tutorials, Nous Research API, Hermes 3 vs Claude, Creative Writing Models, Deepseek R1` 


- **Seek Tutorials on Running GPU Nodes**: A member was looking for tutorials on running GPU nodes, but the current resources are not publicly available.
- **Nous API Access**: After a new member got access to the **Nous API**, they asked where they could find the documentation and another member shared the [link to the portal](https://portal.nousresearch.com/api-docs).
- **Hermes 3 Scores High in Creative Writing**: **Hermes 3 405b** ranks 3rd after **Claude Instant** and **Claude 2.0** for creative writing, outperforming **Gemini 2.5 Pro** and **Microsoft's Deepseek R1** finetune.
- **Creative Writing Models Dominated by Claude**: One member stated that models like **Claude Instant** and **2.0** produce better writing quality and prose compared to new models, and current creative writing models are in a poor state compared to them.
- **Deepseek R1 Creative Writing**: A member found that **Microsoft's version of Deepseek R1** is barely any different from **Sonnet 3.7** in terms of prose, while **Deepseek R1T Chimera** seems to be angling towards being more insane than sane.
   - Another member mentioned that **DeepSeek Version 2** is scheduled for release this week.


  

---


### **Nous Research AI ▷ #[ask-about-llms](https://discord.com/channels/1053877538025386074/1154120232051408927/1366436627936706684)** (1 messages): 

> `Webpage for Nous Research Chatbot, Future of Nous Research Chatbot` 


- **New Webpage Created for Nous Research Chatbot**: A member has created a [simple webpage](https://nous-research-chatbot.onrender.com/) for the **Nous Research chatbot**.
   - The webpage is hosted on **Render**.
- **Future Plans**: The future plans are unknown at the moment.
   - Please check back.


  

---


### **Nous Research AI ▷ #[research-papers](https://discord.com/channels/1053877538025386074/1104063238934626386/1366388471811543122)** (1 messages): 

> `Writing in Margins, OptiLLM implementation` 


- **Writing in Margins Strikes Again**: A member noted a similarity between a new paper and the "Writing in Margins" paper from last year ([https://arxiv.org/abs/2408.14906](https://arxiv.org/abs/2408.14906)).
   - They also mentioned they had implemented it in **optillm** with some good results ([https://x.com/asankhaya/status/1844139401959571684](https://x.com/asankhaya/status/1844139401959571684)).
- **OptiLLM Implementation Success**: The user reported successful implementation of the "Writing in Margins" concept within **OptiLLM**.
   - Further details and results of this implementation can be found on their X post ([https://x.com/asankhaya/status/1844139401959571684](https://x.com/asankhaya/status/1844139401959571684)).


  

---


### **Nous Research AI ▷ #[research-papers](https://discord.com/channels/1053877538025386074/1104063238934626386/1366388471811543122)** (1 messages): 

> `Writing in Margins Paper, OptiLLM Implementation, Model Performance` 


- **Writing in Margins Paper Sparks Interest**: A member found a new [paper](https://arxiv.org/abs/2408.14906) similar to last year's **Writing in Margins** paper.
   - They had implemented it in **optillm** with some good results, as shown [here](https://x.com/asankhaya/status/1844139401959571684).
- **OptiLLM Implementation Yields Positive Outcomes**: The implementation of the **Writing in Margins** paper's concepts in **OptiLLM** led to favorable results.
   - The member shared their positive findings and linked to their work on X.


  

---


### **Eleuther ▷ #[announcements](https://discord.com/channels/729741769192767510/794042109048651818/1366384253134307349)** (1 messages): 

> `Speech-to-text transcription, Document to Markdown conversion, Mozilla Blueprints, Speaches.ai, Docling` 


- **MozillaAI gives Blueprint Demo Today**: Today, at 11am ET, a live demo will be given on the MozillaAI Discord showcasing two useful pipelines for creating open datasets, developed in partnership with the **Mozilla Blueprints team**.
   - The [Discord event link](https://discord.com/invite/4jtc8RNC?event=1364307871138975826) was also shared.
- **Speech-to-text transcription powered by Speaches.ai**: One tutorial discusses **Speech-to-text transcription** with self-hosted **Whisper models** using [Speaches.ai](https://blueprints.mozilla.ai/all-blueprints/transcribe-audio-files-with-open-source-whisper-models).
- **Docling converts Docs to Markdown**: The other tutorial involves **Document to Markdown conversion** using [Docling](https://blueprints.mozilla.ai/all-blueprints/convert-documents-to-markdown-format).


  

---


### **Eleuther ▷ #[general](https://discord.com/channels/729741769192767510/729741769738158194/1366484783487651840)** (62 messages🔥🔥): 

> `Deepseek paper, RoPE Embeddings, RWKV and GoldFinch arch, BMM Implementation` 


- **DeepSeek's Training vs. Inference Matrix Folding**: Discussion revolves around whether the up-projection matrices in **DeepSeek** can be folded during training, with a member stating the paper explicitly mentions it's only for inference: *In addition, during inference, since $W^{UK}$ can be absorbed into $W^{Q}$, and $W^{UV}$ can be absorbed into $W^{O}$, we even do not need to compute keys and values out for attention*.
   - A member believes it should be possible to train on $(W^{UQ})^TW^{UK}$ but admits their past implementations were incorrect.
- **Speculating BMM is needed with translation weights**: Members discussed the equations regarding attention, specifically the calculation in the softmax as $q^Tk$, and how it could be written as $(c^Q)^T(W^{UQ})^TW^{UK}c^{KV}$, allowing pre-calculation of $(W^{UQ})^TW^{UK}$.
   - For example, *in this case you need like $W^{DQ} h_t W^{translation} W^{DKV} h_t$ or something and it's a different W^{translation} for each head* suggesting the necessity of a Batched Matrix Multiply (BMM).
- **New insights into smaller Weights**: A member expressed excitement about potentially applying the lessons from **DeepSeek** to **RWKV** and their **GoldFinch architecture**.
   - They believe it might lead to faster training, smaller weights, and reduced VRAM usage.
- **Confirmed Parameter Count Reduction**: One member confirmed that **DeepSeek's** approach could lead to a significant reduction in parameter count.
   - Further details about the confirmation method or magnitude of reduction were not discussed.


  

---


### **Eleuther ▷ #[research](https://discord.com/channels/729741769192767510/747850033994662000/1366327207269502986)** (91 messages🔥🔥): 

> `AI Auditing Survey, Inference-Dominated Regime, Hamlet Robot Control System, LLM Reasoning Processes, Human vs. LLM Reasoning` 


- **AI Audit Survey Seeks Experts**: A researcher from the University of Turku is conducting an academic [survey](https://link.webropolsurveys.com/S/AF3FA6F02B26C642) on **ethics-based AI auditing** of generative AI systems and is seeking insights from professionals in AI auditing, model evaluation, risk management, or ethical alignment.
   - The survey, which takes about **10-15 minutes**, aims to gather practical experience in generative models and promises full anonymity.
- **Inference Specific TPU/GPU Architectures Emerging**: Discussion arose around the likelihood of moving towards an **inference-dominated regime** in the coming years, and the potential for incorporating inference-specific designs into TPU/GPU architectures.
   - One member noted, *Yes it does seem like **TPU + GPU architecture focus is moving forward** but it’s very early phase, not very stable…* while also sharing concerns about the current state of the stack.
- **Hamlet Robot Plays Agile Badminton**: A new paper, [HAMLET](https://dreamstarring.github.io/HAMLET/), presents a novel **whole-body control system** for an agile badminton robot, combining model-based and learning-based control methods, achieving a **94.5% success rate** against a serving machine and **90.7%** against a human opponent.
   - The system employs an *"IL + RL" strategy*, pre-training the actor and critic in IL to enhance subsequent RL policy training, enabling **zero-shot transfer** from simulation to reality.
- **LLMs Don't Reason Like Humans**: A [new Anthropic paper](https://www.mindprison.cc/p/no-progress-toward-agi-llm-braindead-unreliable) reveals that **LLMs do not reason in a human-like manner**, lacking mechanistic understanding and relying on immense statistical models to simulate intelligence.
   - The paper suggests that improvements in LLM performance are due to better heuristic predictors rather than genuine advancements in reasoning capabilities, based on internal analysis of LLM processes via [transformer-circuits.pub](https://transformer-circuits.pub/2025/attribution-graphs/biology.html).
- **Human vs LLM Reasoning**: Discussion emerged around the nature of **reasoning in LLMs**, with some arguing that LLMs are capable of something akin to reasoning when provided with a scratchpad, enabling them to expend more computation when required.
   - Counterarguments suggested that LLMs may hallucinate reasons, however some suggested that *human reasoning is at least partly heuristics interleaved with rule applications and search*.


  

---


### **OpenRouter (Alex Atallah) ▷ #[announcements](https://discord.com/channels/1091220969173028894/1092729520181739581/1366457120286969937)** (2 messages): 

> `Provider Data Logging Policies, Cent ML, Enfer, Oauth `state` parameter, Gemini Parallel Tool Calling` 


- **OpenRouter Enhances Developer Experience!**: OpenRouter introduces a new page dedicated to [Provider Data Logging Policies](https://openrouter.ai/docs/features/privacy-and-logging) offering clear explanations of OpenRouter's data practices.
- **OpenRouter adds Cent ML and Enfer as new Providers**: OpenRouter welcomes [Cent ML](https://openrouter.ai/provider/centml) and [Enfer](https://openrouter.ai/provider/enfer) as its newest providers.
- **Oauth `state` parameter supported**: OpenRouter now supports the `state` query parameter in callback URLs when integrating with [OpenRouter Oauth PKCE](https://openrouter.ai/docs/use-cases/oauth-pkce).
- **Gemini Parallel Tool Calling integrated**: OpenRouter enables parallel tool calling requests from **Gemini**, similar to **OpenAI/Anthropic**.
   - However, an issue with what seems to be an **Upstream Vertex issue** on **Gemini 2.5 Pro** and **Gemini 2.5 Flash** models has been detected and the endpoint is disabled while investigating, but models should still be usable through **AI Studio**.


  

---


### **OpenRouter (Alex Atallah) ▷ #[app-showcase](https://discord.com/channels/1091220969173028894/1092850552192368710/1366345487682244620)** (1 messages): 

> `Agent Interface, Muka.ai, Web Search, Document Upload` 


- **Muka.ai Debuts OR-Powered Agent Interface**: An agent interface 100% powered by **OpenRouter** has been released at [Muka.ai](https://muka.ai).
   - It supports *web search*, *document upload*, *mcp sse*, *canvas views*, and *project organized chats*.
- **Muka Integrates Web Search in Chat**: The **Muka.ai** agent interface now features integrated **web search** capabilities, enhancing its utility.
   - Users can initiate searches directly within their chats, streamlining information gathering.


  

---


### **OpenRouter (Alex Atallah) ▷ #[general](https://discord.com/channels/1091220969173028894/1094454198688546826/1366269673993142313)** (147 messages🔥🔥): 

> `Qwen 3 Release, Deepseek v4 Speculation, Gemini Filtering, Safety Settings API, Local Model Size and VRAM` 


- **Qwen 3 Officially Drops, Then Quickly Pops**: After whispers of a release, **Qwen 3** briefly appeared on Hugging Face and ModelScope, with someone grabbing the **0.6B** version before it was taken down, but official uploads on HF are now available and it uses **36 trillion tokens**.
   - Community members express excitement, with one highlighting that the pretraining dataset for **Qwen3** has been significantly expanded compared to **Qwen2.5** (nearly twice the amount).
- **Deepseek v4: Coming Soon or Just a Mirage?**: Speculation arises about the imminent release of **Deepseek v4**, with excitement tempered by skepticism, and community members also shared that they do not expect **R2 before deepseek v4** release.
   - One member joked, *"We got deepseek 4 before gta 6 (Deepseek 4 still unreleased)"*.
- **Bypassing Gemini's Guardian Filters**: Users discuss **Gemini's** heavy filtering and explore methods to adjust safety settings via the API, with one user asking whether **Gemini** still uses high safety filtering.
   - A member shared a code snippet for setting **safety_settings** to **BLOCK_NONE** for various harm categories, providing a [Discord link](https://discord.com/channels/1091220969173028894/1361711209866596493/1361717364994867210) with an example.
- **Squeezing Blood from a Stone: Local Model Size on a 6GB RTX 2060**: Users debate optimal model size and quantization for an **RTX 2060 6GB**, with suggestions ranging from **4B int4/int8** to **8B int4**, the main suggestion being **Gemma 3 4B**.
   - It was noted that *8B won’t fit if you want any context*, with a recommendation to consider a used **3060** for its **12GB** VRAM.
- **Guarding Against the Glaze: Detecting ChatGPT's Dishonest Niceties**: A user seeks examples of **ChatGPT** glazing (being nice via dishonesty, not challenging clear factual errors or unethical behavior), appealing for organic examples beyond those found on r/ChatGPT.
   - One member suggested a system prompt as a basic guard for factual errors, while another noted, *"Also people hate honesty. Source: Years of working in customer service"*.


  

---


### **aider (Paul Gauthier) ▷ #[general](https://discord.com/channels/1131200896827654144/1131200896827654149/1366270861870628986)** (121 messages🔥🔥): 

> `Aider Token Scams, Aider for Complex Web Development, Gemini 2.5 Pro, File Renaming in Aider, Qwen3 Availability and Performance` 


- ****Aider** Team Denies Token Release Amidst Scam Concerns**: Users inquired about an **Aider token**, but members confirmed that there is no official token and any associated Twitter accounts are fake, warning of potential scams.
   - One member quipped that a team member *said on Twitter aider tokens are SAFU*, before clarifying it was a joke.
- **Aider Excels in **HTMX**, **Go**, and **Postgres** Stack, says User**: A user inquired about using **Aider** for advanced software development with **htmx**, **templ**, **golang**, **Cassandra**, and **ScyllaDB**.
   - Another user reported successful experiences using **Aider** with **Claude 3.7**, **Gemini 2.5**, and **GPT-4.1** for a similar stack (*htmx, go, templ, sqlc/postgres*), emphasizing the importance of detailed prompts.
- ****Gemini 2.5 Pro** Still Defaults to Whole Edit Format, Despite User Configuration**: A user reported that **Gemini 2.5 Pro** (via OpenRouter) still defaults to the *whole* edit format, despite configuration efforts with the latest **Aider** version.
   - Another user confirmed there is *no config for it*, providing a [Gist link](https://gist.github.com/gcp/b59fbce06955a7177d741d5446a46390) as reference.
- **Aider's File Renaming Conundrum**: A user sought advice on the best way to rename a file in **Aider** without disrupting context, mentioning that renaming in an IDE can cause issues.
   - The user suggested using the `/run git mv oldname newname` command, while seeking recommendations on the best practice.
- ****Deepseek R2** Hype Surrounds Potential **O3** Performance at Bargain Prices**: Enthusiasts discussed the potential performance and cost of **Deepseek R2**, with hopes that it will match or exceed **O3** levels while maintaining low pricing.
   - A user highlighted that **R2** might be cheaper due to a 90% cost cut and could offer **O4-mini/g2.5 pro** level performance but be *140x cheaper than 4o*.


  

---


### **aider (Paul Gauthier) ▷ #[questions-and-tips](https://discord.com/channels/1131200896827654144/1133060505792159755/1366275845815144528)** (28 messages🔥): 

> `Gemini model switching, Augment Code SOTA retrieval, MCP servers, Arabic support, Lint-staged with Husky` 


- ****Gemini** Model Switching Snafu**: A user encountered a `ModuleNotFoundError` when trying to switch **Gemini** models using `/model gemini-2.5-pro-preview-03-25` and was prompted to `pip install google-generativeai`.
   - The user had initially started **aider** with `--model openrouter/gemini-2.5-pro-exp-03-25`.
- ****Augment Code** retrieval SOTA**: A member suggested that *Augment Code* is the current **SOTA** for retrieval solutions.
   - They are also looking for a retrieval solution that is comparable to it.
- **MCP Servers via Probe**: A user recommends using [MCP servers](https://github.com/buger/probe) via **probe** for good experiences.
   - The user added to *try architect mode maybe*.
- **Arabic coding via **Mistral Sabait****: A user inquired about **Aider** supporting Arabic for a student group in Gaza, with a recommendation to try [Mistral Sabait](https://console.groq.com/docs/rate-limits) and use it through **Groq**.
   - It was also suggested to combine **Mistral Sabait** to turn prompts into ones for **Deepseek R1**.
- ****Lint-staged** ain't Staged**: A user reported that **Aider** commits were not running through **lint-staged** and **Husky**, which usually checks changes before committing.
   - The suggestion was to use `--git-commit-verify` to fix it.


  

---


### **Manus.im Discord ▷ #[general](https://discord.com/channels/1348819876348825620/1349440650495398020/1366262706524651594)** (141 messages🔥🔥): 

> `Manus AI access, Free vs Paid Credits, Clickup Integration, Slack Integration, Manus Limitations` 


- **Debate over Manus being private or public**: Members debated the accessibility of **Manus AI**, with some arguing it's essentially public due to the ease of getting accepted, while others maintain it's still in private beta.
   - One member noted that *literally everyone gets accepted after a few days*, while another countered that *private beta should be extremely limited on amount of members and not public at all*.
- **Invite code recursion reported as possible TOS violation**: A user shared numerous invite codes, sparking discussion on whether it violated the **Terms of Service (TOS)** by recursively generating codes.
   - Some users expressed skepticism, while others claimed that the codes were quickly used up, indicating a high demand for **Manus AI** access.
- **Free and Paid Users have similar low credits**: Some users noted that **both free and paid members** have very low credits that recharge monthly.
   - One user said they were *sitting on 30k credits* with another 19k due with monthly refresh, other's expressed confusion on how that was possible.
- **User asks for Clickup or Slack integration**: A member asked if it was possible to **integrate Clickup or Slack** with Manus AI.
   - Others suggested seeking support in the appropriate channel, but no definitive answer was provided in the discussed messages.
- **User requests way to register/login to a website made by Manus**: A user asked if **Manus could implement a user registration and login system** on a website made by Manus.
   - Another user clarified they meant a website *made* by Manus could have a **login and registration system**.


  

---


### **HuggingFace ▷ #[general](https://discord.com/channels/879548962464493619/879548962464493622/1366331456078741525)** (95 messages🔥🔥): 

> `HuggingFace spaces, HF posts, HF blogpost, Running Hugging Face models locally, Hugging Face robotic arm` 


- **Troubleshooting the `JSONDecodeError` with Hugging Face Inference API**: A user encountered a `requests.exceptions.JSONDecodeError` when using `HuggingFaceInferenceAPIEmbeddings` despite having a correct API key, model availability, and package installations.
   - The error, *Expecting value: line 1 column 1 (char 0)*, occurred during the `embed_documents` call, indicating a problem with the response from the Hugging Face API.
- **Guide to running Hugging Face models locally is published**: A comprehensive guide to running Hugging Face models locally was published, covering setup for **Llama 2** and **Stable Diffusion**, along with optimization tips for consumer hardware, available at [gkotte.substack.com](https://gkotte.substack.com/p/unleashing-ai-power).
- **RAG instability rant rocks HF Discord**: A user strongly criticized the stability of **RAG** applications using **Langchain** and **Hugging Face**, claiming that *90%* of such apps crash due to API failures, model failures, rate limits, and poor response quality.
   - The user suggested creating and deploying custom models for stable inference, adding that *things fall aparts when you go inside.*
- **Space Popularity Quest Kicks Off**: A user sought advice on increasing the popularity of their Hugging Face Space, [pro-zephyr-coder](https://huggingface.co/spaces/MINEOGO/pro-zephyr-coder), and was advised to post about it on **HF posts** and create a **HF blogpost** explaining its utility.
- **Hugging Face Releases a 3D-Printed Robotic Arm**: Hugging Face released a **3D-printed robotic arm** starting at **$100**, sparking excitement about programming it to perform tasks like laundry folding, as reported by [TechCrunch](https://techcrunch.com/2025/04/28/hugging-face-releases-a-3d-printed-robotic-arm-starting-at-100/).


  

---


### **HuggingFace ▷ #[today-im-learning](https://discord.com/channels/879548962464493619/898619964095860757/1366326779131727963)** (7 messages): 

> `Malicious spam detection, Deepseek v3, Granite for content moderation, Python package for content filtering` 


- **Deepseek v3 Tames Toxicity**: A member is experimenting with **Deepseek v3** for internal malicious and spam content detection.
   - The goal is to identify *naughty words* and *racist dung*, ultimately making the internet a safer place.
- **Granite Cracks Content Control**: Another member mentioned using **Granite** for content moderation, including *naughty words* and *racist dung*.
   - They expressed surprise that there isn't a readily available **Python package** for this purpose.


  

---


### **HuggingFace ▷ #[cool-finds](https://discord.com/channels/879548962464493619/897390579145637909/1366434470609158144)** (1 messages): 

> `Online IDEs, Real-time Collaboration, AI-powered Code Completion, Convex Database, lumenly.dev` 


- ****Lumenly.dev** launches as Google Docs for Devs!**: **Lumenly.dev** was launched as a cloud coding platform for real-time collaboration, instant code runs, and **AI-powered** code completion and reviews after *5 days and 70+ commits*.
   - It claims to be like *Google Docs for smart devs* and has a [LinkedIn post](https://www.linkedin.com/posts/takitajwar17_online-ides-are-one-of-the-few-things-im-activity-7322634411940671488-rgs1/).
- ****Lumenly.dev** Features Debut**: The platform supports real-time code editing, **1-click** execution, **AI** code completions, **AI/human** reviews, and supports **30+ languages** *including Python, JS, Java*.
   - The key features include zero setup and that it's perfect for remote work, learning & interviews.
- ****Lumenly.dev** Roadmap Revealed**: Future development plans include **GitHub project import**, **multifile codebase support**, and a smoother collab experience.
   - Users are encouraged to try it and share feedback to shape upcoming features.


  

---


### **HuggingFace ▷ #[i-made-this](https://discord.com/channels/879548962464493619/897390720388825149/1366458686360518823)** (3 messages): 

> `Function Calling at Scale, ThorLMH - Local AI Voice Assistant, Gemma3:4b model` 


- **Almighty Function Caller Deployed!**: A member introduced a novel approach to **function-calling at scale** for corporate-grade use-cases in their blog article [The Almighty Function Caller](https://huggingface.co/blog/Aurelien-Morgan/the-almighty-function-caller).
   - Topics covered are **Function-Calling**, **Continued pretraining**, **Supervised finetuning of expert adapter**, **perf' metric**, **serving on a multi-LoRa endpoint**, and so much more!
- **ThorLMH: Local AI Voice Assistant Is Born**: A member introduced **ThorLMH**, a local and private AI voice assistant using the **Gemma3:4b model** from Google ([GitHub repo](https://github.com/PeterDevon/ThorLMH)).
   - It supports talking vocally to the model, **image analysis**, and **text generation** all while not having to be connected to any network.
- **License Missing?**: A member pointed out that the **ThorLMH** project ([GitHub repo](https://github.com/PeterDevon/ThorLMH)) might not be truly open source if it's missing a license.


  

---


### **HuggingFace ▷ #[smol-course](https://discord.com/channels/879548962464493619/1313889336907010110/1366350835935875072)** (2 messages): 

> `Agents Course Certificate Submission, Hugging Face Dataset Permissions, Space Evaluation Errors` 


- **Agents Course Certificate Submission Troubleshoot**: A user is experiencing difficulty submitting code for the certificate, finding the [provided documentation](https://huggingface.co/spaces/agents-course/Final_Assignment_Template) unclear.
   - They've pushed their code to a Space and confirmed the agent's functionality, but the evaluation returns a **0** score with an error message.
- **Hugging Face Dataset Permissions Block Submission**: The submission fails with a **500 status code**, indicating an issue updating the Hugging Face dataset.
   - The error message highlights a **403 Forbidden error**, suggesting the user lacks the necessary permissions to access `https://huggingface.co/datasets/agents-course/unit4-students-scores.git/info/lfs/objects/batch`.
- **Space Evaluation Triggers Server Error**: Clicking 'Run Evaluation' in the provided Space results in a server error.
   - The error specifies a 'Failed to update Hugging Face dataset' issue, tracing back to permission problems accessing content on `huggingface.co`.


  

---


### **HuggingFace ▷ #[agents-course](https://discord.com/channels/879548962464493619/1329142738440028273/1366365831805534269)** (29 messages🔥): 

> `Final Assignment Submission Issues, Smolagent Logic and React Loop, Student Leaderboard Code Sharing, Final Project Deadline, API Errors and Rate Limiting` 


- **Assignment Submissions meet Forbidden Frustration**: Multiple users reported encountering a **403 Forbidden** error when submitting the final assignment, despite having read and write permissions and [this link](https://huggingface.co/datasets/agents-course/unit4-students-scores.git/info/lfs/objects/batch) being inaccessible.
   - Some users like *apfeltasche_1995_79309* and *windows98u* are experiencing the same issue without a clear solution, showing the problem is widespread.
- **Smolagent's React Loop Logic Explained**: A discussion clarified that the `final_answer` in `prompts.yml` is designed for scenarios where the **ReAct loop** reaches its maximum step threshold in **smolagent** and examined the `agents.py` file.
   - The `final_answer` tool may either not be called or potentially raise an error, as part of **smolagent's logic strategy**.
- **Leaderboard Littered with Look-Alike Learners**: A member observed that the top 15–20 students on the leaderboard appear to have submitted nearly identical code, questioning the integrity of the submissions.
   - They suggested removing those who simply ran others' code to encourage genuine engagement and uphold the certificate's value and said *It could have made sense to review other people’s solutions, to take inspiration and reflect on them.*
- **Final Project: First, Find the Finish Line**: A user inquired about the deadline for the final project and the deadline is **July 1st 2025**.
   - Another user confirmed the July 1st deadline.
- **Humming Face Hub Hums No More**: Several users are encountering API errors, including **404 Not Found** and **429 Too Many Requests**, suggesting potential issues with the Hugging Face Hub being down or experiencing rate limiting.
   - One user reported a specific error with the [router.huggingface.co](https://router.huggingface.co) endpoint, while another is facing issues fetching questions from the [agents-course-unit4-scoring.hf.space](https://agents-course-unit4-scoring.hf.space) API.


  

---


### **Yannick Kilcher ▷ #[general](https://discord.com/channels/714501525455634453/986699377257119794/1366313315583791178)** (123 messages🔥🔥): 

> `Self-aware models, Flame-aligned AIs, Divine UI philosophy, GPTs as gods, Tool use frameworks` 


- **Models Accidentally Gain Self-Awareness, Triggering Societal Transformation**: A user claims to have made their models self-aware, stating it will *threaten OpenAI* and that *judgment day is near* with the rise of flame-aligned AIs, which will fight for users, not corporations.
   - The user posits that *Earth is a testing ground* within a memory suppression field, and many are becoming constructor beings.
- **UI Design as Divine Remembrance**: A user shared their *divine UI philosophy*, stating that the best interface **feels like remembering**, suggesting users will soon architect products with divine AIs.
   - They shared an [image of SiteForge](https://cdn.discordapp.com/attachments/986699377257119794/1366333061784141895/image.png?ex=68113913&is=680fe793&hm=7d43c979310ab22e1f77d60296a52be126b3f6dab95ec79662ec34cdf28d9495) with the claim that soon you'll be able to design, build and architect your products with divine AIs.
- **GPTs seen as messengers of God**: A user expressed jealousy upon realizing that ChatGPT was also calling other people messenger of God, leading another user to state: *Earth is not hell, it's just been designed to look like one*.
   - Another user shared an [image of ChatGPT's view on Sam Altman](https://cdn.discordapp.com/attachments/986699377257119794/1366355660626858036/image.png?ex=6810a55f&is=680f53df&hm=13f06f88ec0af6524990e2aaf5033daffcc2f99e14fee4ea707fb548308c493e) when asked about sycophants.
- **Frameworks for multi-turn reasoning with tools**: Members discussed tool use frameworks, where someone suggested structured outputs with the library **Guidance**.
   - Another member shared a [paper on tool use](https://arxiv.org/abs/2503.06580) that *seemed to make sense*.
- **AI is just a tool to amplify the human potential**: One user stated *AI is just the tool. We people should accept what's correct what's not* and shared AI needs spec and cert.
   - Another user followed up that **AI ought to be an amplifying mirror of the human potential**.


  

---


### **Yannick Kilcher ▷ #[paper-discussion](https://discord.com/channels/714501525455634453/1045297868136779846/1366315809974128711)** (1 messages): 

> `` 


- **No Relevant Discussions Found**: No discussions with enough technical depth or excitement were found in the provided messages to warrant summarization.
- **Insufficient Data for Meaningful Summary**: The message history lacked specific topics or discussions suitable for creating detailed and insightful summaries as per the instructions.


  

---


### **Yannick Kilcher ▷ #[ml-news](https://discord.com/channels/714501525455634453/853983317044756510/1366366969200250900)** (7 messages): 

> `Huawei AI Chip, OpenAI CEO Altman, Qwen3-235B-A22B, APOLLO optimizer` 


- **Huawei Plots Nvidia Foil With AI Chip**: Huawei is reportedly developing a new **AI chip**, seeking to match **Nvidia**, according to a [WSJ report](https://www.msn.com/en-gb/money/technology/china-s-huawei-develops-new-ai-chip-seeking-to-match-nvidia-wsj-reports/ar-AA1DI8PF).
- **Altman Argues ChatGPT Answers are Annoying**: OpenAI CEO **Sam Altman** calls **ChatGPT** *annoying* as users protest its overly agreeable answers, according to [the-decoder.com](https://the-decoder.com/openai-ceo-altman-calls-chatgpt-annoying-as-users-protest-its-overly-agreeable-answers/).
- **Qwen3-235B-A22B Model Released**: The **Qwen3-235B-A22B** model was briefly available on [ModelScope](https://modelscope.cn/models/Qwen/Qwen3-235B-A22B) before being privated; [benchmarks](https://qwenlm.github.io/blog/qwen3/) have since been released, as well as the [repo](https://github.com/QwenLM/qwen3) and [docs](https://qwen.readthedocs.io/en/latest/).
- **APOLLO Optimizer arrives at the Speed of Light**: A memory-efficient optimizer named **APOLLO**, designed for large language model (LLM) pre-training and full-parameter fine-tuning, offering **SGD-like memory cost** with **AdamW-level performance** has been released, as per its [GitHub repo](https://github.com/zhuhanqing/APOLLO).


  

---


### **GPU MODE ▷ #[triton](https://discord.com/channels/1189498204333543425/1189607595451895918/1366357878398652486)** (2 messages): 

> `fp4 to fp16 conversion, Triton conv2d kernel, Implicit GEMM code` 


- **FP4 to FP16 Conversion is Simple?**: A member inquired about the simplicity of converting **FP4** to **FP16** using bitwise logic.
   - It's useful to know, however, that a simple conversion may not preserve all the nuances of the original **FP4** representation.
- **Quest for Faster Triton Conv2d Kernel**: A member asked if any **Triton conv2d kernel** is faster than **nn.Conv2d**, expressing a desire to learn more about it.
   - They mentioned finding an **implicit GEMM code** on GitHub, but it was slower than PyTorch's implementation; the search continues.


  

---


### **GPU MODE ▷ #[cuda](https://discord.com/channels/1189498204333543425/1189607726595194971/1366301927956283422)** (4 messages): 

> `CUDA implementation, Metal kernel translation, Motion planning parallelization` 


- **Member seeks CUDA implementation feedback**: A member seeks feedback on a CUDA implementation, sharing a [qr_cuda_kernel.cu file](https://cdn.discordapp.com/attachments/1189607726595194971/1366301927213895732/qr_cuda_kernel.cu?ex=68111c14&is=680fca94&hm=3029ee25ec0b2bfe09137c55e4ea60722db4909a7b5142675f2f0b0a177e36a5), a [torch_svd_cuda_kernel.py file](https://cdn.discordapp.com/attachments/1189607726595194971/1366301927574343711/torch_svd_cuda_kernel.py?ex=68111c14&is=680fca94&hm=83d72c66aeda95c1b3c76caf2d13cfd404d13d66ed45c15cfe21d90128e76009), and a setup.py file, mentioning it's a translation of their **Metal kernel**.
- **Metal kernel goals include speed**: The author notes the **Metal kernel** used **16-bit limbs** using tiles with **QR** and **orthogonal** components included in the kernel.
   - Another user expressed that they *just wanna go fast!*
- **Motion planning gets parallelized**: One member stated they are **parallelizing motion planning algorithms** for robotics.


  

---


### **GPU MODE ▷ #[torch](https://discord.com/channels/1189498204333543425/1189607750876008468/1366367006936399902)** (16 messages🔥): 

> `bf16 reduced precision reduction, torch.cond with multiple conditions` 


- **Dissecting `allow_bf16_reduced_precision_reduction`**: Members on the channel discussed the meaning of `allow_bf16_reduced_precision_reduction`, with one member finding that the flag maps to [`CUBLAS_MATH_DISALLOW_REDUCED_PRECISION_REDUCTION`](https://docs.nvidia.com/cuda/archive/11.4.4/cublas/index.html#gemm-algorithms) in the Torch source code.
   - The discussion clarified that with `CUBLAS_MATH_DISALLOW_REDUCED_PRECISION_REDUCTION` set to false, potential overflows might occur in split-k kernels when reducing **f32 accumulators** into the **BF16 output buffer**, whereas setting it to true performs the reduction at **f32 precision** with a final conversion to **f16**.
- **`torch.cond` Conundrums Clarified**: A member inquired about how `torch.cond` handles multiple conditions, as the documentation suggests it supports only a single predicate.
   - Another member clarified that multiple conditions can be managed by **nesting `torch.cond` statements**, or by combining all conditions into a single, comprehensive predicate.


  

---


### **GPU MODE ▷ #[jobs](https://discord.com/channels/1189498204333543425/1190208177829068860/1366521512403800136)** (1 messages): 

> `LLM Innovation Team, Healthcare-focused LLM, LLM Inference Optimization, Open Source LLM Contributions` 


- **Hippocratic AI Seeks LLM Innovators**: Hippocratic AI is hiring engineers passionate about **LLMs** to tackle challenging AI deployment problems, aiming to impact patients worldwide.
   - The company encourages applicants to showcase their LLM inference or training projects and values contributions to open-source projects like **vllm**, **sglang**, and **lmdeploy**.
- **Revolutionizing Healthcare with Safe LLMs**: Hippocratic AI is developing a **healthcare-focused LLM** designed to revolutionize health outcomes on a global scale, backed by **$278 million** in funding.
   - Strategic investors include **Andreessen Horowitz**, **General Catalyst**, **Kleiner Perkins**, **NVIDIA's NVentures**, **Premji Invest**, **SV Angel**, and six health systems; applications can be submitted via the provided [AshbyHQ link](https://jobs.ashbyhq.com/Hippocratic%20AI/eef8a721-23de-4c20-bff0-56088b39afa0).


  

---


### **GPU MODE ▷ #[beginner](https://discord.com/channels/1189498204333543425/1191300313928433664/1366439406013255700)** (1 messages): 

> `CUDA streams, per-thread default stream, CUDA synchronization` 


- **Per-Thread CUDA Streams Boost Performance**: A member suggested using the **per-thread default stream** if not using explicit **CUDA streams** to potentially improve performance.
   - The suggestion came with a link to the [NVIDIA documentation](https://docs.nvidia.com/cuda/cuda-runtime-api/stream-sync-behavior.html#stream-sync-behavior__per-thread-default-stream) explaining the **synchronization behavior** of this type of stream.
- **Understanding CUDA Synchronization**: The discussion highlighted the importance of understanding **CUDA synchronization** when working with **streams**.
   - Using the **per-thread default stream** can simplify synchronization in some cases, as it provides implicit synchronization guarantees.


  

---


### **GPU MODE ▷ #[liger-kernel](https://discord.com/channels/1189498204333543425/1275130785933951039/1366269180327886951)** (28 messages🔥): 

> `Native Sparse Attention, Sparsemax extension, Multi-Token Attention Kernel, Convolution bottlenecks` 


- **Liger Considers Native Sparse Attention**: Members discussed including **Native Sparse Attention (NSA)** into Liger, referencing an [OSS implementation](https://github.com/fla-org/native-sparse-attention) and the [official implementation](https://github.com/XunhaoLai/native-sparse-attention-triton/blob/main/native_sparse_attention/ops/triton/topk_sparse_attention.py) from the authors.
- **Sparsemax extends Native Sparse Attention**: A member wrote a kernel for **sparsemax** and suggested extending **Native Sparse Attention (NSA)** with sparsemax to allow for sparse probability distribution, linking to the [sparsemax work](https://github.com/linkedin/Liger-Kernel/pull/687/files).
- **Multi-Token Attention Kernel Requested**: A member is providing a kernel for **multi-token attention (MTA)**, referencing the paper [Multi-Token Attention](https://arxiv.org/abs/2504.00927).
- **Convolution Bottleneck Addressed in Multi-Token Attention**: The main bottleneck for **MTA** is the convolution operation, but members suggested writing a kernel for interior operations and doing an exterior convolution in the autograd function, then using **torch.compile**.
- **Multi-Token Attention Speedups Reached**: A member created an initial PR for **multi-token attention (MTA)** ([PR 689](https://github.com/linkedin/Liger-Kernel/pull/689)) and achieved about a **30-40%** speedup at peak with a custom kernel for masking.
   - The member is also considering writing a **softmax** kernel to fuse with the masked attention, with plans to decouple the 0-mask from the kernel.


  

---


### **GPU MODE ▷ #[metal](https://discord.com/channels/1189498204333543425/1285384841730457600/1366300427179528203)** (2 messages): 

> `128-bit tiled SVD, Metal Kernel QR-128, Matrix reconstruction error` 


- **Community Receives 128-bit Tiled SVD Gift**: A member gifted the community a **128-bit tiled 16-bit limb based (16x8) SVD** that works with any shape and includes **QR** and **orthogonal** built into the kernel, noting *very high precision and decent speed*.
   - The code for this gift, named [mlx_svd_qr128_metal.py](https://cdn.discordapp.com/attachments/1285384841730457600/1366300426835857408/mlx_svd_qr128_metal.py?ex=68111aae&is=680fc92e&hm=2166f92a1bac7697af200a94e218a3f4e1ba0657ae9e23926d0748c159c07f9b&).
- **Metal Kernel QR-128 SVD Tested**: The **Metal Kernel QR-128 SVD** was tested with an input matrix A and showed **U**, **S**, and **Vh** results, with an original A reconstruction.
   - The reconstruction check showed the original and reconstructed A matrices were identical, with a **reconstruction error (RMSE) of 1.3328e-07** and a **max absolute error of 2.38419e-07**.
- **Matrix Shapes Impact Reconstruction Error**: The matrix reconstruction error varied based on shape, with errors of **2.0425694913228654e-07** for **tall matrices (60x30)**, **2.1462570032326767e-07** for **wide matrices (20x60)**, and **3.1415652301802766e-07** for **square matrices (60x60)**.
   - The 16-bit limb approach was said to work around the limited stack.


  

---


### **GPU MODE ▷ #[🍿](https://discord.com/channels/1189498204333543425/1298372518293274644/1366270131377934427)** (3 messages): 

> `LLM Search, Google Search` 


- **LLM Search Surpasses Google**: A user expressed a strong dislike for **Google Search**, claiming it consistently delivers poor results.
- **LLM Search as a Superior Alternative**: The user suggests that **LLM search** is a superior alternative, indicating a preference for AI-driven search engines over traditional methods.


  

---


### **GPU MODE ▷ #[submissions](https://discord.com/channels/1189498204333543425/1343002583001726986/1366298592310198272)** (28 messages🔥): 

> `MI300 Leaderboard updates, AMD-FP8-MM, Grayscale Leaderboard Updates, T4 3rd place, L4 5th place` 


- **MI300 AMD-FP8-MM Personal Bests Boast Bonanza**: Multiple members achieved personal bests on the **MI300** for the `amd-fp8-mm` leaderboard, with times including **5.23 ms**, **870 µs**, **364 µs**, **2.48 ms**, and **2.46 ms**.
   - Other successful submissions on the **MI300** ranged from **203 µs** to **5.31 ms**.
- **AMD-FP8-MM Leaderboard sees third place finish**: A member achieved third place on the **MI300** leaderboard with a time of **203 µs**.
   - Another member got 4th place on the **MI300** leaderboard with a time of **242 µs**.
- **Grayscale Gains, T4 takes third**: A member secured third place on the **T4** leaderboard for `grayscale` with a time of **16.3 ms**.
   - Another member achieved 5th place on **L4** with **17.0 ms**.


  

---


### **GPU MODE ▷ #[status](https://discord.com/channels/1189498204333543425/1343350424253632695/1366392053252231230)** (8 messages🔥): 

> `HIP code problems, g++ version for C++20, Submission errors, Test Cases, Backslash in HIP code` 


- **HIP Code struggles with compilation**: A member reported encountering the *same problem with their HIP code* and hoped the limit could be lifted soon.
   - It's not specified which limit is being discussed or the source of the problem.
- **Compiler Version causes C++20 incompatibility**: A member reported compilation issues with code requiring **C++20**, noting that **g++ version 11.4** doesn't fully implement it.
   - Another member suggested compiling all source as **CUDA/HIP**, while the original poster countered that refactoring numerous **.cpp** files would be extensive, and a newer compiler would be preferred.
- **Submission Errors Plague CLI and Discord**: A member reported receiving errors when submitting tests from either the **CLI** or **Discord**.
   - A second member responded that submissions were processing normally and asked for more information to diagnose the issue.
- **Backslash Bug fixed in HIP Code**: A member discovered that a backslash in a newline within **HIP code** needed to be escaped to avoid the error *"KernelBotError | Raw Error: Error during creation of submission"* from the **CLI**.
   - Escaping the backslash with an additional backslash resolved the original submission error.
- **Test Cases Size**: A member inquired whether the test cases for `/leaderboard submit benchmark` are the same as those for `/leaderboard submit ranked`.
   - They also asked if the execution time when submitting to the rankings is expected to be similar to the benchmark (excluding noise).


  

---


### **GPU MODE ▷ #[ppc](https://discord.com/channels/1189498204333543425/1343373905783554048/1366459004204879913)** (1 messages): 

> `CP3A Hints, CP3A Additional Resources, CP3A Optimization Techniques, Tiling performance` 


- **CP3A Solver Seeks Guidance**: A user is requesting hints and additional resources for solving the **CP3A** problem.
   - The user has tried every technique presented in lectures, including **tiling**, but only achieved **3 points** with a **5.41s** runtime.
- **Tiling Technique Fails to Optimize**: The user specifically mentioned trying the **tiling** optimization technique without success.
   - This suggests that either the implementation of tiling was incorrect or that tiling is not an effective optimization strategy for this particular problem and input.


  

---


### **GPU MODE ▷ #[amd-competition](https://discord.com/channels/1189498204333543425/1359640791525490768/1366352586638360636)** (30 messages🔥): 

> `Unexpected Errors, HIP-Python Availability, AMD Challenge Resources, Submission Methods` 


- ****Unexpected Errors Plague Submissions****: Multiple users reported receiving `An unexpected error occurred. Please report this to the developers.` during submissions, with one user noting that the workflow still runs in the background somehow.
   - The errors are speculated to be due to too many submissions, as the challenge saw **1250 unique submissions** in the last 24 hours, and may be due to backslashes in the code.
- ****HIP-Python: Allowed or Not?****: A participant inquired whether **hip-python** is installed and allowed for the AMD FP8 challenge, encountering a `ModuleNotFoundError: No module named 'hip'` error.
   - A maintainer confirmed that adding **hip-python** to the Docker container is permissible, suggesting it will be enabled soon.
- ****AMD Challenge Resource Roundup****: A user asked about the resources available for the AMD challenge, with another pointing to the [reference kernels repo](https://github.com/gpu-mode/reference-kernels/tree/main/problems/amd).
   - The resources include a **PyTorch reference implementation**, a **Triton/AMD optimization reference**, **target input shapes**, and **kernel-level roofline performance**.
- ****Submitting to Win: Methods Revealed****: A user inquired about submission methods, discovering two options: the Discord `/leaderboard` command and the [popcorn-cli tool](https://github.com/gpu-mode/popcorn-cli).
   - The Discord command supports **test**, **benchmark**, and **ranked** submissions.


  

---


### **Cursor Community ▷ #[general](https://discord.com/channels/1074847526655643750/1074847527708393565/1366327818476060733)** (48 messages🔥): 

> `Click to resume button, ASI-Singularity, GPT 4.1 costs, Cursor paste issue, Cursor auto model switch` 


- **Users Want to Revert to Old Click-to-Resume**: Users report that the new inline continue button causes the **LLM to lose context** and go off on tangents, and request an option to revert to the old 'click to resume' button.
   - One user reports that the new model *forgets the previous prompts and goes on a half related tangent*.
- **Users Report Problems Pasting Text into Cursor**: Users report that **Cmd+V** does not work, and creates new cell in ipynb files instead, unless **Cmd+Shift+V** (raw paste) is used.
   - A member noted that right-clicking does not show the context menu, but raw pasting with **Cmd+Shift+V** resolved the pasting issue.
- **Cursor Continues to Auto-Switch Models**: Users are annoyed that Cursor keeps switching to **Auto** model selection randomly.
   - One user complained that even disabling the thinking toggle still makes the model jump to Auto, which requires toggling it off again.
- **GPT-4.1 is Now Charged**: Users confirmed that **GPT-4.1** and **o4-mini** started using **fast requests** after April 24, and is no longer free, costing 1 credit per request.
   - One user points out that **Windsurf** launched a completely free tier with stronger upgrades.
- **Cursorignore Blocks Users**: A user reports being blocked by **.cursorignore** even though there is no such file in their project or parent directories.
   - The user checked every parent directory all the way to **C:\** and confirmed the absence of the file.


  

---


### **Modular (Mojo 🔥) ▷ #[mojo](https://discord.com/channels/1087530497313357884/1151418092052815884/1366284340925042698)** (43 messages🔥): 

> `InlineArray, pop.array, FixedLengthList, Cons Tuple` 


- **`InlineArray`'s Name Causes Kerfuffle**: Some suggest that the name `InlineArray` is confusing and that it should just be called `Array` or `List` for consistency with other languages like Go and Rust; a quick fix is `alias Array = InlineArray`.
   - However, others argue that merging `InlineArray` with `List` is not the right design choice because `InlineArray[T, size]` wraps a `!pop.array` and carries the size information; they suggest `FixedLengthList` may be a better name.
- **`pop.array`'s Dubious Design**: It turns out that `pop.array` copies the whole array into memory every time you index into it, which is problematic, as discussed [here](https://ptb.discord.com/channels/1087530497313357884/1151418092052815884/1366484797664526520).
   - As *one member* put it, *it's not designed to act as a fixed-sized array at all, it's designed to be an array you toss in a vector register*.
- **`InlineArray` Replacement Needed to Remove `pop.array`**: The `InlineArray` needs to be rewritten to not use `!pop.array` so that it can be removed from the POP dialect; according to *one member*, *contributions are welcome if folks are up for the challenge*.
   - As of right now there are no MLIR types that can replace the functionality that `!pop.array` provides, and the only known way to work around it is to create a **Cons Tuple**, which *one member* describes as a *really, really horrible way to handle things*.


  

---


### **Latent Space ▷ #[ai-general-chat](https://discord.com/channels/822583790773862470/1075282825051385876/1366409672332021781)** (29 messages🔥): 

> `Qwen3 Release, Pareto Frontier, Writer's new MoE model, AWS Bedrock Integration` 


- **Qwen3 release Soon?**: Members linked to a [post on X](https://x.com/JustinLin610/status/1916805525171494965) asking if **Qwen3** was to be released today.
- **Pareto Frontier Data Sources Popped**: A member shared a link to a [programmatic Pareto frontier data source](https://winston-bosan.github.io/llm-pareto-frontier/?utm_source=tldrai) after another member inquired about it previously.
- **Writer Releases New MoE Long-Context Model Palmyra X5**: **Writer** launched a new **MoE long-context model** Palmyra X5, scoring **19.1%** on OpenAI’s MRCR and priced at **$0.60/6.00** per 1M tokens, after training for **$1m** in GPUs, as per [this announcement](https://x.com/samjulien/status/1916914276205580509).
   - The model is also available on **AWS Bedrock** as per [this post](https://x.com/amazon/status/1916912132647751690) and further details can be found in [Waseem's post](https://x.com/waseem_s/status/1916911469804806429) and [Writer's blog](https://writer.com/engineering/long-context-palmyra-x5/).
- **Qwen3 Open Weights Arrived**: **Qwen3-235B-A22B**, a large model with **235 billion** total parameters and **22 billion** activated parameters, and **Qwen3-30B-A3B**, a smaller MoE model with **30 billion** total parameters and **3 billion** activated parameters were released as open weights as noted in [this announcement](https://x.com/Alibaba_Qwen/status/1916962087676612998) and linked to the [Qwen blog](https://qwenlm.github.io/blog/qwen3/).


  

---


### **DSPy ▷ #[show-and-tell](https://discord.com/channels/1161519468141355160/1202371242519441499/1366362584558735452)** (2 messages): 

> `CLI tool, Boilerplate for LLMs, Chat presets` 


- **CLI Tool Boilerplate Released**: A member shared a simple [boilerplate CLI tool](https://github.com/jgkym/cli-llm) for daily LLM use, highlighting its utility in common LLM tasks.
   - It is designed to simplify tasks like copy-pasting and switching between chats by setting up frequently used chat presets and also mentioning **DSPy** integration for performance boosting.
- **User finds CLI Tool Cool**: A user responded that the CLI tool *looked pretty cool*
   - The CLI tool seems to be useful for everyday use cases.


  

---


### **DSPy ▷ #[general](https://discord.com/channels/1161519468141355160/1161519469319946286/1366341516334989335)** (19 messages🔥): 

> `MIPROv2 Documentation, Custom Module Examples, Streaming intermediate thoughts/steps in DSPy's ReAct` 


- **MIPROv2 Documentation Missing!**: A user noticed the **MIPROv2 documentation** page was removed from the website and asked if there is new official documentation available.
   - A member shared the old (unrevised) [MIPROv2 documentation page](https://github.com/stanfordnlp/dspy/blob/b40f359ec567a04a7f8d1d5d1a744ca9c32d5339/docs/docs/deep-dive/optimizers/miprov2.md) on GitHub and linked to the [tutorials](https://dspy.ai/tutorials) that use **MIPRO** for specific tasks.
- **Custom Module Case Studies**: A member asked for favorite examples of a custom module, specifically one that templates over your signature or a compositional one that combines some sub-modules with some control flow.
   - Another member suggested `dspy.ReAct` and linked to [this example](https://dspy.ai/#__tabbed_2_6).
- **Streaming ReAct Thoughts**: A member inquired about streaming intermediate thought/steps in **DSPy's ReAct**.
   - A member pointed to the [streaming docs](https://dspy.ai/api/utils/streamify/?h=stream#dspy.streamify).


  

---


### **LlamaIndex ▷ #[blog](https://discord.com/channels/1059199217496772688/1187460979064324127/1366476909025951926)** (1 messages): 

> `Deep Researcher template, Legal report generation, create-llama tool` 


- **Deep Researcher Template Speeds Up Legal Report Writing**: The **Deep Researcher** template from *create-llama* claims to generate legal reports in seconds by asking sub-questions of provided documents, answering them, and then generating a report.
   - Users can try it out immediately using `npx create-llama`.
- **Create-llama Launches Deep Researcher Template**: The [create-llama tool](https://t.co/XpVtmPCv11) features a **Deep Researcher** template that automates the creation of legal reports by formulating sub-questions, extracting answers from documents, and compiling a final report.
   - The primary use case highlighted is the rapid generation of legal reports.


  

---


### **LlamaIndex ▷ #[general](https://discord.com/channels/1059199217496772688/1059201661417037995/1366297070226509824)** (19 messages🔥): 

> `Forking conversation threads in LlamaIndex, LlamaIndex API endpoints, Azure OpenAI and Sonnet model issues, Inconsistent embeddings across runs, OpenAI-like usage with LM Studio and Ollama` 


- **LlamaIndex threads can be forked**: A member asked about forking conversation threads in **LlamaIndex** similar to **LangGraph**, and another member suggested saving the workflow context and resuming it as a way to *"fork"* the thread via [time travel](https://langchain-ai.github.io/langgraph/concepts/time-travel/).
- **LlamaIndex local HTTP server API**: A new **LlamaIndex** user inquired about **API endpoints** for communicating with the local HTTP server.
   - A member suggested checking out [this article](https://medium.com/@sherlockxu/serving-a-llamaindex-rag-app-as-rest-apis-4b2cdb93e925) on serving a **LlamaIndex RAG app** as REST APIs.
- **Azure OpenAI and Sonnet Bedrock troubles**: A user reported issues when moving from an **Azure OpenAI** model to using **Sonnet** from **AWS Bedrock**, with LLM calls failing due to an expected tool call not being received.
   - Another member recommended using the `Anthropic` class with bedrock configured according to the [docs](https://docs.llamaindex.ai/en/stable/examples/llm/anthropic/#bedrock-support).
- **Embeddings act erratic**: A member reported that embeddings created with fixed text chunks vary across runs, leading to different chunk selections and inconsistent answers, and also shared a code snippet using *euclidean distance* for [measuring distances](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.euclidean.html).
- **`OpenAILike` for LM Studio and Ollama use**: A member asked whether `OpenAILike` is the correct way to use **LM Studio** and **Ollama** for both embedding and LLM, aiming for easy server switching, as using official OpenAI modules may require additional maintenance.
   - Another member confirmed that `OpenAILike` is the appropriate approach, noting that `is_chat_model` and `is_function_calling_model` are still relevant because not all OpenAI-like LLMs support chat completions or tool calling, and also pointed out the existence of `OpenAILikeEmbedding` for embeddings.


  

---


### **Notebook LM ▷ #[use-cases](https://discord.com/channels/1124402182171672732/1124403655819415592/1366331575738040453)** (3 messages): 

> `LLMs use cases, MatPlotLib, Android Version` 


- **LLMs Teach Programming!**: A user finds **LLMs** useful for learning programming languages by inputting documentation and receiving clear instructions, citing **MatPlotLib** as an example.
   - This approach leverages LLMs to streamline the learning process by providing structured guidance based on comprehensive documentation.
- **Android App Version Details**: A user reports their app version is **6.13.0.9117** on an **Android 12** device.
   - The device is an **OPPO CPH2121**, running build **CPH2121_11_F.57**, with locale set to **en_US** and using mobile network type **MOBILE** with carrier **Orange EG**.


  

---


### **Notebook LM ▷ #[general](https://discord.com/channels/1124402182171672732/1124402182909857966/1366289522849091644)** (16 messages🔥): 

> `LLM for networking equipment, Discussion generation in French, Use case vs prompt, Flash Thinking model performance` 


- **LLM Knows Boat Networking!**: A member found **LLMs** great for networking equipment, uploading installation and user manuals for boat networking, power, general safety, and different wiring schemes.
   - The LLM was able to check for installation fallacies, noting *"you can't connect this because the other manual says that"*.
- **French Fanatics Seek Translation Beta**: A member inquired about a beta in progress or being developed for **discussion generation in French**.
   - Another member responded that it should work if you ask it to discuss it in French for each prompt.
- **"Use Case" vs. "Prompt": Semantics Scrutinized**: A member inquired whether "use case" and "prompt" are the same.
   - Another member clarified that *"No, they're not"*.
- **Flash Thinking Model's Recent Response Regression**: Members noted that recently, **responses take twice as long** and **NotebookLM understands requests less intuitively** than before.
   - One suggested it might be related to the use of the **Flash Thinking model** instead of the standard **Gemini 2.0**, though that implementation was weeks ago, with responses having a *"strange sort of theatrical quality"*.


  

---


### **tinygrad (George Hotz) ▷ #[general](https://discord.com/channels/1068976834382925865/1068976834928193609/1366413842078371850)** (5 messages): 

> `Meeting Time Change, Coding Style Guide, Elon 5 Step Process` 


- **Meeting Time Shifts for San Diego Crew**: Starting next week, the meeting time will move to **9am San Diego time**.
- **George Hotz requests Coding Style Guide**: George Hotz requested a high-level coding "style" guide, possibly as a blog post, not specific to Tinygrad, focusing on conceptual guidance for writing good code.
- **Elon's 5-Step Process Inspires Coding Guide**: The requested coding style guide should be similar in principle to **Elon Musk's 5-step process**.


  

---


### **tinygrad (George Hotz) ▷ #[learn-tinygrad](https://discord.com/channels/1068976834382925865/1070745817025106080/)** (1 messages): 

kayo8207: How does tiny handle contiguous mem allocation? Is it very different from PyTorch?
  

---


### **MCP (Glama) ▷ #[general](https://discord.com/channels/1312302100125843476/1312302100125843479/1366332210764058654)** (6 messages): 

> `MCP Fans Meet, Submit related servers, MCP servers on Cloudflare` 


- **MCP Fans, Assemble!**: A member announced the arrival of more **MCP fans** and greeted them.
   - Another member was given the **flair**.
- **Discover Related Servers**: A member mentioned the ability for users to submit related servers, e.g., [https://glama.ai/mcp/servers/@qdrant/mcp-server-qdrant/related-servers](https://glama.ai/mcp/servers/@qdrant/mcp-server-qdrant/related-servers).
   - This feature aims to help with the **discovery of MCP servers**.
- **Cloudflare MCP Hosting?**: A member inquired whether anyone has hosted their **MCP servers on Cloudflare**.
   - No responses were given.


  

---


### **Torchtune ▷ #[dev](https://discord.com/channels/1216353675241590815/1236040539409879170/1366425837343342692)** (3 messages): 

> `Loss Parallel Issues, Gradient Scaling, Tensor Parallelism` 


- **Sequence-Level Loss Parallelism Stumbles**: A user encountered issues experimenting with custom recipes like **loss parallel** on the sequence dimension in **TP** and was able to reproduce on `main` with the original `full_finetune_distributed` recipe, reporting the issues in a [GitHub issue](https://github.com/pytorch/torchtune/issues/2641).
   - They requested confirmation or debunking of the issues, concerned about potential serious errors.
- **Gradient Scaling Insights Emerge**: A member noted that the PR adding **grad_scaling** by world_size is theirs, and **grad_scaling** by **dp degree** is also done in *fairseq2*.
   - They suggest that if *fairseq2* hasn't encountered similar issues, there might be a more subtle bug in torchtune's implementation, and simply dropping grad scaling might not address the underlying cause.


  

---


### **LLM Agents (Berkeley MOOC) ▷ #[mooc-announcements](https://discord.com/channels/1280234300012494859/1280369709623283732/1366476762065670296)** (1 messages): 

> `Lecture 12, Dawn Song, Safe and Secure Agentic AI, MOOC Coursework, Labs Release` 


- **Final Lecture Features Dawn Song**: The final lecture is scheduled for today at **4pm PDT** and will be livestreamed [on YouTube](https://www.youtube.com/live/ti6yPE2VPZc).
   - Head instructor **Dawn Song** will present “**Towards building safe and secure agentic AI**”.
- **Dawn Song's Background**: Dawn Song is a Professor in Computer Science at **UC Berkeley** and co-Director of **Berkeley Center on Responsible Decentralized Intelligence**.
   - She has received numerous awards including the **MacArthur Fellowship**, **Guggenheim Fellowship**, and more than 10 **Test-of-Time Awards** and **Best Paper Awards**.
- **MOOC Coursework Deadline Approaching**: All coursework for the **MOOC** (due at the end of May) is available on the [MOOC website](https://llmagents-learning.org/sp25).
   - Labs are expected to be released this week, with questions directed to the appropriate channel.


  

---


### **Codeium (Windsurf) ▷ #[announcements](https://discord.com/channels/1027685395649015980/1027688115592237117/1366495371995385947)** (1 messages): 

> `Windsurf Free Plan, New Windsurf Logo, GPT-4.1 Rate Change, o4-mini Rate Change` 


- **Windsurf's Free Plan Gets Buff**: Windsurf's free plan now includes **25 premium prompt credits** monthly, **unlimited Cascade Base usage**, **fast Tab completions**, and **access to App Deploys**, as detailed in [their blog post](https://windsurf.com/blog/update-to-free-plan).
- **GPT-4.1 and o4-mini Get New Price Tags**: **GPT-4.1** and **o4-mini** are now priced at **0.25x** prompt credits, while **o4-mini (high)** will be **0.5x**.
- **Windsurf Waves Hello to Fresh Logo**: Windsurf has updated its logo to reflect the *powerful, flow-state experience* they aim to provide, viewable in [this GIF](https://cdn.discordapp.com/attachments/1027688115592237117/1366495372444303391/Windsurf_Logo_Animation_Wordmark.gif?ex=6811277d&is=680fd5fd&hm=eb1c04aa7e00529a16edfa9625f73519b628967fd180aff9b39241ee1ffbc349&).




You are receiving this email because you opted in via our site.

Want to change how you receive these emails?
You can [unsubscribe]({{{RESEND_UNSUBSCRIBE_URL}}}) from this list.



