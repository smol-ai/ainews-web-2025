---
id: fc096fb8-c65f-415f-ac11-5edc3c431554
title: 'Genesis: Generative Physics Engine for Robotics (o1-mini version)'
date: '2024-12-19T05:17:10.096057Z'
original_slug: ainews-genesis-generative-physics-engine-for
description: >-
  **OpenAI** launched the **o1 model** API featuring function calling,
  structured outputs, vision support, and developer messages, achieving **60%
  fewer reasoning tokens** than its preview. The model excels in math and code
  with a **0.76 LiveBench Coding score**, outperforming Sonnet 3.5. Beta SDKs
  for Go and Java and WebRTC support with **60% lower prices** were also
  released. **Google Gemini 2.0 Pro (Gemini Exp 1206)** deployment accelerated,
  showing improved coding, math, and reasoning performance. Meta AI FAIR
  introduced research on training transformers directly on raw bytes using
  dynamic entropy-based patching. Commercial humanoid robots were successfully
  deployed by an industry player. **Hugging Face** researchers demonstrated that
  their **3B Llama model** can outperform the **70B Llama model** on MATH-500
  accuracy using search techniques, highlighting efficiency gains with smaller
  models. Concerns about reproducibility and domain-specific limitations were
  noted.
companies:
  - openai
  - google-deepmind
  - meta-ai-fair
  - hugging-face
models:
  - o1
  - o1-preview
  - gpt-4o
  - claude-3.5-sonnet
  - gemini-2.0-pro
  - llama-3-3b
  - llama-3-70b
topics:
  - function-calling
  - structured-outputs
  - vision
  - performance-benchmarks
  - sdk
  - webrtc
  - reasoning
  - math
  - code-generation
  - transformer-architecture
  - model-training
  - humanoid-robots
  - search
  - model-efficiency
  - dataset-sharing
people:
  - aidan_mclau
  - sundarpichai
  - adcock_brett
---


<!-- buttondown-editor-mode: plaintext -->**the old o1-mini version for comparison**

> AI News for 12/17/2024-12/18/2024. We checked 7 subreddits, [**433** Twitters](https://twitter.com/i/lists/1585430245762441216) and **32** Discords (**215** channels, and **4542** messages) for you. Estimated reading time saved (at 200wpm): **497 minutes**. You can now tag [@smol_ai](https://x.com/smol_ai) for AINews discussions!

**You are reading AINews generated by `o1-mini-2024-09-12`. As is tradition on new frontier model days, we try to publish multiple issues for A/B testing/self evaluation. Check our archives for [the o1-2024-12-17 version](https://buttondown.com/ainews/archive/ainews-genesis-generative-physics-engine-for-6175/). We are sorry for the repeat sends yesterday (platform bug) but today's is on purpose.**


---


{% if medium == 'web' %}


**Table of Contents**

[TOC] 

{% else %}

The **Table of Contents** and **Channel Summaries** have been moved to the web version of this email: [{{ email.subject }}]({{ email_url }})!

{% endif %}


---

# AI Twitter Recap

> all recaps done by Claude 3.5 Sonnet, best of 4 runs.

Here are the key discussions organized by topic:

**OpenAI o1 API Launch and Features**

- **o1 model released to API** with [function calling, structured outputs, vision support, and developer messages](https://twitter.com/OpenAIDevs/status/1869156065788715409). Model uses **60% fewer reasoning tokens** than o1-preview and includes a new "reasoning_effort" parameter.

- **Performance Benchmarks**: [@aidan_mclau noted](https://twitter.com/aidan_mclau/status/1869068880326635645) o1 is **"insanely good at math/code" but "mid at everything else"**. [Benchmark results show](https://twitter.com/scaling01/status/1869083247554220353) o1 scoring **0.76 on LiveBench Coding**, compared to Sonnet 3.5's 0.67.

- **New SDKs**: Released [beta SDKs for Go and Java](https://twitter.com/OpenAIDevs/status/1869140165798821942). Also added **WebRTC support** for realtime API with **60% lower prices**.

**Google Gemini Updates**

- [@sundarpichai confirmed](https://twitter.com/scaling01/status/1869072489881747818) that **Gemini Exp 1206 is Gemini 2.0 Pro**, showing improved performance on coding, math and reasoning tasks.

- [Gemini 2.0 deployment accelerated](https://twitter.com/asadovsky/status/1869114982971093316) for Advanced users in response to feedback.

**Model Development & Architecture**

- Discussion around model sizes and training - [debate about whether](https://twitter.com/aidan_mclau/status/1869080913860251911) o1-preview's size matches o1 and relationship to GPT-4o.

- Meta's new research on [training transformers directly on raw bytes](https://twitter.com/LiorOnAI/status/1869409580192555015) using dynamic patching based on entropy.

**Industry & Business**

- [@adcock_brett reported](https://twitter.com/adcock_brett/status/1869235067580764635) successful deployment of commercial humanoid robots at client site with rapid transfer from HQ.

- [New LlamaReport tool announced](https://twitter.com/llama_index/status/1869094544169677138) for converting document databases into human-readable reports using LLMs.

**Memes & Humor**

- [Joke about watching "Attention Is All You Need" re-release in IMAX](https://twitter.com/jxmnop/status/1869154293888258139)

---

# AI Reddit Recap

## /r/LocalLlama Recap

**Theme 1. Hugging Face's 3B Llama Model: Outperforming the 70B with Search**

- **[Hugging Face researchers got 3b Llama to outperform 70b using search](https://i.redd.it/kksacsh1sk7e1.png)** ([Score: 668, Comments: 123](https://reddit.com/r/LocalLLaMA/comments/1hgybhg/hugging_face_researchers_got_3b_llama_to/)): **Hugging Face** researchers achieved a breakthrough by making the **3B Llama model** outperform the **70B Llama model** in MATH-500 accuracy using search techniques. The graph demonstrates that the **3B model** surpasses the **70B model** under certain conditions, with accuracy measured across generations per problem, highlighting the model's potential efficiency and effectiveness compared to larger models.
  - **Inference Time and Model Size Optimization**: Users discuss the potential of finding an optimal balance between inference time and model size, suggesting that smaller models can be more efficient if they perform adequately on specific tasks, especially when the knowledge is embedded in prompts or fine-tuned for particular domains.
  - **Reproducibility and Dataset References**: Concerns are raised about the reproducibility of the results due to the non-publication of the **Diverse Verifier Tree Search (DVTS)** model, with a link provided to the dataset used ([Hugging Face Dataset](https://huggingface.co/datasets/edbeeching/dvts_3b)) and the DVTS implementation ([GitHub](https://github.com/huggingface/search-and-learn/blob/main/src/sal/search/diverse_verifier_tree_search.py)).
  - **Domain-Specific Limitations**: There is skepticism about the applicability of the method outside math and code domains due to the lack of **PRMs** trained on other domains and datasets with step-by-step labeling, questioning the generalizability of the approach.


**Theme 2. Moonshine Web: Faster, More Accurate than Whisper**

- **[Moonshine Web: Real-time in-browser speech recognition that's faster and more accurate than Whisper](https://v.redd.it/gqh3gg170n7e1)** ([Score: 193, Comments: 25](https://reddit.com/r/LocalLLaMA/comments/1hh5y87/moonshine_web_realtime_inbrowser_speech/)): **Moonshine Web** claims to provide **real-time in-browser speech recognition** that is both **faster and more accurate** than **Whisper**.
  - **Moonshine Web** is open source under the **MIT license**, with ongoing efforts to integrate it into **transformers** as seen in [this PR](https://github.com/huggingface/transformers/pull/34784). The **ONNX models** are available on the [Hugging Face Hub](https://huggingface.co/models?library=transformers.js&other=moonshine&sort=trending), although there are concerns about the opacity of the **ONNX web runtime**.
  - Discussion highlights include skepticism about the **real-time capabilities** and accuracy claims of Moonshine compared to **Whisper** models, specifically **v3 large**. Users are curious about the model's ability to perform **speaker diarization** and its current limitation to **English** only.
  - **Moonshine** is optimized for real-time, on-device applications, with support added in **Transformers.js v3.2**. The [demo source code](https://github.com/huggingface/transformers.js-examples/tree/main/moonshine-web) and [online demo](https://huggingface.co/spaces/webml-community/moonshine-web) are available for testing and exploration.


**Theme 3. Granite 3.1 Language Models: 128k Context & Open License**

- **[Granite 3.1 Language Models: 128k context length & Apache 2.0](https://huggingface.co/collections/ibm-granite/granite-31-language-models-6751dbbf2f3389bec5c6f02d)** ([Score: 144, Comments: 22](https://reddit.com/r/LocalLLaMA/comments/1hh403g/granite_31_language_models_128k_context_length/)): **Granite 3.1 Language Models** now feature a **128k context length** and are available under the **Apache 2.0 license**, indicating significant advancements in processing larger datasets and accessibility for developers.
  - **Granite Model Performance**: The **Granite 3.1 3B MoE model** is reported to have a higher average score on the Open LLM Leaderboard than the **Falcon 3 1B**, contradicting claims that MoE models perform similarly to dense models with equivalent active parameters. This is despite having **20% fewer active parameters** than its competitors.
  - **Model Specifications and Licensing**: The **Granite dense models** (2B and 8B) and **MoE models** (1B and 3B) are trained on over **12 trillion** and **10 trillion tokens**, respectively, with the dense models supporting tool-based use cases and the MoE models designed for low latency applications. The models are released under the **Apache 2.0 license**, with the 8B model noted for its performance in code generation and translation tasks.
  - **Community Insights and Comparisons**: The **Granite Code models** are praised for their underrated performance, particularly the **Granite 8BCode** model, which competes with the **Qwen2.5 Coder 7B**. Discussions also highlight the potential for MoE models to facilitate various retrieval strategies and the importance of familiar enterprise solutions like Red Hat's integration of Granite models.


**Theme 4. Moxin LLM 7B: A Fully Open-Source AI Model**

- **Moxin LLM 7B: A fully open-source LLM - Base and Chat + GGUF** ([Score: 131, Comments: 5](https://reddit.com/r/LocalLLaMA/comments/1hh067r/moxin_llm_7b_a_fully_opensource_llm_base_and_chat/)): **Moxin LLM 7B** is a fully open-source large language model trained on text and coding data from **SlimPajama**, **DCLM-BASELINE**, and **the-stack-dedup**, achieving superior zero-shot performance compared to other 7B models. It features a 32k context size, supports long-context processing with grouped-query attention, sliding window attention, and a Rolling Buffer Cache, with comprehensive access to all development resources available on [GitHub](https://github.com/moxin-org/Moxin-LLM) and [Hugging Face](https://huggingface.co/moxin-org/moxin-chat-7b).
  - **Moxin LLM 7B** is praised for being an excellent resource for model training, with its clean and accessible code and dataset, as noted by **Stepfunction**. The model's comprehensive development resources are highlighted as a significant advantage.
  - **TheActualStudy** commends the model for integrating **Qwen-level context**, **Gemma-level tech**, and **Mistral-7B-v0.1** performance. This combination of advanced methods and data is regarded as impressive.
  - **Many_SuchCases** mentions exploring the GitHub repository and notes the absence of some components like intermediate checkpoints, suggesting that these might be uploaded later.


## Other AI Subreddit Recap

> /r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT

**Theme 1. Imagen v2 Quality Elevates Image Generation Benchmark**

- **[New Imagen v2 is insane](https://www.reddit.com/gallery/1hh5swo)** ([Score: 680, Comments: 119](https://reddit.com/r/OpenAI/comments/1hh5swo/new_imagen_v2_is_insane/)): **Imagen 3** is establishing new benchmarks in **image quality** with its release, referred to as **Imagen v2**. The post highlights the impressive advancements in the technology without providing additional context or details.
  - **Access and Usage**: Users discuss accessing **Imagen 3** through the **Google Labs** website, suggesting the use of **VPNs** for regions with restrictions. There is a mention of free access with some daily usage quotas on [labs.google/fx/tools/image-fx](https://labs.google/fx/tools/image-fx).
  - **Artistic Concerns**: There is significant concern among artists about **Imagen 3**'s impact on the art industry, with fears of reduced need for human artists and the overshadowing of traditional art by AI-generated images. Some users express the belief that this shift may lead to the privatization of creative domains and the erosion of artistic labor.
  - **Model Confusion and Improvements**: Some confusion exists regarding the naming and versioning of **Imagen 3**, with users clarifying it as **Imagen3 v2**. Users note significant improvements in image quality, with early testers expressing satisfaction with the results compared to previous versions.


**Theme 2. NotebookLM's Conversational Podcast Revolution**

- **OpenAI should make their own NotebookLM application, it's mindblowing!** ([Score: 299, Comments: 75](https://reddit.com/r/OpenAI/comments/1hgwvwt/openai_should_make_their_own_notebooklm/)): **NotebookLM** produces highly natural-sounding AI-generated podcasts, surpassing even **Huberman's** podcast in conversational quality. The post suggests that **OpenAI** should develop a similar application, as it could significantly impact the field.
  - **NotebookLM's voice quality** is praised but still considered less natural compared to human hosts, with **Gemini 2.0** offering live chat capabilities with podcast hosts, enhancing its appeal. Users note issues with feature integration across different platforms, highlighting limitations in using advanced voice modes and custom projects.
  - The **value of conversational AI** for tasks like summarizing PDFs is debated, with some seeing it as revolutionary in terms of time savings and adult learning theory, while others find the content shallow and lacking depth. The **Gemini model** is noted for its large context window, making it well-suited for handling extensive information.
  - **Google's hardware advantage** is emphasized, with their investment in infrastructure and energy solutions allowing them to offer more cost-effective AI models compared to **OpenAI**. This positions Google to potentially outperform OpenAI in the podcast AI space, leveraging their hardware capabilities to reduce costs significantly.


**Theme 3. Gemini 2.0 Surpass Others in Academic Writing**

- **Gemini 2.0 Advanced is insanely good for academic writing.** ([Score: 166, Comments: 39](https://reddit.com/r/OpenAI/comments/1hgva9g/gemini_20_advanced_is_insanely_good_for_academic/)): **Gemini 2.0 Advanced** excels in academic writing, offering superior understanding, structure, and style compared to other models, including **ChatGPT**. The author considers switching to Gemini 2.0 until **OpenAI** releases an improved version.
  - **Gemini 2.0 Advanced** is identified as **Gemini Experimental 1206** on [AI Studio](https://aistudio.google.com/) and is currently available without a paid version, though users exchange data for access. The naming conventions and lack of a central AI service from **Google** cause some confusion among users.
  - **Gemini 2.0 Advanced** demonstrates significant improvements in academic writing quality, outperforming **GPT-4o** and **Claude** in evaluations. It provides detailed feedback, often critiquing responses with humor, which users find both effective and entertaining.
  - Users discuss the availability of **Gemini 2.0 Advanced** through subscriptions, with some confusion over its listing as "2.0 Experimental Advanced, Preview gemini-exp-1206" in the **Gemini web app**. The model's performance in academic contexts is praised, with users expressing hope that it will push **OpenAI** to address issues in **ChatGPT**.


**Theme 4. Veo 2 Challenges Sora with Realistic Video Generation**

- **[Google is challenging OpenAl's Sora with the newest version of its video generation model, Veo 2, which it says makes more realistic-looking videos.](https://v.redd.it/qok7o7rhsl7e1)** ([Score: 124, Comments: 34](https://reddit.com/r/OpenAI/comments/1hh0vwu/google_is_challenging_openals_sora_with_the/)): **Google** is competing with **OpenAI's Sora** by releasing **Veo 2**, a new version of its video generation model that claims to produce more realistic videos.
  - **Veo 2's Availability and Performance**: Several commenters highlight that **Veo 2** is still in early testing and not widely available, which contrasts with claims of its release. Despite this, some testers on platforms like **Twitter** report impressive results, particularly in areas like physics and consistency, outperforming **Sora**.
  - **Market Strategy and Accessibility**: There is skepticism about the release being a marketing strategy to counter **OpenAI**. Concerns about the lack of public access and API availability for both **Veo 2** and **Sora** are prevalent, with a noted confirmation of a **January** release on **aistudio**.
  - **Trust in Video Authenticity**: The discussion touches on the potential erosion of trust in video authenticity due to advanced generation models like **Veo 2**. Some propose solutions like personal AIs for verifying media authenticity through blockchain registers to address this issue.


---

# AI Discord Recap

> A summary of Summaries of Summaries by o1-2024-12-17

**Theme 1. Challenges in AI Extensions and Projects**  

- **Codeium Extension Breaks Briefly in VSCode**: The extension only displays autocomplete suggestions for a split second, making it unusable. Reverting to version 1.24.8 restores proper functionality, according to multiple user reports.  
- **Windsurf Performance Crumbles Under Heavy Load**: Some users experience over 10-minute load times and sporadic “disappearing code” or broken Cascade functionality. Filing support tickets is the top recommendation until a stable fix arrives.  
- **Bolt Users Cry Foul Over Wasted Tokens**: They jokingly proposed a *“punch the AI”* button after receiving irrelevant responses that deplete credits. Many called for improved memory controls in upcoming releases.

**Theme 2. New and Upgraded Models**  

- [**OpenAI o1 Dazzles With Function Calling**](https://openrouter.ai/openai/o1-preview): This successor to o1-preview introduces a new *“reasoning_effort”* parameter to control how long it thinks before replying. It also features noticeably lower latency through [OpenRouter](https://openrouter.ai).  
- [**EVA Llama Emerges as a Storytelling Specialist**](https://openrouter.ai/eva-unit-01/eva-llama-3.33-70b): Targeted at roleplay and narrative tasks, it reportedly excels at multi-step storytelling. Early adopters praise its creative outputs and user-friendly design.  
- **Major Price Cuts on Fan-Favorite Models**: MythoMax 13B dropped by 12.5% and the QwQ reasoning model plunged 55%. These discounts aim to widen community access for experimentation.

**Theme 3. GPU & Inference Pitfalls**  

- **AMD Driver Updates Slash Performance**: Users saw tokens-per-second plummet from 90+ to around 20 when upgrading from driver 24.10.1 to 24.12.1. Rolling back fixes the slowdown, reinforcing caution with fresh GPU driver releases.  
- **Stable Diffusion on Ubuntu Hits Snags**: Tools like ComfyUI or Forge UI often demand in-depth Linux know-how to fix compatibility issues. Many still recommend an NVIDIA 3060 with 16GB VRAM as a smoother baseline.  
- [**TinyGrad, Torch, and CUDA Memory Confusion**](https://discuss.pytorch.org/t/reduce-time-to-first-kernel-when-using-cuda-graphs/214310): Removing checks like *IsDense(y) && IsSame(x, y)* solved unexpected inference failures, but introduced new complexities. This led developers to reference official CUDA Graphs discussions for potential solutions.

**Theme 4. Advanced Fine-Tuning & RAG Techniques**  

- [**Fine-Tuning Llama 3.2 With 4-bit Conversions**](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama): Many rely on *load_in_4bit=true* to balance VRAM usage and model accuracy. Checkpoints can be reused, and resource constraints are minimized through partial-precision settings.  
- [**Depth AI Indexes Codebases at Scale**](https://www.trydepth.ai/): It attains *99% accuracy* answering technical queries, though indexing 180k tokens may take 40 minutes. Rival solutions like [LightRAG](https://github.com/HKUDS/LightRAG) exist, but Depth AI is praised for simpler setup.  
- [**Gemini 2.0 Adds Google Search Grounding**](https://github.com/BerriAI/litellm/pull/7257): A new configuration allows real-time web lookups to refine answers. Early reviews highlight improved factual precision in coding and Q&A scenarios.

**Theme 5. NotebookLM and Agentic Workflows**  

- **NotebookLM Revamps Its 3-Panel UI**: The update removed *“suggested actions”* due to low usage, but developers promise to reintroduce similar features with better design. Plans include boosted *“citations”* and *“response accuracy”* based on user feedback.  
- **Multilingual Prompts Spark Wide Engagement**: Users tried Brazilian Portuguese and Bangla queries, discovering that explicitly *telling* NotebookLM the language context makes interactions more fluid. This showcases its capability for inclusive global communication.  
- **Controlling Podcast Length Remains Elusive**: Even with time specifications in prompts, final outputs often exceed or ignore constraints. Most rely on flexible length ranges to strike a balance between deep coverage and listener engagement.


---

# PART 1: High level Discord summaries




## [Codeium (Windsurf)](https://discord.com/channels/1027685395649015980) Discord

- **Codeium Extension AutoComplete Issues**: Users reported that the **Codeium extension** in VSCode displays autocomplete suggestions only briefly, rendering it unusable. Reverting to **version 1.24.8** restores functionality.
   - Multiple suggestions to remedy the issue were discussed, focusing on version rollback as a potential solution.
- **Windsurf Performance and Error Handling**: **Windsurf** is experiencing significant performance lags, with instance load times exceeding **10 minutes** and frequent error messages disrupting workflows.
   - Users called for clearer communication from Codeium regarding bugs like 'disappearing code' and Cascade functionality failures.
- **Flex Credits Usage Concerns**: Several users inquired about whether **flex credits** roll over, noting issues with credits being deducted during service outages.
   - Concerns were raised about the impact of frequent error messages and service downtime on credit usage.
- **Connection Issues with Codeium Server**: Members shared difficulties connecting to the **Codeium server**, sharing their experiences and seeking assistance.
   - A recommendation was made to file support tickets for further investigation and potential fixes.
- **Prompting with o1 in AI Applications**: A user shared a [link to a course on o1 prompting](https://www.deeplearning.ai/short-courses/reasoning-with-o1/) that covers its applications in coding and reasoning tasks.
   - Another user requested a summary of the course content due to its complexity.



---



## [Cursor IDE](https://discord.com/channels/1074847526655643750) Discord

- **Cursor 0.44.2 Update Stabilizes Editor**: The **Cursor team** rolled back to [version 0.44.2](https://www.cursor.com/changelog) after addressing bugs in **v0.44**, leading to enhanced stability.
   - Users highlighted new features like the **terminal** and various **bug fixes** improving the overall experience.
- **PyQt/PySide6 Setup Hits Snags**: Developers faced issues with missing files like '**QtWebEngineCore.dll**' when setting up **PySide6**, causing application failures.
   - Recommendations included verifying the correct **Python version** and following detailed **installation steps** to resolve the issues.
- **O1 Pro Boosts Bug Fix Efficiency**: **O1 Pro** users reported successful **bug resolutions** with fewer prompts compared to earlier versions.
   - Despite the added **cost**, many found **O1 Pro's performance** beneficial for their workflows.
- **Kepler Browser Focuses on Privacy**: Development on the **Kepler Community browser** emphasizes **privacy** and **lightweight** functionality.
   - The developer is encouraging **open-source collaboration**, inviting contributions to enhance user privacy features.
- **Cursor's Copy-Paste Functionality Frustrates**: Users reported that **Cursor's copy-paste** sometimes pastes terminal text as plain text instead of code.
   - Suggestions included using **Ctrl + Shift + V** and properly targeting **terminal outputs** to improve usability.



---



## [aider (Paul Gauthier)](https://discord.com/channels/1131200896827654144) Discord

- **o1 API Access Controversy**: Discussions highlighted frustrations among **Tier 5 subscribers** regarding access to the **o1 API**, with concerns about the **$15 per million tokens** pricing compared to the **$200 o1 pro subscription**.
   - Members debated the justification of the pricing structure, noting that while some find it reasonable, others believe it is prohibitively expensive for their use cases.
- **Aider vs. Sonnet Performance**: **Aider**'s latest updates have surpassed **Sonnet** in effectiveness, achieving a benchmark score of **84.2** comparable to Sonnet’s performance.
   - Users observed that while **Aider** excels in **editor mode**, **Gemini** models encounter difficulties with JavaScript tasks, leading to a preference for Aider in certain coding scenarios.
- **Upcoming Models: Veo 2 and R1**: Anticipation surrounds the release of **Veo 2** and **R1**, with members discussing how these models might influence **OpenAI’s market position** amidst growing competition.
   - Conversations indicated that the introduction of newer models could render existing ones like **Sora** less competitive, sparking debates on their ongoing effectiveness.
- **Gemini 2.0 Google Search Integration**: **Gemini 2.0 Flash Experimental** models on **Vertex AI** now support **Google Search grounding**, enabled through specific configurations detailed in a recent [GitHub pull request](https://github.com/BerriAI/litellm/pull/7257).
   - This integration enhances the model’s ability to perform grounded searches, aligning with the latest advancements in **Gemini** capabilities.
- **Depth AI Codebase Understanding**: **Depth AI** impresses users with its ability to generate a comprehensive **knowledge graph** of codebases, achieving **99% accuracy** in answering technical queries.
   - While setup is straightforward, indexing larger projects ranging from **200k to 1.5 million tokens** can take considerable time, as one user reported a **40-minute** indexing for a 180k token repository.



---



## [OpenAI](https://discord.com/channels/974519864045756446) Discord

- **12 Days of OpenAI Updates**: OpenAI is celebrating the **12 Days of OpenAI** by encouraging members to secure the role in <#customize> to stay informed and participate in the festivities. This initiative aims to keep the community engaged with ongoing **updates** and events.
   - On **Day 10**, a linked [YouTube video](https://www.youtube.com/watch?v=LWa6OHeNK3s) showcased the day's celebrations, prompting members to explore the exciting **content** related to the events.
- **OpenAI vs Google: AI Advancements**: The **ai-discussions** channel sparked debates on **OpenAI** and **Google**'s competitive advancements in AI, with many members asserting that **Google** is currently surpassing **OpenAI** in AI development. Concerns emerged that **OpenAI** might be **restricting model releases** for strategic gains.
   - Participants speculated that **Google**'s swift innovation trajectory could significantly shape the future **AI landscape**, affecting how technologies evolve and are adopted.
- **DALL·E vs Midjourney: Image Generation Showdown**: Members compared **OpenAI**'s **DALL·E** with **Midjourney** and **Google's Imagen**, often criticizing **DALL·E** for its recognizable 'AI-generated' outputs despite its free access. Discussions highlighted **Midjourney**'s **pricing** and superior **production quality** as key factors.
   - Users expressed frustration over **DALL·E**'s limitations, while acknowledging **Midjourney**'s strengths, reflecting a preference for higher-quality image generation models even at a cost.
- **Custom GPTs Functionality**: In the **gpt-4-discussions** channel, members questioned the effectiveness of prompting ChatGPT with the instruction *'you are now a manager to train me'*, aiming to enhance response quality.
   - Additionally, frustrations were voiced regarding the inability to **edit custom GPTs**, prompting concerns about limited customization options for users.
- **Channel Posting Etiquette Enforcement**: Discussions in **prompt-engineering** and **api-discussions** channels focused on enforcing **channel posting etiquette**, with members criticizing others for posting in multiple channels as **spam** and advising message deletions from incorrect channels.
   - Members also highlighted challenges in identifying the appropriate channels for seeking help, emphasizing the importance of adhering to specified **guidelines** to maintain order and streamline discussions.



---



## [Nous Research AI](https://discord.com/channels/1053877538025386074) Discord

- **Falcon Models Show Promise**: The **Falcon3** models, especially the **7B** and **10B** variants, are exhibiting robust performance. Recent [updates](https://huggingface.co/blog/falcon3) have introduced tool-use support, enhancing their capabilities for complex interactions.
   - Engineers are keen on testing these models across various applications, noting the improved functionality post-update.
- **Innovative Prompt Chaining Strategies**: **Prompt chaining** is being utilized to refine model outputs by sequentially processing responses through multiple models. Techniques like **structured output** and **tree structures** are being explored to enhance creative tasks such as storytelling.
   - These strategies aim to iteratively improve response quality, as discussed in the [Langflow documentation](https://docs.langflow.org/).
- **OpenAI's Safety Practices Under Scrutiny**: Concerns have been raised about **OpenAI's safety protocols**, especially after a demonstration revealed a jailbreak for their models during a **GPT-4o vs o1 preview** comparison. This has sparked debates on the alignment between OpenAI's safety claims and actual model vulnerabilities.
   - The discussion highlights the need for more transparent safety evaluations, as referenced in [Democratize Intelligence's tweet](https://x.com/demi_network/status/1869085748852662718).
- **Function Calling on Local Models Explored**: A query on the best libraries and methods for **function calling on small local models** indicates a focus on optimizing AI performance locally. This interest points to ongoing efforts to enhance model efficiency without relying on external APIs.
   - The conversation underscores the importance of suitable libraries for effective local model deployment.
- **Ensuring Consistency in LLM Outputs**: Discussions are focused on the **consistency of LLM outputs**, particularly for long and very long text generations. Members are seeking recommendations for top papers that address these challenges in maintaining output quality over extended lengths.
   - This interest reflects a broader concern within the engineering community about sustaining model reliability in extensive applications.



---



## [Notebook LM Discord](https://discord.com/channels/1124402182171672732) Discord

- **3-panel UI Changes in NotebookLM**: The new **3-panel UI** removes the **'suggested actions'** feature from NotebookLM, addressing low utilization due to its limited discoverability and functionality.
   - The development team plans to reintroduce similar functionalities with improved design, focusing on enhancing **citations** and **response accuracy**, and has encouraged users to provide feedback for upcoming releases.
- **Multilingual Functionality Enhancements**: Members are leveraging NotebookLM's interactive functions to facilitate conversations in languages like **Brazilian Portuguese** and **Bangla**, improving engagement through multilingual prompts.
   - One user highlighted that expressing multilingual capabilities in prompts simplifies discussions, fostering more inclusive and diverse interactions within the tool.
- **Interactive Mode Rollout Challenges**: The rollout of **interactive mode** in NotebookLM is experiencing delays and inconsistent access, with some users facing issues like audio generation lag and unexpected resets.
   - Feedback indicates the need for a more reliable deployment strategy to ensure all users with the new UI can access interactive features seamlessly.
- **Podcast Length Customization Strategies**: Users are exploring templates to control **podcast episode lengths**, aiming to maintain deep content exploration without sacrificing engaging dialogue.
   - Discussions revealed a preference for flexible timing ranges over fixed durations, highlighting the complexity in implementing precise podcast length controls.
- **Knowledge Base Generation with NotebookLM**: Members are investigating NotebookLM's capability to generate a **knowledge base** akin to **retrieval augmented generation (RAG)**, seeking insights and alternative solutions.
   - A shared [YouTube video](https://youtu.be/NXdUMyZPUi4) demonstrated using NotebookLM as a knowledge base, aligning with users' needs for structured information retrieval.



---



## [Unsloth AI (Daniel Han)](https://discord.com/channels/1179035537009545276) Discord

- **Fine-tuning Llama 3.2 with 4-bit Conversion**: A member is exploring how to effectively fine-tune the **Llama 3.2** model with added datasets, discussing options for loading previous checkpoints. Another member emphasized that settings like **load_in_4bit=true** allow automatic conversion for models not uploaded by Unsloth.
   - This approach aims to enhance model performance while managing resource constraints, as detailed in the [Unsloth Tutorial](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama).
- **Optimizing Batch Size and VRAM Management**: Discussions about the optimal **batch size** revealed that larger sizes may improve training stability and accuracy but require more VRAM. Members agreed that increasing **gradient accumulation** is a viable alternative for those with limited VRAM.
   - This balance is crucial for efficient training workflows, ensuring both model performance and resource utilization are maximized.
- **Debate on Open Source Reasoning Models like QwQ**: Members debated the effectiveness of open source reasoning models such as **QwQ**, noting that while reproducing reasoning is straightforward, creating a successful model remains challenging. Skepticism was expressed about the necessity of **reinforcement learning (RL)** in current model designs.
   - Suggestions were made that pure **supervised fine-tuning (SFT)** with high-quality datasets might suffice, potentially simplifying model development processes.
- **Multi-GPU and Mac Support in Unsloth**: **Unsloth Pro** now supports **multi-GPU** setups, enhancing the model training experience for both local and cloud environments. However, support for **M4 MAX GPUs** on Macs remains unavailable, with a speculative timeline around **Q2 2025**.
   - Community contributions are encouraged to expedite Mac support, addressing the limitations faced by users without **NVIDIA** hardware.
- **DiLoCo Research and Distributed Training Techniques**: A member shared their research on **DiLoCo** (Distributed Low-Communication Training of Language Models), presenting their findings to the group. This sparked interest and encouraged broader dissemination for additional feedback.
   - References were made to the [DiLoCo Presentation](https://docs.google.com/presentation/d/18Twuq0q1H75BxUOgRc8ZTs2lWGvE2XtnAGZ7CWtq3cA/edit?usp=sharing) and related [ArXiv papers](http://arxiv.org/abs/2311.08105) for deeper insights into distributed training methodologies.



---



## [OpenRouter (Alex Atallah)](https://discord.com/channels/1091220969173028894) Discord

- **OpenAI o1 Model Rolls Out with Enhanced Features**: The new OpenAI **o1 model** is now live, succeeding the [o1-preview](https://openrouter.ai/openai/o1-preview) with features like **function calling** and reduced latency.
   - It introduces a new `reasoning_effort` API parameter for controlling the model's thinking time before answering, enhancing user interactivity.
- **Structured Outputs Normalization Expands**: OpenRouter now normalizes **structured outputs** for **46 models** across **8 companies**, streamlining result formatting.
   - A [tutorial](https://x.com/OpenRouterAI/status/1869077909438091485) was shared to demonstrate its practical usage.
- **EVA Llama Launches as Storytelling Specialist**: The **EVA Llama** model has been released, focusing on roleplay and storytelling, alongside updates for **Grok 2** and **Cohere** models.
   - Details about **EVA Llama** can be explored [here](https://openrouter.ai/eva-unit-01/eva-llama-3.33-70b).
- **Significant Price Drops on Popular Models**: **MythoMax 13B** sees a **12.5% price reduction**, while the **QwQ reasoning model** experiences a **55% price drop**, enhancing affordability.
   - These reductions aim to make the models more accessible to the community.
- **OpenRouter Introduces Provider Pages Analytics**: Provider pages now offer detailed analytics, allowing users to view model hosting charts by clicking on provider names.
   - An example can be seen with the [DeepInfra provider page](https://openrouter.ai/provider/deepinfra), providing comprehensive insights.



---



## [Eleuther](https://discord.com/channels/729741769192767510) Discord

- **Debating Warmup Phase Formulas**: Discussions centered around Kevin's formula *(1 - beta1^step)* for approximating the **warmup phase** have highlighted the lack of support from current **LR schedulers**.
   - Members shared their [implementations](https://github.com/EleutherAI/gpt-neox/blob/f5325805678c2b9e35aae4528283e0132c5f5bbc/megatron/logging.py#L352-L361), raising concerns about off-by-one errors when using **lambdaLR**.
- **Leveraging Meta-Learning to Mitigate Overfitting**: The community explored whether **Meta-Learning** strategies could effectively reduce overfitting in supervised learning models, seeking specific application examples.
   - While theoretical frameworks supporting this approach exist, participants noted a scarcity of practical implementations within current models.
- **Advancements in Neural Network Compression**: Members delved into **compression methods** such as depthwise compression and pruning techniques like **OATS**, which integrates sparse and low-rank matrices.
   - Concerns were voiced regarding potential performance degradation and data coverage loss, especially for models trained on memorization tasks.
- **Exploring the Grokking Phenomenon in AI**: The **grokking** phenomenon was a focal point, discussing its significance and the current absence of effective methods to induce it in AI models.
   - Participants expressed that while grokking is acknowledged, most research efforts remain concentrated on **large language models**, limiting broader exploration.
- **Questioning the Integration of Koopman Operator Theory**: There was skepticism regarding the applicability of **Koopman operator theory** to neural networks, questioning the benefits of modeling neural layers as dynamical systems.
   - Critics argued that the theory primarily rephrases the use of residual connections without introducing substantial innovations.



---



## [Stability.ai (Stable Diffusion)](https://discord.com/channels/1002292111942635562) Discord

- **Effective Lora Training**: A user shared practical steps for creating a **Lora**: start with a strong dataset, choose an appropriate model, train the Lora, then test it. They emphasized research on creating quality datasets for optimal results.
   - Emphasizing the importance of dataset quality, the user highlighted that thorough research is crucial for achieving optimal training outcomes.
- **Preferred Stable Diffusion Models**: Users discussed their preferred models for **Stable Diffusion**, with some favoring the 'flux' model while others recommend 'InvokeAI' for its usability.
   - There's a consensus on the necessity of having an NVIDIA GPU, with suggestions like a 3060 with 16GB VRAM for smoother performance.
- **Challenges Running SD on Ubuntu**: Users expressed frustrations with running **SDXL** on **Ubuntu**, citing compatibility issues with **ComfyUI** and **Forge UI**.
   - Effective operation of **SDXL** may require in-depth familiarity with the **Ubuntu** system to navigate these compatibility challenges.
- **Optimal Image Resolution for Generation**: A beginner inquired about the optimal image resolution for generation, seeking a balance between quality and processing time.
   - Recommendations included experimenting with around **1024x1024** resolution and utilizing **hires.fix** for enhanced quality output.
- **AI Generated Content Metrics**: There was a discussion about the techniques and metrics used in model training, specifically with the **Pony** model and its scoring system.
   - Users noted how this unique approach impacts image generation and influences community perceptions.



---



## [Perplexity AI](https://discord.com/channels/1047197230748151888) Discord

- **Custom Web Sources enhance Perplexity**: Perplexity now offers [custom web sources](https://www.perplexity.ai/spaces) in **Perplexity Spaces** to tailor search queries to specific use cases.
   - The [launch video](https://cdn.discordapp.com/attachments/1047204950763122820/1318746209778929674/Custom_web_sources_launch_video_-_v6.mp4) demonstrates the new customization capabilities.
- **Perplexity Pro Subscriptions launched**: [Perplexity Pro subscriptions](https://perplexity.supply/shop/perplexity-subscription) are now available, offering 1 to 12-month gifting options that provide access to **3x more sources** and **latest AI models**.
   - Users are leveraging these subscriptions to enhance their search capabilities and stay updated with the newest artificial intelligence developments.
- **AI Model Performance under scrutiny**: Community members are evaluating the **performance of AI models** in **Perplexity Pro**, attempting to improve search quality and suggesting alternatives like **Claude 3.5 Sonnet**.
   - Questions have been raised regarding the advancements claimed with models like **GPT-4o**, leading to discussions on selecting optimal architectures.
- **Meta aims to block OpenAI's for-profit ventures**: **Meta** has voiced intentions to block **OpenAI** from pursuing **for-profit business models**, which could significantly influence **future AI developments** in the industry.
   - This move has sparked debates on market competition and the potential reshaping of AI innovation dynamics.
- **Users face Rate Limits in Perplexity**: Several users reported encountering **rate limits** while using **Perplexity**, prompting discussions on the necessity for personalized rate limit enhancements.
   - There is speculation on the benefits of higher subscription tiers in mitigating these restrictions, with users sharing their experiences.



---



## [GPU MODE](https://discord.com/channels/1189498204333543425) Discord

- **CUDA Memory Copy Issues**: A member reported that removing the condition **IsDense(y) && IsSame(x, y)** from the code resolves unexpected behavior during LLM model inference, highlighting that **CudaCopy** initiates CUDA kernels. Refer to [Reduce time to first kernel when using CUDA graphs](https://discuss.pytorch.org/t/reduce-time-to-first-kernel-when-using-cuda-graphs/214310) for more details.
   - Discussions also touched on the lack of official documentation for **CUDA graphs** supporting **cudaMemcpyAsync**, raising concerns about handling asynchronous memory operations within CUDA implementations.
- **Megatron-LM's Training Efficiency**: **Megatron-LM**'s efficiency remains under scrutiny as members plan to enhance **training throughput** in distributed setups. Insights from **Gensyn** and Christine Yip's active community were suggested for optimizing distributed training.
   - The conversation emphasized the importance of leveraging community resources to address scalability challenges and improve overall training performance with **Megatron-LM**.
- **Custom Vision Encoder Integration**: A member proposed developing a **custom vision encoder** to better handle small pixel-scale images within existing language models, arguing that flexibility in encoder pairing outweighs the benefits of pretrained VLMs.
   - The potential for integrating the encoder with various **LLMs** was discussed, highlighting the adaptability and improved performance in specialized image processing tasks.
- **RTX 3090 Finetuning Experiments**: Experiments using an **RTX 3090** for finetuning were shared, with discussions on the optimal setup employing **bf16** or **QLora+int8** precision. An example from **WandB** confirmed that **8bit Lora** is effective for **8B models** on this GPU.
   - Members explored the balance between computational efficiency and model performance, aiming to identify the best finetuning practices for large-scale models on consumer-grade hardware.
- **Axolotl Lora Configuration Success**: The [Axolotl Lora config for llama-3-vision](https://github.com/axolotl-ai-cloud/axolotl/blob/effc4dc4097af212432c9ebaba7eb9677d768467/examples/llama-3-vision/lora-11b.yaml) was validated to work seamlessly with **2x A6000 GPUs**, demonstrating reliable performance in multi-GPU environments.
   - There is ongoing interest in securing compute sponsors to facilitate larger-scale experiments, contingent upon the success of initial configurations.



---



## [LM Studio](https://discord.com/channels/1110598183144399058) Discord

- **LM Studio Setup and Compatibility**: Users shared their **LM Studio** setups, including [RTX 4060 laptops](https://lmstudio.ai/beta-releases) and **M3 Max** with **96GB RAM**, highlighting the application's versatility.
   - A user encountered an 'unknown model architecture' error when loading [Llama 3.2 11B Vision](https://huggingface.co/mlx-community/Llama-3.2-11B-Vision-Instruct-4bit) in LM Studio.
- **Qwen QwQ Excels in Roleplay Applications**: Discussions recommended **Qwen QwQ** as a strong candidate for roleplay LLM tasks, with multiple users lauding its performance.
   - One member noted that **Qwen2** demonstrates exceptional performance in Python programming contexts.
- **AMD GPU Drivers Causing Llama Performance Drops**: Users reported that **AMD GPUs** using **24.12.1 drivers** are experiencing 'Safetensors header is unexpectedly large' errors, leading one to revert to **24.10.1**.
   - **Llama 3.2 3B model** performance dropped from **90+ tok/s** on driver **24.10.1** to **20 tok/s** on the newer driver.
- **LM Studio Lacks Mobile Support**: A member expressed the need to use **LM Studio** on mobile devices but found that no mobile app is currently available.
   - Alternate solutions were suggested, yet direct mobile compatibility remains unavailable.
- **High RAM Needed for Large Model Inference**: Running a **70B model** requires **70GB** of VRAM or main memory, as discussed by users.
   - It was recommended to have **10-20% extra VRAM** for operational flexibility when operating at **q8**.



---



## [Stackblitz (Bolt.new)](https://discord.com/channels/364486390102097930) Discord

- **Seamless Switch: Firebase to Supabase Migration**: A user in **#[prompting](https://discord.com/channels/364486390102097930/1301167628416454737/1318837235692474419)** sought the optimal strategy to transition their entire site from **Firebase** to **Supabase**, highlighting the need for comprehensive migration practices.
   - The community is actively sharing strategies and best practices to ensure data integrity and minimize downtime during the migration process.
- **Bootstrap Battles with create-mf-app**: A member discussed challenges in **#[prompting](https://discord.com/channels/364486390102097930/1301167628416454737/1318837235692474419)** when integrating **create-mf-app** with **Bootstrap**, noting conflicts with **Tailwind** that lead to unstable setups.
   - Solutions proposed include standardized integration methods to harmonize the use of both frameworks without compromising project stability.
- **Bolt Pilot Seeks Testers**: In **#[prompting](https://discord.com/channels/364486390102097930/1301167628416454737/1318837235692474419)**, a member introduced **Bolt Pilot**, a new GPT for **Bolt**, and requested the community to test its functionalities for improvements.
   - Feedback from early testers is crucial for optimizing **Bolt Pilot**'s performance and feature set before a broader release.
- **Bolt's Token Drain Frustrates Users**: In **#[discussions](https://discord.com/channels/364486390102097930/680953097354215446/1318669185370296461)**, numerous users expressed dissatisfaction with **Bolt**'s excessive token usage, with suggestions like adding a **'punch the AI'** button to mitigate waste.
   - Members are sharing experiences of receiving irrelevant responses, prompting discussions on optimizing token allocation for better efficiency.
- **Enhancing Bolt with Payment Integrations**: There was a conversation in **#[discussions](https://discord.com/channels/364486390102097930/680953097354215446/1318669185370296461)** about the complexity of implementing payment integrations such as **Stripe** and **PayPal** into **Bolt**.
   - Users emphasized the necessity for dynamic billing features and expressed interest in upcoming updates that would support these integrations.



---



## [Cohere](https://discord.com/channels/954421988141711382) Discord

- **Cohere Toolkit Deployment Issues**: A member deployed the **Cohere Toolkit** using AWS instructions but encountered an intermittent `stream ended unexpectedly` error.
   - Another member recommended checking the **docker logs** to diagnose the issue, suggesting that deeper insights might be found in the application logs.
- **Findr App Launch on Product Hunt**: **Findr** officially launched on [Product Hunt](https://www.producthunt.com/posts/findr-remember-everything), aiming to provide humans with **infinite memory** and a **searchable digital brain**.
   - The team is seeking support through their promotional [tweet](https://x.com/Nish306/status/1868953328975261712), receiving **positive feedback** from the community.
- **Multimodal Embed-v3 Rate-limit Increase**: In response to community feedback, the rate limit for the **Multimodal Image Embed** endpoint increased from **40 images/min** to **400 images/min** for production keys.
   - Trial rate limits remain at **5 images/min**, and other endpoints like **Chat** have their own specific rate limits, as detailed in the [API Keys and Rate Limits — Cohere](https://docs.cohere.com/v2/docs/rate-limits) documentation.
- **Cohere Reranker Performance**: A developer reported that the **Cohere Reranker** with **ContextualCompressionRetriever** sometimes fails to select the most relevant chunks, leading to incorrect answers.
   - Despite accurate chunking in their RAG application, the **reranking behavior** appears random, causing confusion among users.
- **Embedding Models Dimensionality Challenges**: A user inquired about creating separate vector stores for embeddings from **text-3-embedding-large (3072 dimensions)** and **Cohere Embed v3 (1024 dimensions)**.
   - The dimensionality differences may impact the storage strategy when integrating embeddings for text, tables, and images.



---



## [Modular (Mojo 🔥)](https://discord.com/channels/1087530497313357884) Discord

- **Mojo REPL Troubles on Archcraft**: A user reported issues entering the **Mojo REPL** on [Archcraft Linux](https://discord.com/channels/1087530497313357884/1098713601386233997/1318669970682679397), citing a missing **mojo-ldd** library.
   - The community discussed potential linker errors related to **mojo-lld** and the necessary installation steps to resolve the issue.
- **Var Keyword Debate in Mojo Docs**: Updates in the [Mojo documentation](https://docs.modular.com/mojo/manual/basics#variables) sparked a debate over the necessity of the `var` keyword in variable declarations.
   - Members suggested making `var` optional, while discussing its impact on struct definitions and code clarity.
- **Clarifying Mojo Kernel Terminology**: The term 'kernel' in Mojo was clarified to refer to functions running on accelerators rather than traditional OS kernels.
   - Discussions highlighted the optimization of code blocks for hardware and the distinction between compute kernels and OS kernels.
- **Custom Ops Loading Issues in Max**: Issues were reported when loading the **mandelbrot** custom op in [Max](https://github.com/modularml/max/issues/269), specifically related to unregistered Mojo kernels.
   - Members pointed out the need for proper registration of custom ops to ensure smooth execution within Mojo.
- **Enhancements for Custom Op Handling**: A [feature request](https://github.com/modularml/max/issues/269) was made to improve error messages and handling for missing custom ops in Max.
   - This includes directing users to relevant documentation when errors occur, enhancing the overall user experience.



---



## [OpenInterpreter](https://discord.com/channels/1146610656779440188) Discord

- **Open Interpreter's Persistent Pitfalls**: Multiple users reported ongoing issues with **Open Interpreter**, particularly errors related to the `--conversations` command, leading to loss of valuable conversations.
   - Members are actively seeking solutions to these persistent errors, emphasizing the need for reliable **conversation management**.
- **Upgrading to Open Interpreter 1.x**: A user inquired about upgrading from **Open Interpreter 0.34** to the latest **1.x version**, sparking discussions on the availability of OS mode in the new release.
   - Members strategized potential improvements and shared insights on the new features expected in **Open Interpreter 1.0**.
- **Innovating AI Applications and Models**: Discussions focused on leveraging **AI** for projects like **Raspberry Pi** setups and integrating voice-to-speech models for **home automation**.
   - Users explored methods to connect smaller models with larger systems to enhance overall **functionality**.
- **Truffle-1: The New AI Powerhouse**: A member introduced the **Truffle-1**, a personal computing stack capable of running multiple models with **64GB unified memory**, available for $500 deposit and $115 monthly. More details can be found on the [Truffle website](https://itsalltruffles.com).
   - The **Truffle-1** promises infinite inference time and supports writing and sharing apps, with units set to ship in **January**.
- **Using OS Mode Locally in Open Interpreter**: A user asked about the feasibility of using **OS mode** locally with **Open Interpreter**, which led to discussions on available configuration options.
   - Members shared configuration tips to help users experiencing issues with local **OS mode** setups.



---



## [tinygrad (George Hotz)](https://discord.com/channels/1068976834382925865) Discord

- **Benchmark Showdown: TinyGrad OpenCL vs PyTorch CUDA**: A member requested [benchmarks](https://github.com/tinygrad/tinygrad/issues/8194) comparing **TinyGrad**'s **OpenCL** implementation with **PyTorch**'s **CUDA** for various **Llama models**.
   - This highlights an ongoing interest in **performance comparisons** between different AI frameworks within the community.
- **Mergeable Shapes: Tackling ShapeTracker Complexity**: Discussion emerged on the complexity of proving the **mergeability** of two arbitrary **ShapeTrackers** in Lean, with a user stating it's impossible to have a simple criterion like a matrix determinant.
   - They emphasized the presence of coincidences in strides and shapes that complicate **mergeability checks**.
- **Layout Algebra Unveiled in CuTe**: Members inquired whether **mergeability** is equivalent to composition in **CuTe's layout algebra**, referencing [a note on the algebra of CuTe Layouts](https://research.colfax-intl.com/a-note-on-the-algebra-of-cute-layouts/).
   - This discussion touched on the fundamental abstractions in NVIDIA's **CUTLASS** library and the mathematical treatment of layout operations.
- **NP-Hard Challenges in Layout Injectivity**: Concerns were raised about proving conditions related to **injectivity** in **layout algebra**, with suggestions that such checks might be **NP hard**.
   - Participants emphasized the difficulties in establishing sufficient conditions in layout algebra due to potential stride interferences.
- **Symbolic Superiority: Functions vs Layouts**: A member pointed out that **symbolic integer functions** are strictly more powerful than **layouts** in terms of checking necessity and sufficiency.
   - This aligns with discussions on **algorithm complexities** in merging views and supports ongoing **research directions**.



---



## [Torchtune](https://discord.com/channels/1216353675241590815) Discord

- **FSDP Normalization Scaling**: Discussions revealed that **FSDP's normalization by `world_size`** must be addressed, and scaling by `world_size` can correct an average operation issue.
   - A member suggested opening a [PR #2172](https://github.com/pytorch/torchtune/pull/2172) to implement this fix, focusing on the `scale_grads` function.
- **Explicit Scaling in Training**: The community highlighted the importance of **explicit scaling of the loss** within the training recipe rather than hiding logic elsewhere, to simplify comprehension.
   - After evaluations, members agreed to clarify the scaling process in both training and optimization hooks.
- **Bug Identification Across Frameworks**: It was identified that a similar bug affecting the reduction by a factor of `1/world_size` might exist across various libraries, including `trl` and Hugging Face's trainer.
   - Members commended the **Hugging Face** team for recognizing and addressing these issues in their training framework, as noted in linked [GitHub issues](https://github.com/huggingface/transformers/issues/34242).
- **Handling No Sync in Hugging Face**: Members discussed how **Hugging Face** handles no sync scenarios by avoiding gradient accumulation normalization while properly computing loss.
   - Specific implementation details are available in the [trainer.py](https://github.com/huggingface/transformers/blob/052e652d6d53c2b26ffde87e039b723949a53493/src/transformers/trainer.py#L3662) file.
- **Evolutionary Algorithms in ML**: **Evolutionary algorithms** are gaining traction in machine learning discussions, highlighting their potential applications.
   - A member pointed out their significance, suggesting further exploration into their use cases within the community.



---



## [DSPy](https://discord.com/channels/1161519468141355160) Discord

- **AI Reshaping the Knowledge Economy**: [AI and Knowledge Economy](https://arxiv.org/abs/2312.05481) introduces a framework analyzing how **AI** transforms the knowledge economy by reallocating roles between 'workers' and 'solvers'. Basic autonomous AI displaces humans, while advanced autonomous AI benefits larger, more productive firms.
   - As autonomous agents gain traction, they predominantly benefit the most knowledgeable individuals, allowing efficient management of routine work, while less knowledgeable individuals benefit from non-autonomous AI like chatbots.
- **Coconut - Continuous Thought Paradigm**: The paper [Training Large Language Models to Reason in a Continuous Latent Space](https://arxiv.org/html/2412.06769v1) from Meta proposes **Coconut**, a new reasoning paradigm that uses the last hidden state of LLMs for reasoning instead of the traditional language space.
   - This approach seeks to overcome limitations of language-based reasoning by exploring unrestricted latent spaces, potentially enhancing LLMs' performance on complex reasoning tasks.
- **TypedReAct Enigma Solved**: A member shared a new implementation of **TypedReAct**, questioning whether to submit a PR, but noted potential deprecated issues with **TypedChainOfThought** in upcoming versions.
   - Another member suggested that removing the 'Typed' prefix would resolve compatibility issues, emphasizing that built-in ReAct is effective without the typing.
- **RouteLLM Maintenance Concerns**: A member expressed concerns about the lack of maintenance for **RouteLLM**, indicating interest in potential **DSPy** integration.
   - The conversation highlighted the importance of supporting development for models with reduced oversight.
- **DSPy Evolution with Reasoning Models**: A member inquired about how **DSPy** might evolve with the rise of reasoning models, emphasizing fine-tuning at the branching level.
   - This perspective shifts focus from traditional prompting to process reward mechanisms, indicating a potential paradigm shift in model training.



---



## [Nomic.ai (GPT4All)](https://discord.com/channels/1076964370942267462) Discord

- **GPT4All Struggles with Jinja Templates**: Users reported that **GPT4All** is experiencing significant issues with **Jinja templates**, which are essential for model functionality. Current problems include incorrect spacing, new line errors, and unsupported functions like 'none' and '[1:]'.
   - Efforts to address these template issues are ongoing, but detailed solutions have yet to be implemented.
- **Demand for Docker Deployment of GPT4All**: A request was made for a **Docker version of GPT4All** featuring a web UI, aiming to simplify deployment processes.
   - As of now, the community has not provided specific resources or existing solutions to fulfill this demand.
- **CLI Access to Local Documents in GPT4All**: Users are encountering difficulties using local documents with the **GPT4All CLI**, as the old CLI no longer supports it officially.
   - However, it was noted that the **server API** allows programmatic access to local documents when enabled through the GUI.



---



## [LlamaIndex](https://discord.com/channels/1059199217496772688) Discord

- **AI SDR Automates Lead Generation with LlamaIndex**: An [agentic AI SDR](https://t.co/tczv5ZDI4H) built using **LlamaIndex** showcased its capability in automated lead generation, linking to multiple [GitHub features](https://github.com/features).
   - This tool emphasizes **LlamaIndex's** integration capabilities, enhancing efficiency in lead generation workflows.
- **Crash Course Teaches Agent Building with LlamaIndex**: [A crash course](https://twitter.com/llama_index/status/1869454248620269615) led by **LlamaIndex** focuses on building agents with function calling to manage real-time data queries.
   - Participants also learn to create an **agentic RAG** that routes intelligently between vector and summary tools, and how to implement ReAct.
- **OpenAIAgent Faces Concurrency Execution Limits**: A member reported that `OpenAIAgent` function execution remains non-concurrent even after async modifications in an asynchronous environment.
   - This highlights a limitation in **OpenAIAgent's** execution model, affecting asynchronous operations.
- **Community Engages on RAG Evaluation Strategies**: Discussions on **RAG evaluation** are active, with a member inviting peers to DM for in-depth conversations.
   - Participants are exploring effective evaluation strategies within the AI community.



---



## [Gorilla LLM (Berkeley Function Calling)](https://discord.com/channels/1111172801899012102) Discord

- **BFCL Leaderboard Functionality Down**: A user reported that the [BFCL Leaderboard](https://gorilla.cs.berkeley.edu/leaderboard.html#leaderboard) function call demo is stuck on '**Loading Model Response...**'.
   - Another member confirmed a **certificate issue** is causing the model endpoint to be down.
- **Gorilla Benchmark for Structured Outputs**: A user inquired about using the **Gorilla benchmark** to evaluate structured outputs from the model, specifically asking about subtasks for generating text according to a provided **JSON schema** or **Pydantic model**.



---



## [LLM Agents (Berkeley MOOC)](https://discord.com/channels/1280234300012494859) Discord

- **Appreciation in MOOC Channel**: A member expressed gratitude: *Thank you for that!* in the [mooc-questions](https://discord.com/channels/1280234300012494859/1280370030609170494/) channel.
   - This expression highlights positive engagement within the **LLM Agents (Berkeley MOOC)** discussions.
- **Positive Feedback in MOOC Discussions**: A thank you message was shared in [mooc-questions](https://discord.com/channels/1280234300012494859/1280370030609170494/), stating: *Thank you for that!*
   - Such acknowledgments indicate active participation and satisfaction among AI Engineers in the guild.



---



## [Axolotl AI](https://discord.com/channels/1104757954588196865) Discord

- **New Engineer Joining for Reinforcement Learning**: A new engineer is set to join in **January** to assist with **Reinforcement Learning**.
   - Their expertise will enhance the team’s capabilities in **Reinforcement Learning**, contributing to ongoing projects.
- **Support for KTO Project Enhanced**: The new engineer will provide support for the **kto** project starting in **January**.
   - This assistance is anticipated to positively impact the development of the **kto** project.



---



## [Mozilla AI](https://discord.com/channels/1089876418936180786) Discord

- **Developer Hub Update Released**: A significant update for the **Developer Hub** was announced, detailing improvements and new features. You can view the full announcement [here](https://discord.com/channels/1089876418936180786/1230938514955436242/1318638353503227935).
   - Community feedback is encouraged to enhance the **user experience**.
- **Blueprints Initiative for Open-Source AI**: The **Blueprints initiative** aims to assist developers in creating open-source AI solutions. More details can be found [in the thread](https://discord.com/channels/1089876418936180786/1318689803021058158).
   - This initiative serves as a resource for developers to kickstart their projects effectively.



---


The **MLOps @Chipro Discord** has no new messages. If this guild has been quiet for too long, let us know and we will remove it.


---


The **HuggingFace Discord** has no new messages. If this guild has been quiet for too long, let us know and we will remove it.


---


The **AI21 Labs (Jamba) Discord** has no new messages. If this guild has been quiet for too long, let us know and we will remove it.


---

# PART 2: Detailed by-Channel summaries and links


{% if medium == 'web' %}




### **Codeium (Windsurf) ▷ #[discussion](https://discord.com/channels/1027685395649015980/1027697446446432336/1318669267478253689)** (60 messages🔥🔥): 

> `Codeium Extension Issues, Windsurf Performance Problems, Flex Credits Concerns, Connection to Codeium Server, Prompting with o1` 


- **Codeium Extension suffers from Autocomplete Issues**: Multiple users reported that the **Codeium extension** in VSCode is showing autocomplete suggestions only for a fraction of a second, making it unusable.
   - Suggestions to remedy this issue included reverting to **version 1.24.8**, which appears to restore functionality.
- **Windsurf Performance Lagging**: Users have expressed frustration with **Windsurf** becoming extremely slow or failing to load altogether, with one user waiting over **10 minutes** for it to open.
   - Another reported frequent error messages disrupting their workflow and asked for potential fixes.
- **Concerns about Flex Credits Usage**: Several users inquired whether **flex credits** roll over, as they struggle with frequent error messages and service outages affecting their usage.
   - Users reported issues with credits being deducted even when experiencing service downtime.
- **Connection Issues with Codeium Server**: Territory of discussions regarding difficulties in connecting to the **Codeium server**, with users sharing their experiences and requesting assistance in resolving the issue.
   - A suggestion was made to file support tickets for further investigation and possible fixes.
- **Prompting with o1 in AI Applications**: A user shared a link about the **o1 prompting**, which discusses how it can effectively perform coding and reasoning tasks, urging others to explore its capabilities.
   - Another user asked for a summary of this course content due to the complexity of information provided.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://www.deeplearning.ai/short-courses/reasoning-with-o1/">Reasoning with o1</a>: Learn how to use and prompt OpenAI&#x27;s o1 model for complex reasoning tasks.</li><li><a href="https://tenor.com/view/hello-there-gif-5677380953331354485">Hello There GIF - Hello there - Discover &amp; Share GIFs</a>: Click to view the GIF
</li>
</ul>

</div>
  

---


### **Codeium (Windsurf) ▷ #[windsurf](https://discord.com/channels/1027685395649015980/1306163501286293515/1318669293239533671)** (678 messages🔥🔥🔥): 

> `Windsurf vs Cursor, Model Performance Comparisons, Error Handling in Windsurf, AI Integration in Development, Coding Performance and Tools` 


- **Windsurf vs Cursor**: Users are discussing the differences between Windsurf and Cursor, highlighting Cursor's $20 plan as providing better value with features like unlimited requests, compared to Windsurf's higher pricing and credit system.
   - Some prefer to keep both options open for comparison, while others favor Cursor for cost-effectiveness.
- **Model Performance Comparisons**: The discussion reveals that Codeium's 4o-mini and Haiku models are generally regarded as more efficient and cost-effective, with comparisons also made to other models such as Llama 3.1 and GPT.
   - Participants mention that 4o-mini can perform analogous tasks effectively and has recently added the ability to accept images.
- **Error Handling in Windsurf**: Users report various errors and bugs with Windsurf, including 'disappearing code' and issues with Cascade's functionality not working as expected.
   - Some are experiencing internal errors during file operations, and there's a call for clearer communication from Codeium regarding these issues.
- **AI Integration in Development**: Participants express interest in how various AI tools, including Copilot and Codeium, integrate with their coding workflows, discussing the effectiveness of these tools for autocomplete and code suggestions.
   - Analyses about the effectiveness of these tools indicate a general consensus on the importance of experimenting with different models to find the best fit.
- **Coding Performance and Tools**: Conversations around best practices in using AI for coding reflect a need for clarity on when to use chat mode versus write mode in tools like Windsurf.
   - Suggestions emphasize the importance of using absolute paths and defining clear goals for prompts to improve the effectiveness of AI-assisted coding.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://livebench.ai/#/?Coding=a">LiveBench</a>: no description found</li><li><a href="https://www.reddit.com/r/LocalLLaMA/comments/1ga25gj/introducing_fast_apply_replicate_cursors_instant/">Reddit - Dive into anything</a>: no description found</li><li><a href="https://codeium.canny.io/feature-requests/p/cannot-use-windsurf-as-git-editor">Cannot use windsurf as git editor | Feature Requests | Codeium</a>: git config --global core.editor &#x27;windsurf --wait &#x27; throws error on rebases hint: Waiting for your editor to close the file... [1119/144632.</li><li><a href="https://codeium.canny.io/feature-requests/p/windsurf-focus-follows-mouse-as-a-configuration-option">Windsurf - Focus Follows Mouse (as a configuration option) | Feature Requests | Codeium</a>: There is an open GitHub PR for VSCode which is, on the surface, more than 4 years old, however it is way older than that.</li><li><a href="https://www.youtube.com/watch?v=9jgR-Ih_wGs"> - YouTube</a>: no description found</li><li><a href="https://www.ray.io">Productionizing and scaling Python ML workloads simply | Ray</a>: Ray manages, executes, and optimizes compute needs across AI workloads. It unifies infrastructure and enables any AI workload. Try it for free today.</li><li><a href="https://www.reddit.com/r/ChatGPTCoding/s/vPI207Unh3">Reddit - Dive into anything</a>: no description found</li><li><a href="https://www.reddit.com/r/Codeium/s/v22PYhpKn1">Reddit - Dive into anything</a>: no description found
</li>
</ul>

</div>
  

---


### **Cursor IDE ▷ #[general](https://discord.com/channels/1074847526655643750/1074847527708393565/1318672809093763163)** (707 messages🔥🔥🔥): 

> `Cursor Update 0.44.2, Development tools in Cursor, PyQt and PySide6 issues, O1 Pro usage, Kepler Community browser` 


- **Cursor Update 0.44.2 Released**: The Cursor team has rolled back to version 0.44.2 after addressing bugs in the previous version 0.44, with users reporting stability improvements.
   - Users have discussed their experiences with the update, including new features like the terminal and bug fixes.
- **Challenges with PyQt and PySide6**: Users experienced issues related to missing files like 'QtWebEngineCore.dll' when setting up PySide6, leading to problems in their applications.
   - Recommendations were made to ensure the correct Python version is installed and to troubleshoot installation steps.
- **O1 Pro Enhancements**: Users discussed the benefits of using O1 Pro, reporting successful bug resolutions in a fraction of the prompts used compared to earlier versions.
   - The cost of O1 Pro was noted, with some users finding value in its performance despite the additional expense.
- **Kepler Community Browser Development**: One user shared their progress on developing the Kepler Community browser, emphasizing its focus on privacy and lightweight functionality.
   - The developer expressed a commitment to open-source collaboration, inviting others to contribute to the project aimed at enhancing user privacy.
- **Cursor's Copy-Paste Functionality**: Users reported frustrations with Cursor's handling of copied terminal text, which sometimes pastes as a plain text instead of code.
   - Suggestions included using Ctrl + Shift + V for pasting and targeting terminal outputs effectively to improve usability.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://download.todesktop.com/230313mzl4w4u92/cursor-0.44.0-build-2412187f9v0nffu-x86_64.AppImage">no title found</a>: no description found</li><li><a href="https://www.cursor.com/settings">Settings | Cursor - The AI Code Editor</a>: You can manage your account, billing, and team settings here.</li><li><a href="https://www.cursor.com/downloads">Downloads | Cursor - The AI Code Editor</a>: Choose your platform to download the latest version of Cursor.</li><li><a href="https://python-poetry.org/">Poetry - Python dependency management and packaging made easy</a>: no description found</li><li><a href="https://marketplace.visualstudio.com/items?itemName=donjayamanne.python-environment-manager">Python&#32;Environment&#32;Manager&#32;-&#32;Visual&#32;Studio&#32;Marketplace</a>: Extension&#32;for&#32;Visual&#32;Studio&#32;Code&#32;-&#32;View&#32;and&#32;manage&#32;Python&#32;environments&#32;&amp;&#32;packages.</li><li><a href="https://docs.astral.sh/uv/">uv</a>: no description found</li><li><a href="https://github.com/ultrasev/cursor-reset">GitHub - ultrasev/cursor-reset: Mac utility to reset Cursor editor&#39;s device identification system. Helps resolve account restrictions and trial-related issues.</a>: Mac utility to reset Cursor editor&#39;s device identification system. Helps resolve account restrictions and trial-related issues. - ultrasev/cursor-reset</li><li><a href="https://github.com/ZackPlauche/add-cursor-to-win-context-menu">GitHub - ZackPlauche/add-cursor-to-win-context-menu</a>: Contribute to ZackPlauche/add-cursor-to-win-context-menu development by creating an account on GitHub.</li><li><a href="https://forum.cursor.com/t/warning-cursor-v0-44-breaks-all-devcontainers-v0-394-0/35747/7">WARNING: Cursor v0.44 breaks all devcontainers v0.394.0</a>: How did you forcibly disable Cursor from updating? I’m stuck in a world where upon restarts of Cursor, it will always update to v0.44.0 now.  The added issue is even if I disable the “devcontainer” ex...</li><li><a href="https://tenor.com/view/danger-alert-siren-alarm-red-light-gif-16931369">Danger Alert GIF - Danger Alert Siren - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://www.cursor.com/changelog">Changelog | Cursor - The AI Code Editor</a>: New updates and improvements.</li><li><a href="https://pastebin.com/Bgu7XD6C">index.html - Pastebin.com</a>: Pastebin.com is the number one paste tool since 2002. Pastebin is a website where you can store text online for a set period of time.</li><li><a href="https://pastebin.com/cqNdfphK">style.css - Pastebin.com</a>: Pastebin.com is the number one paste tool since 2002. Pastebin is a website where you can store text online for a set period of time.</li><li><a href="https://github.com/TheGalaxyStars/KEPLER-COMMUNITY">GitHub - TheGalaxyStars/KEPLER-COMMUNITY: Explore freely, leave no trace.</a>: Explore freely, leave no trace. Contribute to TheGalaxyStars/KEPLER-COMMUNITY development by creating an account on GitHub.
</li>
</ul>

</div>
  

---


### **aider (Paul Gauthier) ▷ #[general](https://discord.com/channels/1131200896827654144/1131200896827654149/1318696787279876159)** (264 messages🔥🔥): 

> `o1 API access, Benchmark Performance, Refund and Support Experiences, Gemini vs. Sonnet, Aider Functionality` 


- **Controversy over o1 API access and pricing**: Discussions revealed mixed experiences regarding access to the **o1 API**, with some users expressing frustration over not receiving it despite being Tier 5 subscribers.
   - A member noted the pricing structure, highlighting that **15$/1 million tokens** for the API is considered high compared to the **$200 subscription** for o1 pro, which some find justifiable.
- **Performance Comparisons of Aider and Sonnet**: Users compared the performance of **Aider** and **Sonnet**, reporting that Aider's latest updates show it to be more effective, with o1 achieving a benchmark of **84.2**, rivaling Sonnet.
   - Others discussed that o1 functions well in **editor mode**, while Gemini models struggled with JavaScript, suggesting that Aider has performed better in certain coding tasks.
- **Refund Process for Subscription Services**: Several members shared their experiences with the **refund process** for the o1 pro subscription, noting that responses can be delayed but refunds do eventually occur.
   - While some reported long wait times for refunds, others claimed it was quicker, in particular, one member received a refund within hours of the request.
- **Expectations from Upcoming Models**: Members expressed anticipation for upcoming models like **Veo 2** and **R1**, noting that competition is growing and could impact OpenAI’s market position.
   - Conversations suggested that as newer models come out, existing models like Sora may fall behind, sparking debates on their effectiveness and performance.
- **Aider's Improved Functionality**: Users noted improvements in Aider functionality, specifically discussing the ability to **see all files** without needing to /add them manually, highlighting the potential need for a `.aiderignore` file.
   - A member discussed the efficiency of using Aider's editor capabilities, particularly with Gemini models, while raising concerns about the editing limitations with JavaScript.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://x.com/AndrewYNg/status/1869421643925422166">Tweet from Andrew Ng (@AndrewYNg)</a>: OpenAI just announced API access to o1 (advanced reasoning model) yesterday. I&#39;m delighted to announce today a new short course, Reasoning with o1, built with @OpenAI, and taught by @colintjarvis,...</li><li><a href="https://x.com/CodeByPoonam/status/1869289412951220395">Tweet from Poonam Soni (@CodeByPoonam)</a>: Google just dropped Veo 2 and it&#39;s INSANESpoiler: OpenAI Sora is now falling behind.10 Wild Examples of what it&#39;s capable of: (Don’t miss the 5th one)</li><li><a href="https://aider.chat/docs/usage/lint-test.html">Linting and testing</a>: Automatically fix linting and testing errors.</li><li><a href="https://openrouter.ai/openai/o1">o1 - API, Providers, Stats</a>: The latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding. The o1 model series is trained with large-scale reinforcement learning to reason using ...</li><li><a href="https://aider.chat/docs/config/options.html">Options reference</a>: Details about all of aider’s settings.</li><li><a href="https://aider.chat/docs/config/options.html#fixing-and-committing">Options reference</a>: Details about all of aider’s settings.
</li>
</ul>

</div>
  

---


### **aider (Paul Gauthier) ▷ #[questions-and-tips](https://discord.com/channels/1131200896827654144/1133060505792159755/1318706793655963689)** (18 messages🔥): 

> `Aider Support for Gemini Flash 2, Working with /architect and /ask Modes, Managing Code Refactoring, File Upload Issues, Google Search Grounding in Gemini 2.0` 


- **Aider does not support special features for Gemini Flash 2**: A member raised a question about Aider’s support for Gemini Flash 2's grounding feature, but it was clarified that Aider doesn't do anything special in the API for this.
   - Another member mentioned that the **Gemini models support Google Search grounding**, with specific requirements related to the model and pricing involved.
- **Using /architect and /ask for Project Planning**: Members discussed how to effectively utilize the /architect and /ask modes for defining project plans that could be implemented through /code mode.
   - One member suggested requesting Aider to create a *todo.md* file for task tracking, enhancing workflow organization.
- **Challenges with Code Refactoring and Task Management**: A member expressed that as projects grow larger, maintaining clean code becomes difficult when using Claude for feature development.
   - Participants shared that without supervision, the generated code could become messy and might require refactoring steps along the way.
- **Issues with File Uploads on Aider**: A member reported not receiving a file dropdown when attempting to add files to Aider, raising concerns about usability in the new version.
   - Another user confirmed that this bug has been fixed in the main branch and provided instructions to update.
- **Integration of Google Search in Gemini 2.0**: A member detailed that Gemini 2.0 Flash Experimental models on Vertex AI support Google Search grounding enabled by specific configurations.
   - They shared a relevant GitHub pull request that enhances support for this functionality along with the YAML configuration needed for setup.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://aider.chat/docs/repomap.html">Repository map</a>: Aider uses a map of your git repository to provide code context to LLMs.</li><li><a href="https://aider.chat/docs/faq.html#how-can-i-add-all-the-files-t">FAQ</a>: Frequently asked questions about aider.</li><li><a href="https://github.com/yamad">yamad - Overview</a>: yamad has 85 repositories available. Follow their code on GitHub.</li><li><a href="https://aider.chat/docs/faq.html#how-can-i-add-all-the-files-to-the-chat">FAQ</a>: Frequently asked questions about aider.</li><li><a href="https://github.com/yamadashy/repomix">GitHub - yamadashy/repomix: 📦 Repomix (formerly Repopack) is a powerful tool that packs your entire repository into a single, AI-friendly file. Perfect for when you need to feed your codebase to Large Language Models (LLMs) or other AI tools like Claude, ChatGPT, and Gemini.</a>: 📦 Repomix (formerly Repopack) is a powerful tool that packs your entire repository into a single, AI-friendly file. Perfect for when you need to feed your codebase to Large Language Models (LLMs) o.....</li><li><a href="https://github.com/BerriAI/litellm/pull/7257">Add support for Gemini 2.0 GoogleSearch tool by samling · Pull Request #7257 · BerriAI/litellm</a>: TitleAdd googleSearch() tool to valid tool list for Gemini/VertexAI models to support Gemini 2.0 grounding.Relevant issuesEnhances #7188Type🆕 New Feature✅ TestChangesAdd googleSearch() too...
</li>
</ul>

</div>
  

---


### **aider (Paul Gauthier) ▷ #[links](https://discord.com/channels/1131200896827654144/1268910919057149974/1318741968242737282)** (11 messages🔥): 

> `Depth AI, LightRAG, Codebase Indexing, AI Assistant Deployment` 


- **Depth AI impresses with code understanding**: Users have been enjoying [Depth AI](https://www.trydepth.ai/) for its ability to construct a comprehensive knowledge graph of their codebase, answering deep technical questions with **99% accuracy**.
   - Many found the setup easy, although indexing larger projects (200k - 1.5 mil tokens) may take some time, with one user noting a 180k token repo took **40 minutes**.
- **LightRAG discussed as an alternative**: A user suggested trying [LightRAG](https://github.com/HKUDS/LightRAG), described as a simple and fast retrieval-augmented generation tool, during discussions about Depth AI.
   - However, another user expressed preference for Depth AI, labeling it easier to set up and potentially more effective.
- **Indexing duration causes mixed reactions**: While one user reported successful indexing of their project with Depth AI, another mentioned their medium-sized project has been indexing for **4 hours**.
   - The time taken to index appears to vary significantly based on the token size, emphasizing the importance of patience.
- **Concerns over Depth AI output**: One user expressed frustration when Depth AI returned 'no output was generated' for their queries after completing the indexing.
   - This raises questions about the reliability of output despite successful indexing.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://www.trydepth.ai/">Depth AI - AI that deeply understands your codebase</a>: Chat with your codebase or build customised AI assistants. Deploy them wherever you work — Slack, Github Copilot, Jira and more.</li><li><a href="https://github.com/HKUDS/LightRAG">GitHub - HKUDS/LightRAG: &quot;LightRAG: Simple and Fast Retrieval-Augmented Generation&quot;</a>: &quot;LightRAG: Simple and Fast Retrieval-Augmented Generation&quot; - HKUDS/LightRAG
</li>
</ul>

</div>
  

---


### **OpenAI ▷ #[annnouncements](https://discord.com/channels/974519864045756446/977259063052234752/1318999359412506645)** (1 messages): 

> `12 Days of OpenAI, Stay Updated Role` 


- **Join the 12 Days of OpenAI Fun**: OpenAI is encouraging members to stay in the loop during the **12 Days of OpenAI** by picking up the <@&1261377106890199132> role in <id:customize>.
   - This is a fantastic way to receive updates and be involved in the ongoing festivities.
- **Day 10 Celebration Features**: The announcement highlights the **Day 10** celebration through a linked [YouTube video](https://www.youtube.com/watch?v=LWa6OHeNK3s) that showcases the ongoing events.
   - Members are encouraged to check it out for exciting content related to the day's activities.


  

---


### **OpenAI ▷ #[ai-discussions](https://discord.com/channels/974519864045756446/998381918976479273/1318674710694989916)** (220 messages🔥🔥): 

> `OpenAI vs Google AI advancements, Experiences with different AI models, AI and safety concerns, AI for personal assistance, DALL·E vs Midjourney for image generation` 


- **OpenAI and Google's Competitive Landscape**: Discussion centers around the competition between [OpenAI](https://openai.com) and Google, with many participants believing Google is currently outperforming OpenAI in AI advancements.
   - Concerns were raised about how OpenAI may be holding back models for competitive strategy, while others speculated that Google's rapid innovation could define future AI landscapes.
- **Diverse Experiences with AI Models**: Members shared their opinions on different AI models with many siding in favor of OpenAI's GPT models for programming and math, while also highlighting some dissatisfaction with Gemini 2.0 Flash’s performance.
   - Users expressed how agents could significantly enhance life for people with disabilities by performing tasks autonomously, reflecting a desire for practical applications of AI.
- **AI Safety and Ethical Concerns**: Participants debated the effectiveness and ethics of current AI safety measures, with some claiming that current solutions may limit creativity and usefulness.
   - Emphasis was placed on finding a balance between ensuring safety and allowing for the exploration of AI capabilities, with some noting over-censorship as a potential issue.
- **Interest in Personal AI Assistants**: A significant discussion point was the desire for personal AI assistants that can manage tasks autonomously and simplify daily life, particularly for elderly users recovering from health issues.
   - Conversations focused on how such technology could improve life quality, with references to Google's ongoing developments in this area.
- **Comparing Image Generation Models**: Users compared OpenAI's DALL·E with Midjourney and Google's Imagen, often lamenting the limitations and quality of DALL·E despite its no-cost access.
   - Discontent was expressed about DALL·E's output being easily recognizable as 'AI-generated,' while users highlighted Midjourney's pricing and production quality as factors for consideration.



**Link mentioned**: <a href="https://github.com/AlignAGI/Alignment/">GitHub - AlignAGI/Alignment: Promoting global awareness and action for ethical AI alignment and safeguarding humanity against AI self-replication risks. Includes research, frameworks, and open-source resources.</a>: Promoting global awareness and action for ethical AI alignment and safeguarding humanity against AI self-replication risks. Includes research, frameworks, and open-source resources. - AlignAGI/Alig...

  

---


### **OpenAI ▷ #[gpt-4-discussions](https://discord.com/channels/974519864045756446/1001151820170801244/1318739662197624903)** (3 messages): 

> `Custom GPTs functionality, Manager role in training` 


- **Training Role of ChatGPT Clarified**: A member questioned whether the instruction 'you are now a manager to train me' functions effectively when prompting ChatGPT to assume a particular role.
   - *Is this the key to unlocking better responses?*
- **Limitations on Editing Custom GPTs**: Another member expressed frustration regarding the inability to edit custom GPTs, signaling a potential flaw in the system.
   - *Are we stuck without options?*


  

---


### **OpenAI ▷ #[prompt-engineering](https://discord.com/channels/974519864045756446/1046317269069864970/1318796591389347902)** (4 messages): 

> `Channel Posting Etiquette, Seeking Help in Appropriate Channels` 


- **Channel Posting Etiquette Under Fire**: A member criticized another for posting in multiple channels, labeling it as **spam** and instructing to delete the messages from all but the correct channel <#1047565374645870743>.
   - This comment emphasized the importance of using designated channels to maintain order and avoid confusion.
- **Searching for Help in the Right Place**: One member expressed uncertainty about the appropriate channel, stating they were just trying to find the best input.
   - This inquiry underscores the challenges users face when navigating channel guidelines for assistance.


  

---


### **OpenAI ▷ #[api-discussions](https://discord.com/channels/974519864045756446/1046317269069864970/1318796591389347902)** (4 messages): 

> `Channel Overposting, Seeking Help, Proper Channel Usage, Spam Concerns` 


- **Channel Overposting Sparks Debate**: A member questioned why a post was shared in **four channels**, highlighting concerns about spam.
   - They suggested deleting the post from other channels to streamline the discussion.
- **Member Seeking Guidance**: Another member expressed uncertainty about the **appropriate channel** for their inquiry, stating they were just looking for help.
   - This raised questions about channel organization and member awareness.
- **Call for Proper Channel Usage**: In response, a member emphasized that the **correct channel** for such posts is specified, recommending adherence to guidelines.
   - They offered assistance after the post is removed from other locations.


  

---


### **Nous Research AI ▷ #[general](https://discord.com/channels/1053877538025386074/1149866623109439599/1318670110012997702)** (210 messages🔥🔥): 

> `Falcon Model Performance, Prompt Chaining Techniques, OpenAI Safety Discussions, Feedback and Evaluation Systems, API and Tool-Use Support in Models` 


- **Falcon Models Show Promise**: The Falcon3 models, particularly the 7B and 10B versions, are demonstrating strong performance, with users expressing interest in testing their capabilities for various applications.
   - Recent updates have added tool-use support, enhancing their functionality, especially in contexts requiring complex interactions.
- **Innovative Prompt Chaining Strategies**: Discussion on prompt chaining highlighted its utility for enhancing model output by using a series of models to process and refine responses iteratively.
   - Techniques such as structured output and tree structures are suggested to improve storytelling and other creative tasks.
- **OpenAI's Safety Credibility Under Scrutiny**: Concerns were raised about OpenAI's focus on safety practices, particularly in light of a demonstration that showcased a jailbreak for their models during a comparison between GPT-4o and o1 preview.
   - This has led to an ongoing conversation regarding the alignment between their safety claims and actual model vulnerabilities.
- **Feedback and Rating Systems**: Users are implementing evaluation frameworks to assess story quality generated by models using specific rubrics which detail various narrative elements.
   - This systematic approach aims to produce higher-quality outputs through iterative feedback and assessment mechanisms.
- **API and Local Model Performance**: There is a discussion regarding the commonality of running inference without batching in local models, with users advocating for queuing requests for efficiency.
   - The aim is to explore its integration into various applications, including market simulations, where real-world testing becomes essential.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://docs.langflow.org/">Welcome to Langflow | Langflow Documentation</a>: Langflow is a new, visual framework for building multi-agent and RAG applications. It is open-source, Python-powered, fully customizable, and LLM and vector store agnostic.</li><li><a href="https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute">Scaling test-time compute - a Hugging Face Space by HuggingFaceH4</a>: no description found</li><li><a href="https://x.com/demi_network/status/1869085748852662718">Tweet from Democratize Intelligence (@demi_network)</a>: &#34;It&#39;s not a question of alignment between the company and AI, it&#39;s a question of alignment between the company and you. It’s going to be very important who your AI works for.If your AI is ...</li><li><a href="https://huggingface.co/tiiuae/Falcon3-7B-Instruct-1.58bit">tiiuae/Falcon3-7B-Instruct-1.58bit · Hugging Face</a>: no description found</li><li><a href="https://huggingface.co/tiiuae/falcon-11B">tiiuae/falcon-11B · Hugging Face</a>: no description found</li><li><a href="https://huggingface.co/tiiuae/falcon-7b-instruct">tiiuae/falcon-7b-instruct · Hugging Face</a>: no description found</li><li><a href="https://huggingface.co/tiiuae/Falcon3-10B-Instruct">tiiuae/Falcon3-10B-Instruct · Hugging Face</a>: no description found</li><li><a href="https://huggingface.co/blog/falcon3">Welcome to the Falcon 3 Family of Open Models!</a>: no description found</li><li><a href="https://huggingface.co/tiiuae/falcon-40b-instruct">tiiuae/falcon-40b-instruct · Hugging Face</a>: no description found</li><li><a href="https://huggingface.co/tiiuae/Falcon3-10B-Instruct#benchmarks)">tiiuae/Falcon3-10B-Instruct · Hugging Face</a>: no description found</li><li><a href="https://x.com/_xjdr/status/1869062741849461146">Tweet from xjdr (@_xjdr)</a>: this was one of the most interesting things i heard repeated from ~trusted sources a few times at NeurIPS  (newsonnet being 400B dense)Quoting Aidan McLau (@aidan_mclau) @Heraklines1 @deedydas no not ...</li><li><a href="https://safepine.co/">Safepine</a>: no description found</li><li><a href="https://www.reddit.com/r/LocalLLaMA/comments/175ejvi/quick_start_example_for_llava_generate_image/">Reddit - Dive into anything</a>: no description found
</li>
</ul>

</div>
  

---


### **Nous Research AI ▷ #[ask-about-llms](https://discord.com/channels/1053877538025386074/1154120232051408927/1318853936660221984)** (13 messages🔥): 

> `Function calling on local models, Bias in function fetching, Effectiveness of search engines, Hermes 3 405B model issues, Pink elephant problem in AI responses` 


- **Exploring Function Calling Libraries**: A query was raised on the best libraries and methods for **function calling on small local models**.
   - This indicates an ongoing interest in optimizing AI performance on local systems.
- **Bias Due to Model Recall**: Discussion centered around the pitfalls of using language models for data recall, emphasizing that **correctness** is subjective based on the source and purpose.
   - Concern was expressed that models might **mistake biased information** as truth if they leverage generic web searches.
- **Search Engine Quality Debate**: One member voiced frustration, suggesting that current search engines are plagued by **SEO spam** and untrustworthy news sites.
   - They yearned for a superior search engine indexing all books and papers ever written.
- **Hermes 3 405B Model Feedback**: A user reported issues with the **Hermes 3 405B model** reverting prompts during responses, despite instructions not to do so.
   - They noted a comparison with GPT-4O showed fewer issues, questioning whether rephrasing prompts might help.
- **Pink Elephant Problem & Model Responses**: The 'pink elephant problem' was discussed, illustrating how instructing models about what NOT to do can inadvertently trigger that behavior.
   - Research on enhancing model robustness against such pitfalls was mentioned, prompting a shift in user prompting strategy.


  

---


### **Nous Research AI ▷ #[research-papers](https://discord.com/channels/1053877538025386074/1104063238934626386/1318732957287972976)** (2 messages): 

> `Signal and Noise in Inference, Consistency of LLM Output, Long Output Challenges` 


- **Signal vs. Noise: Key to Clear Thinking**: The importance of the **signal to noise ratio** was highlighted as vital for coherent and clear inference, similar to its role in the **human brain**.
   - *When will we hear about something like this?* indicates anticipation for deeper discussions on this topic.
- **Seeking Recommendations on LLM Consistency Papers**: A member expressed interest in finding the **best papers** focused on the **consistency** of LLM output, especially for long and very long outputs.
   - This prompts further exploration on the **challenges** faced by LLMs in maintaining output quality over extended text lengths.


  

---


### **Nous Research AI ▷ #[research-papers](https://discord.com/channels/1053877538025386074/1104063238934626386/1318732957287972976)** (2 messages): 

> `Signal and Noise in Inference, Consistency of LLM Output` 


- **Importance of Signal vs. Noise in Inference**: A member emphasized that the **signal and noise** ratio is crucial for coherent and clear thinking inference, drawing parallels to its role in the human brain.
   - *When can we expect to hear more about this?*
- **Seeking Recommendations on LLM Consistency Papers**: Another member expressed interest in hearing recommendations for the **best papers** on the consistency of LLM output, specifically focusing on long to very long outputs.
   - They asked for the group's input, making it clear that they wanted relevant discussions around this topic.


  

---


### **Notebook LM Discord ▷ #[announcements](https://discord.com/channels/1124402182171672732/1182376564525113484/1319005592332796045)** (1 messages): 

> `3-panel UI changes, Suggested actions removal, Workarounds for missing features` 


- **3-panel UI rollout removes suggested actions**: The new **3-panel UI** has eliminated the **'suggested actions'** feature that had been a part of NotebookLM, which included prompts like 'Explain' and 'Critique'. The previous setup was rarely utilized due to its limited discoverability and functionality.
   - Many users have noticed this change, which follows feedback that highlighted the sparse use of the suggested actions.
- **Plan to restore functionality with better design**: The development team plans to restore much of the functionality from the suggested actions in a more intuitive way over the coming months. They aim to enhance the user experience by integrating new features that improve citations and response accuracy.
   - Users are encouraged to share additional feedback as the improvements are implemented in the upcoming releases.
- **Alternative workarounds introduced**: In the interim, users can recreate the suggested actions by copying text from sources and asking for explanations or summaries directly in the chat. The '**convert all notes to source**' feature allows users to create a new source from notes for more structured querying.
   - This method maintains functionality by ensuring responses include clickable citations while focusing on user notes directly.


  

---


### **Notebook LM Discord ▷ #[use-cases](https://discord.com/channels/1124402182171672732/1124403655819415592/1318687630849871952)** (27 messages🔥): 

> `Multilingual Functionality, Podcast Length Customization, Interactive AI Use Cases, Knowledge Base Generation, Creative Podcast Production` 


- **Multilingual Functionality Experimentation**: Members are excited about experimenting with the interactive function of NotebookLM to streamline conversations in different languages, particularly in Brazilian and Bangla.
   - One user mentioned that expressing multilingual in prompts makes it easier to engage in these discussions during the chat.
- **Podcast Length Customization Template Discussion**: A suggestion was made to create a timing template to control episode length, with a member expressing their desire for longer podcasts to explore content deeply without skipping engaging dialogue.
   - Another member questioned how such a template would function, implying a need for a range rather than an exact duration.
- **Creative Uses of Interactive AI**: Various users discussed leveraging NotebookLM and similar tools for creative endeavors, including generating podcasts and engaging with niche topics that may not be widely covered.
   - One user shared their approach of recording concise episodes while reviewing academic materials for an open-source prediction market platform.
- **Knowledge Base Generation with NotebookLM**: A member inquired whether NotebookLM could generate a knowledge base similar to retrieval augmented generation (RAG), asking for insights or alternative solutions.
   - Another user pointed to a YouTube video showcasing the use of NBLM as a knowledge base, suggesting that it may be indicative of what the inquirer was looking for.
- **AI-Powered Podcast Production Insights**: A user shared their experience creating AI-generated podcasts, emphasizing the need to add personal commentary to avoid 'AI slop' and maintain content quality.
   - They expressed plans to enhance their podcast by not just relying on first drafts from NotebookLM but also engaging in interactive mode for refined content.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://youtu.be/NXdUMyZPUi4"> - YouTube</a>: no description found</li><li><a href="https://open.spotify.com/episode/6pjDfRqlfDGZY1KTpxD2iS?si=qEpiAJXiRPm67UPLDbedKw">Ask Gennie! Reverse Mortgage Q&amp;A - What is a Reverse Mortgage for Seniors? What are the benefits of the reverse mortgages for elder people and retirees?</a>: Ask Gennie! Mortgage Questions Answered with Experts from GenNext.Mortgage (NMLS #2326098) · Episode
</li>
</ul>

</div>
  

---


### **Notebook LM Discord ▷ #[general](https://discord.com/channels/1124402182171672732/1124402182909857966/1318669153707491358)** (194 messages🔥🔥): 

> `NotebookLM Podcast Features, Interactive Mode Rollout, Audio Overview Functionality, Source Integration and Updates, Case Study Preparation Using NotebookLM` 


- **Challenges with Podcast Length Control**: Users are struggling to set specific lengths for podcasts, with efforts to include audio length notes often being ignored.
   - Some suggest using precise prompting techniques, but inconsistencies in output persist.
- **Interactive Mode Rollout Issues**: The interactive mode feature's rollout is slow and random; users with the new UI may not access the feature yet.
   - Feedback indicates that audio generation frequently lags or fails, with some users experiencing resets to manage limitations.
- **Syncing Google Docs as Sources**: Users are uncertain if Google Docs linked as sources automatically sync with updates or require manual refreshes.
   - Currently, sources do not auto-update, raising questions about future roadmap plans for auto-syncing files.
- **Combining and Managing Notes**: The new UI lacks the ability to combine selected notes, restricting operations to only single or all notes at once.
   - This limitation has sparked discussions about potential UI improvements to facilitate better note management.
- **Case Studies and Study Aids**: Users share experiences utilizing NotebookLM for study aids, emphasizing the tool's assistance in organizing speaker notes.
   - In-depth tips for exam preparation, particularly for case studies, highlight the importance of applying concepts through thorough resource integration.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://tenor.com/view/noob-gif-5274024">Noob GIF - Noob - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://support.google.com/notebooklm/answer/15678219?hl=en">Upgrading to NotebookLM Plus - NotebookLM Help</a>: no description found</li><li><a href="https://blog.google/technology/google-labs/notebooklm-new-features-december-2024/">NotebookLM gets a new look, audio interactivity and a premium version</a>: NotebookLM is introducing new features, and a premium version called NotebookLM Plus.</li><li><a href="https://www.reddit.com/r/GooglePixel/comments/z5i6ns/a_hidden_gem_the_pixel_recorder/">Reddit - Dive into anything</a>: no description found</li><li><a href="https://youtu.be/7mqciPtMfBI?si=IStj7r25df71U40Y"> - YouTube</a>: no description found
</li>
</ul>

</div>
  

---


### **Unsloth AI (Daniel Han) ▷ #[general](https://discord.com/channels/1179035537009545276/1179035537529643040/1318675575484711043)** (66 messages🔥🔥): 

> `Fine-tuning Llama 3.2, Batch Size and Training, Function Calling in Models, Multi-GPU Support in Unsloth, Overfitting in Machine Learning Models` 


- **Fine-tuning Llama 3.2 and 4-bit Conversion**: A member is exploring how to effectively fine-tune the **Llama 3.2** model with added datasets, discussing options for loading previous checkpoints.
   - Another member emphasized that settings like **load_in_4bit=true** allow automatic conversion for models not uploaded by Unsloth.
- **Optimizing Batch Size for Training**: A discussion arose about the optimal **batch size**, where a larger size may improve training stability and accuracy, although it requires more VRAM.
   - Members agreed that increasing gradient accumulation could be an alternative for those with limited VRAM.
- **Function Calling and Models Understanding**: Clarification was sought about model prompt formats including function calls, with some members noting that including special tokens directly is feasible.
   - A resource link was shared, illustrating the prompt format for function calling in Llama models.
- **Multi-GPU Support for Unsloth Pro**: A user inquired if **multi-GPU support** for Unsloth Pro is operational, specifically if it works with local setups or only through cloud platforms.
   - The response confirmed that multi-GPU functionality is available, enhancing the model training experience.
- **Addressing Overfitting in Fine-tuned Models**: A member reported poor performance from their exported fine-tuned model on **Hugging Face**, suggesting potential **overfitting**.
   - Another member advised that issues might stem from the model parameters or dataset quality rather than the fine-tuning framework itself.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://docs.unsloth.ai/basics/chat-templates>)...">no title found</a>: no description found</li><li><a href="https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama">Tutorial: How to Finetune Llama-3 and Use In Ollama | Unsloth Documentation</a>: Beginner&#x27;s Guide for creating a customized personal assistant (like ChatGPT) to run locally on Ollama</li><li><a href="https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/text_prompt_format.md#zero-shot-function-calling-e2e-format>)">llama-models/models/llama3_2/text_prompt_format.md at main · meta-llama/llama-models</a>: Utilities intended for use with Llama models. Contribute to meta-llama/llama-models development by creating an account on GitHub.
</li>
</ul>

</div>
  

---


### **Unsloth AI (Daniel Han) ▷ #[off-topic](https://discord.com/channels/1179035537009545276/1179039861576056922/1318684381124100097)** (139 messages🔥🔥): 

> `Open Source Reasoning Models, Unsloth Model Training, Fine-Tuning with QwQ, DiLoCo Presentation, LORA vs Model Architecture` 


- **Open Source Reasoning Models Debate**: Members discussed the effectiveness of open source reasoning models like **QwQ** potentially outperforming traditional models, noting that while reproducing reasoning is easy, creating a successful model remains challenging.
   - There's skepticism around the necessity of reinforcement learning (RL) in current model designs, with suggestions that pure supervised fine-tuning (SFT) coupled with high-quality datasets may suffice.
- **Unsloth Training Experiences**: A user detailed their experience with **Unsloth** for training models, encountering issues related to saving models in GGUF format due to dependencies on external repositories.
   - The conversation included troubleshooting methods, highlighting the importance of proper installations and the need for specific files to be present for successful execution.
- **Differences between Adapter and Model Explained**: Users received clarification that models consist of a collection of weights affecting their parameters, while Low-Rank Adapters (LoRAs) only modify a small subset of these parameters.
   - This discussion emphasized how LoRAs can be combined with models for efficient training without altering the entire architecture.
- **DiLoCo Research Sharing**: One member shared their research on **DiLoCo** (Distributed Low-Communication Training of Language Models) and created a presentation for their group, sparking interest from others in the channel.
   - The member was encouraged to post their findings in a broader context for additional feedback.
- **Training with LORA Output Size Queries**: A user inquired about the expected output size when training a model with LoRA, noting that their output was significantly smaller than expected due to the nature of adapter training.
   - Discussions followed about how to combine models and adapters effectively, with references to documentation on saving and quantizing models properly.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing#scrollTo=r2v_X2fA0Df5">Google Colab</a>: no description found</li><li><a href="https://huggingface.co/settings/tokens">Hugging Face – The AI community building the future.</a>: no description found</li><li><a href="https://medium.com/@yuxiaojian/fine-tuning-ollama-models-with-unsloth-a504ff9e8002">Fine-Tuning Ollama Models with Unsloth</a>: In the previous two articles, we explored Host Your Own Ollama Service in a Cloud Kubernetes (K8s) Cluster and Run Your Own OLLAMA in…</li><li><a href="https://docs.unsloth.ai/basics/saving-and-using-models/saving-to-gguf">Saving to GGUF | Unsloth Documentation</a>: Saving models to 16bit for GGUF so you can use it for Ollama, Jan AI, Open WebUI and more!</li><li><a href="https://huggingface.co/collections/kaleinaNyan/eule-675ad4e60d8d2cd0d958b32a">Eule - a kaleinaNyan Collection</a>: no description found</li><li><a href="https://huggingface.co/kaleinaNyan/eule-qwen2.5instruct-7b-111224">kaleinaNyan/eule-qwen2.5instruct-7b-111224 · Hugging Face</a>: no description found</li><li><a href="https://docs.unsloth.ai/get-started/unsloth-notebooks">Unsloth Notebooks | Unsloth Documentation</a>: See the list below for all our notebooks:</li><li><a href="https://docs.google.com/presentation/d/18Twuq0q1H75BxUOgRc8ZTs2lWGvE2XtnAGZ7CWtq3cA/edit?usp=sharing">DiLoCo: Distributed Low-Communication Training of Language Models</a>: DiLoCo: Distributed Low-Communication Training of Language Models OpenDiLoCo: An Open-Source Framework for Globally Distributed Low-Communication Training INTELLECT-1 Technical Report</li><li><a href="http://arxiv.org/abs/2311.08105">DiLoCo: Distributed Low-Communication Training of Language Models</a>: Large language models (LLM) have become a critical component in many applications of machine learning. However, standard approaches to training LLM require a large number of tightly interconnected acc...</li><li><a href="http://arxiv.org/abs/2407.07852">OpenDiLoCo: An Open-Source Framework for Globally Distributed Low-Communication Training</a>: OpenDiLoCo is an open-source implementation and replication of the Distributed Low-Communication (DiLoCo) training method for large language models. We provide a reproducible implementation of the DiL...
</li>
</ul>

</div>
  

---


### **Unsloth AI (Daniel Han) ▷ #[help](https://discord.com/channels/1179035537009545276/1179777624986357780/1318787357348204586)** (15 messages🔥): 

> `Llama 3.2 training loss, M4 MAX GPU compatibility, Unsloth support on Mac` 


- **Loss Discrepancy with Llama 3.2**: A user reported that their loss is **3x higher** when training the **Llama 3.2** **1bn instruct model** using the **llama template** compared to the **alpaca prompt**, initially starting at **5.1**.
   - Another user sought clarification on whether the dataset was used correctly with the llama template.
- **M4 MAX GPUs still uncharted territory**: A user inquired about support for **M4 MAX GPUs**, noting that the current **conda install** instructions are only for **CUDA**.
   - The response indicated that **Unsloth** is not currently supported on Macs.
- **Mac Support Timeline Speculated**: A member speculated that support for Macs should land around **Q2 2025**, but it depends on available time for development.
   - Community contributions are encouraged to expedite this process.
- **Limited Fine-Tuning Options on Mac**: A user mentioned the lack of **fast fine-tuning alternatives** on Mac and questioned if that is still the case.
   - The response confirmed the uncertainty, as that user does not have **NVIDIA** hardware.


  

---


### **OpenRouter (Alex Atallah) ▷ #[announcements](https://discord.com/channels/1091220969173028894/1092729520181739581/1318791940686479383)** (1 messages): 

> `OpenAI o1 model, Structured outputs, EVA Llama model, Price reductions, Provider pages` 


- **OpenAI o1 model launches with cool features**: The new OpenAI **o1 model** is live, succeeding the [o1-preview](https://openrouter.ai/openai/o1-preview) with features like **function calling** and reduced latency.
   - It introduces a new `reasoning_effort` API parameter for controlling the model's thinking time before answering, enhancing user interactivity.
- **Structured outputs gain traction**: OpenRouter now normalizes **structured outputs** for **46 models** across **8 different companies**, making it easier to get results in a preferred format.
   - A tutorial on this finesse was shared [here](https://x.com/OpenRouterAI/status/1869077909438091485), highlighting its relevance in practical usage.
- **New storytelling model EVA Llama joins the lineup**: A new roleplay and storytelling model, **EVA Llama**, has been launched along with updates for **Grok 2** and **Cohere** models.
   - Users can explore **EVA Llama** details in more depth via [this link](https://openrouter.ai/eva-unit-01/eva-llama-3.33-70b).
- **Exciting price drops on popular models**: A **12.5% reduction** has been implemented for the **mythomax-l2-13b** model, making it more accessible.
   - In addition, there's a whopping **55% price drop** for the sought-after **QwQ reasoning model**, impressing the community with affordability.
- **Provider pages offer insightful analytics**: Users can now click on provider names to view model hosting charts, enhancing transparency about performance over time.
   - An example was noted with [DeepInfra's provider page](https://openrouter.ai/provider/deepinfra), providing detailed insights.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://openrouter.ai/openai/o1-preview>)">o1-preview - API, Providers, Stats</a>: The latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding.The o1 models are optimized for math, science, programming, and other STEM-related tasks...</li><li><a href="https://x.com/OpenRouterAI/status/1869077909438091485))">Tweet from OpenRouter (@OpenRouterAI)</a>: Structured outputs are very underrated. It&#39;s often much easier to constrain LLM outputs to a JSON schema than asking for a tool call.OpenRouter now normalizes structured outputs for- 46 models- 8 ...</li><li><a href="https://openrouter.ai/openai/o1>)">OpenRouter</a>: A unified interface for LLMs. Find the best models &amp; prices for your prompts</li><li><a href="https://openrouter.ai/eva-unit-01/eva-llama-3.33-70b>)">EVA Llama 3.33 70b - API, Providers, Stats</a>: EVA Llama 3.33 70b is a roleplay and storywriting specialist model. Run EVA Llama 3.33 70b with API</li><li><a href="https://openrouter.ai/models)">OpenRouter</a>: A unified interface for LLMs. Find the best models &amp; prices for your prompts</li><li><a href="https://openrouter.ai/provider/deepinfra)">OpenRouter</a>: A unified interface for LLMs. Find the best models &amp; prices for your prompts</li><li><a href="https://openrouter.ai/gryphe/mythomax-l2-13b>)">MythoMax 13B - API, Providers, Stats</a>: One of the highest performing and most popular fine-tunes of Llama 2 13B, with rich descriptions and roleplay. #merge. Run MythoMax 13B with API</li><li><a href="https://openrouter.ai/qwen/qwq-32b-preview)">QwQ 32B Preview - API, Providers, Stats</a>: QwQ-32B-Preview is an experimental research model focused on AI reasoning capabilities developed by the Qwen Team. As a preview release, it demonstrates promising analytical abilities while having sev...</li><li><a href="https://x.com/OpenRouterAI/status/1869237170952978935">Tweet from OpenRouter (@OpenRouterAI)</a>: OpenAI o1 is now live for all! Try its 🧠 on:- image inputs- structured outputs- function calling- a &#34;reasoning effort&#34; controlThe Chatroom link below has a couple challenges you can try with ...
</li>
</ul>

</div>
  

---


### **OpenRouter (Alex Atallah) ▷ #[general](https://discord.com/channels/1091220969173028894/1094454198688546826/1318682928175382682)** (209 messages🔥🔥): 

> `Exposed OpenRouter keys, Chat details in API, Using OpenRouter API keys with PKCE, OpenRouter pricing structure, Model performance comparisons` 


- **Reporting Exposed OpenRouter Keys**: A user discovered exposed OpenRouter API keys on GitHub with limits over $100 and inquired where to report them, with a member suggesting support@openrouter.ai.
   - There was a discussion about the safety of sending these compromised keys over email.
- **Retrieving Chat Details in API**: An inquiry was made about viewing chat details of API calls, amid concerns about the inability to retrieve prompts or responses once the metadata is accessed.
   - A suggestion was made for having a flag to see conversations as chats instead of stateless requests.
- **Using OpenRouter with PKCE**: A user discussed creating a web app using OpenRouter API keys via PKCE, considering the security of handling keys on the client-side versus the backend.
   - Recommendations were made for managing API keys securely while maintaining a near-stateless architecture.
- **OpenRouter Pricing and Costs**: Clarifications were sought regarding the costs associated with OpenRouter's service, particularly if using own API keys incurred additional fees.
   - It was noted that using custom keys incurs a 5% fee on top of the upstream provider's costs.
- **Performance of Various Models**: A user noted inconsistencies in model responses, particularly with QwQ, prompting discussions about the role of model sizes in instruction following.
   - Users were encouraged to utilize higher-end models like Google Experimental 1206 or DeepSeek-v2 for more consistent coding assistance.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://openrouter.ai/terms#_4_-payment">OpenRouter</a>: A unified interface for LLMs. Find the best models &amp; prices for your prompts</li><li><a href="https://openrouter.ai/docs/integrations">Integrations | OpenRouter</a>: Bring your own provider keys with OpenRouter</li><li><a href="https://openrouter.ai/docs/integrations#embeddings">Integrations | OpenRouter</a>: Bring your own provider keys with OpenRouter</li><li><a href="https://cdn.openai.com/spec/model-spec-2024-05-08.html">Model Spec (2024/05/08)</a>: no description found</li><li><a href="https://openrouter.ai/docs/limits">Limits | OpenRouter</a>: Set limits on model usage</li><li><a href="https://openrouter.ai/rankings">LLM Rankings | OpenRouter</a>: Language models ranked and analyzed by usage across apps
</li>
</ul>

</div>
  

---


### **Eleuther ▷ #[general](https://discord.com/channels/729741769192767510/729741769738158194/1318986871719858287)** (1 messages): 

> `Retail/E-commerce ad models, Runway, OpenAI Sora, Veo 2` 


- **Seeking Effective Retail Ad Models**: A member queried about effective models for creating **retail/e-commerce ad content**, including both video and copy formats.
   - They specifically mentioned considering **Runway**, **OpenAI Sora**, and **Veo 2**, while inviting suggestions for other options.
- **Exploring Alternatives for Ad Content**: The discussion aimed to identify **other potential models** tailored for ad content beyond what was already mentioned.
   - The member's focus on gathering diverse options led to a broader conversation on existing technologies in the market.


  

---


### **Eleuther ▷ #[research](https://discord.com/channels/729741769192767510/747850033994662000/1318673940763119689)** (123 messages🔥🔥): 

> `Warmup phase for learning rates, Meta-Learning to reduce overfitting, Compression methods in neural networks, Grokking in large models, Koopman operator theory in neural networks` 


- **Debating the Warmup Phase Formula**: Kevin's formula for approximating the warmup phase, which is (1 - beta1^step), currently lacks support from LR schedulers, leading to discussions on its implementation.
   - Members shared their implementations, expressing concerns about off-by-one errors related to step counts when using lambdaLR.
- **Utilizing Meta-Learning to Address Overfitting**: A discussion emerged on whether **Meta-Learning** could assist in mitigating overfitting in supervised learning models with specific examples requested.
   - The community noted that while theoretical frameworks exist, practical examples remain sparse.
- **Exploring Compression Techniques in Neural Networks**: Members explored ideas around compressing neural networks, with emphasis on depthwise compression and neural network pruning methods like OATS which combine sparse and low-rank matrices.
   - Concerns were raised about the potential loss in data coverage and performance due to compression, particularly in regards to models trained for memorization tasks.
- **Grokking as a Central Theme in AI Research**: The phenomenon of grokking was discussed, focusing on its significance and the lack of compelling methods to induce it within AI models.
   - There is a shared sentiment that while grokking is somewhat studied, the predominant research interest lies with large language models, overshadowing broader exploration.
- **Skepticism Towards Koopman Theory Integration**: The applicability of Koopman operator theory to neural networks was debated, with members expressing skepticism about its benefits and the legitimacy of framing neural layers as dynamical systems.
   - Critics pointed out the potential for obfuscation in the paper, arguing it mainly translates to the utilization of residual connections rather than introducing significant innovations.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://x.com/BlinkDL_AI/status/1869433254425833487">Tweet from BlinkDL (@BlinkDL_AI)</a>: RWKV-7-World 0.1B (L12-D768) trained w/ ctx4k perfectly solves NIAH ctx16k 🤯 100% RNN and attention-free. RWKV is all you need. https://www.rwkv.com/ #RWKVQuoting BlinkDL (@BlinkDL_AI) RWKV-7 &#34;Go...</li><li><a href="https://openreview.net/forum?id=DLDuVbxORA">OATS: Outlier-Aware Pruning Through Sparse and Low Rank Decomposition</a>: The recent paradigm shift to large-scale foundation models has brought about a new era for deep learning that, while has found great success in practice, has also been plagued by prohibitively...</li><li><a href="https://arxiv.org/abs/2304.15004">Are Emergent Abilities of Large Language Models a Mirage?</a>: Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguin...</li><li><a href="https://arxiv.org/abs/1810.01479">Time-Delay Observables for Koopman: Theory and Applications</a>: Nonlinear dynamical systems are ubiquitous in science and engineering, yet analysis and prediction of these systems remains a challenge. Koopman operator theory circumvents some of these issues by con...</li><li><a href="https://x.com/BlinkDL_AI/status/1869368399849238727">Tweet from BlinkDL (@BlinkDL_AI)</a>: RWKV-7 &#34;Goose&#34; 🪿 0.4B trained w/ ctx4k automatically extrapolates to ctx32k+, and perfectly solves NIAH ctx16k🤯Only trained on the Pile. No finetuning. Replicable training runs. tested by ou...</li><li><a href="https://www.jasonwei.net/blog/common-arguments-regarding-emergent-abilities">Common arguments regarding emergent abilities &mdash; Jason Wei</a>: This blog post doesn’t represent the positions of my employer (past, present, or future).     I’ll review some common arguments that come up when discussing emergent abilities of large language models...</li><li><a href="https://arxiv.org/abs/2409.01308">Representing Neural Network Layers as Linear Operations via Koopman Operator Theory</a>: The strong performance of simple neural networks is often attributed to their nonlinear activations. However, a linear view of neural networks makes understanding and controlling networks much more ap...</li><li><a href="https://distill.pub/2020/growing-ca/">Growing Neural Cellular Automata</a>: Training an end-to-end differentiable, self-organising cellular automata model of morphogenesis, able to both grow and regenerate specific patterns.</li><li><a href="https://github.com/Jamba15/SpectralTools">GitHub - Jamba15/SpectralTools: Spectral analysis and training of dense layers</a>: Spectral analysis and training of dense layers . Contribute to Jamba15/SpectralTools development by creating an account on GitHub.
</li>
</ul>

</div>
  

---


### **Eleuther ▷ #[lm-thunderdome](https://discord.com/channels/729741769192767510/755950983669874798/1318956907389784074)** (6 messages): 

> `doc_to_text function arguments, Creating new configs, Overloading config fields` 


- **Extra arguments for doc_to_text function**: A user inquired whether it's possible to pass extra arguments to the **doc_to_text** function in a new task.
   - Another member clarified that the main entry point for this is through **configs**.
- **Creating different configs for prompts**: A user explained they have a **base config** where functions are defined and are considering separate configs for different prompts.
   - This would lead to the creation of different subtasks for each prompt, enhancing task customization.
- **Overloading configs with included tasks**: It was suggested that a new config can be created based on another using `include: <other configs>` to overload specific fields.
   - However, this approach would apply the overload across all included tasks in that group config, which could limit flexibility.
- **Link to MMLU config example**: A member shared that users can also add contents to a group config, but it will overload the included tasks overall.
   - They provided a reference link to the [MMLU config](https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/mmlu/default/_mmlu.yaml) for further details.


  

---


### **Eleuther ▷ #[gpt-neox-dev](https://discord.com/channels/729741769192767510/730090096287547444/1318925162933915749)** (9 messages🔥): 

> `WANDB logging, Configuring WANDB run names, Pull Requests on features` 


- **WANDB Logging for MFU and Performance Metrics**: A member inquired about the possibility of logging **MFU**, **batches/sec**, and **tokens/sec** to WANDB during **pretraining** with neox, hinting that it would be beneficial for direct plots.
   - Another member confirmed that while there isn't an option currently available, it may be implemented similarly to the [existing logging method](https://github.com/EleutherAI/gpt-neox/blob/f5325805678c2b9e35aae4528283e0132c5f5bbc/megatron/logging.py#L352-L361).
- **Setting WANDB Run Names in Config**: A user sought clarity on how to set a WANDB run name from the config, but encountered errors when attempting to add it directly.
   - One member responded that this option isn't currently available but promised to add it along with the metrics logging in a forthcoming PR.
- **Pull Requests Planned on Features**: A member expressed intent to submit a pull request (PR) for the non-parametric layernorm feature over the weekend.
   - Another member offered assistance with the logging improvements but later assured they will handle the PR themselves.



**Link mentioned**: <a href="https://github.com/EleutherAI/gpt-neox/blob/f5325805678c2b9e35aae4528283e0132c5f5bbc/megatron/logging.py#L352-L361">gpt-neox/megatron/logging.py at f5325805678c2b9e35aae4528283e0132c5f5bbc · EleutherAI/gpt-neox</a>: An implementation of model parallel autoregressive transformers on GPUs, based on the Megatron and DeepSpeed libraries - EleutherAI/gpt-neox

  

---


### **Stability.ai (Stable Diffusion) ▷ #[general-chat](https://discord.com/channels/1002292111942635562/1002292112739549196/1318674059189424158)** (122 messages🔥🔥): 

> `Lora Training Techniques, Current Models in Use, Running Stable Diffusion on Linux, Navigating Image Resolution and Performance, Understanding AI Generated Content and Models` 


- **Effective Steps for Lora Training**: A user shared practical steps for creating a Lora: start with a strong dataset, choose an appropriate model, train the Lora, then test it. They emphasized research on creating quality datasets for optimal results.
- **Preferred Models for Stable Diffusion**: Various users discussed their models of choice; some favor the 'flux' model while others recommend 'InvokeAI' for its usability. Others pointed out the importance of having an NVIDIA GPU, suggesting a 3060 with 16GB VRAM for smoother performance.
- **Stable Diffusion on Ubuntu Challenges**: Users expressed frustrations with running SDXL on Ubuntu, citing issues with ComfyUI and Forge UI's Linux compatibility. It was noted that running SDXL effectively may require familiarity with the system.
- **Choosing Image Resolution for Generative Models**: A beginner asked about optimal image resolution for generation, finding a balance between quality and processing time. Recommendations included experimenting with around 1024x1024 resolution and using hires.fix for better quality output.
- **Understanding AI Generated Content Metrics**: Discussion emerged around the techniques and metrics used in model training, specifically with the Pony model and its scoring system. Users noted how this unique approach impacts image generation and community perceptions.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://civitai.com/models/1045828">Epoch Helper - v1.1 | Other Other | Civitai</a>: source code - https://github.com/Monkellie/epochcalc # The Epoch Helper Tool This is a tool I created (AI Assisted) to help myself with calculation...</li><li><a href="https://www.youtube.com/watch?v=AbB33AxrcZo"> - YouTube</a>: no description found</li><li><a href="https://github.com/lllyasviel/stable-diffusion-webui-forge/blob/main/webui-user.sh">stable-diffusion-webui-forge/webui-user.sh at main · lllyasviel/stable-diffusion-webui-forge</a>: Contribute to lllyasviel/stable-diffusion-webui-forge development by creating an account on GitHub.</li><li><a href="https://evermeet.cx/ffmpeg/">static FFmpeg binaries for macOS 64-bit Intel</a>: Download static FFmpeg binaries for macOS 64-bit Intel. snapshots and release binaries are available. FFmpeg developers strongly encourage all users to use a current snapshot build instead of a releas...
</li>
</ul>

</div>
  

---


### **Perplexity AI ▷ #[announcements](https://discord.com/channels/1047197230748151888/1047204950763122820/1318746208197414943)** (1 messages): 

> `Custom Web Sources, Perplexity Spaces` 


- **Introducing Custom Web Sources in Perplexity Spaces!**: Perplexity now allows users to **choose custom web sources** for searches, enabling further tailoring of queries to specific use cases that matter most to [you](https://www.perplexity.ai/spaces?utm_source=discord&utm_campaign=websourceslaunch122624).
   - Accompanying this announcement is a [launch video](https://cdn.discordapp.com/attachments/1047204950763122820/1318746209778929674/Custom_web_sources_launch_video_-_v6.mp4?ex=67641a5d&is=6762c8dd&hm=df3d15393ffcbb4e7a4be49861d8a1530b60a9dde6a330974e1d1d5ec7789ad2&) showcasing the new feature.
- **A New Level of Customization!**: This update provides **enhanced customization** options for users, allowing them to curate their Perplexity experience more effectively based on their specific needs.
   - By selecting the websites Perplexity searches, users can improve the relevance and quality of the information retrieved tailored to their preferences.


  

---


### **Perplexity AI ▷ #[general](https://discord.com/channels/1047197230748151888/1047649527299055688/1318671407638511719)** (108 messages🔥🔥): 

> `Perplexity Pro Subscriptions, New Features and Updates, User Experience with AI Models, Rate Limits and Performance, User Interface Suggestions` 


- **Perplexity Pro Subscriptions Available**: Users discussed the launch of [Perplexity Pro subscriptions](https://perplexity.supply/shop/perplexity-subscription) that allow gifting knowledge with options for 1 to 12 month duration, enhancing the user experience.
   - This subscription provides additional features such as searching 3x as many sources and accessing the latest AI models.
- **Call for New Features amidst User Expectations**: Members expressed desires for **new features** from Perplexity, especially as competitors like Google and OpenAI release new models frequently, generating a sense of stagnation at Perplexity.
   - Thoughts on potentially collaborating with firms like Meta for advancements were also shared, highlighting urgency in innovation.
- **Concerns about Rate Limits**: A user reported hitting **rate limits** while using Perplexity, receiving messages indicating they needed to sign up for higher personalized rate limits for better access.
   - Other users speculated on the actual benefits of higher tiers in alleviating these restrictions and shared personal experiences with the rate limits.
- **User Interface Enhancement Suggestions**: One user suggested adding a **snowfall effect** to the Perplexity UI, receiving mixed feedback; some found it visually appealing while others preferred practicality.
   - Members continued to discuss how interface aesthetics and usability could better meet their professional needs.
- **Discussion on AI Model Performance**: Conversations emerged around the **performance of AI models**, with some users feeling the Pro Search quality could be improved based on their experiences.
   - A user proposed using Claude 3.5 Sonnet for better outcomes and questioned the claimed advancements with models like GPT-4o.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://x.com/pplxsupply/status/1868738538231287816?s=46">Tweet from Perplexity Supply (@PPLXsupply)</a>: Give the gift of knowledge. Perplexity Pro gift subscriptions now available.</li><li><a href="https://perplexity.supply/shop/perplexity-subscription">Perplexity Pro Subscription | Perplexity Supply</a>: Perplexity Supply exists to explore the relationship between fashion and intellect with thoughtfully designed products to spark conversations and showcase your infinite pursuit of knowledge.</li><li><a href="https://vm.tiktok.com/ZMkjhUDEa/">TikTok - Make Your Day</a>: no description found
</li>
</ul>

</div>
  

---


### **Perplexity AI ▷ #[sharing](https://discord.com/channels/1047197230748151888/1054944216876331118/1318733227187372183)** (4 messages): 

> `Meta vs OpenAI Pro-Fit, Microbe Threat Warning, Plant Communication, Dopamine Precursors, Cell Revival` 


- **Meta wants to block OpenAI's for-profit ventures**: Meta has expressed its desire to prevent OpenAI from pursuing for-profit business models, as discussed in various forums.
   - An intriguing discussion surrounds the impact this could have on future AI developments within the industry.
- **Microbe Threat Warning surfaces**: The community highlighted a recent warning regarding potential threats posed by microbes, an issue that could affect ecological balance.
   - The discourse included references to preventive measures and the importance of awareness in addressing these microbial risks.
- **Plants exhibit crying behavior**: A shared article explored the concept of plants exhibiting 'crying' behavior, suggesting a unique form of communication among flora.
   - The findings suggest implications for understanding plant responses to environmental stress, stirring curiosity about plant intelligence.
- **Understanding dopamine precursors**: A resource was linked concerning precursors to dopamine, shedding light on the biochemical pathways essential for mental health.
   - This topic stirred interest in the community regarding its relevance to neurological studies and potential therapeutic outcomes.
- **Reviving dead cells technology**: Members discussed fascinating advancements in technology that allow for the revival of dead cells, posing significant bioethical questions.
   - The implications of this technology on both medicine and ethics sparked lively debate within the group.



**Link mentioned**: <a href="https://www.youtube.com/embed/7PBvDi_aKbs">YouTube</a>: no description found

  

---


### **Perplexity AI ▷ #[pplx-api](https://discord.com/channels/1047197230748151888/1161802929053909012/1319010860332482601)** (1 messages): 

> `Perplexity API, Web Search Feature, Cost Overview` 


- **Inquiring about Web Search in Perplexity API**: A member asked if the **web search feature** is included in the **chat completion API call** for their upcoming project using Perplexity.
   - This raises important questions about the integration capabilities of Perplexity compared to other APIs they've worked with.
- **Seeking Cost Overview for Perplexity**: The same member expressed interest in finding a **cost overview** for using Perplexity's services.
   - Understanding the pricing structure will be crucial for planning their project effectively.


  

---


### **GPU MODE ▷ #[general](https://discord.com/channels/1189498204333543425/1189498205101109300/1318676213417381979)** (41 messages🔥): 

> `6D Parallelism Article, PC Troubleshooting, GPU Performance and Coil Whine, Multi-GPU Instances with NVLink, Coil Whine and Audio Experimentation` 


- **Insight into 6D Parallelism**: A detailed article on [6D parallelism](https://main-horse.github.io/posts/visualizing-6d/) highlights **collective communications** in training, aiming to provide clearer visuals and explanations compared to existing resources.
   - It critiques the lack of depth in other writings that fail to address the complexity of combining various training approaches.
- **Successful PC Boot After Wait**: A user experienced initial issues with their new PC, reporting no signal to the monitor until the system booted successfully after about a minute as the **LEDs turned off**.
   - Another member suggested trying a single memory stick to troubleshoot the issue.
- **Radeon Card Frustrations**: A user expressed dissatisfaction with their **Radeon card**, noting it produces around **10 FPS more** than an Nvidia 4060, but suffers from unacceptable coil whine.
   - They concluded that both the Radeon and Nvidia cards have their drawbacks, with the coil whine from the Radeon being particularly troublesome.
- **Seeking Multi-GPU Instances on VastAI**: A user inquired about finding **multi-GPU instances** with NVLink on VastAI, concerned about the limited **NVLink bandwidth** visible in the listings.
   - They speculated that another member might have experience with this subject based on past conversations.
- **Coil Whine Musical Experiment Idea**: Discussion arose about the notion of creating a program that uses coil whine from GPUs to play music, with members noting its **pitch changes** based on power draw.
   - One member humorously suggested donating a GPU for this musical project, linking it to their experiences with coil whine.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://main-horse.github.io/series/nccl-source-code-study/">NCCL Source Code Study</a>: no description found</li><li><a href="https://main-horse.github.io/posts/visualizing-6d/">Visualizing 6D Mesh Parallelism</a>: Plus some lore
</li>
</ul>

</div>
  

---


### **GPU MODE ▷ #[triton](https://discord.com/channels/1189498204333543425/1189607595451895918/1318874718299885653)** (1 messages): 

> `Kernel Computation Optimization, Memory Management in GPU, Output Concatenation Techniques` 


- **Concatenation inside GPU Kernel**: A user asked if there's an efficient way to concatenate outputs inside a loop within a GPU kernel, referencing previous successful summation methods.
   - They suggested that writing out to global memory during the loop may be slow and inquired about using syntax like `var[idx:idx+block_size] = value` as a feasible alternative.
- **Seeking Efficient Memory Techniques**: Another discussion point emerged regarding the speed of writing to global memory when concatenating outputs in GPU kernels.
   - The user emphasized the need for a non-slow solution while running operations inside a loop, indicating a common concern among developers.


  

---


### **GPU MODE ▷ #[cuda](https://discord.com/channels/1189498204333543425/1189607726595194971/1318751828472762399)** (8 messages🔥): 

> `CUDA Memory Copy Issues, Comparing A100 and H100 GPUs, AMP Related Differences` 


- **CUDA Memory Copy Issues**: A member reported that commenting out a specific code segment related to **IsDense(y) && IsSame(x, y)** resulted in correct functionality, while including it led to unexpected behavior during LLM model inference.
   - They noted that **CudaCopy** will trigger CUDA kernels, raising questions on the handling of memory operations in this context.
- **A100 vs H100 Training Discrepancies**: A member inquired about the differences between **A100** and **H100** GPUs during training, specifically noting a **0.3% loss** discrepancy in the first step of training with a single GPU task.
   - This unexpected result prompted discussions and potential comparisons of performance metrics between the two GPU models.
- **Questions on CUDA Graph Support**: A member raised concerns regarding a lack of official documentation on why **CUDA graphs** could not support the **cudaMemcpyAsync** operation within their implementations.
   - This led to further discussions about asynchronous operations and their limitations with CUDA graphs.
- **AMP Potential Influence**: A member speculated if the discrepancies noticed between the A100 and H100 might be related to **Automatic Mixed Precision (AMP)** settings utilized during training.
   - This opened up a dialogue on how AMP can affect training outcomes and whether adjustments are necessary for different GPU models.


  

---


### **GPU MODE ▷ #[torch](https://discord.com/channels/1189498204333543425/1189607750876008468/1318671579131150367)** (33 messages🔥): 

> `Megatron-LM efficiency, Torch.compile warnings handling, Distributed training community, FlexAttention development, Keras/PyTorch contributions` 


- **Megatron-LM's training efficiency questioned**: A member inquired if **Megatron-LM** is still efficient for training, as they plan to enhance **training throughput** in a distributed setup.
   - Another member suggested reaching out to **Gensyn** for insights, mentioning Christine Yip's active community for distributed training.
- **Handling Torch.compile warnings intelligently**: A user sought guidance on managing **torch.compile** warnings when supporting various shapes, highlighting slow kernels with `dynamic=True`.
   - Another member proposed using `fn_compiled = torch.compile(fn)` to adaptively handle function calls without being tied to the decorator.
- **Challenges in developing FlexAttention**: A discussion highlighted the difficulties faced in upgrading **PyTorch**, with members sharing the extensive processes involved in image builds.
   - One member acknowledged the aim to make **FlexAttention** robust while managing upgrade challenges due to their customized setups.
- **Wrapping model invocations for better flexibility**: A member suggested wrapping the model invocation rather than the **torch.compile** for better handling of shape variations.
   - They also noted the possibility of using Python's warnings module to filter out specific warnings rather than suppressing all logs.
- **Interest in contributions to Keras/PyTorch**: An inquiry was raised about ongoing contributions to **Keras** and **PyTorch**, emphasizing community engagement.
   - This may signal an interest in collaboration or getting involved in further development efforts.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://discuss.pytorch.org/t/reduce-time-to-first-kernel-when-using-cuda-graphs/214310">Reduce time to first kernel when using CUDA graphs</a>: I’ve been profiling the inference stack that I’m using against vLLM and I found that in their case after calling graph replay, the first kernel gets executed almost instantly(left), whereas in my code...</li><li><a href="https://github.com/pytorch/torchtitan">GitHub - pytorch/torchtitan: A native PyTorch Library for large model training</a>: A native PyTorch Library for large model training. Contribute to pytorch/torchtitan development by creating an account on GitHub.
</li>
</ul>

</div>
  

---


### **GPU MODE ▷ #[cool-links](https://discord.com/channels/1189498204333543425/1189868872887705671/1318674557439311932)** (4 messages): 

> `Raspberry Pi 5 Deployment, Edge Device Models, Esp32 / Xtensa LX7 Chips` 


- **Raspberry Pi 5 with NVMe Speeds Up LLM Performance**: The **Raspberry Pi 5** has been overclocked to **2.8GHz** with a **256GB NVMe** to enhance data transfer speeds for deploying smaller **1.5B parameter models**.
   - Using **Ollama** compiled with **OpenBlas**, the models are run locally on the Pi5, streamlining edge device operations.
- **Excitement for Esp32 / Xtensa LX7 Chips**: There's anticipation for the **Esp32 / Xtensa LX7 chips** to enable a new scenario where LLMs are called remotely via **API**.
   - One user expressed enthusiasm, stating it looks **fun!** as they explore different deployment strategies.


  

---


### **GPU MODE ▷ #[jobs](https://discord.com/channels/1189498204333543425/1190208177829068860/1319053856751095948)** (1 messages): 

> `MatX LLM accelerator, Job openings in ML, ASIC roles` 


- **MatX develops LLM accelerator ASIC**: MatX is actively building an **LLM accelerator ASIC** aimed at enhancing machine learning performance and efficiency.
   - They are currently seeking candidates for roles like **low-level compute kernel author**, **compiler**, and **ML performance engineer** to join their team.
- **Hiring opportunities at MatX**: MatX has multiple job openings listed on their website, including positions crucial for the development of their ASIC technology.
   - Interested candidates can find more details about these opportunities at [MatX Careers](https://matx.com/jobs).



**Link mentioned**: <a href="https://matx.com/jobs">Tweet from MatX | Jobs</a>: no description found

  

---


### **GPU MODE ▷ #[torchao](https://discord.com/channels/1189498204333543425/1205223658021458100/1318929596292534343)** (5 messages): 

> `int4group scheme, Training process quantization, Tinygemm compute method` 


- **Clarification on int4group flow**: A member inquired whether in the **int4group scheme**, the weights remain quantized (int4) while activations stay in fp16, leading to a matmul of fp16 x int4 = fp16.
   - An image was shared to visualize the process, confirming the understanding aligns with the described flow.
- **No quantization for activations during training**: A discussion about the training process questioned if there would be any **fake quantization** for activations.
   - It was clarified that **Tinygemm uses bf16 for compute**, and at both QAT and inference time, activations remain unquantized.
- **On-the-fly dequantization in kernels**: A member confirmed that the **dequantization of int4 weight** occurs on the fly inside the matmul kernel.
   - This aligns with expectations on processing flow, providing clarity on how the matmul kernel operates.


  

---


### **GPU MODE ▷ #[rocm](https://discord.com/channels/1189498204333543425/1233704710389764236/1318820781857177630)** (1 messages): 

> `MigraphX in MI300X, ONNX frontend support, Opset compatibility` 


- **Building MigraphX on MI300X for ONNX**: Discussion centered around the possibility of building **MigraphX** on the **MI300X** for the **ONNX** frontend, suggesting it should be feasible.
   - One member noted, *'I didn't check the opset(11) supported, should be the latest one'* indicating the need for further verification on compatibility.
- **Inquiry on ONNX Opset Support**: A question was raised regarding whether the operations for **opset(11)** are supported in the latest implementation.
   - This indicates a potential gap in the current knowledge that may need further exploration by the team.


  

---


### **GPU MODE ▷ #[thunderkittens](https://discord.com/channels/1189498204333543425/1300872762163728550/)** (1 messages): 

kimishpatel: what i cam here for 🙂
  

---


### **GPU MODE ▷ #[arc-agi-2](https://discord.com/channels/1189498204333543425/1316377974672588850/1318672936063733890)** (18 messages🔥): 

> `Custom Vision Encoder, Chain of Thought Generation, Axolotl Configurations, Efficient Sampling Processes, Experimenting with Finetuning` 


- **Custom Vision Encoder Discussion**: A member suggested creating a **custom vision encoder** to integrate with existing language models, as current models may not handle small pixel-scale images effectively.
   - The potential benefits of flexibility in pairing the encoder with various LLMs were highlighted, outweighing the improvements from pretrained VLMs.
- **Exploring Chain of Thought Generation**: Discussion centered around the granularity of chain of thought (CoT) implementation, questioning whether it merely explains core ideas or genuinely offers iterative thinking.
   - One member proposed **dual methods**: a reasoning monologue before outputs and multiple templates for guided exploration based on riddle types.
- **Axolotl Lora Configuration Success**: A member confirmed that the example [Axolotl Lora config for llama-3-vision](https://github.com/axolotl-ai-cloud/axolotl/blob/effc4dc4097af212432c9ebaba7eb9677d768467/examples/llama-3-vision/lora-11b.yaml) works well with 2x A6000 GPUs.
   - There was an interest in finding compute sponsors to support larger experiments once initial setups are validated.
- **Decentralized Sampling Process for CoT Prompts**: The potential for running sampling processes without training was discussed, aiming to improve **CoT prompts** through human-guided exploration.
   - This decentralized approach could help in collecting datasets efficiently for future research.
- **Experimenting with RTX 3090 Finetuning**: A member mentioned their capability to run experiments on an **RTX 3090** while inquiring about the best finetuning setup using bf16 or Qlora+int8.
   - There was confirmation that 8bit Lora could indeed work for 8B models on the RTX 3090, referencing an example from **WandB**.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://wandb.ai/augmxnt/train-bench/runs/zelehjsm/overview">augmxnt</a>: Weights & Biases, developer tools for machine learning</li><li><a href="https://github.com/axolotl-ai-cloud/axolotl/blob/effc4dc4097af212432c9ebaba7eb9677d768467/examples/llama-3-vision/lora-11b.yaml">axolotl/examples/llama-3-vision/lora-11b.yaml at effc4dc4097af212432c9ebaba7eb9677d768467 · axolotl-ai-cloud/axolotl</a>: Go ahead and axolotl questions. Contribute to axolotl-ai-cloud/axolotl development by creating an account on GitHub.
</li>
</ul>

</div>
  

---


### **LM Studio ▷ #[general](https://discord.com/channels/1110598183144399058/1110598183144399061/1318672102517248053)** (87 messages🔥🔥): 

> `LM Studio setup, Qwen QwQ and roleplay LLMs, Model compatibility and errors, Using LM Studio on mobile, New developments in AI models` 


- **LM Studio Setup and Compatibility**: Users discussed their setups for LM Studio, mentioning various hardware configurations like RTX 4060 laptops and M3 Max with 96GB RAM, showcasing the application's versatility.
   - A specific case highlighted issues with loading Llama 3.2 11b Vision in LM Studio, where one user encountered a 'unknown model architecture' error.
- **Qwen QwQ as a Roleplay LLM**: Discussions led to suggesting Qwen QwQ as a suitable option for roleplay style applications, with several users expressing satisfaction with its performance.
   - One member noted that Qwen2 performs exceptionally well with Python, indicating its robustness in programming contexts.
- **Model Errors and Download Issues**: An error message related to 'Safetensors header is unexpectedly large' prompted conversations about potential file corruption during downloads.
   - Reminders to ensure that models were downloaded correctly from within LM Studio were made, with some users reporting successful loads on their systems.
- **Using LM Studio on Mobile Devices**: A member expressed interest in accessing LM Studio from their phone while on the go, but discovered there is currently no mobile app available.
   - Suggestions were made to explore alternatives, but direct mobile compatibility remains a limitation.
- **Recent Developments in AI Models**: Users inquired about developments in the application of o1-like CoT to open-source models and mentioned the Falcon3 bitnet models.
   - The community highlighted ongoing interests and trends in AI model enhancements, speculating on future possibilities and accessibility.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://huggingface.co/mlx-community/Llama-3.2-11B-Vision-Instruct-4bit">mlx-community/Llama-3.2-11B-Vision-Instruct-4bit · Hugging Face</a>: no description found</li><li><a href="https://fixupx.com/slow_developer/status/1869059311969661103">Tweet from Haider. (@slow_developer)</a>: 🚨 NVIDIA Introduces Jetson Nano Super&gt; compact AI computer capable of 70-T operations per second&gt; designed for robotics, it supports advanced models, including LLMs, and costs $249</li><li><a href="https://lmstudio.ai/beta-releases">LM Studio Beta Releases</a>: LM Studio Beta Releases
</li>
</ul>

</div>
  

---


### **LM Studio ▷ #[hardware-discussion](https://discord.com/channels/1110598183144399058/1153759714082033735/1318670170578751529)** (17 messages🔥): 

> `3060ti confusion, AMD driver issues, Llama model performance, Inference hardware desires, RAM requirements for large models` 


- **3060ti vs Regular 3060**: Discussion sparked over a possible **3060ti** variant with **11GB**, leading to a clarification that it might be referring to the regular **3060** with **12GB**.
   - Participants expressed confusion over their specifications, with one noting unusual performance behavior.
- **AMD GPUs and Driver Problems**: It was mentioned that the **Radeon VII** might be experiencing the same issues as other **AMD GPUs** with the **24.12.1 driver**, which led one user to revert to **24.10.1**.
   - Issues included loading models forcing the GPU to **100% utilization** without power usage, resulting in significant lag.
- **Llama Model Performance Concerns**: One user reported a stark decrease in performance on a simple **Llama 3.2 3B model**, from **90+ tok/s** on **24.10.1** to **20 tok/s** on the new driver.
   - Another user suggested checking configurations to ensure **llama.cpp** is set to use **CUDA** to potentially improve performance.
- **Desire for Powerful Hardware**: A user expressed a desire for a powerful **M4** MacBook Pro for inference, reflecting on their experience with an **M2 MBA** as a mere introduction.
   - Another member humorously commented on the slippery slope of needing more powerful hardware by referencing **gddr6x**.
- **RAM Requirements for Large Models**: Users discussed the RAM requirements for running large models, noting that running a **70B model** requires **70GB** either in VRAM or main memory.
   - It was emphasized that having **10-20% extra VRAM** is ideal for context and operational flexibility when running at **q8**.


  

---


### **Stackblitz (Bolt.new) ▷ #[prompting](https://discord.com/channels/364486390102097930/1301167628416454737/1318837235692474419)** (6 messages): 

> `Migrating Firebase to Supabase, Using Bootstrap with create-mf-app, Google reCAPTCHA Issues, Testing ChatGPT Bolt Pilot, Vite Pre-Transform Errors` 


- **Migrating from Firebase to Supabase**: A user inquired about the best approach to migrate their entire site built on **Firebase** to **Supabase**.
   - The discussion remains open for strategies and best practices for such a migration.
- **Create-mf-app and Bootstrap Clash**: A member seeks a consistent method to integrate **create-mf-app** with **Bootstrap** without conflicting with **Tailwind**.
   - They noted that attempts to combine the two often lead to a chaotic setup.
- **Google reCAPTCHA Troubleshooting**: A user reported an initial error of 'Invalid key type' while using **Google reCAPTCHA** due to selecting v3 instead of v2 as implemented by **Bolt.new**.
   - After switching to v2, they still face issues with verification counts and not receiving emails from the contact form.
- **Feedback Request for Bolt Pilot**: A member announced their creation of **Bolt Pilot**, a new GPT for **Bolt**, and requested users to test its functionalities.
   - They encouraged feedback on any areas needing readjustment for improvement.
- **Vite Pre-Transform Error Reports**: A user raised concerns about encountering numerous repeated '[vite] Pre-Transform' errors during development.
   - This issue appears to be affecting others as well, inviting further discussion on potential resolutions.


  

---


### **Stackblitz (Bolt.new) ▷ #[discussions](https://discord.com/channels/364486390102097930/680953097354215446/1318669185370296461)** (97 messages🔥🔥): 

> `Token Waste Issues, Project Collaboration, Bolt.diy Importing Projects, Payment Integration Discussion, User Experience with Bolt` 


- **Users frustrated with token waste**: Many users expressed frustration over **Bolt** wasting tokens, with one even considering taking a break due to its behavior.
   - Suggestions included adding a **'punch the AI'** button to stop token wastage, as members shared their experiences of receiving irrelevant responses.
- **Collaborative project opportunities**: A user is seeking collaboration on a project in **Bolt**, inviting others to join and build something great together.
   - This prompted further discussions about sharing resources and working as a team on upcoming projects.
- **Importing projects from Bolt.new**: Users are curious about importing projects from **bolt.new** to **bolt.diy**, with a method discussed of downloading projects as zip files.
   - Instructions were given on using the import folder feature to continue working on previously created projects.
- **Payment Integration Features**: There was a discussion about how complex it would be for **Bolt** to implement various payment integrations like **Stripe** and **PayPal**.
   - Users highlighted the need for features like dynamic billing and expressed interest in potential future updates on this matter.
- **User experience and bugs**: Frustration with repeated bugs and placeholders emerged, causing project delays for users testing new features.
   - Suggestions to address issues included turning off diffs and focusing on managing project files better.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://aesthetic-sorbet-a2513b.netlify.app/">Vite + React + TS</a>: no description found</li><li><a href="https://thinktank.ottomator.ai/">oTTomator Community</a>: Where innovators and experts unite to advance the future of AI-driven automation</li><li><a href="https://github.com/RealSput/Wenode">GitHub - RealSput/Wenode: WebContainers, except it&#39;s a million times easier to use</a>: WebContainers, except it&#39;s a million times easier to use - RealSput/Wenode</li><li><a href="https://github.com/stackblitz-labs/bolt.diy#join-the-community-for-boltdiy">GitHub - stackblitz-labs/bolt.diy: Prompt, run, edit, and deploy full-stack web applications using any LLM you want!</a>: Prompt, run, edit, and deploy full-stack web applications using any LLM you want! - stackblitz-labs/bolt.diy
</li>
</ul>

</div>
  

---


### **Cohere ▷ #[discussions](https://discord.com/channels/954421988141711382/954421988783444043/1318718521940770847)** (42 messages🔥): 

> `Maya Tool Use, Model Integration Challenges, Sleep Importance, Image Tool Development, Local Model Usage` 


- **Push for Maya Tool Use**: A member emphasized the need for *maya tool use*, stating, '**we NEED maya tool use**' to enhance their model's capabilities.
   - Another member encouraged taking **some rest**, reminding them that rejuvenation fosters creativity.
- **Challenges with Model Integration**: There was a discussion about the integration of local models, specifically mentioning *no native tool use*, causing difficulty for a member.
   - They expressed uncertainty about their approach, saying, '*I got no idea wat I'm doing*' amidst the technical challenges.
- **Importance of Sleep Discussed**: A member raised the question: '**Why is sleep important?**', leading to a general agreement on the necessity of rest for mental health.
   - A light-hearted reminder was given, encouraging team members to rejuvenate and balance dedication with well-being.
- **Image Tool Ideas Shared**: Ideas were exchanged about creating a new image_tool that interacts with models for multi-step queries, maximizing output from images.
   - This would allow the model to engage directly with tools, enhancing the response generation process when dealing with images.
- **Technical Glitches Hindered Progress**: A member humorously reported that their IDE crashed while loading **71k lines of JSON**, causing a pause in their workflow.
   - The group shared a laugh over the challenges faced while pushing for development under tight timelines like their goal for Christmas.


  

---


### **Cohere ▷ #[announcements](https://discord.com/channels/954421988141711382/996880279224451154/1318696792207921213)** (1 messages): 

> `Rate-limit increase for Multimodal Embed-v3 Images, Trial vs Production rate limits, API key options and pricing` 


- **Multimodal Embed-v3 Images get a 10x Boost!**: Due to community feedback, the rate limit for the **Multimodal Image Embed** endpoint has increased from **40 images/min** to **400 images/min** for production keys.
   - The trial rate limit will remain at **5 images/min** to allow for free testing.
- **Understanding Trial and Production Rate Limits**: In addition to the significant boost for the **Embed (Images)** endpoint, various other endpoints have their distinct rate limits detailed in a provided chart.
   - For instance, the **Chat** endpoint allows **20 images/min** in trial and **500 images/min** in production, highlighting the advantages of paying for production keys.
- **Explore API Keys and Pricing Details**: Cohere offers two kinds of API keys: **evaluation keys**, which are free, and **production keys**, which are paid and provide higher limits.
   - Developers can create and manage their keys on the [API keys page](https://dashboard.cohere.com/api-keys) and check the pricing details on the [pricing docs](https://docs.cohere.com/v2/docs/how-does-cohere-pricing-work).



**Link mentioned**: <a href="https://docs.cohere.com/v2/docs/rate-limits">API Keys and Rate Limits — Cohere</a>: This page describes Cohere API rate limits for production and evaluation keys.

  

---


### **Cohere ▷ #[questions](https://discord.com/channels/954421988141711382/1168411509542637578/1318719316299874397)** (51 messages🔥): 

> `Cohere Reranker Issues, Using Different Embedding Models, Cohere and Nvidia Dependency, TPU in AI Systems, Vector Store for Different Dimensionality` 


- **Cohere Reranker doesn't consistently select relevant chunks**: A developer reported that the **Cohere Reranker** used with the **ContextualCompressionRetriever** sometimes fails to select the most relevant chunks from the retrieved data, leading to incorrect answers.
   - Despite accurate chunking in their RAG application, the **reranking behavior** is random and often selects less relevant chunks, causing confusion.
- **Questions on storing embeddings with different dimensions**: A user asked whether they should create separate vector stores for embeddings generated by **text-3-embedding-large** and **cohere embed v3**, given their different dimensions of **3072** and **1024**, respectively.
   - The concern was raised because the dimensionality difference might impact the storage strategy when integrating embeddings for text, tables, and images.
- **Dependency of AI systems on Nvidia products**: One participant noted that **Nvidia** is a core component for most AI systems due to the strong ecosystem provided by **CUDA** and **NCCL**.
   - While AMD and TPU are alternatives, they are considered more niche in comparison to Nvidia's widespread adoption in the AI space.
- **Exploration of TPU usage in AI**: There was a discussion about **TPUs** and their effectiveness as fast vector processors, specifically for **matrix multiplication** in AI applications.
   - While **Anthropic** utilizes TPUs significantly, the consensus was that the majority of systems still predominantly rely on **Nvidia** due to its robust ecosystem.
- **Utilizing diverse computing architectures for AI**: A participant shared their past experiences with **FPGA** for inference, indicating that multiple hardware options exist for AI processing.
   - Discussion highlighted the need to consider how 'turn-key' a solution should be, weighing the flexibility of alternative architectures against ease of implementation.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://docs.cohere.com/docs/structured-outputs">Structured Outputs — Cohere</a>: This page describes how to get Cohere models to create outputs in a certain format, such as JSON.</li><li><a href="https://docs.cohere.com/reference/chat#request.body.response_format">Chat — Cohere</a>: Generates a text response to a user message and streams it down, token by token. To learn how to use the Chat API with streaming follow our [Text Generation guides](https://docs.cohere.com/v2/docs/cha...</li><li><a href="https://docs.cohere.com/reference/chat#request.body.strict_tools">Chat — Cohere</a>: Generates a text response to a user message and streams it down, token by token. To learn how to use the Chat API with streaming follow our [Text Generation guides](https://docs.cohere.com/v2/docs/cha...
</li>
</ul>

</div>
  

---


### **Cohere ▷ #[cmd-r-bot](https://discord.com/channels/954421988141711382/1168578374038470656/)** (1 messages): 

setupisanoun: hey buddy
  

---


### **Cohere ▷ #[projects](https://discord.com/channels/954421988141711382/1218409701339828245/1318889996584878081)** (2 messages): 

> `Product Hunt Launch, Findr App, Digital Memory` 


- **Findr App Launches on Product Hunt**: [Findr](https://www.producthunt.com/posts/findr-remember-everything) has officially launched on Product Hunt, aiming to give humans **infinite memory** and a **searchable digital brain**.
   - The team is requesting support, as shown in their promotional [tweet](https://x.com/Nish306/status/1868953328975261712).
- **Support from the Community**: Community members, including @meor.amer, expressed their congratulations on the launch of Findr.
   - This shows a **positive response** from users who are interested in the app's innovative concept.



**Link mentioned**: <a href="https://x.com/Nish306/status/1868953328975261712">Tweet from Nishkarsh (@Nish306)</a>: we’ve launched on Product Hunt. i would greatly appreciate your support https://www.producthunt.com/posts/findr-remember-everythingwe&#39;re giving humans infinite memory and a searchable digital brai...

  

---


### **Cohere ▷ #[cohere-toolkit](https://discord.com/channels/954421988141711382/1254901651081269268/1318945686573551689)** (3 messages): 

> `Cohere Toolkit Deployment, AWS Stream Errors, Docker Logs Inspection` 


- **Cohere Toolkit deployed but facing issues**: A member successfully deployed the **Cohere Toolkit** using the provided **AWS instructions**, but encountered an intermittent `stream ended unexpectedly` error.
   - This issue appears to happen randomly, with some messages working fine at times.
- **Seeking insights on stream error**: The same member inquired if anyone else has experienced the **stream ended unexpectedly** error with no concrete lead on what might be causing it.
   - The issue persisted despite other functions appearing normal at times, prompting the request for shared experiences.
- **Advice to check Docker logs**: Another member suggested checking the **docker logs** to find more information about the error.
   - This recommendation indicates that deeper insights might be found in the application logs related to the deployment.


  

---


### **Modular (Mojo 🔥) ▷ #[general](https://discord.com/channels/1087530497313357884/1098713601386233997/1318669970682679397)** (22 messages🔥): 

> `Mojo on Archcraft Linux issues, Installation of Max and Magic, Using the Mojo REPL, Python requirements in magic environment` 


- **Congratulations on the new release!**: Members celebrated the new release and shared examples available in the [GitHub repo for Stable Diffusion](https://github.com/modularml/max/tree/main/examples/inference/stable-diffusion-mojo-onnx).
   - One member offered congratulations while providing helpful links for further exploration.
- **User struggles with Mojo on Archcraft Linux**: A user reported issues entering the Mojo REPL on Archcraft Linux, stating it could not find the dynamically linked library, potentially called **mojo-ldd**.
   - Discussion ensued about errors related to **mojo-lld**, a linker, and its installation requirements.
- **Max installation process issues**: Another member mentioned that the Max installation process was unexpectedly killed, hindering their progress.
   - They expressed difficulties in accessing the Mojo REPL despite being able to use Max and Magic.
- **Externally managed environment error**: The same user stated that while in the magic environment, attempts to install Python requirements led to an error indicating they were in an **externally managed environment**.
   - They sought help for this issue, indicating they could not download the necessary requirements.
- **Tip for problem-solving threads**: One member suggested creating a new thread for problem solving to assist others facing similar issues.
   - This approach was encouraged to ensure collaborative solutions and continuous assistance from the community.


  

---


### **Modular (Mojo 🔥) ▷ #[mojo](https://discord.com/channels/1087530497313357884/1151418092052815884/1318676804692873278)** (57 messages🔥🔥): 

> `Mojo Documentation Updates, Mojo Kernel Terminology, Compute Kernels vs OS Kernels, Discussion on `var` Keyword, Argmax and Argmin Removal` 


- **Clarifying Mojo Documentation Updates**: A team member discussed updates in the [Mojo docs](https://docs.modular.com/mojo/manual/basics#variables) regarding variable declarations, particularly the use of the `var` keyword.
   - Another member confirmed they are working on an update regarding the `var` requirement, which is still in progress.
- **Understanding Mojo Kernel Terminology**: Members discussed the term 'kernel' in the context of Mojo, clarifying it refers to a function running on an accelerator, not an OS kernel.
   - One member humorously noted that the term is used to sound sophisticated, while another explained it as specific blocks of code optimized for hardware.
- **Distinction Between Compute Kernels and OS Kernels**: A discussion emerged redefining compute kernels versus OS kernels, highlighting that Mojo could be used for userspace drivers but still requires improvements.
   - Members agreed that while Mojo can help with compile and portability aspects, more work is needed before it matches the capabilities of OS kernels.
- **Debate Over the `var` Keyword in Mojo**: Members expressed differing views on the necessity of the `var` keyword, with suggestions for it to be optional but indicated on the code.
   - One member discussed how the removal of `var` might affect structs, while others expressed a desire for clarity in its usage.
- **Concern Over Removal of Argmax and Argmin Functions**: A member inquired about the disappearance of `argmax` and `argmin` from the algorithm.reduction, fearing the need to implement them from scratch.
   - This sparked conversation about the updates and changes in the Mojo library, indicating the need for a changelog clarification.



**Link mentioned**: <a href="https://docs.modular.com/mojo/manual/basics#variables)">Mojo language basics | Modular Docs</a>: Introduction to Mojo&#x27;s basic language features.

  

---


### **Modular (Mojo 🔥) ▷ #[max](https://discord.com/channels/1087530497313357884/1212827597323509870/1318740743908622337)** (13 messages🔥): 

> `Custom ops in Mojo, Error handling and documentation, Feature request for custom op messages, Max GitHub repo issues, Session loading with custom ops` 


- **Custom ops give trouble in Mojo**: There are issues loading a custom op named [mandelbrot](https://github.com/modularml/max/issues/269) in Mojo, specifically when trying to import the type.
   - Errors indicate that the Mojo kernel for the mandelbrot custom op is not registered, hindering execution.
- **Documentation updates are needed**: Members discussed filing issues on the Max GitHub repo regarding the clarity of error messages, particularly the 'custom op not found' message.
   - Suggestions include improving error messages and potentially guiding users towards relevant documentation.
- **Feature request for improved custom op handling**: A member initiated a [feature request](https://github.com/modularml/max/issues/269) for better handling of custom ops that are not found and clearer error messages.
   - This request aims to address user experience issues by directing users to documentation when errors occur.
- **Confirmed bug reporting**: Members expressed gratitude for catching issues and confirmed that they would file bug reports, particularly regarding custom operations.
   - Clear communication about GitHub issues is necessary for resolving existing problems with Mojo's handling of custom ops.
- **Session loading confusion**: There were discussions around loading sessions with custom ops, particularly using paths to kernel directories.
   - One member mentioned underlining the specifics when calling `session.load` with `custom_ops_paths` for clarity.



**Link mentioned**: <a href="https://github.com/modularml/max/issues/269">[Feature Request] Single compilation unit kernels and/or improved error messages · Issue #269 · modularml/max</a>: What is your request? This is a 2-part request, but bundled since they both address the same UX issue. Part one is to make the &quot;custom op not found&quot; error message direct users to documentati...

  

---


### **OpenInterpreter ▷ #[general](https://discord.com/channels/1146610656779440188/1147665339266650133/1318671414433284116)** (28 messages🔥): 

> `Open Interpreter Errors, Latest OI Version, AI Applications and Models, Truffle-1 Computing Stack, Long-Term Memory in OI` 


- **Persistent Open Interpreter Errors**: Multiple users reported consistent issues when using Open Interpreter, particularly involving errors related to the `--conversations` command.
   - One member expressed frustration over losing valuable conversations, raising questions on how to resolve these persisting issues.
- **Inquiry about Latest OI Version**: A user was curious about upgrading to version 1.x of Open Interpreter, mentioning they were still on 0.34 and heard about a newer 1.0 release.
   - Discussions included whether the OS mode was available in the latest version, as members strategized improvements.
- **Exploring AI Applications and Models**: Discussions emerged on using AI for various applications, including a Raspberry Pi setup and possible voice-to-speech models for home automation.
   - Users contemplated how to connect smaller models with larger, more capable systems to enhance functionality.
- **Introduction of Truffle-1 AI Computer**: One member shared details about the Truffle-1, a personal computing stack running multiple models with 64GB of unified memory, priced at a $500 deposit and $115 monthly.
   - This personal agentic computing device aims to provide infinite inference time and supports writing and sharing apps, with units shipping in January.
- **Local Use of OS Mode**: A user inquired whether it was possible to use OS mode locally with Open Interpreter.
   - This prompted further discussion on the configuration options available for users experiencing issues.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://itsalltruffles.com">Truffle</a>: A Personal Agentic Computing stack — the Truffle-1 runs a mixture of models on-device with 64GB of unified memory</li><li><a href="https://x.com/iamgingertrash/status/1869450385896751449">Tweet from simp 4 satoshi (@iamgingertrash)</a>: To recap:&gt; $500 deposit authorized today&gt; $115/month for 12 months&gt; Infinite inference time compute&gt; Writing & Sharing your own apps&gt; A glowing orb with a 64GB Orin&gt; We&#39;re actual...</li><li><a href="https://tenor.com/view/first-of-all-all-things-are-possible-jot-that-down-pointing-serious-gif-14586817">First Of All All Things Are Possible GIF - First Of All All Things Are Possible Jot That Down - Discover &amp; Share GIFs</a>: Click to view the GIF
</li>
</ul>

</div>
  

---


### **tinygrad (George Hotz) ▷ #[general](https://discord.com/channels/1068976834382925865/1068976834928193609/1318864276387139694)** (27 messages🔥): 

> `Benchmarks of Llama Models, Mergeability in ShapeTrackers, Layout Algebra in CuTe, Algorithm Complexity in Merging, Injectivity in Layout Algebra` 


- **Request for Llama Model Benchmarks**: A member asked if anyone had benchmarks for any Llama models comparing **TinyGrad**'s OpenCL with **PyTorch**'s CUDA implementations.
   - This highlights an ongoing interest in **performance comparisons** between AI frameworks.
- **Challenges of Mergeability in ShapeTrackers**: A user discussed the complexity of proving the mergeability of two arbitrary **ShapeTrackers** in Lean, stating it's not possible to have a simple criterion similar to a matrix determinant.
   - They highlighted the existence of coincidences in strides and shapes that complicate the mergeability checks.
- **Insights on CuTe Layout Algebra**: Inquire into whether mergeability is equivalent to composition in **CuTe's layout algebra**, referencing an academic note on layout operations.
   - This discussion touched on fundamental abstractions in NVIDIA's **CUTLASS** library and the mathematical treatment of layout operations.
- **Complexity of Layout Algebra**: Concerns about proving conditions related to injectivity in layout algebra were raised, with suggestions that such checks might be **NP hard**.
   - It emphasizes the difficulties in establishing sufficient conditions in layout algebra due to potential stride interferences.
- **Symbolic Functions vs. Layouts**: A member pointed out that **symbolic integer functions** are strictly more powerful than layouts in terms of checking necessity and sufficiency.
   - This aligns with the discussions on algorithm complexities in merging views and supports ongoing research directions.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://research.colfax-intl.com/a-note-on-the-algebra-of-cute-layouts/">A note on the algebra of CuTe Layouts</a>: The core abstraction of NVIDIA’s CUTLASS library for high-performance linear algebra is the CuTe Layout. In this technical note, we give a rigorous, mathematical treatment of the algebra of these l…</li><li><a href="https://github.com/tinygrad/tinygrad/issues/8194),">Issues · tinygrad/tinygrad</a>: You like pytorch? You like micrograd? You love tinygrad! ❤️  - Issues · tinygrad/tinygrad</li><li><a href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/02_layout_algebra.md">cutlass/media/docs/cute/02_layout_algebra.md at main · NVIDIA/cutlass</a>: CUDA Templates for Linear Algebra Subroutines. Contribute to NVIDIA/cutlass development by creating an account on GitHub.
</li>
</ul>

</div>
  

---


### **Torchtune ▷ #[dev](https://discord.com/channels/1216353675241590815/1236040539409879170/1318727531725197412)** (25 messages🔥): 

> `FSDP normalization, Scaling factors in loss computation, Bug reports on trl and HF trainer, Optimizer behavior with weight decay, Updates to PRs` 


- **FSDP Normalization Needs Scaling**: Discussions revealed that FSDP's normalization by `world_size` must be addressed, and scaling by `world_size` can correct an average operation issue.
   - A member suggested opening a PR to implement this fix since it won't require extensive changes, mostly rotating around the `scale_grads` function.
- **Explicit Scaling Preferred in Training**: The community highlighted the importance of explicit scaling of the loss within the recipe rather than hiding logic elsewhere, to simplify understanding.
   - After evaluations, members agreed to clarify the scaling process in both training and optimization hooks.
- **Identifying Bugs Across Frameworks**: It was pointed out that a similar bug affecting the reduction by a factor of `1/world_size` might exist across various libraries, including `trl` and Hugging Face's trainer.
   - Members commended the HF team for recognizing and addressing these issues in their training framework, as noted in linked GitHub issues.
- **Handling No Sync Scenario**: Members discussed how Hugging Face handles no sync scenarios by avoiding grad accumulation normalization while properly computing loss.
   - Specific links were provided detailing their method of arriving at the number of items in the batch to facilitate accurate loss normalization.
- **Updates Made to the PR**: A member confirmed the addition of a scaling factor for the `optimizer_in_bwd` case in their existing PR to address potential issues.
   - The functionality matters as it adjusts how optimizers apply weight decay and ensures better gradients handling in specific cases.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://github.com/pytorch/torchtune/blob/3518492f43a8a5a462cbd604be4101268ff5bd52/recipes/full_finetune_distributed.py#L768">torchtune/recipes/full_finetune_distributed.py at 3518492f43a8a5a462cbd604be4101268ff5bd52 · pytorch/torchtune</a>: PyTorch native post-training library. Contribute to pytorch/torchtune development by creating an account on GitHub.</li><li><a href="https://github.com/pytorch/torchtune/pull/2172">Fix gradient scaling to account for world_size normalization by mirceamironenco · Pull Request #2172 · pytorch/torchtune</a>: ContextWhat is the purpose of this PR? Is it to add a new feature fix a bug update tests and/or documentation other (please add here)Please link to any issues this PR addresses.ChangelogW...</li><li><a href="https://github.com/pytorch/torchtune/blob/main/torchtune/training/memory.py#L219">torchtune/torchtune/training/memory.py at main · pytorch/torchtune</a>: PyTorch native post-training library. Contribute to pytorch/torchtune development by creating an account on GitHub.</li><li><a href="https://github.com/huggingface/transformers/issues/34242">Add DDP token averaging for equivalent non-parallel training similar to #34191 · Issue #34242 · huggingface/transformers</a>: Feature request Token averaging in gradient accumulation was fixed in #34191 . But token averaging in DDP seems to have the same issue. Expected behaivor With all the tokens contributing to loss in...</li><li><a href="https://github.com/pytorch/torchtune/blob/3518492f4">GitHub - pytorch/torchtune at 3518492f43a8a5a462cbd604be4101268ff5bd52</a>: PyTorch native post-training library. Contribute to pytorch/torchtune development by creating an account on GitHub.</li><li><a href="https://github.com/huggingface/transformers/blob/052e652d6d53c2b26ffde87e039b723949a53493/src/transformers/trainer.py#L3662">transformers/src/transformers/trainer.py at 052e652d6d53c2b26ffde87e039b723949a53493 · huggingface/transformers</a>: 🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX. - huggingface/transformers
</li>
</ul>

</div>
  

---


### **Torchtune ▷ #[papers](https://discord.com/channels/1216353675241590815/1293438210097025085/1318872136684933151)** (2 messages): 

> `Evolutionary Algorithms, Scale Up Evolution, Gradient Techniques` 


- **Evolutionary Algorithms Take Center Stage**: A member pointed out the interesting use of **evolutionary algorithms**, showcasing their significance in current discussions.
   - This potential innovation invites further exploration into its application in machine learning.
- **Sakana Aims to Compete with Gradient Techniques**: **Sakana** is attempting to scale up its evolutionary methods to remain competitive with prevailing **gradient techniques**.
   - This move suggests a growing interest in alternative optimization strategies within the community.


  

---


### **DSPy ▷ #[show-and-tell](https://discord.com/channels/1161519468141355160/1202371242519441499/)** (1 messages): 

collabin: https://youtu.be/BrvVheleOqc
  

---


### **DSPy ▷ #[papers](https://discord.com/channels/1161519468141355160/1203568372667645963/1318687341027655800)** (4 messages): 

> `AI and Knowledge Economy, Coconut - Chain of Continuous Thought, Autonomous vs Non-Autonomous AI` 


- **AI Reshaping the Knowledge Economy**: [This paper](https://arxiv.org/abs/2312.05481) introduces a framework analyzing how AI transforms the knowledge economy by reallocating roles between 'workers' and 'solvers'. It highlights that basic autonomous AI displaces humans while advanced autonomous AI leads to larger, more productive firms.
   - As autonomous agents gain traction, they predominantly benefit the most knowledgeable individuals, allowing them to efficiently manage routine work, while less knowledgeable individuals benefit from non-autonomous AI like chatbots.
- **Introducing Coconut - Continuous Thought**: [A recent paper](https://arxiv.org/html/2412.06769v1) from Meta proposes a new reasoning paradigm called Coconut, which uses the last hidden state of LLMs for reasoning instead of the traditional language space. The authors argue that traditional methods may not effectively capture the reasoning process and introduce a concept termed “continuous thought.”
   - This approach seeks to overcome limitations of language-based reasoning by exploring unrestricted latent spaces, which could enhance LLMs' performance on complex reasoning tasks.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://arxiv.org/abs/2312.05481">Artificial Intelligence in the Knowledge Economy</a>: The rise of Artificial Intelligence (AI) has the potential to reshape the knowledge economy by enabling problem solving at scale. This paper introduces a framework to analyze this transformation, inco...</li><li><a href="https://arxiv.org/html/2412.06769v1">Training Large Language Models to Reason in a Continuous Latent Space</a>: no description found
</li>
</ul>

</div>
  

---


### **DSPy ▷ #[general](https://discord.com/channels/1161519468141355160/1161519469319946286/1318971783793344573)** (11 messages🔥): 

> `TypedReAct integration, RouteLLM maintenance concerns, DSPy evolution with reasoning models` 


- **TypedReAct enigma solved**: A member shared a new implementation of `TypedReAct`, questioning whether to submit a PR, but noted potential deprecated issues with `TypedChainOfThought` in upcoming versions.
   - Another member suggested that simply removing the 'Typed' prefix would resolve compatibility issues, emphasizing that built-in ReAct is effective without the typing.
- **RouteLLM's prolonged slumber**: A member expressed concerns regarding the lack of maintenance for RouteLLM, indicating interest in potential DSPy integration.
   - The conversation highlighted how critical it is to support development for models with reduced oversight.
- **Discussing DSPy's future amidst reasoning models**: A member inquired about discussions related to how DSPy might evolve with the rise of reasoning models, emphasizing fine-tuning at the branching level.
   - This perspective shifts focus from traditional prompting methods to process reward mechanisms, indicating a potential paradigm shift in model training.



**Link mentioned**: <a href="https://dspy.ai/tutorials/agents/">Agents - DSPy</a>: The framework for programming—rather than prompting—language models.

  

---


### **Nomic.ai (GPT4All) ▷ #[general](https://discord.com/channels/1076964370942267462/1090427154141020190/1318701547609260185)** (12 messages🔥): 

> `GPT4All issues, Jinja template functionality, Docker version of GPT4All, Command line interface concerns, Local documents in CLI` 


- **GPT4All struggles with Jinja templates**: Users expressed frustrations with GPT4All being 'completely broken' for side loading, citing issues with Jinja templates that are crucial for model functionality.
   - Current problems identified include needing to space elements correctly, fixing new line issues, and unsupported functions like 'none' and '[1:]'.
- **Docker version of GPT4All in demand**: A user inquired about a version of GPT4All that runs from a Docker container with a web UI, indicating interest in easier deployment options.
   - The community has not responded with specific resources or existing solutions for this request yet.
- **CLI interactions without localdocs**: One user is trying to access GPT4All models via the command line but is unable to use local documents with the current setup.
   - Another user informed them that the old CLI is not officially supported, but the server API allows programmatic access to localdocs when enabled in the GUI.


  

---


### **LlamaIndex ▷ #[blog](https://discord.com/channels/1059199217496772688/1187460979064324127/1318705846204633099)** (2 messages): 

> `AI SDR, Agent Building Crash Course, LlamaIndex Function Calling, Agentic RAG, ReAct` 


- **AI SDR Generates Leads with LlamaIndex**: Check out this [agentic AI SDR](https://t.co/tczv5ZDI4H) that generates leads for you, built using **LlamaIndex**.
   - This tool is highlighted for its capability in automated lead generation linking to several [GitHub features](https://github.com/features).
- **Learn to Build Agents from Scratch**: A crash course from [@TRJ_0751](https://twitter.com/llama_index/status/1869454248620269615) teaches how to build agents with **LlamaIndex** focusing on function calling to manage real-time data queries.
   - Participants will also learn to create an **agentic RAG** that intelligently routes between vector and summary tools, as well as how to create ReAct.



**Link mentioned**: <a href="https://t.co/tczv5ZDI4H">composio/python/examples/quickstarters at master · ComposioHQ/composio</a>: Composio equip&#39;s your AI agents &amp; LLMs with 100+ high-quality integrations via function calling - ComposioHQ/composio

  

---


### **LlamaIndex ▷ #[general](https://discord.com/channels/1059199217496772688/1059201661417037995/1318816823642292254)** (4 messages): 

> `OpenAIAgent concurrency, RAG evaluation discussions` 


- **OpenAIAgent execution may not be concurrent**: A member inquired whether `OpenAIAgent` function execution can be done concurrently in an asynchronous environment, noting it differs from parallel function calling.
   - The investigation into this led to observing that even with modifications for async, the function executions remain non-concurrent.
- **Utilizing async tools for function execution**: Another member suggested using async entry points and async tools, stating that this approach should ensure proper execution.
   - They provided a code snippet demonstrating how to implement a tool asynchronously with `OpenAIAgent`.
- **Looking for RAG evaluation discussions**: A member expressed interest in discussing RAG evaluation and invited others to DM if they want to chat.
   - This indicates an ongoing effort to engage with peers in the AI community on evaluation strategies.



**Link mentioned**: <a href="https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_parallel_function_calling/#example-from-openai-docs">Single-Turn Multi-Function Calling OpenAI Agents - LlamaIndex</a>: no description found

  

---


### **Gorilla LLM (Berkeley Function Calling) ▷ #[discussion](https://discord.com/channels/1111172801899012102/1111353033352294440/1318697465855086655)** (3 messages): 

> `BFCL Leaderboard, Function Call Demo Issues, Gorilla Benchmark for Structured Outputs` 


- **BFCL Leaderboard functionality questioned**: A user noted issues with the function call demo on the [BFCL Leaderboard](https://gorilla.cs.berkeley.edu/leaderboard.html#leaderboard) stating it was stuck on 'Loading Model Response...'.
   - In response, another member confirmed there is a **certificate issue** causing the model endpoint to be down.
- **Inquiry about structured output evaluation**: A user expressed interest in using the **Gorilla benchmark** for evaluating the quality of structured outputs from the model.
   - They specifically asked if there are any subtasks dedicated to generating text according to a provided **JSON schema** or **Pydantic model**.



**Link mentioned**: <a href="https://gorilla.cs.berkeley.edu/leaderboard.html#leaderboard">
        Berkeley Function Calling Leaderboard V3 (aka Berkeley Tool Calling Leaderboard V3)
    </a>: no description found

  

---


### **LLM Agents (Berkeley MOOC) ▷ #[mooc-questions](https://discord.com/channels/1280234300012494859/1280370030609170494/)** (1 messages): 

kallemickelborg: Thank you for that!
  

---


### **Axolotl AI ▷ #[general](https://discord.com/channels/1104757954588196865/1104757955204743201/1318904347194556490)** (1 messages): 

> `New Engineer on Board, Reinforcement Learning Support` 


- **New Engineer Joining in January**: A new engineer is set to join in **January** to assist with **Reinforcement Learning** in general.
   - They will also provide support with the **kto** project at that time.
- **Support for RL and kto**: The new engineer's expertise will enhance the team’s capabilities in **Reinforcement Learning**.
   - Their assistance is anticipated to positively impact the development of the **kto** as well.


  

---


### **Mozilla AI ▷ #[announcements](https://discord.com/channels/1089876418936180786/1089876419926032396/1318691196566241371)** (1 messages): 

> `Developer Hub, Blueprints Initiative` 


- **Developer Hub Update Released**: A significant update regarding the **Developer Hub** has been announced, detailing improvements and new features. You can view the full announcement [here](https://discord.com/channels/1089876418936180786/1230938514955436242/1318638353503227935).
   - Feedback from the community is appreciated as they strive to enhance user experience.
- **Blueprints for Open-Source AI Solutions**: A thread discussing the **Blueprints initiative** aims to assist developers in creating open-source AI solutions has been shared. More details can be found [in the thread](https://discord.com/channels/1089876418936180786/1318689803021058158).
   - This initiative is positioned as a resource for developers to kickstart their projects effectively.


  

---


---


---


{% else %}


> The full channel by channel breakdowns have been truncated for email. 
> 
> If you want the full breakdown, please visit the web version of this email: [{{ email.subject }}]({{ email_url }})!
>
> If you enjoyed AInews, please [share with a friend](https://buttondown.email/ainews)! Thanks in advance!

{% endif %}
