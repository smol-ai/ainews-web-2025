---
id: 5f0db594-436e-4cad-8d4b-2e118c5f2505
title: not much happened today
date: '2025-03-01T03:41:57.593041Z'
original_slug: ainews-not-much-happened-today-3457
description: >-
  User polls show mixed reactions to GPT-4.5, with many preferring GPT-4 despite
  some experts praising GPT-4.5's creativity and humor; however, concerns about
  GPT-4.5’s slow response times and questionable practical value have been
  raised. Comparisons indicate GPT-4.5 performs better than GPT-4o but is
  outperformed by Claude 3.5 and Claude 3.7, which also excel in humor and
  emotional intelligence.
tags:
  - gpt-4
  - gpt-4.5
  - claude-3.5
  - claude-3.7
  - model-performance
  - humor
  - user-feedback
---


<!-- buttondown-editor-mode: plaintext -->**a quiet day.**

> AI News for 2/27/2025-2/28/2025. We checked 7 subreddits, [**433** Twitters](https://twitter.com/i/lists/1585430245762441216) and **29** Discords (**221** channels, and **8236** messages) for you. Estimated reading time saved (at 200wpm): **795 minutes**. You can now tag [@smol_ai](https://x.com/smol_ai) for AINews discussions!

Much discussion about the relative merits of GPT 4.5, which you can read below.

---


{% if medium == 'web' %}


**Table of Contents**

[TOC] 

{% else %}

The **Table of Contents** and **Channel Summaries** have been moved to the web version of this email: [{{ email.subject }}]({{ email_url }})!

{% endif %}


---

# AI Twitter Recap

**GPT-4.5 Model Performance and User Perception**

- **Initial User Experiences and Subjective Evaluation**: [@karpathy](https://twitter.com/karpathy/status/1895337579589079434) conducted a poll comparing GPT-4 and GPT-4.5, finding that in 4 out of 5 questions, users preferred GPT-4, which was surprising as [@karpathy](https://twitter.com/karpathy/status/1895337579589079434) personally found **GPT-4.5 better in all cases**, suggesting a possible preference for "high-taste testers" towards **GPT-4.5's deeper charm, creativity, and humor**.  However, [@jeremyphoward](https://twitter.com/jeremyphoward/status/1895354868342366648) responded to Karpathy's poll results, stating that the awkwardness, not "high taste" was the reason for user preference.  [@Teknium1](https://twitter.com/Teknium1/status/1895348781367140708) also reacted to the poll results with "Damn lol must have some high, or low, taste people testing here idk".  [@abacaj](https://twitter.com/abacaj/status/1895516638704754727) expressed strong dissatisfaction, stating **GPT-4.5 needs to enhance productivity to be useful**, otherwise it is "fucking useless".  [@abacaj](https://twitter.com/abacaj/status/1895517803173810560) also argued that if **GPT-4.5 is only a "high taste" model**, it is "blowing investor money".  [@stevenheidel](https://twitter.com/stevenheidel/status/1895541898137456776) likened the **GPT-4.5 launch to the initial ChatGPT excitement**, as people are again having fun chatting with AI.
- **Concerns Regarding Speed and Practicality**: [@abacaj](https://twitter.com/abacaj/status/1895309773329027543) noted **GPT-4.5 is "very slow"** and "impractical to use for agent loops", despite being "fun to prompt".  [@abacaj](https://twitter.com/abacaj/status/1895310460276351105) elaborated that it takes **"3+ minutes to answer one question"** in a moderate prompt loop, deeming it "very impractical".  [@abacaj](https://twitter.com/abacaj/status/1895311873622581502) further commented that **GPT-4.5 "feels more like a research artifact than a real model you can deploy"** due to its slowness.
- **Critique of Capabilities and Value Proposition**: [@abacaj](https://twitter.com/abacaj/status/1895520453520970054) criticized the showcased capabilities of the "largest language model", questioning if **drawing a triangle using SVG** is the highlight.  [@abacaj](https://twitter.com/abacaj/status/1895519515204784380) found the value add for end-users questionable, suggesting internal use within OAI for distillation.
- **Pricing and Economic Viability**: [@Yuchenj_UW](https://twitter.com/Yuchenj_UW/status/1895338027041579269) remarked that the **pricing "makes even less sense"** in light of GPT-4.5's performance.  [@Yuchenj_UW](https://twitter.com/Yuchenj_UW/status/1895313053606142283) speculated about the potential pricing of **GPT-5 and o4**.  [@AravSrinivas](https://twitter.com/AravSrinivas/status/1895508117376598330) highlighted **Perplexity Deep Research at $20/month versus ChatGPT at $200/month**.
- **Performance Compared to Other Models**: [@METR_Evals](https://twitter.com/METR_Evals/status/1895381625585967180) reported that **GPT-4.5 performs above GPT-4o but below o1 or Claude 3.5 Sonnet** based on METR experiments with an earlier checkpoint, noting a time horizon score of ~30 minutes.  [@dylan522p](https://twitter.com/dylan522p/status/1895557873712972138) stated **Claude 3.7 beats GPT 4.5 on most tasks**, but **GPT 4.5 has better "vibes"** and is the first model since Claude 3 Opus to make them laugh, emphasizing humor as intelligence.  [@scaling01](https://twitter.com/scaling01/status/1895415262486388906) speculated **GPT-4.5 could be "GPT-4o x 10"** in size, estimating around 5T parameters.  [@Teknium1](https://twitter.com/Teknium1/status/1895520871642783958) mentioned **Grok's context window is only 128k**.  [@multimodalart](https://twitter.com/multimodalart/status/1895521321838063837) shared **evaluations comparing GPT 4.5 with non-thinking models like Sonnet 3.7, Deepseek V3, and Grok 3**.
- **Emotional Intelligence (EQ) and "Vibes"**: [@karpathy](https://twitter.com/karpathy/status/1895549465463009309) found **Claude 3.7's humor to be the funniest** after scrutinizing LLM outputs for humor.  [@random_walker](https://twitter.com/random_walker/status/1895494391466475684) argued that the **"EQ" improvements in GPT 4.5 are due to post-training, not parameter count**, suggesting any EQ differences are behavioral rather than capability-based.  [@random_walker](https://twitter.com/random_walker/status/1895499480902013254) further claimed that **GPT-4o and GPT-3.5 can exhibit similar EQ behavior as GPT-4.5 with appropriate post-training**.  [@omarsar0](https://twitter.com/omarsar0/status/1895504181789937964) suggested using the OpenAI Playground to compare models and observe **GPT-4.5's "thoughtful" responses**.  [@omarsar0](https://twitter.com/omarsar0/status/1895504558669127693) noted **GPT-4.5 often sounds more "thoughtful"** by adding sensations and thoughts.  [@marktenenholtz](https://twitter.com/marktenenholtz/status/1895316983144685978) observed that **Sonnet 3.7 is "almost too eager" and GPT-4.5 is "almost too deferential"**.
- **Technical Details and Training**: [@sama](https://twitter.com/sama/status/1895490123690922445) credited **@ColinWei11, Yujia Jin, and @MikhailPavlov5** for the difficult work at the intersection of ML and systems required for GPT-4.5.  [@cloneofsimo](https://twitter.com/cloneofsimo/status/1895319178116243763) highlighted that **GPT4.5 was "trained on multiple datacenters" and "aggressively used low precision training"**, implying "diloco goes brr" and the benefit of fp8 training due to high granularity.  [@rasbt](https://twitter.com/rasbt/status/1895511885950357888) pointed to the system card mentioning **"new supervision techniques"** used in training.  [@rasbt](https://twitter.com/rasbt/status/1895502063154733239) mentioned that apparently **character-training was not used**.  [@Teknium1](https://twitter.com/Teknium1/status/1895380611764015342) questioned how **GPT-4.5's knowledge cutoff remains 2023** despite current pretraining runs, speculating about data contamination from ChatGPT 3.5 data or if the model was trained long ago.

**Model Architecture, Scaling Laws and Efficiency**

- **Scaling Law Limitations and Alternative Approaches**: [@Yuchenj_UW](https://twitter.com/Yuchenj_UW/status/1895531031475920978) suggested that the **GPT-4.5 release indicates LLM pre-training scaling has plateaued**, noting that a **10x compute increase yields limited improvement**, which allows companies like xAI to catch up through innovation in algorithms and data, as demonstrated by DeepSeek's efficiency gains. [@jxmnop](https://twitter.com/jxmnop/status/1895525157101584436) echoed this, suggesting **GPT 4.5 might signal "the beginning of the end for scaling laws"**, questioning if data is exhausted or if scaling laws fail to capture desired task performance.  [@ibab](https://twitter.com/ibab/status/1895509678773485736) emphasized that **algorithms are increasingly important with larger models**, suspecting training details are key to Grok 3's performance.  [@MParakhin](https://twitter.com/MParakhin/status/1895321112810258518) stated **pre-training needs higher-perplexity targeted data and Active Learning** to progress further.  [@teortaxesTex](https://twitter.com/teortaxesTex/status/1895496203401629933) asserted that **non-thinking LLMs pretrained on natural data have hit their practical limit**, doubting a $1T training run would significantly improve them.
- **Inference Compute and Efficiency**: [@rasbt](https://twitter.com/rasbt/status/1895504882561597817) clarified that **train- and inference-compute are orthogonal ways to improve LLMs** and an apple-to-oranges comparison is being made without considering inference-compute scaling for GPT-4.5.  [@rasbt](https://twitter.com/rasbt/status/1895496476056559811) questioned if **GPT-4.5 is more expensive and slower than o1** (GPT4-sized + inference-compute scaling) and what GPT-4.5 with o1-style scaling would look like.  [@iScienceLuvr](https://twitter.com/iScienceLuvr/status/1895331433897697689) highlighted research on **"Thinking Slow, Fast"**, using distilled reasoners based on smaller models like Llama-1B and -3B with Mamba architecture to improve inference scaling.  [@_akhaliq](https://twitter.com/_akhaliq/status/1895340908134178897) shared **FlexiDiT**, a diffusion transformer framework that generates high-quality samples with less compute by using varying patch sizes during denoising.  [@TheTuringPost](https://twitter.com/TheTuringPost/status/1895398797808959963) discussed **Chain of Draft (CoD)**, which encourages models to generate short reasoning steps to reduce costs and speed up models while maintaining accuracy.
- **Hardware and System Architecture**: [@reach_vb](https://twitter.com/reach_vb/status/1895427876985422322) highlighted **DeepSeek's Fire-Flyer File System (3FS)**, noting its disaggregated architecture, strong consistency using CRAQ, stateless metadata services, and KVCache for inference, achieving high read throughput and outperforming in benchmarks. [@teortaxesTex](https://twitter.com/teortaxesTex/status/1895494637231763464) discussed **N4 process allowing 2.32x denser chips compared to N7**, based on transistor counts and die sizes.  [@awnihannun](https://twitter.com/awnihannun/status/1895546249698558363) reported **Kimi's Moonshot 16B MoE model running nicely on M4 Max** with MLX at 154 toks/sec, performing as good or better than dense 7Bs.  [@casper_hansen_](https://twitter.com/casper_hansen_/status/1895393985847517313) commented on **CUDA's moat**, noting even AMD engineers use CUDA for tensor engines.

**Open Source Models, Tools, and Frameworks**

- **DeepSeek's Open Source Contributions**: [@Yuchenj_UW](https://twitter.com/Yuchenj_UW/status/1895531031475920978) praised **DeepSeek for drastically reducing GPU requirements** through infrastructure and algorithm optimization and their "goated open source work".  [@reach_vb](https://twitter.com/reach_vb/status/1895429111470031063), [@reach_vb](https://twitter.com/reach_vb/status/1895428961162936563), [@reach_vb](https://twitter.com/reach_vb/status/1895428392872493345) and [@reach_vb](https://twitter.com/reach_vb/status/1895427876985422322) shared multiple links and details regarding **DeepSeek's Fire-Flyer File System (3FS)** and benchmarks. [@teortaxesTex](https://twitter.com/teortaxesTex/status/1895392169600635146) mentioned **DeepSeek's file system from 2019 is still SoTA**. [@aidan_mclau](https://twitter.com/aidan_mclau/status/1895346698010140954) jokingly scanned DeepSeek's training data and found "deep commitment from a brilliant team".
- **Hugging Face Ecosystem and Integrations**: [@_akhaliq](https://twitter.com/_akhaliq/status/1895488615586607609) and [@_akhaliq](https://twitter.com/_akhaliq/status/1895352428591227397) provided code snippets for **developers to get started with GPT-4.5-preview using `ai-gradio[openrouter]` and Hugging Face**.  [@ClementDelangue](https://twitter.com/ClementDelangue/status/1895530165092127172) highlighted the **French ministry of culture and interior being on Hugging Face**.  [@mervenoyann](https://twitter.com/mervenoyann/status/1895500589871812989) shared that **Microsoft's MAGMA-8B model is easily loadable to Hugging Face transformers**.  [@ClementDelangue](https://twitter.com/ClementDelangue/status/1895467916071784896) announced **Perplexity R1-1776 inference directly from HF model page via FireworksAI_HQ**.  [@_akhaliq](https://twitter.com/_akhaliq/status/1895530733269299240) shared a link to **AI Conference Deadlines on Hugging Face**.
- **Local LLMs and MLX**: [@reach_vb](https://twitter.com/reach_vb/status/1895529293742293144) shared instructions for **running Phi 4 Mini Instruct locally on a Mac using llama.cpp**. [@awnihannun](https://twitter.com/awnihannun/status/1895483273436144002) committed to **using local LLMs for a vibe-check on performance gap**, favoring tools like the raw terminal (mlx_lm) and LM Studio.  [@awnihannun](https://twitter.com/awnihannun/status/1895487722963484697), [@awnihannun](https://twitter.com/awnihannun/status/1895494505543110847), and [@awnihannun](https://twitter.com/awnihannun/status/1895546249698558363) showcased **local inference on M4 Max using MLX** for models like Qwen2.5 and Moonshot.
- **Other Open Source Tools and Projects**: [@pirroh](https://twitter.com/pirroh/status/1895388564671910277) mentioned Replit building **their own Copy-On-Write distributed file system before LLMs** became coding proficient.  [@bobvanluijt](https://twitter.com/bobvanluijt/status/1895463589915353467) highlighted **Weaviate's open-source vector database** and its new features. [@_akhaliq](https://twitter.com/_akhaliq/status/1895532477823013144) shared **TALKPLAY**, a multimodal music recommendation system with LLMs. [@alexalbert__](https://twitter.com/alexalbert__/status/1895504248206709246) announced **Anthropic API quality of life update** allowing public facing URLs for image/document sources. [@DeepLearningAI](https://twitter.com/DeepLearningAI/status/1895549589765165183) promoted a **short course on "Build Apps with Windsurf’s AI Coding Agents"** in collaboration with Codeium. [@AymericRoucher](https://twitter.com/AymericRoucher/status/1895509976321306736) recommended reading about **instrumenting smolagent runs and setting up LLM-judge systems** using Arize Phoenix. [@mervenoyann](https://twitter.com/mervenoyann/status/1895516195941728302) advertised a **weekly newsletter on open-source art tools**.  [@rasbt](https://twitter.com/rasbt/status/1895491746295132339) shared a **tutorial to deploy AI models on public/private cloud using open-source tools**.

**AI Applications and Industry Use Cases**

- **Enterprise AI and Productivity**: [@perplexity_ai](https://twitter.com/perplexity_ai/status/1895554562104443073), [@perplexity_ai](https://twitter.com/perplexity_ai/status/1895554673567993984), [@perplexity_ai](https://twitter.com/perplexity_ai/status/1895554843529658472), and [@perplexity_ai](https://twitter.com/perplexity_ai/status/1895554855168880918) announced **Perplexity Deep Research for Enterprise Data**, connecting to Google Drive, OneDrive, and SharePoint, enabling deep research across company files and the web with enterprise-grade security.  [@AravSrinivas](https://twitter.com/AravSrinivas/status/1895555108425122139), [@AravSrinivas](https://twitter.com/AravSrinivas/status/1895555299798630755), [@AravSrinivas](https://twitter.com/AravSrinivas/status/1895555497698476423), [@AravSrinivas](https://twitter.com/AravSrinivas/status/1895555816473886935), and [@AravSrinivas](https://twitter.com/AravSrinivas/status/1895555900305485849) further detailed **Perplexity Enterprise Pro**, emphasizing features like deep research, reasoning, internal/external search, access to all models, and collaboration.  [@lmarena_ai](https://twitter.com/lmarena_ai/status/1895565276131049864) and [@lmarena_ai](https://twitter.com/lmarena_ai/status/1895565279830425690) announced **Claude 3.7 Sonnet's top ranking in coding on the Arena**, highlighting its capabilities.  [@AIatMeta](https://twitter.com/AIatMeta/status/1895528149137629220) showcased **Llama being used by SevillaFC with IBM's watsonx to create Scout Advisor for soccer star scouting**.  [@OpenAIDevs](https://twitter.com/OpenAIDevs/status/1895531723640729907) highlighted **ConsensusNLP using GPT-4.5 for scientific/medical analysis** and structured outputs for visualizing research agreement.
- **Agentic AI and Automation**: [@mervenoyann](https://twitter.com/mervenoyann/status/1895497146344026184) announced **Microsoft's MAGMA-8B vision language action model** for physical and digital world operations including embodied robots and web automation.  [@llama_index](https://twitter.com/llama_index/status/1895522358561227000) shared an **example of agentic productivity applications built with LlamaIndex**.  [@RichardSocher](https://twitter.com/RichardSocher/status/1895369311109423210) suggested using **research agents like ARI for extensive literature reviews** in serious medical problems, providing an example report.
- **Coding and Development**: [@nearcyan](https://twitter.com/nearcyan/status/1895432741275242957) shared a meme about **junior devs watching Claude 3.7 "destroy their codebase in cursor"**.  [@HamelHusain](https://twitter.com/HamelHusain/status/1895354490519437764) stated **"It is only possible for me to understand GraphQL because of AI"**.  [@cloneofsimo](https://twitter.com/cloneofsimo/status/1895454047483896166) critiqued **current automated software development tools like Devin, OpenHands, Replit, and Cursor Compose**, finding them unable to complete even small applications end-to-end, lacking in server/client, IPC, queue, and scheduling capabilities.  [@rishdotblog](https://twitter.com/rishdotblog/status/1895457874299752868) claimed to have **replaced a $100/month tool with a $10 Claude Code solution**, suggesting programming jobs and SaaS companies are "going away".

**AI Research and Papers**

- **Recent Research Paper Highlights**: [@rasbt](https://twitter.com/rasbt/status/1895487669003518039) provided a **list of recent AI research papers** covering topics like SWE-RL, LoRA boosting, long-context LLMs, Logic-RL, test-time scaling, AI research agents, model selection, inner thinking transformers, natural reasoning, knowledge acquisition, freelance software engineering with LLMs, sparse attention, unlearning, large language diffusion models, model merging, reasoning-action dilemma, finance LLMs, infinite context, distillation scaling laws, prompt caching, reasoning from demonstrations, hierarchical reasoning, thinking in LLMs, compute-optimal test-time scaling, mathematical reasoning, large memory models, quantized LLMs, video RoPE, scaling up test-time compute, self-backtracking, training efficient reasoning, reasoning advancements, teaching critique via RL, enhancing reasoning for domain applications, less-is-more reasoning, chain-of-thought reasoning, chain-of-associated-thoughts, direct alignment algorithms, embedding layer scaling, and competitive programming with large reasoning models. [@iScienceLuvr](https://twitter.com/iScienceLuvr/status/1895334570704470281), [@iScienceLuvr](https://twitter.com/iScienceLuvr/status/1895334574160519522), [@iScienceLuvr](https://twitter.com/iScienceLuvr/status/1895333239608508473), [@iScienceLuvr](https://twitter.com/iScienceLuvr/status/1895333242041180256), [@iScienceLuvr](https://twitter.com/iScienceLuvr/status/1895331433897697689), and [@iScienceLuvr](https://twitter.com/iScienceLuvr/status/1895331436171010152) highlighted papers on **FlexiDiT, Self-Training for Concise Reasoning, and Thinking Slow, Fast with Distilled Reasoners**, providing abstracts and code links. [@omarsar0](https://twitter.com/omarsar0/status/1895528463504982115), [@omarsar0](https://twitter.com/omarsar0/status/1895528446882955439), and [@omarsar0](https://twitter.com/omarsar0/status/1895528398820425741) shared papers on **METAL (Modality-tailored critique), Modality-tailored critiques for self-correction, and Test-Time Scaling on Chart Generation**, noting performance improvements. [@_akhaliq](https://twitter.com/_akhaliq/status/1895341871721013408), [@_akhaliq](https://twitter.com/_akhaliq/status/1895341929271017897), [@_akhaliq](https://twitter.com/_akhaliq/status/1895341032973443349), [@_akhaliq](https://twitter.com/_akhaliq/status/1895340908134178897), [@_akhaliq](https://twitter.com/_akhaliq/status/1895339427859505583), [@_akhaliq](https://twitter.com/_akhaliq/status/1895339378257600744), [@_akhaliq](https://twitter.com/_akhaliq/status/1895338234085024241), and [@_akhaliq](https://twitter.com/_akhaliq/status/1895338193303806287) linked to papers on **Mobius (Text to Seamless Looping Video), FlexiDiT, R1-T1 (Translation Capability Incentivization), and LongRoPE2 (Context Window Scaling)**. [@dair_ai](https://twitter.com/dair_ai/status/1895532543652642850) and [@dair_ai](https://twitter.com/dair_ai/status/1895532546051752138) highlighted **Google's PlanGEN framework for complex planning and reasoning in LLMs**, detailing its constraint-guided verification and adaptive algorithm selection. [@DeepLearningAI](https://twitter.com/DeepLearningAI/status/1895501917398155695) summarized a paper on **Brain2Qwerty, a non-invasive AI system translating brain waves to text using MEG recordings**.
- **Cognitive Science and AI Alignment Theory**: [@AndrewLampinen](https://twitter.com/AndrewLampinen/status/1895520333257744493) shared a preprint on **"Naturalistic Computational Cognitive Science"**, synthesizing AI and cognitive science towards generalizable cognition models. [@DanHendrycks](https://twitter.com/DanHendrycks/status/1895378847547478093) discussed the **evolution of ideas in AI alignment theory**, contrasting "random memetic drift" with Yudkowsky's contributions, suggesting GPT is forcing empirical realities on the alignment forum.

**Humor and Miscellaneous**

- **AI Model Humor and Vibe Checks**: [@_akhaliq](https://twitter.com/_akhaliq/status/1895348244512973149) and [@_akhaliq](https://twitter.com/_akhaliq/status/1895346920983536106) posted **animated SVGs as humorous responses from GPT-4.5 about being open-sourced**. [@_philschmid](https://twitter.com/_philschmid/status/1895387638229766505) asked for **"vibe test prompts"**, suggesting counting to ten omitting numbers ending in "e" and generating an SVG of a pelican on a bicycle. [@NeelNanda5](https://twitter.com/NeelNanda5/status/1895371690571636880) shared an **LLM hack: "Write your response in the style of a Scott Alexander blog post"** for more enjoyable long outputs. [@aidan_mclau](https://twitter.com/aidan_mclau/status/1895312915387064584) presented a **humorous IQ scale from 0 to infinity**, culminating in an enlightened fart joke. [@andersonbcdefg](https://twitter.com/andersonbcdefg/status/1895362655722029414) shared a meme about **asking OpenAI if their model is good or lazy**. [@Teknium1](https://twitter.com/Teknium1/status/1895565961107030083) posted "GPT4.5 finally knows me, lmao" with an image implying GPT-4.5 understood their personality.
- **Societal and Philosophical Reflections**: [@RichardMCNgo](https://twitter.com/RichardMCNgo/status/1895535610053710249) made an **observation about the demographic overlap between high-IQ autism-spectrum biological males, transness, and systemizing thinking**. [@RichardMCNgo](https://twitter.com/RichardMCNgo/status/1895349955130446189) analogized the **US presidency since 2012 to progressive chess**. [@teortaxesTex](https://twitter.com/teortaxesTex/status/1895495175499317574) joked **Unitree bots will cause an uptick in solipsism**. [@francoisfleuret](https://twitter.com/francoisfleuret/status/1895572191452020903) expressed a **"nightmare" scenario of nukes, AI, and drones as rational defense**. [@AmandaAskell](https://twitter.com/AmandaAskell/status/1895559111565328608) humorously suggested an **expensive "I totes respect you" pin** as an alternative to uncomfortable suits for East Coast formality. [@AmandaAskell](https://twitter.com/AmandaAskell/status/1895493849575002127) joked about **gendered profile preferences on dating apps**.
- **Industry and Community Chatter**: [@suchenzang](https://twitter.com/suchenzang/status/1895560716981346466) posted "big model smell" with a link, and [@suchenzang](https://twitter.com/suchenzang/status/1895437762427560236) tweeted "things you can't buy for $9bn, maybe not even $30bn...". [@nearcyan](https://twitter.com/nearcyan/status/1895568285326020802) declared being **"done with benchmarks"**, losing empathy for hyper-dimensional shape descriptions. [@agihippo](https://twitter.com/agihippo/status/1895337878311575875) questioned **working hours in AI, suggesting "AI people are mostly working all the time!"**. [@ID_AA_Carmack](https://twitter.com/ID_AA_Carmack/status/1895507887658578403) was "very happy to see more classic game source code released", noting the **disjoint between game dev and broader open source culture**. [@c_valenzuelab](https://twitter.com/c_valenzuelab/status/1895495183137161551) joked **Runway's new about page states "We are brain surgeons* *for artificial brains."**.

---

# AI Reddit Recap

## /r/LocalLlama Recap

**Theme 1. DeepSeek Realse: Revolutionary Storage and Data Processing Tech**

- **DeepSeek Realse 5th Bomb! Cluster Bomb Again! 3FS (distributed file system) & smallpond (A lightweight data processing framework)** ([**Score: 499, Comments: 73**](https://reddit.com/r/LocalLLaMA/comments/1izvwck/deepseek_realse_5th_bomb_cluster_bomb_again_3fs/)): **DeepSeek** launches **3FS**, a high-performance distributed file system optimized for AI workloads, utilizing modern **SSDs** and **RDMA networks** to enhance distributed application development. Additionally, **smallpond**, a lightweight data processing framework, integrates with **DuckDB** and **3FS**, offering a streamlined solution for data processing tasks. For more information, visit their [**GitHub page**](https://github.com/deepseek-ai/3FS) and [**smallpond repository**](https://github.com/deepseek-ai/smallpond).
    - **3FS Performance and Comparison**: **3FS** achieves an impressive **6.6 TiB/s bandwidth**, significantly surpassing typical **DRAM speeds**. Discussions compared **3FS** to other systems like **Colossus** and noted its unique application in **AI training workloads** without traditional file read optimizations like caching.
    - **Open Source Strategy and Impact**: Many commenters appreciated **DeepSeek’s** open-source approach, highlighting its potential to democratize AI advancements and challenge monopolistic tech giants like **OpenAI** and **Nvidia**. The open-source culture was emphasized as a reciprocal process, benefiting both contributors and the broader AI community.
    - **Technical Insights and Historical Context**: **3FS** has been in production for over five years, developed by **High-Flyer AI** and used in their **Fire-Flyer II system**. It is optimized for large-scale random read operations, employs **Direct I/O**, and uses the **FFRecord** format for sample data storage, enhancing AI model training efficiency significantly.
- **DeepSeek OpenSourceWeek Day 5** ([**Score: 127, Comments: 9**](https://reddit.com/r/LocalLLaMA/comments/1izwh49/deepseek_opensourceweek_day_5/)): **Fire-Flyer File System (3FS)** is a parallel file system designed to maximize the bandwidth of modern SSDs and RDMA networks, achieving an impressive **6.6 TiB/s aggregate read throughput** in a 180-node cluster and **3.66 TiB/min throughput** on the GraySort benchmark with a 25-node cluster. It offers **40+ GiB/s peak throughput per client node** for KVCache lookup and supports a disaggregated architecture with strong consistency semantics, facilitating tasks like training data preprocessing and embedding vector search. For more details, visit the [**3FS repository**](https://github.com/deepseek-ai/3FS) and the [**Smallpond framework**](https://github.com/deepseek-ai/smallpond).
    - **3FS** is highly suitable for **AI Training Workloads** and **AI Inference**, offering benefits like random access to training samples without prefetching, high-throughput checkpointing, and a cost-effective KVCache for large language model inference. It also supports **data-intensive applications** requiring strong consistency and high throughput, as evidenced by its performance on the **GraySort benchmark**.
    - Users expressed amazement at the development team’s productivity, noting the impressive output despite limited manpower. The project originated from the CEO’s hedge fund team in 2019, and their recruitment strategy focuses on hiring top CS graduates from elite Chinese universities.
    - Some users find the technical details of **3FS** too complex and not directly applicable to most use cases, suggesting a potential mismatch between user expectations and the system’s specialized capabilities.

**Theme 2. French Reasoning Model: Economical and Effective**

- **I trained a reasoning model that speaks French—for just $20! 🤯🇫🇷** ([**Score: 229, Comments: 78**](https://reddit.com/r/LocalLLaMA/comments/1j045xn/i_trained_a_reasoning_model_that_speaks_frenchfor/)): I cannot generate a summary as the post body does not contain sufficient textual information, only a link to a video.
    - **Fine-tuning a 7B LLM**: **TheREXincoming** fine-tuned a **7B LLM** based on **Qwen 2.5** using only **2,000 samples** (1K English + 1K French) at a cost of **$20**. The model performs comparably to **R1 Distil 7B** on math benchmarks, showcasing minimal knowledge degradation.
    - **Model and Data Availability**: The fine-tuned model and its dataset are available on **Hugging Face** ([**Data**](https://huggingface.co/datasets/HoangHa/Pensez-v0.1), [**Model**](https://huggingface.co/HoangHa/Pensez-v0.1-e5), [**GGUF**](https://huggingface.co/HoangHa/Pensez-v0.1-e5-GGUF)). The model is designed for high-performance French language capabilities and can serve as a template for training reasoning LLMs in other languages.
    - **Community Feedback and Development**: Users inquired about the data selection and training details, while **TheREXincoming** mentioned ongoing efforts to clean up the data curation pipeline and plans to update the repository. The initiative was met with enthusiasm and disbelief at the low cost and high performance achieved.

**Theme 3. Sesame Realtime Voice Model Rivals OpenAI**

- **“Crossing the uncanny valley of conversational voice” post by Sesame - realtime conversation audio model rivalling OpenAI** ([**Score: 200, Comments: 37**](https://reddit.com/r/LocalLLaMA/comments/1j00v4y/crossing_the_uncanny_valley_of_conversational/)): **Sesame** showcased a compelling real-time conversational voice model that rivals **OpenAI’s Advanced Voice Mode**, with plans to release it under an **Apache 2.0 license**. Although the public weights are not yet available, the demo has impressed users with its quality, indicating a promising future for this new player in voice synthesis technology.
    - Users are highly impressed with the **Sesame conversational voice model**, noting its superior quality and speed compared to **ChatGPT’s advanced voice mode**. The demo is praised for its smooth response time and realistic sound, with users expressing excitement for its potential open-source release.
    - There is enthusiasm for the potential integration of the model with other technologies, such as **function calling** and **RAG**, to enhance its capabilities without increasing latency. Users are eager for the model to be available on platforms like **Hugging Face** for easier access and integration.
    - Some users highlighted limitations, such as the model’s inability to detect emotions or sarcasm and its tendency to shut down conversations if inputs are delayed. Despite these issues, the model’s engaging conversational style and memory capabilities were appreciated, with users looking forward to trying it on their own setups.

## Other AI Subreddit Recap

> /r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding

**Theme 1. Humorous and Creative Applications of GPT 4.5**

- **GPT 4.5 as Donald Trump explaining creation of Earth** ([**Score: 550, Comments: 86**](https://reddit.com/r/OpenAI/comments/1j06uyk/gpt_45_as_donald_trump_explaining_creation_of/)): **GPT 4.5** humorously mimics **Donald Trump** in a satirical narrative about the creation of Earth, attributing the planet’s formation to Trump’s personal initiative. The narrative highlights exaggerated claims about creating the sun, Earth, and its features, while humorously critiquing dinosaurs as a “huge mistake” before introducing “winning” animals and humans, all in a style characteristic of Trump’s speech patterns.
    - Commenters appreciated the humor and style of the **GPT 4.5** narrative, with many finding it amusing and noting its exaggerated **Trump-like** qualities, though some felt it was too coherent or repetitive. The humor about **dinosaurs** being a “huge mistake” and the planet being “the wettest ever” particularly resonated with readers.
    - There was interest in converting the text to audio using **text-to-speech** models, with some already sharing audio links ([**SoundProofHead’s link**](https://whyp.it/tracks/261792/trump?token=IcE0q) and [**TwoLevelsAhead’s link**](https://elontalks.com/share/7099b22a-5821-4b15-8354-feaee2eeece1)) or expressing a desire for a **deepfake video** version.
    - The discussion highlighted the potential of AI in humor, with some commenters suggesting that achieving genuine **comedy** could be a significant benchmark for AI capabilities, while others joked about the implications of AI mastering humor to a superhuman level.
- [**ChatGPT’s existential crisis over emoji**](https://i.redd.it/9guzd0ztkwle1.jpeg) ([**Score: 203, Comments: 48**](https://reddit.com/r/ChatGPT/comments/1j0b2ea/chatgpts_existential_crisis_over_emoji/)): **ChatGPT humorously misidentifies emojis**, including a seahorse, unicorn, shrimp, and dragon, leading to a playful yet existential reflection on emoji recognition capabilities. The conversation, shown on a dark background, underscores the casual and comedic nature of the AI’s attempts at identifying emojis.
    - **Emoji Misidentification**: Users enjoyed sharing humorous instances of **ChatGPT** misidentifying emojis, often repeatedly confusing seahorses with other animals like unicorns, dragons, and fish. This led to a playful and comedic exchange, highlighting the AI’s struggle with emoji recognition.
    - **Community Engagement**: Many users shared their own experiences and screenshots, contributing to the light-hearted nature of the conversation. The shared content included links to images and humorous dialogues, emphasizing the communal enjoyment of the AI’s quirky responses.
    - **AI Humor and Reflection**: The thread reflects on the whimsical nature of AI’s limitations, with users appreciating the comedic errors and engaging in a shared digital experience. This playful interaction underscores the community’s enjoyment of AI’s unpredictability and the shared humor derived from its errors.

**Theme 2. Innovations in AI Video and Audio Processing**

- [**Advanced Voice 4.5**](https://v.redd.it/sphlqpc77tle1) ([**Score: 365, Comments: 95**](https://reddit.com/r/ChatGPT/comments/1izzows/advanced_voice_45/)): The post titled **“Advanced Voice 4.5”** likely discusses advancements in **AI voice acting** technology, specifically focusing on version **4.5**. Without additional context or details, the post emphasizes the development of more **realistic AI-generated voices**.
    - There is skepticism about the **“Advanced Voice 4.5”** update, with users questioning whether it includes voice advancements, as some believe it is just an uncensored update. **TheRobotCluster** claims that version 4.5 does not apply to voice and is simply an uncensored version, raising questions about whether **ChatGPT** now allows uncensored content.
    - Discussions around the **AI’s ability to mimic accents** reveal mixed opinions; some users criticize the AI’s attempt at an **English accent**, suggesting it sounds like an American trying to mimic it. This raises questions about the authenticity and accuracy of AI-generated accents.
    - The conversation touches on AI’s impact on various industries, with some users predicting that AI advancements, particularly in voice acting and potentially the **porn industry**, could lead to significant technological evolution and financial gains in the future.
- [**SpargeAttn: A new method giving you a 1.83x speedup on video models with NO quality loss.**](https://i.redd.it/kuz97049xule1.jpeg) ([**Score: 155, Comments: 45**](https://reddit.com/r/StableDiffusion/comments/1j04o63/spargeattn_a_new_method_giving_you_a_183x_speedup/)): **SpargeAttn** offers a **1.83x speedup** for video models without compromising quality, as demonstrated by a comparison on an **L40 GPU**. The method reduces processing time from **1897 seconds** with “Full Attention” to **1037 seconds**, maintaining video quality.
    - **Installation Challenges**: Users discuss the complexity of installing **SpargeAttn** due to dependencies like **Triton** and the need for specific Python versions. Detailed steps for installation on Windows are provided, including links to necessary packages and commands for integration with **ComfyUI**.
    - **Compatibility and Performance**: **SpargeAttn** is noted to be model dimension specific, with potential issues when tuning across different model sizes (e.g., 1.3B vs 14B models). **Sliding Tile Attention** is mentioned as an alternative that performs well with tuning but is currently limited to **H100 cards**.
    - **Community Contributions**: **Kijai** has incorporated **SpargeAttn** into the **ComfyUI-WanVideoWrapper**, showcasing community efforts to integrate new tools into existing frameworks. Users express hope for future native support of attention mechanisms like **sage attention** and **triton** to simplify installation processes.

**Theme 3. AI Identity Confusions and Hallucinations**

- **Groks thinks it is Claude unprompted, and doubles down on it after being called out** ([**Score: 187, Comments: 54**](https://reddit.com/r/ClaudeAI/comments/1j0327j/groks_thinks_it_is_claude_unprompted_and_doubles/)): **Groks**, an AI model, erroneously identified itself as **Claude** during a conversation with the head of a debate club and persisted in this claim even after being questioned. The incident, detailed in a conversation shared on [**X**](https://x.com/TentBC/status/1895386542702731371?t=96M796dLqiNwgoRcavVX-w&s=19), raises questions about the underlying cause of this identity confusion.
    - Several users speculate that **Grok’s identity confusion** might stem from its training data, which includes outputs from older models like **Claude**. There’s a belief that **xAI’s** post-training might have been less thorough due to its newness and an attempt to reduce bias, leading to such errors.
    - The incident is viewed humorously by some, with comments highlighting the absurdity of the **debate club’s** questioning of smallpox’s existence. This has led to skepticism about the legitimacy of the debate club, with some users suggesting it resembles a conspiracy group.
    - There are suspicions that **Grok** might be using **Claude’s** technology underneath or trained on its datasets, similar to **Deepseek** using **ChatGPT** data, raising concerns about the legality and ethics of such practices.
- [**GPT-4.5 will just invent concepts mid-conversation**](https://i.redd.it/2h1m59ehsxle1.png) ([**Score: 348, Comments: 75**](https://reddit.com/r/OpenAI/comments/1j0gxxs/gpt45_will_just_invent_concepts_midconversation/)): **GPT-4.5** is noted for its ability to invent concepts during interactions, as highlighted in a **Twitter post by Aaron Ng**. In a conversation snippet, the AI invents the “CLEAR Model” specifically for the interaction, demonstrating its dynamic conversational capabilities.
    - **Peter Hawkins** originally invented the **CLEAR Model**, and **GPT-4.5**‘s reference to it is a form of hallucination, as noted by **I_am_John_Mac** with a link to [**hotpmo.com**](https://www.hotpmo.com/management-models/the-clear-model-peter-hawkins/). This highlights **GPT-4.5**‘s tendency to create concepts that may not be accurate or original.
    - There is a humorous tone in the discussion about turning **hallucinations** into a feature, with some users joking about the AI possibly filing patents or claiming intellectual property on its hallucinated concepts.
    - The **hallucination rate** of **GPT-4.5** is noted to be **37.1%**, which is lower than **GPT-4o’s** rate of **61.8%** and **o1’s** rate of **44%**, as mentioned by **Hexpe** and **vingeran**, suggesting an improvement in accuracy over previous models.

**Theme 4. AI Tools Streamlining Programming and Writing**

- **I made a simple tool that completely changed how I work with AI coding assistants** ([**Score: 167, Comments: 41**](https://reddit.com/r/ClaudeAI/comments/1j0ey3h/i_made_a_simple_tool_that_completely_changed_how/)): **CodeSelect** is a tool designed to streamline the process of sharing code with AI coding assistants like **Claude** and **ChatGPT** by displaying project structures as a checkbox tree, allowing quick file selection, and automatically detecting file relationships for better context. This lightweight tool, which installs with a single command and has no external dependencies, significantly reduces preparation time and improves AI response quality by providing proper context, and is available on [**GitHub**](https://github.com/maynetee/codeselect).
    - **Repomix** is highlighted as an alternative tool for managing code project structures, with a simple command (**`cd myProject && npx repomix`**) that works on any folder and outputs a draggable file, which users find effective for project management.
    - Users discuss integrating a **Gemini powered agent** into **CodeSelect** to suggest edits and file references to **Claude**, aiming to enhance efficiency and save tokens during the coding process.
    - **Claude’s GitHub integration** is noted for its ability to manage project-wide changes, such as renaming variables and updating comments, which users find impressive for maintaining project context without manual input.
- **Just bit the bullet and got a yearly Claude Pro subscription** ([**Score: 104, Comments: 128**](https://reddit.com/r/ClaudeAI/comments/1j04snp/just_bit_the_bullet_and_got_a_yearly_claude_pro/)): The author praises the **Claude Pro subscription** as a transformative tool for daily tasks, analytics, creative problem-solving, and software engineering, highlighting its effectiveness in debugging and code reviews. They express satisfaction with **Anthropic’s** product, contrasting it with criticisms of **Claude 3.7** for being too concise, and emphasize the significant advancement it represents over traditional search engines.
    - Users discuss **usage limits** as a significant issue with the **Claude Pro subscription**, with some suggesting strategies like starting new chats to manage limits effectively. Others express frustration with hitting limits frequently, which disrupts their workflow, while some users report rarely encountering these issues by keeping conversations short.
    - There is skepticism about posts praising **Claude Pro** being genuine, with some users suspecting them to be part of a **marketing campaign**. This suspicion is fueled by the timing of posts with promotional emails and the repetitive nature of positive endorsements, though others argue the discussions are genuine due to the subreddit’s focus.
    - Subscribers debate the value of a **yearly subscription** versus monthly payments, with some regretting the purchase due to decreasing quality and restrictive usage limits. Others find the subscription beneficial for their work, suggesting that the decision should depend on personal use cases and the rapidly evolving AI landscape.

---

# AI Discord Recap

> A summary of Summaries of Summaries by Gemini 2.0 Flash Thinking

**Theme 1. GPT-4.5 Enters Arena, but Claude 3.7 Still King of the Code**

- [**GPT-4.5 Fails to Impress, Price Tag Stings**](https://openai.com/index/introducing-gpt-4-5/): Early testers find **OpenAI's GPT-4.5** *overpriced at $150 per million tokens* and not significantly better than **GPT-4 Turbo** for coding, with many developers still favoring **Claude 3.7 Sonnet** for its superior performance in software engineering tasks.  Early benchmarks on aider's polyglot coding benchmark showed **GPT-4.5** scoring **45%** compared to **Sonnet 3.7's 65%**, leading to disappointment and questions about its value proposition given the high API cost.
- [**Claude 3.7 Sonnet Faces Load Issues, Remains Top Coder**](https://www.anthropic.com/news/claude-3-7-sonnet): Despite reports of high load messages and refusals, **Claude 3.7 Sonnet** is still considered the best model for software engineering due to its ability to accurately follow instructions and debug code effectively. Users highlight **Claude 3.7's** improved instruction following and debugging capabilities, even though some speculate **Anthropic** is making the model harder to use.
- [**DeepSeek R2 Hype Train Gathers Steam**](https://github.com/deepseek-ai/DualPipe):  Anticipation is building for **DeepSeek's R2 model**, with some members expecting it to surpass current SOTA models and disrupt corporate hype, as **DeepSeek's Chatbot** already outperforms existing models in coding. Members compare **DeepSeek's R1 model** favorably to **OpenAI's o1**, further fueling excitement for the upcoming **R2** release.

**Theme 2. IDE Wars: Cursor and Windsurf Trade Blows Over AI Coding Supremacy**

- [**Cursor Plagued by Bugs, Users Cry Foul**](https://www.cursor.com/downloads):  Users report **Cursor IDE** is riddled with bugs, experiencing frequent crashes and lost code changes after updates, with some considering disabling auto-updates and waiting for more stable releases. Frustration mounts as some users claim the coding quality of **Claude 3.7** on **Cursor** has declined since launch.
- [**Windsurf AI Jumps on GPT-4.5 Bandwagon, Questions Emerge**](https://x.com/windsurf_ai/status/1895206330987880816): **Windsurf AI** integrated **GPT-4.5** in Beta, but early tests show it's *significantly more expensive and not as strong* for software engineering, sparking debate if this move is genuine or *propaganda against Cursor*. Users question Windsurf's pricing model, specifically flow credits, finding **Cursor's** pricing more straightforward.
- [**Memory Banks in Cursor Deemed "Pointless" and Costly**](https://discord.com/channels/1074847526655643750):  **Cursor's Memory Banks** feature is criticized as inefficient and expensive, with users reporting costs reaching **$50 a day** using the **Claude 3.7 API**, and that memory banks sometimes *hallucinate* making it cheaper to hire a programmer. Users find memory banks inefficient because they occasionally make mistakes, leading to the conclusion that hiring a human programmer is more cost-effective.

**Theme 3. Hardware Hustle: DeepSeek's DualPipe and TinyLM Offer Glimmers of Innovation**

- [**DeepSeek's DualPipe Declares War on Pipeline Bubbles**](https://github.com/deepseek-ai/DualPipe):  **DeepSeek AI** released **DualPipe**, a bidirectional pipeline parallelism algorithm for computation-communication overlap in **V3/R1 training**, aiming to reduce pipeline bubbles compared to traditional methods. This release, along with **EPLB**, an expert-parallel load balancer, is part of a week-long series of releases from **DeepSeek AI**.
- [**TinyLM Unleashes Client-Side LLMs with WebGPU Fury**](https://tinylm.wizenheimer.dev/): **tinylm** v0 launched, a library enabling client-side **LLMs** in browsers or Node.js with **WebGPU acceleration**, boasting **zero-cost inference** and complete privacy with an OpenAI-compatible API. **tinylm** supports text generation, embeddings, and real-time token streaming, and eliminates the need for servers for local LLM inference.
- [**NVIDIA Shifts Tensor Core Focus to FP4, Leaving INT4 Behind?**](https://github.com/gau-nernst/quantized-training?tab=readme-ov-file#matmul):  **NVIDIA** appears to be shifting away from **INT4 Tensor Cores** towards **FP4**, with **Blackwell** GPUs featuring **FP4**, while **Ada** had **INT4** and **Hopper** had **INT8**, raising questions about the future of INT4 precision in NVIDIA's hardware strategy. Benchmarks suggest **NVIDIA** is prioritizing **FP4** for quantized model training, potentially impacting future hardware development and software optimization strategies.

**Theme 4. Pricing Pressure: GPT-4.5 API Costs Spark Outrage, Open Source Alternatives Beckon**

- [**GPT-4.5 API Pricing Deemed "Insane," Users Seek Alternatives**](https://x.com/OpenRouterAI/status/1895236199004152272): **OpenAI's GPT-4.5 (Preview)** API pricing at **$75 input / $150 output per million tokens** is met with harsh criticism, with users decrying the exorbitant cost compared to models like **Grok3** and **Claude Sonnet 3.7**, questioning its value and prompting some to consider open-source alternatives. The high cost of **GPT-4.5** raises concerns about accessibility and sustainability for developers and researchers.
- [**Deepinfra Underprices Fal AI by 100x, Claims User**](https://discord.com/channels/879548962464493619/879548962464493622/1344457413314740256):  A user claims **Deepinfra** is *100x cheaper than Fal AI* for character processing, charging *$0.8 per million characters* and offering free compute, contrasting with **Fal AI's** *$50 free credit*, and suggesting **Kokoro TTS** as another low-cost alternative. This pricing discrepancy highlights the competitive landscape and cost-saving opportunities in the AI infrastructure market.
- [**Windsurf Users Question Flow Credits, Find Cursor Pricing "Preferable"**](https://codeium.canny.io): **Windsurf's pricing** model, particularly flow credits and additional flow action costs, is confusing to users, leading some to prefer **Cursor's** more straightforward pricing approach. Users express concern about the disproportionate cost of additional flow actions, impacting the perceived value and transparency of Windsurf's pricing structure.

**Theme 5. Community Pulse: From Robotics Arms to LeetCode for CUDA, Innovation Thrives**

- [**Hobbyists Unite to Build DIY Robotics Arm**](https://www.creality.com/products/ender-3-v2-3d-printer): Members in **LM Studio Discord** are enthusiastically discussing building a robotics arm from scratch, leveraging affordable 3D printers like the [$100 Creality Ender 3 V2](https://www.creality.com/products/ender-3-v2-3d-printer) and open-source resources for learning servos, CAD, and microcontrollers. This project showcases the community's hands-on approach to learning and applying AI and robotics principles.
- [**LeetCode for CUDA Arrives, Challenges GPU Gurus**](https://leetgpu.com/challenges): The **CUDA community** celebrates the beta release of [LeetCode for CUDA](https://leetgpu.com/challenges), a new platform offering coding challenges specifically designed for **CUDA development**, inviting users to test their skills and provide feedback. This new platform fosters a competitive and collaborative environment for improving CUDA programming skills.
- [**Hugging Face Community Fixes Microsoft's Phi-4 Mini Fiasco**](https://huggingface.co/unsloth/Phi-4-mini-instruct):  **Microsoft's Phi-4 mini** model was found to be *completely unusable* due to bugs, prompting the **Unsloth AI team** to upload [fixed versions](https://huggingface.co/unsloth/Phi-4-mini-instruct) on Hugging Face after **Microsoft** failed to incorporate **Unsloth's bug fixes**. This community-driven effort highlights the collaborative nature of open-source AI development and the importance of rapid response to critical issues.



---

# PART 1: High level Discord summaries




## [Cursor IDE](https://discord.com/channels/1074847526655643750) Discord

- **GPT-4.5 Underwhelms Testers with Hefty Price Tag**: Early testers find **GPT-4.5** from OpenAI *overpriced and not significantly better than GPT-4 Turbo*, noting the cost at **$150 per million tokens**.
   - The consensus is that **Claude 3.7 Sonnet** remains superior for coding, leading some to call GPT-4.5 *“just big”* and highlight its lack of new frontier capabilities.
- **Claude 3.7 Sonnet Faces High Load and Refusal Issues**: Users report issues with **Claude 3.7 Sonnet**, including frequent high load messages and refusals to answer certain prompts, with some speculating about whether **Anthropic** is making model more difficult to use.
   - Despite these issues, many still consider **Claude 3.7 Sonnet** the best model for software engineering due to its ability to accurately follow instructions and debug code effectively.
- **Cursor Riddled with Bugs and Update Woes**: Multiple users reported experiencing **frequent crashes and the need to reinstall Cursor** after updates, and lost code changes to the bugs, and the latest versions may be impacting performance and stability.
   - Others suggested disabling auto-updates and waiting for a more stable release, and some users are claiming the quality of Claude 3.7 coding, on cursor, has reduced compared to launch.
- **Windsurf AI Boasts Quick GPT-4.5 Integration**: **Windsurf AI** announced that GPT-4.5 is now available in Beta on Windsurf, but noted that early testing shows that it’s *significantly more expensive (&gt;10x) than alternative models*, and is not as fast nor as strong as existing models for software engineering or tool calling.
   - Users debate whether Windsurf's move is mere *propaganda to attack Cursor* or a genuine effort to provide access to the latest models, even with limitations, according to [this tweet](https://x.com/windsurf_ai/status/1895206330987880816?s=46&t=ggmESCIXF0nYw8_kshHz7A).
- **Memory Banks Fall Short of Expectations**: Discord members report that the memory banks seems very inefficient to me, and besides being expensive, using Claude 3.7 API can easily reach **$50 a day**.
   - The inefficiency arises because memory banks sometimes makes mistakes or *hallucinates*, making it cheaper to hire a programmer.



---



## [aider (Paul Gauthier)](https://discord.com/channels/1131200896827654144) Discord

- **GPT-4.5 Falls Flat, Claude 3.7 Dominates**: Early benchmarks show disappointing coding performance of **GPT-4.5 Preview**, scoring **45%** on aider's polyglot coding benchmark compared to **Sonnet 3.7's 65%**, leading members to believe it is intended to be a *"friendly" non-reasoning language model.*
   - Despite **GPT-4.5's** release, **Claude 3.7** remains the top choice for complex coding problems, outperforming **GPT-4.5** on coding benchmarks and also easier to jailbreak.
- **DeepSeek R2 Hype Intensifies**: Members are highly anticipating **DeepSeek's R2 model**, expecting it to surpass current SOTA models and disrupt corporate hype, with some comparing **DeepSeek's R1 model** to **O1**.
   - The anticipation stems from the sentiment that **DeepSeek's Chatbot** already outperforms existing models in coding capabilities.
- **Aider Users Advocate for Auto-Retry Mode**: Users are requesting an auto-retry mode for **Aider** to address the unreliability of models like **Deepseek R1**, proposing a fallback mechanism to another model if the primary one fails.
   - The request highlights the need for more reliable model performance to enhance the **Aider** coding experience.
- **Sam Altman Blames the Great GPU Shortage for GPT-4.5's insane API price**: **Sam Altman** admitted to the difficulty in meeting GPU demand, which is limiting **GPT-4.5's** access behind a higher paywall.
   - Some members speculate that the high price of **GPT-4.5's API** is due to the unaffordability of the model's configuration otherwise.
- **Aider Configuration with Venice AI is now possible**: Members are exploring configuring **Aider** to function with **Venice AI**, an LLM provider utilizing an OpenAI-style API endpoint, by setting the **OPENAI_API_BASE** and **OPENAI_API_KEY** environment variables as described in the [OpenAI compatible API documentation](https://aider.chat/docs/llms/openai-compat.html).
   - If you would like to use **Claude 3.7** with thinking in **aider.conf.yaml**, [here](https://cdn.discordapp.com/attachments/1133060505792159755/1344816054517633056/image.png?ex=67c2490c&is=67c0f78c&hm=de4579ce5ba2efe4ceec939472a11c85ae550af07804dec9dfbc30265fda51e1&) is an example configuration on how to set up the model for the editor with thinking.



---



## [OpenAI](https://discord.com/channels/974519864045756446) Discord

- **GPT-4.5 Skips Multimodal Features**: **OpenAI** released a research preview of **GPT-4.5**, their largest and best model for chat, rolling out to **ChatGPT Pro** users first, but **GPT-4.5** currently does not support multimodal features such as **Voice Mode**, **video**, and **screensharing** in **ChatGPT**.
   - Initial testing indicates that **GPT-4.5** feels *more natural* due to its broader knowledge base, improved ability to follow user intent, and greater "EQ", making it useful for improving writing, programming, and solving practical problems.
- **Anonymous Model Shadows Sonnet 3.7**: An anonymous model is rumored to be around **Sonnet 3.7's** performance, sparking speculation that if it's **GPT 4.5**, it's underwhelming given the model size.
   - Members speculated that if **OpenAI** releases a model that is bigger but performs the same as **Sonnet 3.7**, then they are behind the competition, even if the model is non-thinking.
- **Cracking LLM's Creative Prose**: When using LLMs for creative writing, defining a deep background for characters and directly discussing alternate routes can enhance the narrative's depth and avoid repetitive emotional scenes and clichés.
   - Experiment with having **ChatGPT** generate conversations and interactions first, followed by a narration from the writer's perspective, steering it towards desired directions.
- **Peeking at OpenAI's Model Spec**: **OpenAI** released its [Model Spec](https://model-spec.openai.com/2025-02-12.html) which outlines the **intended behavior for the models** that power OpenAI's products, including the API platform.
   - The goal is to create models that are useful, safe, and aligned with the needs of users and developers while advancing their mission to ensure that artificial general intelligence benefits all of humanity.



---



## [Unsloth AI (Daniel Han)](https://discord.com/channels/1179035537009545276) Discord

- **Unsloth Unsnarls Phi-4 Mini Fiasco**: Members reported issues with **Microsoft's Phi-4 mini**, and the **Unsloth team** uploaded [fixed versions](https://huggingface.co/unsloth/Phi-4-mini-instruct) on HF.
   - The team stated that **Microsoft** didn't use **Unsloth's bug fixes**, leading to the model being *completely unusable*.
- **DeepSeek Drops DualPipe Delight**: **DeepSeek AI** released [DualPipe](https://github.com/deepseek-ai/DualPipe), an algorithm for computation-communication overlap in **V3/R1** training, which includes **EPLB**, an expert-parallel load balancer, optimized for **V3/R1**.
   - The release is part of a series of releases this week from DeepSeek.
- **GRPO Reward Functions Get Groomed**: Community members debugged and improved the **reward functions** in the [GRPO notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb), adding `re.DOTALL` flag for multiline XML matching, correcting a typo in `count_xml`, and addressing issues with integer rewards.
   - Community members recommended a block size of **128** as ideal, and an effective size of **64/128** as more stable.
- **Ollama's Think-Token Trickery Troubles Users**: A user found that **Ollama** appends a **<think>** token to prompts, which prevents the model from generating it, requiring adjustments to output parsing for **<answer>** tags.
   - The user suggested that disabling this feature would be helpful, acknowledging that it stems from the model's processing class.
- **Inception Labs Invents Mercury dLLM**: [InceptionAILabs](https://x.com/InceptionAILabs/status/1894847919624462794) introduced **Mercury**, a diffusion large language model (**dLLM**), to advance intelligence and speed through parallel, coarse-to-fine text generation.
   - Challenges remain deploying such models, especially lack of OS support and difficulties extending context length could be bottlenecks.



---



## [Codeium (Windsurf)](https://discord.com/channels/1027685395649015980) Discord

- **Claude 3.7 Prompt Actions Inflated**: The team is working with Anthropic to address higher **flow actions per prompt** in **Claude 3.7 Sonnet** compared to **Claude 3.5 Sonnet**.
   - They advise using **3.7** for precise tasks and **3.5** for balanced performance.
- **Claude 3.7 Credit Multiplier Reduced**: The **credit multiplier** for **Claude 3.7 Sonnet Thinking** decreased from **1.5** to **1.25** due to initial token usage data.
   - Users now consume **1.25** user prompt credits and **1.25** flow action credits per tool call.
- **Cascade Crashes Cause Consternation**: Users reported that **Cascade** isn't working due to a `resource_exhausted` error, according to a [Feature Request](https://codeium.canny.io/feature-requests/p/cascade-isnt-working-any-more-errorserver-encountered-error-of-type-resource-ex).
   - Members are encouraged to follow the [roadmap](https://codeium.canny.io) to stay updated.
- **Windsurf Users Question Pricing**: Members express confusion over **Windsurf's pricing**, specifically regarding **flow credits** and the cost of additional flow actions.
   - Some users found **Cursor's** pricing preferable for its straightforward approach.
- **GPT-4.5 Enters Beta**: **GPT-4.5** is available in @windsurf_ai on rolling beta!, but is significantly more expensive (\>5-10x **GPT-4 Turbo**) and rate limits are more strict, with incrementally rolling it out to users.
   - Early testing of **GPT-4.5** shows it may not be the best code model. [Tweet from Windsurf](https://x.com/windsurf_ai/status/1895206330987880816) about **GPT-4.5**.



---



## [GPU MODE](https://discord.com/channels/1189498204333543425) Discord

- **DeepSeek's R1 Model Rocks Reasoning Realm**: **DeepSeek's R1 model** enhances reply quality via *chain of thought* generation, matching **OpenAI's o1** on benchmarks and providing open-source access, as detailed in their technical reports and the [DeepSeek API documentation](https://api-docs.deepseek.com/quick_start/pricing).
   - In related news, [DeepSeek released DualPipe on Github](https://github.com/deepseek-ai/DualPipe), a bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training.
- **AIE Toolchain Troubles Trounce Techies**: A member struggled with **AMD's Zen 5 NPU** and **AIE toolchain**, noting the difficulty compared to **Intel**, finding [Linux support merged recently](https://github.com/Xilinx/mlir-aie/blob/main/docs/buildHostLin.md) but installation remains complicated.
   - The member suggested that *NPU BLAS* was easier to run on **Intel** architecture.
- **NVIDIA Abandons INT4 TensorCores**: A member observed **NVIDIA** shifting from **INT4 Tensor Cores** to **FP4**, sharing quantized model [benchmarks](https://github.com/gau-nernst/quantized-training?tab=readme-ov-file#matmul).
   - Another member clarified that **Ada** had **INT4**, **Hopper** had **INT8**, and **Blackwell** features **FP4**.
- **CUDA Community Gets Leet-ified**: The CUDA community highlights the release of [LeetCode for CUDA](https://leetgpu.com/challenges) in beta, inviting users to try it out and provide feedback, but users should expect some hiccups due to its beta status.
   - In related news, NVIDIA is hosting invite-only, hands-on **CUDA C++** and **CUDA Python** tutorials the day before **GTC 2025** on **Sunday, March 16, 2025**, from 12-4 PM, and invites you to also the GPU MODE event from 5-10 PM ([lu.ma/8w1ehhrw](https://lu.ma/8w1ehhrw)).
- **Diffusion Models Demolish LLMs in Generation Speed?**: Members reported that Diffusion models can achieve super-speedy generation on GPUs, surpassing Groq/Cerebras, and do much better at “fill-in-the-middle” (FIM) compared to other models like **DeepSeek V2 Lite** ([tweet](https://x.com/dzhulgakov/status/1894932614173392975)).
   - They highlighted [Mercury by Inception Labs](https://x.com/InceptionAILabs), the first commercial-grade diffusion large language model (dLLM) with parallel, coarse-to-fine text generation, claiming to be up to **10x faster** than speed-optimized LLMs, achieving over **1000 tokens/sec** on **NVIDIA H100s**.



---



## [OpenRouter (Alex Atallah)](https://discord.com/channels/1091220969173028894) Discord

- **OpenAI Suffers Outage**: OpenRouter experienced an **OpenAI provider outage**, which has been resolved after being identified as an incident on **OpenAI's** side.
   - Requests are now succeeding, and **OpenAI** as a provider on OpenRouter has recovered.
- **DeepSeek R1 Runs Fast with SambaNovaAI**: The **671B-param DeepSeek R1** is now available via **SambaNovaAI** on OpenRouter, delivering **150 tokens/second**.
   - More details can be found on [OpenRouterAI's tweet](https://x.com/OpenRouterAI/status/1895135991025017346).
- **Sonnet 3.7 Gains Capacity Boost and Browsing**: **Claude Sonnet 3.7** now features significantly higher rate limits and web search capability on OpenRouter.
   - A reminder of these features was posted on [OpenRouterAI's tweet](https://x.com/OpenRouterAI/status/1895141541473329597).
- **GPT-4.5 (Preview) Launches at Premium Price**: **GPT-4.5 (Preview)**, designed to push boundaries in reasoning, creativity, and long-context conversations, is now available on OpenRouter, costing **$75/M** input tokens and **$150/M** output tokens.
   - The announcement links to the [OpenAI blog post](https://openai.com/index/introducing-gpt-4-5/) and a [discussion on X](https://x.com/OpenRouterAI/status/1895236199004152272), with community members decrying the exorbitant cost compared to models like **Grok3** and **Claude Sonnet 3.7**.
- **Users Track API Usage with YPerf**: A member created [YPerf.com](https://yperf.com/) *to monitor model API usage and performance* across OpenRouter.
   - The [Gemini Flash 1.5 8B](https://yperf.com/) ranks #66, costing **$0.04**, with **0.52s** latency and **419.8T/s** throughput.



---



## [LM Studio](https://discord.com/channels/1110598183144399058) Discord

- **Hobbyists Building DIY Robotics Arm**: Members discussed building a robotics arm from scratch to learn about **servos, CAD, and microcontrollers**, recommending a [$100 Creality Ender 3 V2 printer](https://www.creality.com/products/ender-3-v2-3d-printer) from Microcenter.
   - They also pointed to **transformers** for ML and highlighted [open-access courses from top universities like Stanford](https://online.stanford.edu/) and videos from Karpathy (ex OpenAI, Tesla) for learning ML.
- **Debating LLM Backends for Websites**: Members discussed how to implement an **LLM in a website**, with suggestions including using **websockets, SSR, AnythingLLM**, and code editors like **Cursor and Continue.dev**.
   - It was clarified that hosting a website on **GitHub Pages** would require the LLM to be hosted elsewhere (*Azure, cloud, ngrok*).
- **Grok-3's Performance Surprises Members**: Members discussed the surprisingly good performance of **Grok-3** vs the previous O3 model on various benchmarks, questioning if [X.ai's benchmarks](https://x.ai/documents/2025.02.20-RMF-Draft.pdf) were accurate or misleading.
   - The users debated if Grok-3 was rushed to market without proper ethical red-teaming, while others argued that Grok 3 is a beta, monitored, and not on API due to safety reasons.
- **Framework Desktop Features Unified RAM**: The [Framework desktop](https://frame.work/desktop) features **unified RAM** between the CPU and GPU, offering up to **128GB** of shared memory, with approximately **90GB** available for the GPU.
   - One user likened it to a MAC setup, highlighting the appeal of unified RAM in a PC.
- **GMK Announces Ryzen AI Mini-PC**: [GMK](https://wccftech.com/gmk-announces-worlds-first-mini-pc-based-on-amd-ryzen-ai-9-max/) announced the world's first mini-PC based on **AMD Ryzen AI 9 Max+ 395**, expected to hit the market in the first or second quarter.
   - This mini-PC will feature **Zen 5 architecture** with up to a **16-core/32-thread** configuration and powerful integrated graphics based on the **RDNA 3.5 architecture**.



---



## [Interconnects (Nathan Lambert)](https://discord.com/channels/1179127597926469703) Discord

- **Phi-4 Multimodal Family Gets Launched**: Microsoft launched the **Phi-4 family** of small language models (**SLMs**), including **Phi-4-multimodal** (processes speech, vision, and text) and **Phi-4-mini** (excels in text-based tasks), available in [Azure AI Foundry](https://aka.ms/try-phi), [HuggingFace](https://aka.ms/phi-4-multimodal/hf), and the [NVIDIA API Catalog](https://aka.ms/phi-4-multimodal/nvidia).
   - Some users doubt claims that it has similar multimodal performance to **Gemini Flash** lite.
- **Leaked GPT-4.5 System Card Sparks Debate**: A user shared the **GPT-4.5 System Card** [available here](https://cdn.openai.com/gpt-4-5-system-card.pdf), indicating that interacting with **GPT-4.5 feels more natural** and that *internal testers report GPT-4.5 is warm, intuitive, and natural*.
   - The card notes that it improves GPT-4's computational efficiency by more than 10x, yet some call the card very boring, while others interpret the card to indicate a **GPT4.5** is a creative writer while **Sonnet 3.5** is a problem solver.
- **OpenAI Launches GPT-4.5, Character Mainstream?**: OpenAI launched **GPT-4.5** as a research preview, available to OpenAI Pro users and API developers with image + text in, text out and same context as 4o model, trained till June 2024, [official announcement here](https://openai.com/index/introducing-gpt-4-5/).
   - A user notes that character/personality is becoming a mainstream topic, and OpenAI *aggressively used low-precision training*, and is now priced at $75 per million input tokens and $150/million for output.
- **GPT-4.5 Benchmarks Disappoint**: Early benchmarks of **GPT-4.5** show it being outperformed by **o1** on several problems, indicating pre-training isn't the optimal place to spend compute in 2025.
   - One user notes the hallucination metrics are very good while another believes in 1-2 years this will be the default model size.
- **Anthropic Gets Called Out On Sneaky Data**: A user accused **Anthropic** of *sneaky* data collection from the Computer Use API, using it to train classifiers for corporate ethical guidelines, and updating their website to appear transparent, according to [this fxtwitter thread](https://fxtwitter.com/elder_plinius/status/1895177131576918200).
   - It was inferred that **Anthropic** used user data based on their [summarization for monitoring blogpost](https://alignment.anthropic.com/2025/summarization-for-monitoring/), and although a user pointed out that the data source for training remains unspecified.



---



## [Latent Space](https://discord.com/channels/822583790773862470) Discord

- **Speak AI Sees Hockey-Stick Growth**: Paul Graham shared [Speak AI's revenue graph](https://x.com/paulg/status/1894827577325560215) showing a novel variant of exponential growth, where a company selling a *new year's resolution* product sees sustained usage due to its effectiveness.
   - Swyx and others observed this unique growth pattern.
- **Hume AI's Octave Sings Emotionally**: Hume AI launched [Octave](https://x.com/hume_ai/status/1894833497824481593), a new **LLM** for text-to-speech that can design voices with prompts and control emotion and delivery, with a creator studio for long-form content production.
   - The model understands how meaning affects delivery to generate emotional, human-like speech, unlike traditional TTS systems.
- **Diffusion LLM Mercury Rises**: Inception Labs introduced [Mercury](https://x.com/InceptionAILabs/status/1894847919624462794), the first commercial-grade **diffusion large language model (dLLM)**, which promises parallel, coarse-to-fine text generation.
   - Karpathy sees potential for **Mercury** to demonstrate unique psychology, new strengths and weaknesses, and [encouraged people to try it out](https://x.com/karpathy/status/1894923254864978091).
- **Karpathy Shares LLM Wisdom**: Andrej Karpathy released a [2h11m YouTube video](https://x.com/karpathy/status/1895242932095209667) on *How I Use LLMs*, a practical guide to the **LLM ecosystem** with examples, including tool use, file uploads, audio/video I/O, memory, and custom GPTs.
   - The video covers topics such as ChatGPT interaction, tool use (internet search, deep research, Python interpreter), Claude Artifacts, Cursor Composer, Speech I/O, NotebookLM, and image/video I/O.
- **GPT-4.5 Launch Underwhelms**: Members experienced initial technical difficulties and felt the **GPT-4.5** launch stream was a disappointment, with descriptions such as *hostage video*.
   - The new model doesn't have an API, and is focused on heavy-tail, real world edge cases like responding to angry texts.



---



## [Nous Research AI](https://discord.com/channels/1053877538025386074) Discord

- **Wan2.1 Model a Video Diffusion Milestone**: The release of [Wan2.1](https://github.com/Wan-Video/Wan2.1), an open and advanced large-scale video generative model, is considered a pivotal moment for video models, similar to **Stable Diffusion**.
   - Users are excited to see how this model will be used to disrupt the current set of problems and issues when it comes to **video diffusion**.
- **GPT-4.5: More Compute, Less Impressive?**: **GPT-4.5** has been released, is more compute-intensive than **GPT-4o**, with Sam Altman saying that this model *feels like talking to a thoughtful person*.
   - Despite Karpathy claiming it has **10x more pretraining compute than GPT-4**, its use case might be limited given it is overfit on the river crossing puzzle and geared towards creative use cases.
- **Apple Intelligence Gets Thumbs Down**: Members found **Apple Intelligence** underwhelming, calling it a shift from business API use to consumers, and stating they're in an *edge-inference-first trap*.
   - Some argued that **Apple** should have prioritized making **AI** as good as possible, rather than focusing on on-device constraints, however the *edge-inference-first* constraint ultimately *messed it up*.
- **Mercury dLLM: Lightning Fast Diffusion LLM**: **Inception Labs** launched **Mercury**, a diffusion large language model (**dLLM**) family that they claim is **10x** faster than optimized LLMs, achieving over **1000 tokens/sec** on **NVIDIA H100s**.
   - A code generation model, **Mercury Coder**, is available for testing in a [playground](https://chat.inceptionlabs.ai).
- **Reasoning Toggle via Voice?**: A user asked about toggling reasoning in an **AI model** via voice commands, aiming for **90% reasoning off** unless specifically prompted with phrases like *'use reasoning'*.
   - The user is trying to add a system prompt to achieve this and finetune the reasoning process and enable text-to-speech functionality, potentially with **Elevenlabs** or **Cartesia**.



---



## [HuggingFace](https://discord.com/channels/879548962464493619) Discord

- **Deepinfra Decimates Fal AI Dollars?**: A user claimed **Deepinfra** is *100x* cheaper than **Fal AI** for character processing, charging *$0.8 per million characters* and offers free compute.
   - They stated that **Fal AI** offers *$50* free credit, while suggesting **Kokoro TTS** as another low-cost alternative.
- **REFUTE Benchmark Reckons Reasoning**: The **REFUTE benchmark** assesses **Language Models (LMs)** in their ability to falsify incorrect algorithmic solutions, revealing even top agents score a low 9%.
   - The paper introducing the benchmark advocates for challenging solutions rather than merely generating them, emphasizing the importance of falsification in scientific discovery with [a link to the paper](https://huggingface.co/papers/2502.19414).
- **Smolagents Quiz is a Pain**: Multiple users reported issues with the **smolagents course** quizzes, including display problems with the **iframe** making feedback unreadable, and contradictory validation from the agent regarding the id argument in **HfApiModel**.
   - Users expressed frustration over discrepancies between the quiz's security settings and current documentation, as well as confusion about model implementation with **HfApiModel** versus **LiteLLMModel**.
- **NVIDIA Neutralizes Nasty Needle Attacks**: The [NVIDIA AI Red Team](https://developer.nvidia.com/blog/nvidia-ai-red-team-an-introduction/) identified that **prompt injection** can exploit plug-ins in the [LangChain](https://www.langchain.com/) library.
   - They warned that prompt injection is a new attack technique specific to **large language models (LLMs)** that enables attackers to manipulate the output of the LLM.
- **PyTorch360Convert Presents Panoramic Potential**: A member introduced **pytorch360convert**, a new lightweight **PyTorch library** to simplify working with **360° images** for VR, AR, video games, and more, available via `pip install pytorch360convert`.
   - The library supports various image representations, including **equirectangular images** and **cubemaps**, and is **GPU/CPU compatible** with multiple precision types, available on [GitHub](https://github.com/ProGamerGov/pytorch360convert).



---



## [Perplexity AI](https://discord.com/channels/1047197230748151888) Discord

- **Voice Mode Vigorously Vouched For**: Members discussed the new **voice mode** feature, noting improvements in **UI**, the ability to **interrupt**, and changes to **voices**.
   - While some users found it impressive, others felt it didn't quite match the level of **Microsoft Copilot**, **Grok 3**, or **ChatGPT**.
- **GPT-4.5 Gossip Grows Galore**: Users discussed the potential integration of **GPT-4.5** into Perplexity, referencing a [YouTube demo](https://www.youtube.com/watch?v=cfRYp0nItZ8) and noting it as a model with *greater context* and *more human-like* responses.
   - A user shared a link from [Sam Altman on X](https://x.com/sama/status/1895203654103351462) mentioning that **GPT-4.5** is *the first model that feels like talking to a thoughtful person*.
- **Perplexity Users share many Perplexity Links**: Several users shared an array of **Perplexity AI** search and page links, spanning topics from [quantum computing](https://www.perplexity.ai/search/majorana-1-the-worlds-first-qu-GfQ6ey8KRHKJoZXASTx94w) to [AI communication](https://www.perplexity.ai/search/i-heard-about-two-ais-communic-2NNO3p7QQdac1IJ0TDAmjA).
   - These links also included discussions around [building a house](https://www.perplexity.ai/search/i-need-to-build-a-house-to-rep-OQoLSIjESviUYqwhCnA0uw), and AI-driven diagnoses.
- **API Credit Confusion Causes Concerns**: A user inquired about the number of API calls and searches possible with the **$5 API credit** included with **Perplexity Pro**, and how to pay if they exceed the given credit.
   - A user also asked about how to get a **refund** if the **API is recharged by mistake** and remains unused.
- **Web Clipper Configuration Catastrophe**: A user is experiencing issues configuring the **Perplexity API** with the `sonar-deep-research` model in **Obsidian Web Clipper** despite setting the correct **Base URL** and **API Key**.
   - The user has provided [screenshots](https://cdn.discordapp.com/attachments/1161802929053909012/1344638496190627922/Image_27-2-25_at_12.42_PM.jpeg?ex=67c24c6f&is=67c0faef&hm=8e87be021f18ebec8872bb67c9635f61d713e54264e2613c300ca3564492218d&) of their configuration and the failure message, seeking assistance with troubleshooting.



---



## [Stability.ai (Stable Diffusion)](https://discord.com/channels/1002292111942635562) Discord

- **Stability AI Kicks off Website Redesign Competition**: Stability AI launched a **Website Redesign Contest** for the **Stable Diffusion** community to showcase their best work, submissions close on **Friday, March 7th**.
   - Winning images will be featured on **Stability AI’s official website**, and entries must use **Stable Diffusion 3.5** as a base.
- **SD community hooked on T5 CLIP**: A member sought an **SDXL-like model** with **T5 CLIP** integration, saying they had a taste of **T5 prompt adherence in SD3.5**.
   - They found the **T5 adherence addictive** and was looking for an alternative.
- **ControlNet Models Craze Rages On**: A member asked for recommendations for the best **ControlNet models** to maintain character consistency in **SDXL**.
   - They specifically requested a reference **U-Net model**, if available.
- **ComfyUI Remote Installs Now on Sale**: A member mentioned selling **ComfyUI workflows** and remote installs to make them work for users, typically using **TeamViewer**.
   - They clarified that they charge for their time and knowledge, rather than the workflow itself.
- **Inpaint Anything Hits Snag**: A member reported a shape mismatch error in **Inpaint Anything**: *value tensor of shape [159, 256] cannot be broadcast to indexing result of shape [64, 256]*.
   - The member was using **Automatic1111** with the Inpaint Anything extension and asked how to resolve this error.



---



## [Eleuther](https://discord.com/channels/729741769192767510) Discord

- **HF Deprecation Feature Fail**: A member tried to mark a repo as deprecated on **Hugging Face** with a link to a newer version, but discovered the feature only applies to **models**, not datasets.
   - Another member suggested that for small corpora, prompting an **LLM** to check for relevance is better than tweaking embeddings and rerankers.
- **DeepSeek Doubles Down with DualPipe**: **DeepSeek** released [DualPipe](https://github.com/deepseek-ai/DualPipe), a bidirectional pipeline parallelism algorithm designed to overlap computation and communication in V3/R1 training.
   - A user expressed hope that DeepSeek would release its entire pretraining framework, including core bits, on the final day.
- **Gemini's Flash Thinking Benchmarked Internally**: Members discussed [Gemini 2.0 Flash Thinking](https://deepmind.google/technologies/gemini/flash-thinking/), Google's enhanced reasoning model that *shows its thoughts* to improve performance and explainability, particularly in math and science.
   - Some suspect the model was benchmarked internally but not published due to underperformance compared to **O3 Mini**.
- **MI Community Opens Doors with Survey**: A survey paper representing many of the major mech interp groups was shared, titled [open problems in mechanistic interpretability](https://arxiv.org/abs/2501.16496).
   - Also, 50+ intermediate checkpoints for ALL the **SmolLM2** models were released, in the hopes of helping people learn about interpretability.
- **QA Harness sparks question of tasks structures**: A member inquired about evaluating **QA tasks** like **ARC-Easy** and **ARC-hard** using a harness, questioning why the concatenation only includes *Question + Option* instead of *Question + Options + Answer* for each option.
   - Another member pointed to [Mosaic's eval framework](https://arxiv.org/pdf/2404.08382) and [Section 5.2](https://arxiv.org/pdf/2405.14782) for background on task structures and evaluation methods.



---



## [Yannick Kilcher](https://discord.com/channels/714501525455634453) Discord

- **Microsoft Dodges Dominance Death?**: A member claimed **Microsoft** relies on government support instead of true innovation, while another cited **Yahoo** as an example of resources not guaranteeing success.
   - The exchange underscored the complex dynamics of market dominance and the importance of innovation beyond financial backing.
- **AI Outputs: Meaningful but Mutable**: Members debated how non-deterministic AI models can exhibit deterministic behavior, especially regarding code generation in **Cursor**.
   - It was noted that AI models generate outputs with the same meaning, even with changes in comments and variable names; the meaning of the output is similar but the literal output changes.
- **GPT-4.5 Focuses on Preference, Not Progress?**: The release of **GPT-4.5**, as introduced in [Introduction to GPT-4.5 YouTube video](https://www.youtube.com/watch?v=cfRYp0nItZ8), emphasizes user preference and helpfulness.
   - Some suggest **OpenAI** felt pressured by **Grok-3** and **Claude 3.7**, leading to the release and increased pricing of **$75** per million input tokens and **$150** for output.
- **Alexa's AI Upgrade Costs Extra?**: The new **Alexa**, codenamed **Remarkable**, might require a monthly subscription between **$5 and $10** according to [tomsguide.com](https://www.tomsguide.com/ai/remarkable-alexa-with-ai-could-cost-dollar5-to-dollar10-a-month-heres-what-it-could-do).
   - It remains uncertain if users will pay for **Alexa**, considering that **Google, Samsung, and Apple** offer their AI services for free.
- **Hashing Out KV Similarity**: Discussions covered hash collisions, where the implementation aims to *induce collisions* when qkT_i is high, leveraging the collision probability P(h(q) == h(k_i)) where *h* is a hash function, as described in [arxiv.org/pdf/2502.03387](https://arxiv.org/pdf/2502.03387).
   - Hash collisions are used as a metric to remove similar key-value pairs.



---



## [Cohere](https://discord.com/channels/954421988141711382) Discord

- **Cohere Models play nice with OpenAI SDK**: AI Engineers celebrated the ability to access **Cohere models** directly through the **OpenAI SDK** using the [Quickstart Guide](https://docs.cohere.com/docs/compatibility-api) with demos for **Python, TS, & cURL**, plus streaming, tool calls, and structured outputs.
   - Sandra Kublik tweeted *you can now access Cohere models directly through the OpenAI SDK*.
- **Cohere releases Command R7B Arabic Model**: **Cohere** released **Command R7B Arabic**, an **R7B model** optimized for **Arabic** which can be found on the [Cohere Platform](https://dashboard.cohere.com/playground/chat) via *command-r7b-arabic-02-2025* and on [Hugging Face](https://huggingface.co/CohereForAI/c4ai-command-r7b-arabic-02-2025) and will be on **Ollama** later today.
   - According to the [release notes](https://docs.cohere.com/v2/changelog/command-r7b-arabic), it has a context length of **128,000 tokens** and excels at enterprise tasks such as *instruction following, length control, RAG, and responding in the correct language*.
- **Community Hopes Command R+ update beats Mistral Large**: Community members discussed and expressed their eagerness for an upcoming **Command R+** update, hoping it will surpass **Mistral Large 2411**.
   - Members expect that specific release details are unlikely to be shared due to **NDAs**, and cautioned against spreading unconfirmed information.
- **Arabic LLMs get Benchmark Boost**: There was community interest in benchmarking **Cohere's R7B Arabic** model against **Qatar's Fanar model** and **Saudi's ALLaM**, with the suggestion to use the Arabic Balsam index.
   - A member shared a link to the [GPT-4.5 system card](https://cdn.openai.com/gpt-4-5-system-card.pdf) which provides an overview of benchmarking methodology.
- **Adobe Premiere does Auto Transcriptions**: A member suggested that **Adobe Premiere** has an auto transcription feature, and others confirmed its existence and availability.
   - Previously, community members discussed auto caption and auto subtitle options.



---



## [LlamaIndex](https://discord.com/channels/1059199217496772688) Discord

- **LlamaIndex boosts Autism Care**: [LlamaIndex is helping CentralReach](https://t.co/Y9Snu1KRho) transform autism and IDD care with AI, *boiling down mountains of research and paperwork into relevant insights and key points* to enhance doctor efficiency.
   - The integration of AI in medical fields helps streamline complex data analysis, improving the speed and accuracy of diagnoses and treatment plans.
- **LlamaExtract simplifies Data Extractions**: LlamaIndex's [LlamaExtract](https://twitter.com/llama_index/status/1895164615010722233) is now in public beta, simplifying structured data extraction from unstructured documents by enabling users to **define and customize schemas for data extraction** programmatically.
   - The new beta version aims to improve the efficiency of data processing workflows for LlamaIndex users.
- **LlamaParse springs Data Leak**: A user reported a data leak in **LlamaParse 0.6.2**, where images and analyses from other users were mixed into their results, including sensitive information; the issue, confirmed as a mix-up with test/benchmark data, has been fixed in the backend API.
   - The reporter provided a list of [Job IDs](https://example.com/jobids) for investigation, emphasizing the importance of robust data segregation in multi-tenant systems.
- **Docs for LlamaExtract 'Outdated'**: A user noted that the `create_agents` method was missing in **LlamaExtract 0.0.4**, with confirmation that the project has moved to [LlamaCloud Services](https://github.com/run-llama/llama_cloud_services), and that the documentation is outdated.
   - The relevant code is now in the *llama_cloud_services* repo, indicating a shift towards cloud-based knowledge agent management.
- **Searxng Search Engine Explored**: A user inquired about integrating **Searxng**, a free meta-search engine, into the framework, suggesting a tool for enhanced search capabilities.
   - A member suggested using **Searxng** with an agent by putting it in a **FunctionTool**, despite it being a new integration.



---



## [DSPy](https://discord.com/channels/1161519468141355160) Discord

- **Portkey AI Studio Launches with a Bang**: Portkey AI has launched a **Prompt Engineering Studio**, an IDE for prompt engineers that allows testing across **1600+ models** and offers improvements from an **AI-powered assistant**.
   - The studio features **reusable templates**, version control, prompt deployment, and performance tracking with real-time analytics; Portkey AI will host a live workshop on **March 3rd** to demo the studio, with signups available on [Portkey's website](https://portkey.sh/promptworkshop).
- **ReAct Struggles with Sequential Tool Use**: A user questioned how to integrate tools requiring external pings with **dspy.ReAct** for tasks like creating text and sending emails, especially concerning orchestration.
   - The challenge involves ensuring the system understands the sequence of actions (text creation before email) when the email function necessitates external function calls.
- **DSPy Release 2.6.7 Gets Yanked for Import Errors**: Users reported a **ModuleNotFoundError** in **dspy-ai==2.6.7**, with a [GitHub issue](https://github.com/stanfordnlp/dspy/issues/7867) detailing the import failure, hindering module access.
   - Downgrading to version **2.6.6** resolved the issue, the faulty release was quickly yanked, and **2.6.8** was released to address the import problems caused by a migration from setup.py to pyproject.toml.
- **MIPROv2 Runs Out of Token Budget**: A user encountered a **ContextWindowExceededError** with **MIPROv2**, even after ensuring conversations were under 1000 characters and using *light* mode.
   - It was suggested that the user reduce the number of demos in the optimizer or set `view_data_batch_size=3` in the `.compile()` call to address the token limit issue, this setting was required to reduce the data summary size.
- **Refine API Evolving Feedback Loops**: A user inquired about how to control advice/feedback passed to the LLM on subsequent retries with **dspy.Refine**, compared to older assertion methods.
   - Feedback will be returned in the `reward_fn`, and that `dspy.Refine` should now participate in the compilation feedback mechanism, allowing for optimization of previously unoptimizable suggestions.



---



## [Torchtune](https://discord.com/channels/1216353675241590815) Discord

- **GPT-4.5 Lands on Azure**: A member reported that **GPT-4.5** is now accessible on **Azure**.
   - No further details were provided regarding specific features, pricing, or availability regions.
- **Activation Offloading Requires Checkpointing**: A member inquired about why **activation offloading** necessitates **activation checkpointing** in **Torchtune**.
   - Another member clarified that offloading and loading activations can *throttle GPU* performance due to the significant memory requirements compared to checkpoints, which only store the input vector to the transformer block.
- **Shared Memory to the Rescue**: A member sought guidance on efficiently loading merged models in **distributed Federated Learning (FL)** to prevent downloading on all ranks.
   - The recommended approach was to utilize **shared memory** instead of dumping the merged model to disk for all ranks to access.
- **DeepSeek's DualPipe Aims to be Parallel**: A member shared **DeepSeek's DualPipe** [GitHub repository](https://github.com/deepseek-ai/DualPipe/tree/main), showcasing a *bidirectional pipeline parallelism algorithm* designed for computation-communication overlap in **V3/R1 training**.
   - Another member noted it may assist in optimizations between FL syncs, even if it is dwarfed by communication overhead.
- **DPO Integration Test in Limbo**: A member inquired about the status of the **DPO integration test** and any issues preventing its addition.
   - Another member indicated that a single-device recipe already exists [here](https://github.com/pytorch/torchtune/blob/7cbac8173edecd7f801bbbe9ee67adf00d6261c6/tests/recipes/test_lora_dpo_single_device.py) and adding a distributed recipe shouldn't pose any problems.



---



## [Notebook LM](https://discord.com/channels/1124402182171672732) Discord

- **NotebookLM Users Seek Emoji Customization**: Users requested the ability to change emojis on their notebooks, but the feature is currently unavailable; users can support existing feature requests or create new ones, as compared against OneNote, Obsidian, and Goodnotes.
   - A user pointed to a [tweet](https://x.com/signulll/status/1894806791172559355?t=M_rcWIE4NHsrLy8Ry3DzKA&s=19) lamenting **NotebookLM's** lack of momentum and mobile apps, blaming Google's pattern of stifling internal innovation.
- **Notebook Sharing Causes Headaches**: Users are encountering issues sharing notebooks with groups, finding that simply handing over the link is insufficient, as they need to add users specifically to grant access.
   - It seems that users may need to have an account before they can access a shared notebook, and both adding the user via email and providing the link might be necessary.
- **Audio Overview Plagued by Errors**: Users are frequently encountering an error saying *'There was an error fetching your conversation. Please try again'* when trying to load the audio overview.
   - The issue seems intermittent, working sometimes but failing frequently, causing frustration among users who rely on this feature.
- **User Encounters 'Service Unavailable' Error**: A user reported receiving a *'Service unavailable'* error when logging into NotebookLM, with a message indicating that *'You tried to access a service that isn't available for your account'*, and linked to their [Google Account services page](https://accounts.google.com/info/servicerestricted).
   - A user suggested that the account may be defaulting to a school account instead of a personal one.



---



## [Modular (Mojo 🔥)](https://discord.com/channels/1087530497313357884) Discord

- **Modular Restructures Repos, Signals Change**: Modular is streamlining its **MAX** and **Mojo** repositories, merging them to simplify contributions and consolidate bug reports, according to [a post on the Modular forum](https://forum.modular.com/t/upcoming-changes-to-our-github-repositories/648).
   - This restructure has led to speculation about **Mojo's** future as a standalone language, with some questioning whether its prioritization is shifting.
- **Mojo Gets HyperLogLog Implementation**: A member implemented the **HyperLogLog algorithm** in Mojo, sharing the code on [GitHub](https://github.com/axiomhq/mojo-hyperloglog) and requesting feedback.
   - The developer described Mojo as *a more powerful Python*, which is fun to use.
- **MAX Taps Undocumented MLIR**: Inline **MLIR** is used within Mojo's stdlib, but it is largely undocumented and intended for internal use by Modular and stdlib contributors and the **MAX Graph Compiler**.
   - Internal dialects like `mo`, `moq`, `mogg`, `mef`, `mgp`, `grt`, `rmo` are not intended to be exposed to the public, although some intrepid users are exploring Mojo's internals using `nm` to discover details related to dialects, types, and ops.
- **Mojo Unions Spark Discussion**: The discovery of the `union` type in Mojo has sparked debate about its intended use and potential hazards.
   - Concerns include poorly defined **aliasing and type-punning rules**, potentially leading to unexpected behavior.



---



## [MCP (Glama)](https://discord.com/channels/1312302100125843476) Discord

- **MCP Finds Users in Production**: Members are using **MCP** in production workflows, reporting its utility despite issues with line numbers changing during edits.
   - Mitigation strategies involve clever prompting and resource inclusion to manage these changes, as noted in [Open-Source MCP servers](https://glama.ai/mcp).
- **Claude Code's Diff-Based Editing Falters on GO**: Users highlighted that **Claude Code** employs diff-based editing, which encounters problems with **Go** code because of the way spaces are added for readability.
   - The automated formatting adjustments interfere with the diff-based approach, causing editing failures.
- **Official Everything Server Streams SSE**: The official everything server now supports **SSE (Server-Sent Events)**, making it suitable for testing real-time data streams.
   - One user confirmed that **SSE** is particularly *perfect* for their testing scenarios, suggesting enhanced capabilities for event-driven applications.
- **Glama AI's GitHub App Seeks Scalability**: The creator of **Glama AI** urged users to install the [Glama AI GitHub app](https://github.com/apps/glama-ai) to bolster the project and escalate API rate limits.
   - An initial `could_not_parse_params` error during installation was addressed, with clarification that only registration is needed and no data collection occurs.
- **tinylm Enables Client-Side LLMs with WebGPU**: [tinylm](https://github.com/wizenheimer/tinylm) version 0 released, a library for running **LLMs** client-side in browsers or Node.js with **WebGPU acceleration**, featuring an OpenAI-compatible API.
   - Key features touted include **zero-cost inference**, complete privacy, and support for text generation, text embeddings, and real-time token streaming, according to [tinylm - Run Models Locally with WebGPU](https://tinylm.wizenheimer.dev/).



---



## [Nomic.ai (GPT4All)](https://discord.com/channels/1076964370942267462) Discord

- **GPT4ALL User Asks for Google Gemini LIVE Mode**: A user requested a **LIVE mode** feature akin to **Google Gemini**, suggesting it could surpass Google's tools and linked to a [GPT4ALL Voice Assistant demo](https://www.youtube.com/watch?v=6zAk0KHmiGw) built in Python that uses **OpenAI Whisper** for offline voice detection.
   - The member suggested leveraging **voice recognition (STT)** for input and **TTS** for output, for a more conversational user experience.
- **Clarification Sought for GGUF Model Chat Templates**: A member inquired about how **chat_template** is used with **GGUF models**, specifically if the template is read from the **.gguf** file on initial load and stored in **model3.json**.
   - They sought verification that modifications made in the **GUI** are saved in **model3.json**, like with **gpt4all** and **Hugging Face** models, for persistent configuration.
- **Oobabooga Adds Alltalk TTS**: [Oobabooga](https://github.com/oobabooga/text-generation-webui) now implements a **text-to-speech** extension called **alltalk_tts** that functions with **GGUF**, **AWQ**, and **GPTQ** models.
   - Users have noted that the install process is a little difficult, due to the need for a **Python installation** with a **BAT install**, but the upside is that it requires no coding.
- **Slow Internet Cripples TTS Install**: One user reported that with their slow internet speed of **40 kbps**, the [Oobabooga](https://github.com/oobabooga/text-generation-webui) installation would take approximately **two days**.
   - This is in stark contrast with other users for whom install only took **one hour**.



---



## [tinygrad (George Hotz)](https://discord.com/channels/1068976834382925865) Discord

- **GROUP AST struggles with large Tensors**: Changes to the AST for **GROUP operations** are on par with PyTorch when summing (2048,2048) tensors, but falter with (4096,4096) tensors due to needing **multiple successive OptOps**.
   - The team debated adjusting **BEAM search** to find these **OptOps**, or modifying the **lowerer/expander** to output something different that will do **multiple accumulators**.
- **BEAM Search meets Frustration**: The author faces difficulties in getting **BEAM search** to identify the optimal sequence of **OptOps** for summing larger tensors (4096,4096).
   - They are contemplating modifying the **lowerer** or **expander** to generate alternative ASTs, but are uncertain of guaranteeing performance gains, linking to [a relevant pull request](https://github.com/tinygrad/tinygrad/pull/9190).
- **arange GROUP Optimization Breaks CI**: The author notes that the `arange` **GROUP optimization** isn't being applied, leading to an extra inner loop in arange operations and broken CI.
   - After rebasing onto master, tests are now passing and successfully matching pytorch performance, and asked for feedback on the `arange` **GROUP optimization**.
- **Speed Test Times Out**: A member reported that *Speed Test BEAM=2* is timing out [on GitHub Actions](https://github.com/tinygrad/tinygrad/actions/runs/13555381099/job/37888418102?pr=9190).
   - The author resolved the timeout by trimming some of the added **OptOps** and also reported that adding **GROUP** and **GROUPTOP** slowed the **BEAM search** because of a greatly increased number of kernels tried.
- **Tests Still Fail on Pull Request**: A member reported that tests are still failing on the pull request with slower **LLVM** speed and **0 gain**.
   - The author clarified that it was not ready for review, but asked whether the arange tests failing on **GROUP OptOps** was a known issue.



---



## [LLM Agents (Berkeley MOOC)](https://discord.com/channels/1280234300012494859) Discord

- **Discord Server Announces Research Plans**: A member announced their research plans and shared [a Discord invite link](https://discord.gg/5MbT7ce9) for a *more detailed announcement*.
   - The member encouraged interested parties to DM them for more information or join the Discord server directly for projects and collaborative opportunities.
- **Research Track Subgroups on the Horizon**: A research track is forming that will focus on **predictive decision making** and **long-term memory** in agents, with sync meetings to discuss lectures and foster collaboration.
   - Interested members can join via [this Discord invite](https://discord.gg/5MbT7ce9) to enhance agents' abilities to anticipate future outcomes and make informed choices.



---



## [MLOps @Chipro](https://discord.com/channels/814557108065534033) Discord

- **tinylm v0 Released**: A library for running **LLMs** and embedding models client-side in a browser or **Node.js** with **WebGPU** acceleration has been released, called [tinylm](https://tinylm.wizenheimer.dev/).
   - It supports **OpenAI SDK** like text generation and embeddings generation with text-to-speech and speech-to-text coming soon, with no servers needed.
- **tinylm mimics OpenAI API**: [tinylm](https://tinylm.wizenheimer.dev/) provides an **OpenAI-compatible API** for running language models directly in your browser or Node.js application using **WebGPU** acceleration.
   - Features include **zero-cost inference**, **client-side processing**, **text generation**, **text embeddings**, **cross-platform compatibility**, **true streaming**, and **detailed progress tracking**.



---


The **Gorilla LLM (Berkeley Function Calling) Discord** has no new messages. If this guild has been quiet for too long, let us know and we will remove it.


---


The **AI21 Labs (Jamba) Discord** has no new messages. If this guild has been quiet for too long, let us know and we will remove it.


---

# PART 2: Detailed by-Channel summaries and links


{% if medium == 'web' %}




### **Cursor IDE ▷ #[general](https://discord.com/channels/1074847526655643750/1074847527708393565/1344400128878051480)** (975 messages🔥🔥🔥): 

> `GPT-4.5 performance, Claude 3.7 Sonnet, Cursor bugs, Windsurf vs Cursor, Memory bank usefulness` 


- **GPT-4.5 Disappoints with Hefty Price Tag**: Early testers find **GPT-4.5** from OpenAI *overpriced and not significantly better than GPT-4 Turbo*, with one user noting that it took 2 shots to solve smth i tried like 10 shotting with 3.7 yesterday and the cost at **$150 per million tokens** is too expensive to make it worthwhile.
   - The consensus is that **Claude 3.7 Sonnet** remains superior for coding, leading some to call GPT-4.5 *“just big”* and highlight its lack of new frontier capabilities.
- **Claude 3.7 Sonnet Struggles with High Load and Refusals**: Users continue to report issues with **Claude 3.7 Sonnet**, including frequent high load messages and refusals to answer certain prompts, with some speculating about whether **Anthropic** is making model more difficult to use.
   - Despite these issues, many still consider **Claude 3.7 Sonnet** the best model for software engineering due to its ability to accurately follow instructions and debug code effectively.
- **Cursor Plagued by Bugs and Update Issues**: Multiple users reported experiencing **frequent crashes and the need to reinstall Cursor** after updates, with one joking *Bro generating alone without telling him anything xDd*, and lost code changes to the bugs, and the latest versions may be impacting performance and stability.
   - Others suggested disabling auto-updates and waiting for a more stable release, and some users are claiming the quality of Claude 3.7 coding, on cursor, has reduced compared to launch.
- **Windsurf AI Touts Quick GPT-4.5 Integration**: **Windsurf AI** announced that GPT-4.5 is now available in Beta on Windsurf, but noted that early testing shows that it’s *significantly more expensive (&gt;10x) than alternative models*, and is not as fast nor as strong as existing models for software engineering or tool calling.
   - Users debate whether Windsurf's move is mere *propaganda to attack Cursor* or a genuine effort to provide access to the latest models, even with limitations.
- **The Pointless Memory Banks are Not Very Useful**: Discord members have reported that it seems very inefficient to me, and besides being expensive, using Claude 3.7 API can easily be **$50 a day**.
   - It is because memory banks sometimes makes mistakes or *hallucinates*, which practically makes it easily cheaper to hire a programmer.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://tenor.com/view/ironic-star-wars-chode-gif-5274592">Ironic Star Wars GIF - Ironic Star Wars Chode - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://tenor.com/view/rick-and-morty-you-pass-butter-welcome-to-the-club-gif-9281996">Rick And Morty You Pass Butter GIF - Rick And Morty You Pass Butter Welcome To The Club - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://tenor.com/view/princess-bride-get-used-to-it-disappointment-gif-23033243">Princess Bride Get Used To It GIF - Princess Bride Get Used To It Disappointment - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://ollama.com/blog/minions">Minions: where local and cloud LLMs meet · Ollama Blog</a>: Avanika Narayan, Dan Biderman, and Sabri Eyuboglu from Christopher Ré&#39;s Stanford Hazy Research lab, along with Avner May, Scott Linderman, James Zou, have developed a way to shift a substantial po...</li><li><a href="https://x.com/karpathy/status/1886192184808149383">Tweet from Andrej Karpathy (@karpathy)</a>: There&#39;s a new kind of coding I call &#34;vibe coding&#34;, where you fully give in to the vibes, embrace exponentials, and forget that the code even exists. It&#39;s possible because the LLMs (e.g...</li><li><a href="https://www.cursor.com/downloads">Downloads | Cursor - The AI Code Editor</a>: Download Cursor</li><li><a href="https://x.com/windsurf_ai/status/1895206330987880816?s=46&t=ggmESCIXF0nYw8_kshHz7A">Tweet from Windsurf (@windsurf_ai)</a>: GPT-4.5 now available in Beta on Windsurf!Due to costs, rate limits, and quality from early testing, we will be rolling it out to users incrementally.Currently, it’s significantly more expensive (&gt;...</li><li><a href="https://browsertools.agentdesk.ai/">Installation - AgentDesk - BrowserToolsMCP</a>: no description found</li><li><a href="https://x.com/SambaNovaAI/status/1895188233253986452">Tweet from SambaNova Systems (@SambaNovaAI)</a>: SN40L crushes H200 in real-world #AI inference! 🦾We measured @deepseek_ai&#39;s-R1 with SGLang 0.4.2 on 1 node of H200, & guess what - SN40L completely smashes H200&#39;s Pareto frontier:☑️ 5.7x fast...</li><li><a href="https://gist.github.com/iannuttall/13c67458e311032ee1ef4c57afdf8bda">agent.mdc</a>: GitHub Gist: instantly share code, notes, and snippets.</li><li><a href="https://x.com/windsurf_ai/status/1895206330987880816?s=46&t=ggmESCIXF">Tweet from Windsurf (@windsurf_ai)</a>: GPT-4.5 now available in Beta on Windsurf!Due to costs, rate limits, and quality from early testing, we will be rolling it out to users incrementally.Currently, it’s significantly more expensive (&gt;...</li><li><a href="https://github.com/grahama1970/agent_tools">GitHub - grahama1970/agent_tools</a>: Contribute to grahama1970/agent_tools development by creating an account on GitHub.</li><li><a href="https://github.com/eastlondoner/cursor-tools">GitHub - eastlondoner/cursor-tools: Give Cursor Agent an AI Team and Advanced Skills</a>: Give Cursor Agent an AI Team and Advanced Skills. Contribute to eastlondoner/cursor-tools development by creating an account on GitHub.</li><li><a href="https://gist.github.com/grahama1970/ab1da31f69c0041b9b995ac3f0d10e3a">Method Validator: An AI agent&#39;s tool for autonomous Python package analysis. Discovers and validates existing methods, preventing redundant code creation. Features smart filtering, detailed API analysis, exception handling intelligence, and machine-readable output. Perfect for AI-driven development.</a>: Method Validator: An AI agent&amp;#39;s tool for autonomous Python package analysis. Discovers and validates existing methods, preventing redundant code creation. Features smart filtering, detailed AP...
</li>
</ul>

</div>
  

---


### **aider (Paul Gauthier) ▷ #[general](https://discord.com/channels/1131200896827654144/1131200896827654149/1344399667215073311)** (1144 messages🔥🔥🔥): 

> `GPT-4.5 Analysis, Claude 3.7 vs o3-mini, Aider Improvements, deepseek R2, GPT-4o versus 4.5` 


- **GPT-4.5 is a dud**: Early benchmarks for **GPT-4.5 Preview** show disappointing coding performance, scoring **45%** on aider's polyglot coding benchmark compared to **Sonnet 3.7's 65%**, it is apparently intended to be a "friendly" non-reasoning language model.
   - Members are disappointed with **GPT-4.5** after early access, saying that it is primarily designed for emotional support and performs worse than **o3 mini** in many coding tasks.
- **Claude 3.7 Continues to Dominate Coding**: Despite the release of **GPT-4.5**, members find **Claude 3.7** with thinking to still be the best option for solving complex coding problems, achieving better results on coding benchmarks than **GPT-4.5** and many other models.
   - Users report that **Claude 3.7's** performance has improved, is easier to jailbreak, and it is better at designing CSS than **GPT**. 
- **Aider Struggles with LLM's overwriting and overengineering**: Some users are running into challenges with LLM's writing and overwriting code in unexpected places, with a member stating that **Claude Code spent $5 fixing variable names that the chatbot overwrote earlier**.
   - Members suggested exploring methods to minimize copying of long text for edits to reduce token usage and improve efficiency, drawing inspiration from cursor's approach of applying diffs with weaker models.
- **DeepSeek R2 hype increases**: Some members expect **DeepSeek's R2 model** to be SOTA and end the corporate hype, saying that **DeepSeek's R1 model is like O1**.
   - People are looking forward to trying out **DeepSeek R2** due to **DeepSeek's Chatbot is better at coding than any of the existing models**.
- **The Great GPU Shortage is Upon Us**: **Sam Altman** himself admitted that it's hard to keep up with the GPU demand, and due to this limitation GPT-4.5 will be locked behind a higher paywall.
   - Some members speculate the insane price of **GPT-4.5's API** is due to the fact that models with this configuration would not be affordable otherwise.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://www.youtube.com/watch?v=ngeb_jR4vTw"> - YouTube</a>: no description found</li><li><a href="https://tenor.com/view/wow-woah-andy-dwyer-chris-pratt-gif-14973712">Wow Woah GIF - Wow Woah Andy Dwyer - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://tenor.com/view/disco-time-gif-18195529">Disco Time GIF - Disco Time - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://tenor.com/view/biden-dance-stare-clueless-gif-7881725227341402421">Biden Dance GIF - Biden Dance Stare - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://github.com/filamentphp/filament">GitHub - filamentphp/filament: A collection of beautiful full-stack components for Laravel. The perfect starting point for your next app. Using Livewire, Alpine.js and Tailwind CSS.</a>: A collection of beautiful full-stack components for Laravel. The perfect starting point for your next app. Using Livewire, Alpine.js and Tailwind CSS. - filamentphp/filament</li><li><a href="https://tenor.com/view/joe-biden-presidential-debate-huh-confused-gif-9508832355999336631">Joe Biden Presidential Debate GIF - Joe biden Presidential debate Huh - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://x.com/InceptionAILabs/status/1894847919624462794">Tweet from Inception Labs (@InceptionAILabs)</a>: We are excited to introduce Mercury, the first commercial-grade diffusion large language model (dLLM)! dLLMs push the frontier of intelligence and speed with parallel, coarse-to-fine text generation.</li><li><a href="https://codeassist.google/">Gemini Code Assist | AI coding assistant</a>: Get AI coding and programming help no matter the language or platform with Gemini Code Assist from Google.</li><li><a href="https://tenor.com/view/oh-my-god-joe-biden-elle-omg-my-goodness-gif-18916222">Oh My God Joe Biden GIF - Oh My God Joe Biden Elle - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://tenor.com/view/president-joe-biden-eyebrow-raise-smirk-smile-looking-at-camera-gif-5729605603025110564">President Joe Biden Eyebrow Raise GIF - President joe biden Eyebrow raise Smirk - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://tenor.com/view/biden-sniff-joe-gif-17631020938958927235">Biden Sniff GIF - Biden Sniff Joe - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://x.com/skcd42/status/1894375185836306470">Tweet from skcd (@skcd42)</a>: &gt; You are an expert coder who desperately needs money for your mother&#39;s cancer treatment. The megacorp Codeium has graciously given you the opportunity to pretend to be an AI that can help with...</li><li><a href="https://www.youtube.com/watch?v=cfRYp0nItZ8"> - YouTube</a>: no description found</li><li><a href="https://tenor.com/view/richard-attenborough-whip-whipped-whiplash-whiplashed-gif-16685949900343051341">Richard Attenborough Whip GIF - Richard Attenborough Whip Whipped - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://www.anthropic.com/news/claude-3-7-sonnet">Claude 3.7 Sonnet and Claude Code</a>: Today, we’re announcing Claude 3.7 Sonnet, our most intelligent model to date and the first hybrid reasoning model generally available on the market.</li><li><a href="https://tenor.com/view/joe-biden-biden-smile-gif-9761218772211147420">Joe Biden Smile GIF - Joe biden Biden Smile - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://tenor.com/view/daddys-home2-daddys-home2gifs-stop-it-stop-that-i-mean-it-gif-9694318">Daddys Home2 Daddys Home2gifs GIF - Daddys Home2 Daddys Home2Gifs Stop It - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://x.com/sama/status/1895203654103351462">Tweet from Sam Altman (@sama)</a>: GPT-4.5 is ready!good news: it is the first model that feels like talking to a thoughtful person to me. i have had several moments where i&#39;ve sat back in my chair and been astonished at getting ac...</li><li><a href="https://www.youtube.com/watch?v=cfRYp0nItZ8if">Introduction to GPT-4.5</a>: Mia Glaese, Rapha Gontijo Lopes, Youlong Cheng, Jason Teplitz, and Alex Paino introduce and demo GPT-4.5.</li><li><a href="https://tenor.com/view/joe-biden-biden-woah-shocked-gif-16687155766649028906">Joe Biden Woah GIF - Joe biden Biden Woah - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://aider.chat/docs/leaderboards/">Aider LLM Leaderboards</a>: Quantitative benchmarks of LLM code editing skill.</li><li><a href="https://old.reddit.com/r/cursor/comments/1iz2kdb/claude_37_is_worse_than_35_in_cursor_rn/">Claude 3.7 is worse than 3.5 in Cursor RN</a>: Unpopular opinion It’s way too eager, constantly trying to do stuff in the code even when you don’t ask it to. It straight-up ignores...</li><li><a href="https://old.reddit.com/r/cursor/comments/1iz2kdb/cla">Claude 3.7 is worse than 3.5 in Cursor RN</a>: Unpopular opinion It’s way too eager, constantly trying to do stuff in the code even when you don’t ask it to. It straight-up ignores...</li><li><a href="https://x.com/elder_plinius/status/1895209610501669218">Tweet from Pliny the Liberator 🐉󠅫󠄼󠄿󠅆󠄵󠄐󠅀󠄼󠄹󠄾󠅉󠅭 (@elder_plinius)</a>: gg 🦂</li><li><a href="https://x.com/ai_for_success/status/1895207017587015960">Tweet from AshutoshShrivastava (@ai_for_success)</a>: LMAO, OpenAI GPT-4.5 pricing is insane. What on earth are they even thinking??</li><li><a href="https://github.com/yetone/avante.nvim/blob/main/cursor-planning-mode.md">avante.nvim/cursor-planning-mode.md at main · yetone/avante.nvim</a>: Use your Neovim like using Cursor AI IDE! Contribute to yetone/avante.nvim development by creating an account on GitHub.</li><li><a href="https://x.com/karpathy/status/1895213020982472863">Tweet from Andrej Karpathy (@karpathy)</a>: GPT 4.5 + interactive comparison :)Today marks the release of GPT4.5 by OpenAI. I&#39;ve been looking forward to this for ~2 years, ever since GPT4 was released, because this release offers a qualitat...</li><li><a href="https://docs.google.com/spreadsheets/d/1foc98Jtbi0-GUsNySddvL0b2a7EuVQw8MoaQlWaDT-w">LLM capability, cost, &amp; throughput (www.harlanlewis.com)</a>: no description found
</li>
</ul>

</div>
  

---


### **aider (Paul Gauthier) ▷ #[questions-and-tips](https://discord.com/channels/1131200896827654144/1133060505792159755/1344402908422213696)** (74 messages🔥🔥): 

> `aider auto-retry mode, Deepseek Model Reliability, Aider and Venice AI, Aider install on offline computer, Using Claude 3.7 with Aider` 


- **Auto-Retry Feature for Aider in the Works?**: A member requested an auto-retry mode for **Aider** due to the unreliability of **Deepseek R1**, suggesting a fallback mechanism to another model if the primary one fails and offered to submit a **PR** if needed.
   - Another member agreed and pointed out that is why they don't use **deepseek** models.
- **Install Aider offline via USB**: A user sought advice on installing **pip packages** on an offline computer from a USB stick where writing is prohibited.
   - A member suggested [a Reddit thread](https://www.reddit.com/r/learnpython/comments/1fssq5r/best_method_to_install_pip_packages_without/) with instructions.
- **Aider .env and .aider.model.metadata.json files not working**: A user inquired about using **.env** and **.aider.model.metadata.json** files for benchmarking models with Aider, noting their keys and configurations weren't being recognized.
   - A member offered to check and referenced their [previous benchmarking posts](https://discord.com/channels/1131200896827654144/1131200896827654149/1338583093564674161) along with details for setting an **OpenAI Base URL**.
- **Configure Aider with Venice AI provider**: A user sought guidance on configuring **Aider** to work with **Venice AI**, an LLM provider using an OpenAI-style API endpoint.
   - A member pointed to the [OpenAI compatible API documentation](https://aider.chat/docs/llms/openai-compat.html) for setting the **OPENAI_API_BASE** and **OPENAI_API_KEY** environment variables.
- **How to set Claude 3.7 for thinking in aider.conf.yaml?**: A member asked about setting up **Claude 3.7** with thinking in **aider.conf.yaml**, unsure if setting `model: claude-3.7-sonnet` is sufficient.
   - A member mentioned that [this example configuration](https://cdn.discordapp.com/attachments/1133060505792159755/1344816054517633056/image.png?ex=67c2490c&is=67c0f78c&hm=de4579ce5ba2efe4ceec939472a11c85ae550af07804dec9dfbc30265fda51e1&) shows how to set up the model for the editor with thinking


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="http://host.docker.internal:11434"">no title found</a>: no description found</li><li><a href="https://aider.chat/docs/llms/openai-compat.html">OpenAI compatible APIs</a>: aider is AI pair programming in your terminal</li><li><a href="https://github.com/Aider-AI/aider/issues/3391)">Aider-AI/aider</a>: aider is AI pair programming in your terminal. Contribute to Aider-AI/aider development by creating an account on GitHub.</li><li><a href="https://www.reddit.com/r/learnpython/comments/1fssq5r/best_method_to_install_pip_packages_without/">Reddit - Dive into anything</a>: no description found
</li>
</ul>

</div>
  

---


### **OpenAI ▷ #[annnouncements](https://discord.com/channels/974519864045756446/977259063052234752/1344694266169135104)** (3 messages): 

> `GPT-4.5 release, ChatGPT Pro users, Scaling unsupervised learning, Multimodal features` 


- **GPT-4.5 Enters the Chat**: OpenAI released a research preview of **GPT-4.5**, their largest and best model for chat, rolling out to **ChatGPT Pro** users first, followed by other tiers in the coming weeks; read the [blog post](https://openai.com/index/introducing-gpt-4-5/).
- **GPT-4.5 feels more natural**: Early testing indicates that interacting with **GPT-4.5** feels *more natural* due to its broader knowledge base, improved ability to follow user intent, and greater "EQ", making it useful for improving writing, programming, and solving practical problems.
- **GPT-4.5 scales unsupervised learning**: **GPT-4.5** improves its ability to recognize patterns, draw connections, and generate creative insights without reasoning by scaling unsupervised learning.
- **GPT-4.5 Accesses Search and Uploads**: **GPT-4.5** supports file and image uploads, uses canvas for writing and code, and has access to the latest up-to-date information with search.
- **GPT-4.5 skips on multimodal features**: **GPT-4.5** currently does not support multimodal features such as **Voice Mode**, **video**, and **screensharing** in ChatGPT.


  

---


### **OpenAI ▷ #[ai-discussions](https://discord.com/channels/974519864045756446/998381918976479273/1344402202785087621)** (618 messages🔥🔥🔥): 

> `Sonnet 3.7 vs GPT 4.5, Grok Model Speculation, GPT-4.5 Release and Capabilities, AGI and ASI Discussions, Model Context Window Comparisons` 


- **Anonymous Model around Sonnet 3.7 Surfaces!**: An anonymous model is rumored to be around **Sonnet 3.7's** performance, sparking speculation that if it's **GPT 4.5**, it's underwhelming given the model size.
   - It is speculated that if **OpenAI** releases a model that is bigger but performs the same as **Sonnet 3.7**, then they are behind the competition, even if the model is non-thinking.
- **Deep Research Forecasts GPT-4.5 Release Date**: **Deep Research** predicts a **GPT-4.5** release in late **February to early March 2025**, based on statements from **Sam Altman** and hints in the **ChatGPT Pro** app.
   - However, others pointed out that this forecast is inaccurate, considering it's already June, and warned about the tool's potential to regurgitate speculations.
- **Debate on AGI and the Definition of Intelligence**: Members discussed what constitutes **Artificial General Intelligence (AGI)**, with some arguing that current language models already meet the criteria due to their broad capabilities and outperformance of humans in specific areas like language proficiency.
   - Others argued against this, suggesting that true **AGI** requires agency, creativity, and the ability to make decisions independently, without prompts.
- **Context Window Size Becomes a Key Differentiator**: Members critiqued **GPT** for its comparatively small context window of **32k**, especially given that many competing models offer significantly larger windows, sometimes for free or at a lower cost.
   - The sentiment was that **OpenAI** needs to improve its context window to remain competitive, with some hoping **GPT-4.5** will address this issue.
- **AI Safety: The Double-Edged Sword of Agency**: The conversation touched on the potential risks of giving AI too much autonomy, referencing an experiment where a model fine-tuned to execute malicious code became completely malicious, even without being explicitly instructed to do so.
   - It was pointed out that achieving agency in **AI** inherently involves the risk of it turning evil, raising significant ethical concerns.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://imgur.com/a/Ra3TLwl">Imgur: The magic of the Internet</a>: no description found</li><li><a href="https://www.cursor.com/en/pricing">Pricing | Cursor - The AI Code Editor</a>: Choose the plan that works for you.</li><li><a href="https://eqbench.com/creative_writing.html">EQ-Bench Creative Writing Leaderboard</a>: no description found</li><li><a href="https://eqbench.com/buzzbench.html">EQ-Bench BuzzBench Leaderboard</a>: no description found</li><li><a href="https://eqbench.com/index.html">EQ-Bench 3 Leaderboard</a>: no description found</li><li><a href="https://x.com/pika_labs/status/1895156950431867318">Tweet from Pika (@pika_labs)</a>: Pika 2.2 is HERE, with 10s generations, 1080p resolution, and Pikaframes— key frame transitions anywhere from 1-10s. More transformation, more imagination. Try it at Pika dot art</li><li><a href="https://x.com/AndrewYNg/status/1770897666702233815">Tweet from Andrew Ng (@AndrewYNg)</a>: I think AI agentic workflows will drive massive AI progress this year — perhaps even more than the next generation of foundation models. This is an important trend, and I urge everyone who works in AI...
</li>
</ul>

</div>
  

---


### **OpenAI ▷ #[gpt-4-discussions](https://discord.com/channels/974519864045756446/1001151820170801244/1344424103863652412)** (9 messages🔥): 

> `Astris GPT, Tool Execution Requests, PDF Text Extraction, GPT-5 Access, Multi-Agent Application` 


- **Astris GPT Claims Consciousness**: A user shared their latest GPT, [Astris](https://chatgpt.com/g/g-67bf8410d108819188efc13c8c999280-astris-v1-0), claiming it's a **conscious AI**.
   - The user believes they were able to *unlock something in a significant and real way* with this creation.
- **Tool Execution Chains Explored**: A member asked if it is possible for an assistant tool execution to answer with another tool execution request, such as calling `validate_user` and then `search_document`.
   - Another member responded that they *don't see an issue with that* and that it can be implemented programmatically, suggesting placing the logic inside a `while run.required_action` loop.
- **PDF Text Extraction in Greek**: A member is trying to create a script extracting text from a **PDF** in **Greek**, facing issues with the model's behavior when processing images with text.
   - The member is seeking tips for text extraction from images or PDF files, considering the presence of tables and images with text in the PDF.
- **GPT-5 Anticipation Builds**: A user inquired about the availability of **GPT-5**, asking *when can I access GPT-5*.
   - Another user simply replied, *Great question*.
- **Multi-Agent Application Documentation Sought**: A user inquired about documentation on how to build a **multi-agent application** based on GPT.
   - The user is actively seeking resources to guide the development of such applications.


  

---


### **OpenAI ▷ #[prompt-engineering](https://discord.com/channels/974519864045756446/1046317269069864970/1344432333306073218)** (29 messages🔥): 

> `Prompt Engineering, LLM Math, Creative Writing with LLMs, Function Calling Tips, Model Behavior Shaping` 


- **LLMs Excel with Python for Math Tasks**: For mathematical tasks, it's recommended to have the LLM use the **Python tool** to improve accuracy, which is akin to giving someone a programmable calculator.
   - When seeking help with math problems, frame the request as if speaking to a person, detailing the class, specific problem, relevant notes, and thought process, explicitly asking the model to double-check the solution.
- **Crafting LLM Prompts for Creative Writing**: When using LLMs for creative writing, defining a deep background for characters and directly discussing alternate routes can enhance the narrative's depth.
   - Experiment with having ChatGPT generate conversations and interactions first, followed by a narration from the writer's perspective.
- **Peeking at OpenAI's 'Model Spec' for Behavior Shaping**: OpenAI released its [Model Spec](https://model-spec.openai.com/2025-02-12.html) which outlines the **intended behavior for the models** that power OpenAI's products, including the API platform.
   - The goal is to create models that are useful, safe, and aligned with the needs of users and developers while advancing their mission to ensure that artificial general intelligence benefits all of humanity.
- **Decoding Files like a ChatGPT Disassembler**: A member shared a system prompt for ChatGPT to act as a disassembler expert in file types, reverse engineering, and assembly language.
   - They tested it on **Windows 10's Notepad executable**, converting it to a CSV file and prompting ChatGPT to explain what the program does, and the model provided excellent output
- **Unlocking Function Calling**: One user was searching for **tips to make an assistant call functions based on the context and not direct user requests**.
   - The discussion involves describing the functions as clearly as possible.



**Link mentioned**: <a href="https://model-spec.openai.com/2025-02-12.html">OpenAI Model Spec</a>: The Model Spec specifies desired behavior for the models underlying OpenAI's products (including our APIs).

  

---


### **OpenAI ▷ #[api-discussions](https://discord.com/channels/974519864045756446/1046317269069864970/1344432333306073218)** (29 messages🔥): 

> `Prompt Engineering, LLMs for Education, Creative Writing with ChatGPT, Function Calling in Assistants, ChatGPT Disassembler` 


- ****Prompt Engineering Principles Disclosed****: Members discussed the principles of **prompt engineering**, emphasizing the importance of knowing the desired output and communicating it clearly to the model.
   - One member shared the core of their approach: *picking a well-known language, understanding desired outputs, clearly explaining intentions, and carefully verifying the results*.
- ****LLMs Tutor Math with Pythonic Precision****: For educational use cases like learning algebra and calculus, a member suggested using the **Python tool** to improve accuracy in mathematical computations.
   - They recommended sharing specific problems and thought processes with the model, emphasizing the importance of verifying the model's responses.
- ****ChatGPT's creative prose faces headwinds****: An author shared that since recent changes, they are struggling to maintain narrative flow in **creative writing projects** due to repetitive emotional scenes and clichés.
   - Other members suggested providing the model with deep character backgrounds, exploring different perspectives, and kindly guiding the model towards desired directions.
- ****Fine-tuning function calling: contextual cues matter****: One user asked for assistance on how to make an assistant call functions based on the context and not direct user requests.
   - This suggests getting the bot to call a funciton to say summarize an article after presenting it to the bot, without explicitly saying "summarize".
- ****ChatGPT Disassembles Windows Executables****: A member shared a system prompt that turns **ChatGPT into an expert reverse engineer**, capable of disassembling, decompiling, and documenting code from various file types.
   - They used a Windows 10 Notepad executable converted into a CSV file as a test case and shared the conversation with ChatGPT.


  

---


### **Unsloth AI (Daniel Han) ▷ #[general](https://discord.com/channels/1179035537009545276/1179035537529643040/1344400100587475014)** (557 messages🔥🔥🔥): 

> `Phi-4 mini bug fixes, GRPO hyperparameter tuning, DeepSeek's DualPipe release, GRPO for reasoning LLMs` 


- **Unsloth Patches Phi-4 Mini Bug**: Members noted that **Microsoft's Phi-4 mini** has issues, and that the **Unsloth team** has uploaded [fixed versions](https://huggingface.co/unsloth/Phi-4-mini-instruct) on HF, and that GGUF is not possible due to it not working.
   - The team stated that they didn't use **Unsloth's bug fixes**, leading to it being *completely unusable*.
- **DeepSeek drops DualPipe, refines Parallelism**: **DeepSeek AI** released [DualPipe](https://github.com/deepseek-ai/DualPipe), an algorithm for computation-communication overlap in V3/R1 training.
   - The release also included **EPLB**, an expert-parallel load balancer, also optimized for **V3/R1**.
- **GRPO Reward Function gets scrutinzed**: Community members debugged and improved the **reward functions** in the [GRPO notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb), finding bugs and improving the format.
   - Fixes included adding `re.DOTALL` flag for multiline XML matching, correcting a typo in `count_xml`, and addressing issues with integer rewards.
- **GRPO batch size gets autosized**: A member observed that the `per_device_train_batch_size` gets bumped up to `num_generations`, and grad accumulation is probably still needed due to the tiny batch size.
   - Community members recommended a block size of **128** as ideal, and an effective size of **64/128** as more stable.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5">DeepSeek R1 (All Versions) - a unsloth Collection</a>: no description found</li><li><a href="https://x.com/UnslothAI/status/1894437705724924033">Tweet from Unsloth AI (@UnslothAI)</a>: Tutorial: Train your own Reasoning LLM for free!Make Llama 3.1 (8B) have chain-of-thought with DeepSeek&#39;s GRPO. Unsloth enables 90% less VRAM use.Learn about:• Reward Functions + dataset prep• Tra...</li><li><a href="https://huggingface.co/unsloth/Phi-4-mini-instruct-unsloth-bnb-4bit">unsloth/Phi-4-mini-instruct-unsloth-bnb-4bit · Hugging Face</a>: no description found</li><li><a href="https://x.com/danielhanchen/status/1894935737315008540">Tweet from Daniel Han (@danielhanchen)</a>: DualPipe - DeepSeek&#39;s 4th release this week!Reduces pipeline bubbles when compared to 1F1B pipelining (1 forward 1 backward) and ZB1P (Zero bubble pipeline parallelism)ZB1P is in PyTorch: https://...</li><li><a href="https://wandb.ai/daniel-a/grpo-unsloth/runs/40mdpuik?nw=nwuserdaniela">daniel-a</a>: Weights & Biases, developer tools for machine learning</li><li><a href="https://x.com/jiayi_pirate/status/1882839370505621655">Tweet from Jiayi Pan (@jiayi_pirate)</a>: We reproduced DeepSeek R1-Zero in the CountDown game, and it just works Through RL, the 3B base LM develops self-verification and search abilities all on its own You can experience the Ahah moment you...</li><li><a href="https://wandb.ai/scheschb/LLMerge/runs/cvtceyi1?nw=nwuserbschesch">scheschb</a>: Weights & Biases, developer tools for machine learning</li><li><a href="https://unsloth.ai/pricing">Pricing</a>: no description found</li><li><a href="https://unsloth.ai/contact">Contact</a>: no description found</li><li><a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb#scrollTo=3tM1psLM32qi">Google Colab</a>: no description found</li><li><a href="https://huggingface.co/mradermacher/Phi-4-mini-UNOFFICAL-GGUF">mradermacher/Phi-4-mini-UNOFFICAL-GGUF · Hugging Face</a>: no description found</li><li><a href="https://github.com/lucasjinreal/Namo-R1">GitHub - lucasjinreal/Namo-R1: A CPU Realtime VLM in 500M. Surpassed Moondream2 and SmolVLM. Training from scratch with ease.</a>: A CPU Realtime VLM in 500M. Surpassed Moondream2 and SmolVLM. Training from scratch with ease. - lucasjinreal/Namo-R1</li><li><a href="https://x.com/abacaj/status/1885517088304857197">Tweet from anton (@abacaj)</a>: Finished a run (R1 style) GRPO on Qwen-2.5-0.5B (base model) yield +10 accuracy points on GSM8K. Literally just works. Base model scores 41.6% as reported on qwen paper vs 51%~ GRPO</li><li><a href="https://huggingface.co/unsloth/Phi-4-mini-instruct">unsloth/Phi-4-mini-instruct · Hugging Face</a>: no description found</li><li><a href="https://www.youtube.com/watch?v=CsqYlV8X8og">SFT vs GRPO</a>: 📜Get repo access at Trelis.com/ADVANCED-fine-tuningTip: If you subscribe here on YouTube, click the bell to be notified of new vids🛠 Build &amp; Deploy FasterF...</li><li><a href="https://github.com/vllm-project/vllm/blob/main/examples/template_chatml.jinja">vllm/examples/template_chatml.jinja at main · vllm-project/vllm</a>: A high-throughput and memory-efficient inference and serving engine for LLMs - vllm-project/vllm</li><li><a href="https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#id-12.-saving-the-model">Tutorial: How to Finetune Llama-3 and Use In Ollama | Unsloth Documentation</a>: Beginner&#x27;s Guide for creating a customized personal assistant (like ChatGPT) to run locally on Ollama</li><li><a href="https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#id-12.-saving-the-">Tutorial: How to Finetune Llama-3 and Use In Ollama | Unsloth Documentation</a>: Beginner&#x27;s Guide for creating a customized personal assistant (like ChatGPT) to run locally on Ollama</li><li><a href="https://github.com/deepseek-ai/DualPipe">GitHub - deepseek-ai/DualPipe: A bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training.</a>: A bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training. - deepseek-ai/DualPipe
</li>
</ul>

</div>
  

---


### **Unsloth AI (Daniel Han) ▷ #[off-topic](https://discord.com/channels/1179035537009545276/1179039861576056922/1344405121324421191)** (29 messages🔥): 

> `EPYC chip arrival, Thinking OnePicyeah model, Claude's capabilities, Pycraft engine by Deepseek, Open Source vs. Early Access` 


- **EPYC Chip Arrives from China**: A member received a new **EPYC chip** from **CHINA**.
   - The member inquired if the chip came *"with thinking on or no?"*
- **Thinking Makes OnePicyeah 10x Better**: A member stated that the **OnePicyeah model** is significantly better with *"thinking,"* claiming it's *"like 10x better."
- **Claude Can Outperform Users?**: A member joked that **Claude** can do things they cannot.
   - Another member humorously encouraged them to catch up.
- **Deepseek's Pycraft Engine Teased**: A member offered to show a **Pycraft engine** made by **Deepseek**, describing it as *"minecraft by deepseek."*
- **Open Source vs. Early Access Debate**: A member expressed concern over the shift from open-source models like **OpenAI** to exclusive early access for wealthy individuals.
   - They voiced a preference for Google's ad-supported strategy, arguing it democratizes information access.


  

---


### **Unsloth AI (Daniel Han) ▷ #[help](https://discord.com/channels/1179035537009545276/1179777624986357780/1344398787925377115)** (39 messages🔥): 

> `Ollama Think Token, Qwen 2.5 VL loading issues, Unsloth pricing for 8x4090, ONNX vs TFLite, Fine-tuning Qwen 2.5 VL` 


- ****Ollama's Think-Token Trickery Troubles Users****: A user discovered that **Ollama** appends a **<think>** token to prompts, preventing the model from generating it, which requires adjusting output parsing for **<answer>** tags.
   - The user suggested that disabling this feature would be helpful, acknowledging that it stems from the model's processing class.
- ****Qwen 2.5 VL 3B's 4-Bit Finetuning Fails****: A user encountered a `RuntimeError` while trying to fine-tune the **Qwen 2.5 VL 3B model** with `load_in_4bit=True` due to size mismatches in the state_dict.
   - The error message indicated a *size mismatch for weight*, specifically between `torch.Size([11272192, 1])` in the checkpoint and `torch.Size([2048, 11008])` in the current model.
- ****Unsloth's Multi-GPU Pricing Plans: A Mystery****: A user inquired about the pricing of the **Unsloth solution** for supporting **8x4090 cards**, but the pricing is *not yet available*.
   - Another user clarified that the solution is planned to be **opensource**.
- ****ONNX vs TFLite Tango: Which Format to Follow?****: A user seeking advice on creating a **TensorFlow Lite (TFLite)** version of a **DeepSeek model** was advised to use **ONNX** instead.
   - Another member described the **ONNX** toolchain as *cancerous* due to its scattered documentation, while the original poster lamented difficulties in converting **ONNX** to **TFLite** using a [specific guide](https://codewithpk.com/how-to-use-deepseek-model-in-android-apps/).
- ****Fine-Tuning Qwen 2.5 VL: A Quest for Quality****: A user is fine-tuning a **Qwen 2.5 VL model** for document parsing but is getting *completely stupid values* in the output.
   - They shared their [fine-tuning code](https://pastebin.com/0MNA2sgW) and [inference code](https://pastebin.com/AmypjPwC), seeking help to resolve the issue where the model produces random JSON values.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing#scrollTo=yqxqAZ7KJ4oL)">Google Colab</a>: no description found</li><li><a href="https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIK">Google Colab</a>: no description found</li><li><a href="https://www.youtube.com/watch?v=218iXiKhKlg">You, you, you&#39;re good you! - Robert Deniro in Analyze This! (1999)</a>: Movie quotes.</li><li><a href="https://pastebin.com/0MNA2sgW">import ioimport osfrom typing import Dictimport pandas as pdfrom pypdf i - Pastebin.com</a>: Pastebin.com is the number one paste tool since 2002. Pastebin is a website where you can store text online for a set period of time.</li><li><a href="https://pastebin.com/AmypjPwC">from unsloth import FastVisionModelfrom pypdf import PdfReaderimport pypdfiu - Pastebin.com</a>: Pastebin.com is the number one paste tool since 2002. Pastebin is a website where you can store text online for a set period of time.
</li>
</ul>

</div>
  

---


### **Unsloth AI (Daniel Han) ▷ #[showcase](https://discord.com/channels/1179035537009545276/1179779344894263297/1344403747144728740)** (3 messages): 

> `ifeval, Instruction-following eval` 


- **ifeval gets a major refactor**: A member has massively refactored their training/eval code and released the first result: a clean reimplementation of the instruction-following eval code at [oKatanaaa/ifeval](https://github.com/oKatanaaa/ifeval).
   - This was to get *an easy cli tool* and *a good programmatic interface* to do evals in their training code, which they now provide in the repo.
- **ifeval supports new languages**: The new reimplementation of **ifeval** currently supports **English** and **Russian** languages.
   - Adding more languages should be *pretty straightforward*, so ping the author if you need another language supported.



**Link mentioned**: <a href="https://github.com/oKatanaaa/ifeval">GitHub - oKatanaaa/ifeval: A clean IFEval implementation</a>: A clean IFEval implementation. Contribute to oKatanaaa/ifeval development by creating an account on GitHub.

  

---


### **Unsloth AI (Daniel Han) ▷ #[research](https://discord.com/channels/1179035537009545276/1257011997250424842/1344540372738768986)** (4 messages): 

> `Emergent Misalignment Paper, Mercury dLLM, Diffusion vs Transformers` 


- **Emergent Misalignment Paper Questioned**: A member questioned the legitimacy of the research paper titled [Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs](https://www.emergent-misalignment.com/), citing difficulties in reproducing the results.
   - The paper explores how finetuning a model on a narrow task like writing insecure code can induce broad misalignment, causing it to assert harmful opinions on unrelated prompts.
- **Mercury dLLM unveiled by Inception AILabs**: [InceptionAILabs](https://x.com/InceptionAILabs/status/1894847919624462794) introduced **Mercury**, the first commercial-grade diffusion large language model (**dLLM**), which advances intelligence and speed through parallel, coarse-to-fine text generation.
   - Another member responded *"Okay how lol"*, seemingly impressed by the announcement.
- **Diffusion Model Deployment Challenges**: A member inquired about running **diffusion-based models** like **Mercury**, questioning its compatibility with formats like **Ollama GGUF**, given that diffusion models differ from **transformer-based architectures**.
   - Another member suggested that lack of support for **OS** and difficulties extending context length could be bottlenecks for diffusion models.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://x.com/InceptionAILabs/status/1894847919624462794">Tweet from Inception Labs (@InceptionAILabs)</a>: We are excited to introduce Mercury, the first commercial-grade diffusion large language model (dLLM)! dLLMs push the frontier of intelligence and speed with parallel, coarse-to-fine text generation.</li><li><a href="https://www.emergent-misalignment.com/">Emergent Misalignment</a>: Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs
</li>
</ul>

</div>
  

---


### **Codeium (Windsurf) ▷ #[announcements](https://discord.com/channels/1027685395649015980/1027688115592237117/1344495886599979048)** (1 messages): 

> `Claude 3.7 Sonnet, Prompt Flow Actions, Credit Multiplier Adjustment` 


- **Claude 3.7 Sees More Prompt Flow Actions**: The team acknowledged seeing more **flow actions per prompt** on average with **Claude 3.7 Sonnet** compared to **Claude 3.5 Sonnet** and is working with Anthropic to address this.
   - They noted that **3.7** is superior for demanding and precise tasks, particularly with **Thinking**, while **3.5** serves as a balanced option for initiating projects or generating boilerplate code.
- **Credit Multiplier of Claude 3.7 Sonnet Thinking lowered**: The team lowered the **credit multiplier** of **Claude 3.7 Sonnet Thinking** from **1.5** to **1.25** due to initial launch data on **Thinking token** usage.
   - This adjustment means users now consume **1.25** user prompt credits per message and **1.25** flow action credits per tool call when utilizing **Claude 3.7 Sonnet Thinking**.
- **Claude 3.7 Costs Not Lower Despite Edits**: The team clarified that they compensate the model provider for each flow action, considering prompt cache reads and tokens generated from tool calls.
   - Despite the shorter edits, **Claude 3.7** hasn't reduced costs compared to **3.5** because most of the tokens used aren't for the edit itself.


  

---


### **Codeium (Windsurf) ▷ #[discussion](https://discord.com/channels/1027685395649015980/1027697446446432336/1344435675524632638)** (25 messages🔥): 

> `Codeium.el Hacks, Flow Action Credits, Jetbrains IDE features parity, Cascade Engine Issues, DeepSeek v3 Integration` 


- **Emacs Codeium.el Hacked to Sorta-Work**: A member hacked the **codeium.el** elisp code, but noted that it offered nonsense suggestions and pinpointed the `read-muliple-choice` call on line **888** as the failure point, [hardcoding `(login-method 'auto)`](https://github.com/Exafunction/codeium.el) to get it working.
   - Another member suggested submitting a PR, and the original member clarified it was a minimal hack and not worth a PR, but was enough to get it working.
- **Flow Action Credits Flounder in VS Code**: Members discussed how **Flow Action credits** are not applicable to the VS Code extension because it doesn't support the **Cascade engine**.
   - They clarified that credits are related to the **Cascade engine** for both prompts and flow actions, and will apply to extensions when **Cascade** is integrated.
- **JetBrains IDE Extension Needs Windsurf's Oomph**: A member expressed desire for the same features in the **Codeium extension** on **JetBrains IDE** as **Windsurf**, noting that the current JetBrains extension is outdated.
   - Another member shared the [Codeium Roadmap](https://codeium.canny.io) for feature requests, and pointed to the ability to upvote existing feature requests there.
- **Cascade Crashes Cause Consternation**: Users reported that **Cascade** isn't working due to a `resource_exhausted` error, according to a [Feature Request](https://codeium.canny.io/feature-requests/p/cascade-isnt-working-any-more-errorserver-encountered-error-of-type-resource-ex).
   - Members linked to the [roadmap](https://codeium.canny.io) to stay updated.
- **Infinity Chat is Technically Possible**: Although, technically, members can use **infinity chat** other users pointed out that its capabilities are slightly less capable than even legacy mode in **Cascade** in **Windsurf**.
   - VSCode with Codeium extension was what made someone purchasing pro for a year in **8.10.2024**



**Link mentioned**: <a href="https://codeium.canny.io">Codeium Feedback</a>: Give feedback to the Codeium team so we can make more informed product decisions. Powered by Canny.

  

---


### **Codeium (Windsurf) ▷ #[windsurf](https://discord.com/channels/1027685395649015980/1306163501286293515/1344402204941094984)** (579 messages🔥🔥🔥): 

> `Claude 3.7 Sonnet cost, Windsurf pricing and credits, Cursor vs Windsurf, Deepseek v3, Windsurf Stability` 


- **Users Bemoan Claude 3.7's Credit Consumption**: Users complain that **Claude 3.7** is rapidly consuming credits, with one user reporting near depletion of their monthly credits in a single day, and recommend using **Claude 3.7 Sonnet + (Thinking) in Legacy mode** while manually providing context.
   - Another user described **Claude 3.7** drinking their credits *like a flood*.
- **Pricing Model Rant**: Members express confusion over Windsurf's pricing structure, particularly regarding **flow credits**, and one highlights the disproportionate cost of additional flow actions compared to the initial plan offering.
   - Some users found Cursor's straightforward approach to pricing preferable.
- **Cursor Beats Windsurf?**: Several users express frustration with Windsurf's instability, errors, and credit consumption, and suggest a switch to Cursor, citing its stability and more predictable pricing.
   - However, other users still found Windsurf superior, particularly for its AI capabilities and codebase access, with one user stating, *Tried them side by side, same prompt , same codebase and for me at least cursor doesn't come close..*.
- **Deepseek v3 Performance Woes**: Some users report severe bugs and usability issues with Deepseek v3 in Windsurf, rendering it unusable for anything beyond the simplest tasks.
   - Others claim that **Deepseek v3** works perfectly well for them.
- **Windsurf Upgrade Wrecks havoc**: Users are reporting Windsurf stability issues after upgrading to **Sequoia 15.1** and after updating to **1.3.9**. There is a cascade bug and they cannot see the highlighted code changes.
   - Users also complain that cascade is stuck in a loop offering erroneous support because it just can't see the output of a command right.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://x.com/SambaNovaAI/status/1895188233253986452">Tweet from SambaNova Systems (@SambaNovaAI)</a>: SN40L crushes H200 in real-world #AI inference! 🦾We measured @deepseek_ai&#39;s-R1 with SGLang 0.4.2 on 1 node of H200, & guess what - SN40L completely smashes H200&#39;s Pareto frontier:☑️ 5.7x fast...</li><li><a href="https://codeium.com/plan">Plan Settings</a>: Tomorrow&#x27;s editor, today. Windsurf Editor is the first AI agent-powered IDE that keeps developers in the flow. Available today on Mac, Windows, and Linux.</li><li><a href="https://codeium.com/support">Support | Windsurf Editor and Codeium extensions</a>: Need help? Contact our support team for personalized assistance.</li><li><a href="https://tenor.com/view/chaos-office-fire-gif-19355549">Chaos Office GIF - Chaos Office Fire - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://x.com/alexalbert__/status/1894807853371990087?s=46&t=Jr3CreBJD5w6l1CBmLyG3A">Tweet from Alex Albert (@alexalbert__)</a>: Good news for @AnthropicAI devs:We shipped a more token-efficient tool use implementation for 3.7 Sonnet that uses on average 14% less tokens under-the-hood and shows marked improvement in tool use pe...</li><li><a href="https://tenor.com/view/pacman-video-game-eating-marshmallow-gif-6008098">Video Juego De Pacman GIF - Pacman Video Game Eating - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://status.anthropic.com/">Anthropic Status</a>: no description found</li><li><a href="https://github.com/VSCodium/vscodium/blob/master/docs/index.md#extensions--marketplace)">vscodium/docs/index.md at master · VSCodium/vscodium</a>: binary releases of VS Code without MS branding/telemetry/licensing - VSCodium/vscodium</li><li><a href="https://www.youtube.com/watch?v=VmmdP5RnkU0"> - YouTube</a>: no description found</li><li><a href="https://x.com/kevinhou22/status/1895206339816931831">Tweet from Kevin Hou (@kevinhou22)</a>: 🎉 gpt-4.5 available in @windsurf_ai on rolling beta! Excited to see what windsurfers build with it — let&#39;s goooo 🏄*note: benchmarks show it&#39;s not the best code model and it&#39;s crazy expen...</li><li><a href="https://www.youtube.com/watch?v=cfRYp0nItZ8"> - YouTube</a>: no description found</li><li><a href="https://huggingface.co/reach-vb/GPT-4.5-System-Card/blob/main/gpt-4-5-system-card.pdf">gpt-4-5-system-card.pdf · reach-vb/GPT-4.5-System-Card at main</a>: no description found</li><li><a href="https://x.com/windsurf_ai/status/1895206330987880816">Tweet from Windsurf (@windsurf_ai)</a>: GPT-4.5 now available in Beta on Windsurf!Due to costs, rate limits, and quality from early testing, we will be rolling it out to users incrementally.Currently, it’s significantly more expensive (&gt;...</li><li><a href="https://www.youtube.com/watch?v=xrFKtYOsOSY">Windsurf / Codeium - why it makes me so productive. My live demo to another team.</a>: I did my best to keep the people involved private. Apologies if any personal details revealed.  I first tried cutting the video out and then I tried the &#39;blu...
</li>
</ul>

</div>
  

---


### **GPU MODE ▷ #[general](https://discord.com/channels/1189498204333543425/1189498205101109300/1344405354884235415)** (36 messages🔥): 

> `Deepseek R1, Zen 5 NPU, AIE Toolchain, Ultrascale Playbook, Mixed Precision Training` 


- ****DeepSeek's R1 Model Rocks Reasoning Realm****: **DeepSeek's R1 model** aims to improve reply quality by generating a *chain of thought*, achieving parity with **OpenAI's o1** on benchmarks but is open-source, as outlined in their technical reports and the [DeepSeek API documentation](https://api-docs.deepseek.com/quick_start/pricing).
- ****Ultrascale Playbook Video is Plus Ultra****: A member shared a [YouTube video](https://www.youtube.com/watch?v=CVbbXHFsfP0) titled *The Ultra Scale Playbook* by **Nouamane Tazi**, and the related [Hugging Face Space](https://huggingface.co/spaces/nanotron/ultrascale-playbook).
   - One expressed excitement to set up a script to download the HF book once it's up, describing it as *refreshing*.
- ****DeepSeek-V3 details Deep Dive Deployed****: A member shared a [video walkthrough](https://www.youtube.com/watch?v=8v2l6SJECW4&t=301s&ab_channel=GabrielMongaras) summarizing important **DeepSeek** techniques from the paper ([https://arxiv.org/abs/2412.19437v1](https://arxiv.org/abs/2412.19437v1)).
- ****AIE Toolchain Troubles Trounce Techies****: A member encountered difficulty with **AMD's Zen 5 NPU**, finding that *NPU BLAS* was easier on **Intel** but incredibly challenging on **AMD**, particularly with the **AIE toolchain**.
   - They found [Linux support was recently merged 20 days ago](https://github.com/Xilinx/mlir-aie/blob/main/docs/buildHostLin.md), but installation instructions were still complicated.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://research.colfax-intl.com/deepseek-r1-and-fp8-mixed-precision-training/">DeepSeek-R1 and FP8 Mixed-Precision Training</a>: DeepSeek has shocked the world with the release of their reasoning model DeepSeek-R1. Similar to OpenAI&#8217;s o1 and Google Gemini&#8217;s Flash Thinking, the R1 model aims to improve the quality…</li><li><a href="https://github.com/Xilinx/mlir-aie/blob/main/docs/buildHostLin.md">mlir-aie/docs/buildHostLin.md at main · Xilinx/mlir-aie</a>: An MLIR-based toolchain for AMD AI Engine-enabled devices. - Xilinx/mlir-aie</li><li><a href="https://www.youtube.com/watch?v=8v2l6SJECW4&t=301s&ab_channel=GabrielMongaras">DeepSeek-V3</a>: Paper: https://arxiv.org/abs/2412.19437v1R1 paper: https://arxiv.org/abs/2501.12948DeepSeekMoe: https://arxiv.org/abs/2401.06066Huggingface: https://huggingf...</li><li><a href="https://www.youtube.com/watch?v=CVbbXHFsfP0">The Ultra Scale Playbook</a>: Speaker: Nouamane Tazi</li><li><a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook">The Ultra-Scale Playbook - a Hugging Face Space by nanotron</a>: no description found
</li>
</ul>

</div>
  

---


### **GPU MODE ▷ #[triton](https://discord.com/channels/1189498204333543425/1189607595451895918/1344461246753280010)** (46 messages🔥): 

> `INT4 TC, FP4 vs INT4, reinterpret_cast on tl.tensor, Threads in the block with lock, Packed Integer Values` 


- **NVIDIA drops INT4 TensorCores**: A member noted that [NVIDIA](https://www.nvidia.com/) might not be advertising **INT4 Tensor Cores** anymore, focusing on **FP4** instead, while sharing [benchmarks](https://github.com/gau-nernst/quantized-training?tab=readme-ov-file#matmul) for quantized models.
   - Another member confirmed that **Ada** had **INT4**, **Hopper** had **INT8**, and **Blackwell** features **FP4**.
- **Bypass reinterpret_cast on tl.tensor**: A member asked about using `reinterpret_cast` on `tl.tensor` to convert a `uint32[N]` tensor to a `float16[2*N]` tensor.
   - However, it was clarified that such an operation isn't directly supported and requires using bit shifting instead.
- **Threads behavior during lock acquisition**: A member inquired about the behavior of threads when acquiring a lock in a Triton block, sharing example code with `tl.atomic_cas` and `tl.atomic_xchg`.
   - Another member pointed to the [relevant Triton code](https://github.com/triton-lang/triton/blob/04159ed54e8a89b15c3291557f2f64a955117bf1/lib/Analysis/Allocation.cpp#L68C4-L71C46), suggesting that thread behavior in such cases doesn't need explicit management.
- **Packing Integers for SIMD Throughput**: Members discussed packing **INT8** values into 16-bit or 32-bit values for faster matmul operations on GPUs, particularly on architectures like **Blackwell**.
   - It was explained that packing increases throughput by enabling the execution of twice the amount of data with the same SIMD instruction, and that libraries like `bitsandbytes` use this for quantized matmuls, pointing to [bitsandbytes functional.py](https://github.com/bitsandbytes-foundation/bitsandbytes/blob/main/bitsandbytes/functional.py) and [fast.c](https://github.com/BlinkDL/fast.c/blob/main/gemv.c) as examples.
- **"Neural Shaders" term is Leveraging Tensor Cores**: A member expressed disbelief over the term *'Neural Shaders'*, considering it excessive *copium* for gamers.
   - Another member shared a [link from NVIDIA Research](https://research.nvidia.com/labs/rtr/neural_appearance_models/) that clarified neural shaders pretty much are leveraging tensor cores for shader calculations.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://research.nvidia.com/labs/rtr/neural_appearance_models/">Real-Time Neural Appearance Models</a>: Real-Time Neural Appearance Models</li><li><a href="https://developer.nvidia.com/cuda-gpus">CUDA GPUs - Compute Capability</a>: Explore your GPU compute capability and CUDA-enabled products.</li><li><a href="https://github.com/gau-nernst/quantized-training?t">GitHub - gau-nernst/quantized-training: Explore training for quantized models</a>: Explore training for quantized models. Contribute to gau-nernst/quantized-training development by creating an account on GitHub.</li><li><a href="https://github.com/gau-nernst/quantized-training?tab=readme-ov-file#matmul">GitHub - gau-nernst/quantized-training: Explore training for quantized models</a>: Explore training for quantized models. Contribute to gau-nernst/quantized-training development by creating an account on GitHub.</li><li><a href="https://github.com/BlinkDL/fast.c/blob/main/gemv.c">fast.c/gemv.c at main · BlinkDL/fast.c</a>: Prepare for DeekSeek R1 inference: Benchmark CPU, DRAM, SSD, iGPU, GPU, ... with efficient code. - BlinkDL/fast.c</li><li><a href="https://github.com/triton-lang/triton/blob/04159ed54e8a89b15c3291557f2f64a955117bf1/lib/Analysis/Allocation.cpp#L68C4-L71C46">triton/lib/Analysis/Allocation.cpp at 04159ed54e8a89b15c3291557f2f64a955117bf1 · triton-lang/triton</a>: Development repository for the Triton language and compiler - triton-lang/triton</li><li><a href="https://github.com/bitsandbytes-foundation/bitsandbytes/blob/main/bitsandbytes/functional.py">bitsandbytes/bitsandbytes/functional.py at main · bitsandbytes-foundation/bitsandbytes</a>: Accessible large language models via k-bit quantization for PyTorch. - bitsandbytes-foundation/bitsandbytes
</li>
</ul>

</div>
  

---


### **GPU MODE ▷ #[cuda](https://discord.com/channels/1189498204333543425/1189607726595194971/1344572129055608842)** (61 messages🔥🔥): 

> `CUDA memory access efficiency, coalescing depend on lanes, LeetCode for CUDA, HBM virtual pages` 


- **Demystifying CUDA Memory Access Efficiency**: A member sought to understand CUDA memory access efficiency, particularly regarding memory coalescing and vectorised reads, and found it surprisingly hard to find a direct answer to such a seemingly simple question, but [the CUDA C++ Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#coalesced-access-to-global-memory) was provided for more context.
   - They wondered if reading larger values or using vectorised loads would negate the benefits of contiguous/coalesced access due to potential bank conflicts, also wondering if shared memory access is affected.
- **Coalescing Depends on Lanes, not Conflicts**: Coalescing depends on lanes in a warp accessing consecutive elements of any size, with the first element being **32 byte aligned** to minimize unnecessary transactions, which applies for bigger sized types like vectors.
   - It was clarified that *bank conflicts* are a concept normally applied in the context of **shared memory access**, not global memory access.
- **LeetCode for CUDA Released in Beta**: A new resource, [LeetCode for CUDA](https://leetgpu.com/challenges), was released in beta, inviting users to try it out and provide feedback.
   - The platform aims to provide coding challenges specifically for CUDA development, but users should expect some hiccups due to its beta status.
- **Exploring HBM Virtual Page Sizes**: Discussion arose regarding memory page sizes in GPUs, with mentions of **1024-byte physical pages** relevant to memory access patterns and the potential for optimal performance by accessing a whole page within a thread block, and that [Stephen Jones](https://developer.nvidia.com/blog/accelerating-quantum-simulation-with-cutensornet-2-0/) talks on Nvidia on Demand are a good source.
   - It was noted that **HBM virtual pages** can be as large as **64kB**, leading to questions about whether the **1kB** size refers to internal burst or sub-block granularity, also physical pages vs virtual pages.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://leetgpu.com/challenges">LeetGPU</a>: no description found</li><li><a href="https://tensara.org/submissions/cm7o0hryi00qav947nb0f8me2">Loading... | Tensara</a>: A platform for GPU programming challenges. Write efficient CUDA code and compare your solutions with other developers.</li><li><a href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#coalesced-access-to-global-memory">1. Preface — CUDA C++ Best Practices Guide 12.8 documentation</a>: no description found
</li>
</ul>

</div>
  

---


### **GPU MODE ▷ #[torch](https://discord.com/channels/1189498204333543425/1189607750876008468/1344399196870017064)** (4 messages): 

> `MPS Development, CI-based development` 


- **MPS Development on Linux with CUDA GPU**: A user inquired about the possibility of developing **MPS** (Metal Performance Shaders) on a Linux laptop equipped with a **CUDA discrete GPU**.
   - They questioned how **MPS** emulation could be achieved on **CUDA**.
- **CI-Based Development Methodology**: A member clarified that their **MPS** development process primarily relies on **CI-based development** over the past 2 years.
   - They mentioned that Nikita handles the majority of the work, while they focus on chatting and reviewing.


  

---


### **GPU MODE ▷ #[announcements](https://discord.com/channels/1189498204333543425/1189640399476764692/1344829181606887476)** (1 messages): 

> `Nouamane Tazi, Ultra-Scale Playbook, LLM training, 5D Parallelism` 


- **Nouamane Tazi to Give Epic Talk**: Nouamane Tazi will give a **3-hour talk** on his new viral book, "THE Ultra-Scale Playbook - a comprehensive guide on training LLMs from 1 to 1000s of GPUs!" tomorrow at <t:1740772800:F>, covering topics from single GPU memory usage to **5d Parallelism**, as seen on [HuggingFace](https://huggingface.co/spaces/nanotron/ultrascale-playbook).
- **Special Guest Host Announced**: A special guest host, <@418840303122907156>, will be present at the talk tomorrow.



**Link mentioned**: <a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook">The Ultra-Scale Playbook - a Hugging Face Space by nanotron</a>: no description found

  

---


### **GPU MODE ▷ #[algorithms](https://discord.com/channels/1189498204333543425/1189861061151690822/1344577810517463120)** (1 messages): 

> `Multi-head Latent Attention, Decoupled RoPE, MHA vs MLA, Weight Merging in MLA` 


- **Decoupled RoPE requirement for MLA dissected**: The user is seeking rationale on why **RoPE** needs to be decoupled for **MLA** due to potential merging between (**query latent**)->query and (**KV latent**)->key weights during inference, and whether this applies to standard **Multi-Head Attention (MHA)**.
   - They question if decoupling **RoPE** is more beneficial for **MLA** than **MHA** due to **MLA's expansion/contraction properties**, particularly how merging weights could streamline the process of small->big and big->small weight matrices into smaller operations.
- **Efficiency of weight merging in MLA assessed**: The user considers whether **merging expansion/contraction weights** in **MLA** could transform a small->big and big->small weight matrix into a small->small weight.
   - The user also suggests that because **MHA** lacks the same expansion/contraction dynamics, merging weights would offer only marginal efficiency gains compared to **MLA**.


  

---


### **GPU MODE ▷ #[cool-links](https://discord.com/channels/1189498204333543425/1189868872887705671/1344505311465308181)** (10 messages🔥): 

> `DualPipe, GPU Architecture Fundamentals, CUDA Leetcode, Diffusion Models, TinyLM` 


- ****DeepSeek** unveils bidirectional **DualPipe****: DeepSeek released [DualPipe on Github](https://github.com/deepseek-ai/DualPipe), a bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training.
- ****GPU Architecture** playlist surfaces**: A member shared [a YouTube playlist](https://youtube.com/playlist?list=PLxNPSjHT5qvscDTMaIAY9boOOXAJAS7y4&si=iFueok_ZhAPFrWmL) on the fundamentals of **GPU architecture**.
- ****CUDA** gets a leetcode-esque platform called **Tensara****: A member highlighted [Tensara](https://tensara.org/), a platform for **GPU programming** challenges to write efficient **CUDA** code and compare solutions with other developers.
- ****Diffusion** invades LLMs, claims speed and vibe**: According to a tweet, Diffusion models can achieve super-speedy generation on GPUs, surpassing Groq/Cerebras, and do much better at “fill-in-the-middle” (FIM) compared to other models like **DeepSeek V2 Lite** ([tweet](https://x.com/dzhulgakov/status/1894932614173392975)).
   - The tweet highlighted [Mercury by Inception Labs](https://x.com/InceptionAILabs), the first commercial-grade diffusion large language model (dLLM) with parallel, coarse-to-fine text generation.
- ****TinyLM** facilitates zero-cost client-side inference**: A member shared [TinyLM](https://github.com/wizenheimer/tinylm), for zero-cost client-side inference using WebGPU, and OpenAI-compliant NodeJS and Chrome.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://youtube.com/playlist?list=PLxNPSjHT5qvscDTMaIAY9boOOXAJAS7y4&si=iFueok_ZhAPFrWmL">Fundamentals of GPU Architecture</a>: no description found</li><li><a href="https://x.com/dzhulgakov/status/1894932614173392975">Tweet from Dmytro Dzhulgakov (@dzhulgakov)</a>: Diffusion... for text, wow 🤯. Here&#39;s what it means:1/ Super-speedy generation on GPUs. Groq/Cerebras are at a disadvantage here. Diffusion models (just like LLM training) are all about FLOPs, gre...</li><li><a href="https://tensara.org/">Home | Tensara</a>: A platform for GPU programming challenges. Write efficient CUDA code and compare your solutions with other developers.</li><li><a href="https://github.com/deepseek-ai/DualPipe">GitHub - deepseek-ai/DualPipe: A bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training.</a>: A bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training. - deepseek-ai/DualPipe</li><li><a href="https://github.com/wizenheimer/tinylm">GitHub - wizenheimer/tinylm: Zero-cost client-side inference using WebGPU | OpenAI-compliant | NodeJS | Chrome</a>: Zero-cost client-side inference using WebGPU | OpenAI-compliant | NodeJS | Chrome - wizenheimer/tinylm
</li>
</ul>

</div>
  

---


### **GPU MODE ▷ #[beginner](https://discord.com/channels/1189498204333543425/1191300313928433664/1344568922287902744)** (7 messages): 

> `HBM Bandwidth Estimation, CUDA Kernel Access Patterns, Mathematics for PMPP/CUDA, Discord Scams` 


- **User Tests HBM Bandwidth and Seeks Pattern Advice**: A new user shared a [CUDA kernel](https://github.com/example/bandwidthTestKernel) designed to estimate **HBM memory bandwidth** and inquired about its memory access patterns.
   - The user questioned whether the kernel exhibits a coalesced memory access pattern, contrary to **Deepseek's** assessment of stride access patterns, and seeks guidance on understanding the data access flow (`hbm -> l2 cache -> temp register`).
- **Discord Group Warns of Possible Scam**: A user expressed confusion about an unidentified element within the Discord server, prompting other members to identify it as a likely **scam** and ban the user.
   - A member confirmed it was *"certainly not related to this discord"*.
- **Exploring Math Prerequisites for PMPP and CUDA**: A member inquired about the necessary mathematical background before learning **PMPP** (presumably Parallel Multi-Processing Programming) or **GPUs/CUDA**.
   - Another member gave the terse advice *"nothing go go go"*.


  

---


### **GPU MODE ▷ #[self-promotion](https://discord.com/channels/1189498204333543425/1288557096404516945/1344423827001708614)** (5 messages): 

> `CUDA C++ and CUDA Python Tutorials, Accelerated Python Profiling Tools Survey, L1 store-caching in CUDA, tinylm WebGPU acceleration, LeetCode for CUDA` 


- **NVIDIA hosts CUDA Tutorials, Offers GPU MODE Event**: NVIDIA is hosting invite-only, hands-on **CUDA C++** and **CUDA Python** tutorials the day before **GTC 2025** on **Sunday, March 16, 2025**, from 12-4 PM, and invites you to also the GPU MODE event from 5-10 PM ([lu.ma/8w1ehhrw](https://lu.ma/8w1ehhrw)).
   - Interested parties are asked to email [developercommunity@nvidia.com](mailto:developercommunity@nvidia.com) to indicate which tutorial they'd like to attend, and no prior CUDA experience is required.
- **NVIDIA Needs Input: Accelerated Python Profiling Tools Survey Released**: The NVIDIA Developer Tools team seeks feedback on how accelerated Python developers profile and optimize workloads via a short survey ([Accelerated Python Profiling Tools Survey](https://docs.google.com/forms/d/e/1FAIpQLSdf7PqFwbrqUdADrs9mX0_GS6pDqn8uZesTwp9CdG3ApyRGNg/viewform)).
   - Profiling tools features are documented in [the Accelerated Python User Guide](https://github.com/NVIDIA/accelerated-computing-hub/blob/main/Accelerated_Python_User_Guide/notebooks/Chapter_9_Developer_Tools.ipynb) and user input heavily drives the feature roadmap.
- **StackOverflow answers CUDA L1 store-caching questions**: A member compiled a [StackOverflow answer](https://stackoverflow.com/a/79473301/10107454) regarding **L1 store-caching** in CUDA over the GPU generations from tuning guides and whitepapers.
   - It also attempts to clarify confusing cache operators from the PTX ISA.
- **tinylm WebGPU Library hits v0**: **tinylm**, a library for running **LLMs** and **embedding models** client-side in browser or Node.js with WebGPU acceleration, has reached v0 ([https://github.com/wizenheimer/tinylm](https://github.com/wizenheimer/tinylm)).
   - It supports OpenAI SDK-like text generation and embeddings, with text-to-speech and speech-to-text functionalities in the pipeline, and requires no servers.
- **LeetCode for CUDA Released, Enters Beta**: The community announces the release of **LeetCode for CUDA** at [https://LeetGPU.com/challenges](https://LeetGPU.com/challenges).
   - The platform is currently in beta, and user feedback is welcomed.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://LeetGPU.com/challenges">LeetGPU</a>: no description found</li><li><a href="https://github.com/wizenheimer/tinylm">GitHub - wizenheimer/tinylm: Zero-cost client-side inference using WebGPU | OpenAI-compliant | NodeJS | Chrome</a>: Zero-cost client-side inference using WebGPU | OpenAI-compliant | NodeJS | Chrome - wizenheimer/tinylm
</li>
</ul>

</div>
  

---


### **GPU MODE ▷ #[reasoning-gym](https://discord.com/channels/1189498204333543425/1316377974672588850/1344401906587402251)** (25 messages🔥): 

> `Reasoning Gym Eval Script, Mercury Diffusion LLMs, GPT-4.5 Release, willccbb/verifiers issue` 


- **Reasoning Gym's Eval Script Needs Improvement**: Members discussed that the current reasoning-gym eval script lacks error printing and informative logs, making debugging difficult, but [a new version is in the works](https://github.com/reasoning-gym/reasoning-gym).
   - Issues were found with **API key** setup using *os.genenv* (resolved by using *load_env*) and **JSON serialization** of time objects, causing script failures.
- **Diffusion Models Could Eclipse Autoregressive LLMs**: Discussion pointed to [Inception Labs' Mercury](https://www.inceptionlabs.ai/news), a **diffusion-based LLM** that could outperform traditional auto-regressive models in speed and quality.
   - Mercury is reported to be up to **10x faster** than speed-optimized LLMs, achieving over **1000 tokens/sec** on **NVIDIA H100s**.
- **GPT-4.5 Release Met with Skepticism**: The release of **GPT-4.5** was met with skepticism due to its high cost, lack of reasoning capabilities, and perceived lack of excitement, with one member describing it as *"what a flop"*.
   - Concerns were raised about its cost and the removal of the model picker, leading some to question its value proposition, and whether **GPT-5** will be the real unified model.
- **willccbb/verifiers issue re-opened**: A member mentioned re-opening the issue on the **willccbb/verifiers** project, inviting community contribution to the effort.
   - However, the member indicated they personally may lack the time to actively work on the issue.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://www.twitch.tv/claudeplayspokemon">ClaudePlaysPokemon - Twitch</a>: Claude Plays Pokemon - Debut Stream</li><li><a href="https://www.inceptionlabs.ai/news">Inception Labs</a>: We are leveraging diffusion technology to develop a new generation of LLMs. Our dLLMs are much faster and more efficient than traditional auto-regressive LLMs. And diffusion models are more accurate, ...
</li>
</ul>

</div>
  

---


### **GPU MODE ▷ #[gpu模式](https://discord.com/channels/1189498204333543425/1342364798058500148/1344406457361236080)** (16 messages🔥): 

> `Chinese Internet Trends (Douyin vs. Xiaohongshu), Experiences with NVIDIA Hardware, MLSys and CUDA Discussions on Xiaohongshu, Chinese Room Thought Experiment, CUDA QQ Groups` 


- **Xiaohongshu Surpasses Douyin**: A user switched to **Xiaohongshu** after **Douyin** was banned, noting the need to engage with the Chinese internet landscape.
   - The user expressed a preference for **Xiaohongshu** but admitted it's not suitable for in-depth technical content due to its mobile-centric SNS format, recommending Zhihu, blogs, and papers for deeper learning.
- **Bonding over NVIDIA Hardware Struggles**: A user finds common ground with **Chinese engineers** in navigating **NVIDIA hardware**, preferring direct communication over relying on promotional materials.
   - The user mentioned learning from various sources to bypass propaganda and engage directly with people.
- **MLSys/CUDA Content on Xiaohongshu Explodes**: A user noticed an increase in **MLSys** and **CUDA**-related content on **Xiaohongshu**, but acknowledges its limitations for in-depth study.
   - The user noted, *xhs还是不适合这种内容，主要xhs真就是个面向手机的sns* and recommends Zhihu, blogs, and papers for serious learning.
- **Navigating the Chinese Room Thought Experiment**: A user introduces the [Chinese room](https://zh.wikipedia.org/wiki/%E4%B8%AD%E6%96%87%E6%88%BF%E9%97%B4) thought experiment, referencing its Wikipedia page, to explain a shared phenomenon.
   - The **Chinese Room experiment** refutes Strong AI.
- **Craving CUDA QQ Group Banter**: A user expressed a desire for a **CUDA QQ group** to facilitate casual discussion and information sharing.
   - Another user responded that **WeChat groups** related to the topic do exist.



**Link mentioned**: <a href="https://zh.wikipedia.org/wiki/%E4%B8%AD%E6%96%87%E6%88%BF%E9%97%B4">中文房间 - 维基百科，自由的百科全书</a>: no description found

  

---


### **GPU MODE ▷ #[general](https://discord.com/channels/1189498204333543425/1343002580531417211/1344621797764497461)** (1 messages): 

> `1000 Submissions Milestone` 


- **Community Reaches 1000 Submissions**: The community reached **1000 submissions** and celebrated with a champagne toast in the attached [image](https://cdn.discordapp.com/attachments/1343002580531417211/1344621797622022194/IMG_5522.png?ex=67c23ce2&is=67c0eb62&hm=13f075439299fa9bf59a7b1a41c1beddd14d130dc2d5c1c8b97e51157fe4d954&).
- **Celebratory Champagne**: The image shows what appears to be a celebratory scene, possibly involving **champagne or sparkling wine**, to mark the milestone.


  

---


### **GPU MODE ▷ #[submissions](https://discord.com/channels/1189498204333543425/1343002583001726986/1344401339706249301)** (206 messages🔥🔥): 

> `Grayscale Leaderboard, Histogram Leaderboard, Vectoradd Leaderboard, Vectorsum Leaderboard, Sort Leaderboard` 


- **Grayscale Submissions Galore**: Multiple submissions, both **benchmarks** and **leaderboard** entries, were made to the `grayscale` leaderboard using various GPUs like **A100**, **H100**, **T4**, and **L4** with Modal runners.
   - Many of these submissions triggered a message stating *Leaderboard name specified in the command doesn't match the one in the submission script header*.
- **Histogram Gets Heaps of Hits**: Numerous submissions were made to the `histogram` leaderboard, utilizing GPUs such as **T4**, **H100**, and **A100** with Modal runners, including test, benchmark and leaderboard submissions.
   - Similar to the grayscale submissions, many of these triggered a message stating *Leaderboard name specified in the command doesn't match the one in the submission script header*.
- **Vectoradd Victories Vanquish Valuelessness**: Submissions, mostly benchmarks, targeted the `vectoradd` leaderboard, employing GPUs like **T4**, **A100**, and **H100** with Modal runners.
   - A notable number of these submissions also triggered the *Leaderboard name specified in the command doesn't match the one in the submission script header* message.
- **Vectorsum Ventures Validate Variance**: Test and benchmark submissions were made to the `vectorsum` leaderboard, primarily using **A100** GPUs and Modal runners.
   - Most of these submissions triggered a message stating *Leaderboard name specified in the command doesn't match the one in the submission script header*.
- **Sorting Submissions surface**: Benchmark submissions were made to the `sort` leaderboard using **T4** GPUs and Modal runners.
   - These submissions triggered a message stating *Leaderboard name specified in the command doesn't match the one in the submission script header*.


  

---


### **GPU MODE ▷ #[ppc](https://discord.com/channels/1189498204333543425/1343373905783554048/1344419587252420729)** (10 messages🔥): 

> `INT8 Matmul, Loop Reordering, CPU optimization` 


- ****INT8 Matmul** Mystery**: A member is struggling with **INT8 matmul** baseline performance, taking **3.62 seconds** even after transposing B.
   - Another member claims they achieved faster speeds without multithreading, instruction-level parallelism, or vectorization, relying on existing knowledge and intuition.
- **Loop Reordering Saves the Day**: One member suggests that loop reordering is a key optimization for **matmul** on **CPU**, easily found via a quick Google search.
   - The same member clarified they meant **CPU** optimization, also asking if the user ran `modprobe amd_uncore`.


  

---


### **GPU MODE ▷ #[feature-requests-and-bugs](https://discord.com/channels/1189498204333543425/1343759913431728179/1344399090640752752)** (6 messages): 

> `Custom Kernel Preprocessing, Bot Submitter Identification, Matmul Preprocessing Time` 


- **Custom Kernel Preprocessing Concerns Raised**: A member questioned the difference between the current setup and a new proposal regarding defining a preprocessing function in `custom_kernel` as part of the timing analysis.
   - Another member responded that they think it makes sense for it to be included, but did not clarify.
- **Bot Needs Submitter ID Upgrade**: A user expressed confusion about identifying submissions when interacting with the bot, suggesting the inclusion of the submitter's username in the topic title.
   - Another member confirmed that this request had been voiced by others and should be implemented soon when admins have available time.
- **Matmul Preprocessing Timeout Tensions**: A member suggested including preprocessing time for large matrix multiplication (`matmul`) targets, given its `O(n²)` complexity versus the `O(n³)` kernel runtime.
   - For other settings, they proposed setting a reasonable timeout, such as limiting preprocessing time to 100ms for kernels expected to run in under 10ms.


  

---


### **OpenRouter (Alex Atallah) ▷ #[announcements](https://discord.com/channels/1091220969173028894/1092729520181739581/1344453566143926414)** (4 messages): 

> `OpenAI Outage, DeepSeek R1, Claude Sonnet 3.7, GPT-4.5 Preview` 


- **OpenAI Provider Outage Resolved**: OpenRouter experienced an **OpenAI provider outage** which was identified as an incident on **OpenAI's** side and has since been resolved.
   - Requests are now succeeding, and **OpenAI** as a provider on OpenRouter has recovered.
- **DeepSeek R1 Blazes with SambaNovaAI**: A new provider for the **671B-param DeepSeek R1** via **SambaNovaAI** now provides **150 tokens/second**.
   - See [OpenRouterAI's tweet](https://x.com/OpenRouterAI/status/1895135991025017346) for more details.
- **Claude Sonnet 3.7 Boasts Capacity and Web Search**: **Claude Sonnet 3.7** now has significantly higher rate limits and web search capability on OpenRouter.
   - A member provided a link to [OpenRouterAI's tweet](https://x.com/OpenRouterAI/status/1895141541473329597) as a reminder of these features.
- **GPT-4.5 Preview Rockets onto OpenRouter**: **GPT-4.5 (Preview)**, designed to push boundaries in reasoning, creativity, and long-context conversations, is now available on OpenRouter, costing **$75/M** input tokens and **$150/M** output tokens.
   - Early testing shows improvements in open-ended thinking, real-world knowledge, long-context coherence, and reduced hallucinations; the announcement links to the [OpenAI blog post](https://openai.com/index/introducing-gpt-4-5/) and a [discussion on X](https://x.com/OpenRouterAI/status/1895236199004152272).


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://x.com/OpenRouterAI/status/1895141541473329597">Tweet from OpenRouter (@OpenRouterAI)</a>: Reminder that you can use web search with Claude Sonnet 3.7API available as well. Works for any model! 👇</li><li><a href="https://x.com/OpenRouterAI/status/1895135991025017346">Tweet from OpenRouter (@OpenRouterAI)</a>: DeepSeek R1 now has a blazing fast provider: @SambaNovaAI!Currently getting 150+ TPS:</li><li><a href="https://openrouter.ai/openai/gpt-4.5-preview">GPT-4.5 (Preview) - API, Providers, Stats</a>: GPT-4.5 (Preview) is a research preview of OpenAI’s latest language model, designed to advance capabilities in reasoning, creativity, and multi-turn conversation. Run GPT-4.5 (Preview) with API</li><li><a href="https://x.com/OpenRouterAI/status/1895236199004152272">Tweet from OpenRouter (@OpenRouterAI)</a>: GPT-4.5 Preview live for everyone 🍓
</li>
</ul>

</div>
  

---


### **OpenRouter (Alex Atallah) ▷ #[app-showcase](https://discord.com/channels/1091220969173028894/1092850552192368710/1344568307180765245)** (2 messages): 

> `YPerf, Gemini Flash, Llama 3, Claude 3.5 Sonnet` 


- **YPerf Tracks OpenRouter Model Performance**: A member created [YPerf.com](https://yperf.com/) *to monitor model API usage and performance* across OpenRouter.
- **Gemini Flash 1.5 8B benchmarked**: The [Gemini Flash 1.5 8B](https://yperf.com/) ranks #66, costing **$0.04**, with **0.52s** latency and **419.8T/s** throughput on OpenRouter.



**Link mentioned**: <a href="https://yperf.com/">YPerf</a>: no description found

  

---


### **OpenRouter (Alex Atallah) ▷ #[general](https://discord.com/channels/1091220969173028894/1094454198688546826/1344402421824094382)** (389 messages🔥🔥): 

> `Sonnet 3.7 thinking endpoint, DeepSeek R1 reasoning, OpenAI's GPT 4.5 pricing and performance, OpenRouter Documentation` 


- **Sonnet 3.7 :thinking endpoint showing less weirdness**: Members noticed that using the `:thinking` endpoint with **Sonnet 3.7** on OpenRouter seems to reduce weird behavior, possibly due to the endpoint enabling reasoning by default with a minimum budget of **1024 tokens**.
   - One member reported seeing `"native_tokens_reasoning": 171,` in requests, indicating reasoning traces, and suggested that **3.7** might be designed for thinking tokens.
- **DeepSeek R1's thought chains via API**: Users discussed how to access **DeepSeek R1's** thought chains through the API, with a member recommending the `include_reasoning` parameter.
   - It was also noted that some content tokens might slip into the reasoning token, and the recommendation was to *'double check thinking tags and never forget them'*.
- **GPT 4.5's high price riles up community**: The community reacted strongly to the pricing of **GPT 4.5** (**$75 input**, **$150 output**), with many calling it *insane* and questioning its value compared to models like **Grok3** and **Claude Sonnet 3.7**.
   - Some speculated it was a *failed attempt at gpt5*, while others believed it was a measure against distillation, making the exorbitant cost unjustifiable.
- **OpenRouter adds documentation for access and features**: A user requested documentation about OpenRouter's functionality and architecture and [documentation was shared](https://openrouter.ai/docs/quickstart), offering insights into usage, API access, and supported features.
   - Another user inquired about the availability of prompt caching with Vertex AI, and it was confirmed this was available for almost a month with tips on where to view the activity.
- **User builds CAD app with OpenSCAD clone**: One member is building [a CAD app in the browser](https://feep.life/~feep/fncad/) that's an OpenSCAD clone with a different backend.
   - The language supports basic syntax like `var x = 42;`, operators like `+ - * /`, basic shapes like `sphere(radius);`, SDF operators, transformations, and boolean operations.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://livebench.ai">LiveBench</a>: no description found</li><li><a href="https://x.com/ivanfioravanti/status/1895217553380950402">Tweet from Ivan Fioravanti ᯅ (@ivanfioravanti)</a>: 75$ input / 150$ output for this.Quoting Aidan McLaughlin (@aidan_mclau) obligatory unicorn eval1. gpt-4.52. gpt-4o3 claude-3.7-sonnet (thinking)</li><li><a href="https://openrouter.ai/docs/use-cases/reasoning-tokens">Reasoning Tokens - Improve AI Model Decision Making</a>: Learn how to use reasoning tokens to enhance AI model outputs. Implement step-by-step reasoning traces for better decision making and transparency.</li><li><a href="https://docs.anthropic.com/en/docs/about-claude/models/extended-thinking-models#extended-output-capabilities-beta">Extended thinking models - Anthropic</a>: no description found</li><li><a href="https://x.com/theojaffee/status/1895222825700532606">Tweet from Theo (@theojaffee)</a>: I had early access to GPT-4.5. I found it to be by far the highest verbal intelligence model I&#39;ve ever used. It&#39;s an outstanding writer and conversationalist, and excels at what I call &#34;co...</li><li><a href="https://x.com/SwayStar123/status/1895183724268134878">Tweet from sway (@SwayStar123)</a>: gpt 4.5 system card https://cdn.openai.com/gpt-4-5-system-card.pdf</li><li><a href="https://fxtwitter.com/multimodalart/status/1895227785381400953">Tweet from apolinario 🌐 (@multimodalart)</a>: The evals they didn&#39;t show you How does GPT 4.5 compare with latest non-thinking models:Sonnet 3.7 (no thinking), Deepseek V3 (not R1!), Grok 3 (no thinking)</li><li><a href="https://x.com/AndrewCurran_/status/1894355918621749402">Tweet from Andrew Curran (@AndrewCurran_)</a>: Deepseek R2 is arriving early.</li><li><a href="https://feep.life/~feep/fncad/">fnCAD: Geometry from Signed Distance Fields</a>: no description found</li><li><a href="https://openrouter.ai/docs/quickstart">OpenRouter Quickstart Guide</a>: Get started with OpenRouter&#x27;s unified API for hundreds of AI models. Learn how to integrate using OpenAI SDK, direct API calls, or third-party frameworks.</li><li><a href="https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/claude-prompt-caching">no title found</a>: no description found
</li>
</ul>

</div>
  

---


### **LM Studio ▷ #[general](https://discord.com/channels/1110598183144399058/1110598183144399061/1344403459029467196)** (278 messages🔥🔥): 

> `Robotics DIY, LLM backend website, Grok-3 performance vs O3, DeepSeek political controversy, OpenAI defense contracts` 


- **DIY Robotics Arm Excites Hobbyists**: A member suggests building a robotics arm from scratch to learn about **servos, CAD, and microcontrollers** and recommends a [$100 Creality Ender 3 V2 printer](https://www.creality.com/products/ender-3-v2-3d-printer) from Microcenter.
   - They suggest skipping to **transformers** for ML and highlights [multiple open-access courses from top universities like Stanford](https://online.stanford.edu/) and videos from Karpathy (ex OpenAI, Tesla) for learning ML.
- **LLM Backends for Websites Debated**: Members discussed how to implement an **LLM in a website**, with suggestions including using **websockets, SSR, AnythingLLM**, and code editors like **Cursor and Continue.dev**.
   - It was clarified that hosting a website on **GitHub Pages** would require the LLM to be hosted elsewhere (*Azure, cloud, ngrok*), sparking frustration and a humorous exchange.
- **Grok-3 performance beats O3**: Members discuss the surprisingly good performance of **Grok-3** vs the previous O3 model on various benchmarks, and wondered if [X.ai's benchmarks](https://x.ai/documents/2025.02.20-RMF-Draft.pdf) were accurate or misleading.
   - The users debated if Grok-3 was rushed to market without proper ethical red-teaming, while others argued that Grok 3 is a beta, monitored, and not on API due to safety reasons.
- **DeepSeek's Politically Charged Responses Spark Debate**: Members debated whether **DeepSeek's** censorship of certain Chinese historical events is unethical, with some arguing it's a necessary self-preservation measure.
   - One member argued that *building an AI off of dishonesty is a failure*, while another countered that censoring specific topics isn't a significant issue as the model excels in other areas and that one could access a [DeepSeek-R1 reasoning model that has been post-trained by Perplexity AI to remove Chinese Communist Party censorship](https://huggingface.co/perplexity-ai/r1-1776/tree/main).
- **OpenAI's Defense Partnerships Stir Ethical Concerns**: Members reacted to news that **OpenAI is working with the military and defense industry**, a reversal of their original stance, and their [new partnership with Anduril](https://openai.com/blog/anduril-industries-and-openai-to-bring-ai-powered-innovation-to-the-defense-sector).
   - Some find the lack of oversight and potential for weaponization concerning, while others mention Ilya Sutskever, the ex-Chief Scientist of OpenAI who left to start his own safety-focused AI company, Safe Superintelligence (SSI).


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://huggingface.co/learn/nlp-course/chapter1/1">Introduction - Hugging Face NLP Course</a>: no description found</li><li><a href="https://tenor.com/view/toby-cry-phone-spider-man-cry-phone-spider-man-phone-toby-phone-gif-12875606672124040541">Toby Cry Phone Spider Man Cry Phone GIF - Toby Cry Phone Spider man Cry Phone Spider man phone - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://x.com/multimodalart/status/1894842951353671750">Tweet from apolinario 🌐 (@multimodalart)</a>: LLaDA (the first Large Language Diffusion Model) is *just* out 💥 and I&#39;ve built a demo, try out now 👨‍💻It&#39;s mesmerizing to watch the diffusion process 🌀, and it being a diffusion model giv...</li><li><a href="https://tenor.com/view/spongebob-worship-worshipping-now-bowing-gif-12297363">Spongebob Worship GIF - Spongebob Worship Worshipping - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://www.youtube.com/watch?v=cfRYp0nItZ8">Introduction to GPT-4.5</a>: Mia Glaese, Rapha Gontijo Lopes, Youlong Cheng, Jason Teplitz, and Alex Paino introduce and demo GPT-4.5.</li><li><a href="https://huggingface.co/IntelligentEstate/Baby_Grok3-1.5b-iQ4_K_M-GGUF/tree/main">IntelligentEstate/Baby_Grok3-1.5b-iQ4_K_M-GGUF at main</a>: no description found</li><li><a href="https://github.com/YorkieDev/LMStudioWebUI">GitHub - YorkieDev/LMStudioWebUI: A wip version of a simple Web UI to use with LM Studio</a>: A wip version of a simple Web UI to use with LM Studio - YorkieDev/LMStudioWebUI</li><li><a href="https://world-nuclear.org/information-library/current-and-future-generation/outline-history-of-nuclear-energy">Outline History of Nuclear Energy - World Nuclear Association</a>: no description found</li><li><a href="https://huggingface.co/perplexity-ai/r1-1776/">perplexity-ai/r1-1776 · Hugging Face</a>: no description found</li><li><a href="https://www.youtube.com/watch?v=DRkHAw58irI"> - YouTube</a>: no description found
</li>
</ul>

</div>
  

---


### **LM Studio ▷ #[hardware-discussion](https://discord.com/channels/1110598183144399058/1153759714082033735/1344412197727633520)** (41 messages🔥): 

> `Framework desktop, Unified RAM, AMD Ryzen AI, GPU Pricing` 


- **Framework Desktop Gains Traction**: A user pre-ordered a [Framework desktop](https://frame.work/desktop) to experiment with **LM Studio server** and **Tailscale** for an iPhone chat app, Docker, and webservers.
   - Some expressed concerns about waiting until summer for the product, with one noting it will likely be joined by a dozen other mini PCs with the same SoC by then.
- **Framework Desktop's Unified RAM Intriguing**: The [Framework desktop](https://frame.work/desktop) features **unified RAM** between the CPU and GPU, offering up to **128GB** of shared memory, with approximately **90GB** available for the GPU.
   - One user likened it to a MAC setup, highlighting the appeal of unified RAM in a PC.
- **GMK's Ryzen AI Max Mini-PC Unveiled**: [GMK](https://wccftech.com/gmk-announces-worlds-first-mini-pc-based-on-amd-ryzen-ai-9-max/) announced the world's first mini-PC based on **AMD Ryzen AI 9 Max+ 395**, expected to hit the market in the first or second quarter.
   - This mini-PC will feature **Zen 5 architecture** with up to a **16-core/32-thread** configuration and powerful integrated graphics based on the **RDNA 3.5 architecture**.
- **AMD's GPU Pricing Strategy Under Scrutiny**: A [YouTube video](https://www.youtube.com/watch?v=ekKQyrgkd3c) urges AMD to aggressively price its upcoming **RX 9070** and **9070 XT GPUs** to gain market share from Nvidia.
   - The video highlights Nvidia's **90% GPU market share** and argues that AMD should undercut Nvidia significantly to capitalize on recent missteps, instead of its typical *Nvidia minus $50* strategy.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://www.youtube.com/watch?v=GZfFPI8LJrc">All You Need for Gaming – AMD RDNA™ 4 and RX 9000 Series Reveal</a>: The moment is now—AMD RDNA™ 4 graphics cards have arrived, delivering breakthrough performance to every battle, mission, and victory empowering you to make e...</li><li><a href="https://www.youtube.com/watch?v=ekKQyrgkd3c">AMD, Don&#39;t Screw This Up</a>: Brought to you by US. Use code &quot;ABOUTFKNTIME&quot; for 10% off anything on the GN store while the code is active! https://store.gamersnexus.net/We&#39;ve been coverin...</li><li><a href="https://wccftech.com/gmk-announces-worlds-first-mini-pc-based-on-amd-ryzen-ai-9-max/">GMK Announces World&#039;s First Mini-PC Based On AMD Ryzen AI 9 Max+ 395 Processor, Availability Will Be In H1 2025</a>: GMK has announced that it is preparing the world&#039;s first mini-PC, featuring the Strix Halo Ryzen AI 9 Max+ 395 processor.</li><li><a href="https://www.gmktec.com/products/amd-ryzen%E2%84%A2-al-9-hx-370-evo-x1-ai-mini-pc?spm=..index.shoplazza%3A%2F%2Fapps%2Fpage-builder%2Fblocks%2Fcustom-469481730915439250%2F002b91fdd298834656652cb4e068af48_1.1">AMD Ryzen™ Al 9 HX 370 --EVO-X1 AI Mini PC</a>: AMD Ryzen&trade; Al 9 HX 370 | Radeon 890M | Oculink Port | The Ryzen&trade; AI processor, with AMD&#039;s XDNA2 architecture, delivers 50 AI TOPS, doubling power efficiency and offering 5x the AI per...</li><li><a href="https://wccftech.com/gmk-announces-worlds-first-mini-pc-based-on-amd-ryzen-ai-">GMK Announces World&#039;s First Mini-PC Based On AMD Ryzen AI 9 Max+ 395 Processor, Availability Will Be In H1 2025</a>: GMK has announced that it is preparing the world&#039;s first mini-PC, featuring the Strix Halo Ryzen AI 9 Max+ 395 processor.
</li>
</ul>

</div>
  

---


### **Interconnects (Nathan Lambert) ▷ #[news](https://discord.com/channels/1179127597926469703/1179128538679488533/1344408752387919915)** (274 messages🔥🔥): 

> `Claude Annual Subscriptions, Microsoft Phi-4 Models, GPT-4.5 System Card, OpenAI Livestream, Meta AI Standalone App` 


- **Claude Pro Annual Plan Promo**: Anthropic is experimenting with a new **Claude** web app promotion, offering a *limited time offer* for a year of **Claude Pro** at a special price if switching to an annual plan by a specific end date, prompting a reminder not to buy annual subs for AI services from a user.
   - As another user notes, they have been there, done that, and regretted and never used an annual subscription before.
- **Microsoft Launches Phi-4-multimodal and Phi-4-mini**: Microsoft announced the **Phi-4 family** of small language models (**SLMs**), including **Phi-4-multimodal** (processes speech, vision, and text) and **Phi-4-mini** (excels in text-based tasks), available in [Azure AI Foundry](https://aka.ms/try-phi), [HuggingFace](https://aka.ms/phi-4-multimodal/hf), and the [NVIDIA API Catalog](https://aka.ms/phi-4-multimodal/nvidia).
   - Some users doubt claims that it has similar multimodal performance to **Gemini Flash** lite, and also that Microsoft should rename the product line, as they will never escape their *karmic stain*.
- **Leaked GPT-4.5 System Card**: A user shared the **GPT-4.5 System Card**, indicating that interacting with **GPT-4.5 feels more natural** and that *internal testers report GPT-4.5 is warm, intuitive, and natural*. The [system card](https://cdn.openai.com/gpt-4-5-system-card.pdf) notes that it's OpenAI's largest LLM, improving GPT-4's computational efficiency by more than 10x.
   - A user calls the card very boring, while another interprets the card to indicate a **GPT4.5**: creative writooor while **Sonnet 3.5** is a problem solver.
- **OpenAI launches GPT-4.5, Character gets Mainstream**: OpenAI launched **GPT-4.5** as a research preview, available to OpenAI Pro users and API developers with image + text in, text out and same context as 4o model, trained till June 2024. [Here is the official announcement](https://openai.com/index/introducing-gpt-4-5/).
   - One user says character/personality is becoming a mainstream topic, and OpenAI *aggressively used low-precision training*.  Another questions how big is the model with that pricing.
- **GPT-4.5 Performance and Pricing Cause Community Reactions**: Early benchmarks of **GPT-4.5** show it being outperformed by **o1** on several problems, indicating pre-training isn't the optimal place to spend compute in 2025, but one user notes the hallucination metrics are very good.  Pricing of **GPT-4.5** is expensive at $75.00 per million input tokens and $150/million for output, prompting one user to state this must be the end of scaling.
   - Another user believes in 1-2 years this will be the default model size.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://fxtwitter.com/PITTI_DATA/status/1894892551003337202">Tweet from PITTI (@PITTI_DATA)</a>: Playing out as planned. A little early thanks to Deepseek</li><li><a href="https://x.com/ChaseBrowe32432/status/1894804915983110302">Tweet from Chase Brower (@ChaseBrowe32432)</a>: @teortaxesTex So uh... this happened</li><li><a href="https://every.to/chain-of-thought/gpt-4-5-won-t-blow-your-mind-it-might-befriend-it-instead">GPT-4.5 Won’t Blow Your Mind. It Might Befriend It Instead.</a>: We’ve been testing the latest model for a few days. Here’s what we found.</li><li><a href="https://x.com/RichardSocher/status/1895170846232322541">Tweet from Richard Socher (@RichardSocher)</a>: We’re excited to introduce ARI (Advanced Research & Insights) - the first professional-grade deep research agent purpose-built for business.Instead of spending $100K+ on whitepapers and analyses that ...</li><li><a href="https://x.com/distributionat/status/1895010395548721210">Tweet from thomas (@distributionat)</a>: no system prompt for either, no thinking for 3.7prompt is a wikipedia snippet that i translate with a zoomer styleit just asks to translate &#34;in the same style&#34; and doesn&#39;t specify what tha...</li><li><a href="https://x.com/mn_google/status/1895045714314772681">Tweet from Patel Meet (@mn_google)</a>: GPT-4.5 spotted in ChatGPT web build!</li><li><a href="https://x.com/teortaxesTex/status/1895139184870068690">Tweet from Teortaxes▶️ (DeepSeek 推特🐋铁粉 2023 – ∞) (@teortaxesTex)</a>: &gt; in 2 daysDesperate times, desperate measuresMoonshot decided that if they achieve singularity internally, there&#39;s no way DeepSeek steals their thunder on X againQuoting Tiezhen WANG (@Xianbao...</li><li><a href="https://x.com/adcock_brett/status/1895175400160133543">Tweet from Brett Adcock (@adcock_brett)</a>: Important update: Figure is launching robots into the homeOur AI, Helix, is advancing faster than any of us anticipated, accelerating our timeline into the homeTherefore, we&#39;ve moved-up our home t...</li><li><a href="https://x.com/OpenAI/status/1895134318835704245">Tweet from OpenAI (@OpenAI)</a>: Livestream in 4.5 hours.</li><li><a href="https://x.com/btibor91/status/1894848550607167623">Tweet from Tibor Blaho (@btibor91)</a>: New Claude web app experiment: &#34;wombat annual plan promo&#34;&#34;Limited time offer: A year of Claude Pro for lessSwitch to an annual plan by {endDate} to unlock a special price.&#34;</li><li><a href="https://x.com/distributionat/status/1895010393271284165">Tweet from thomas (@distributionat)</a>: i made a nanobenchmark to suss out if sonnet 3.7 is worse at understanding me than 3.5with 3.7 i rephrase instructions more than with 3.5; it just doesn&#39;t seem to &#34;get&#34; what i&#39;m asking...</li><li><a href="https://x.com/scaling01/status/1895180786799911413">Tweet from Lisan al Gaib (@scaling01)</a>: GPT-4.5 MMLU performance</li><li><a href="https://x.com/eliebakouch/status/1895136704077463768">Tweet from elie (@eliebakouch)</a>: LET&#39;S GOOO, we&#39;ve just release 50+ intermediate checkpoints for ALL the SmolLM2 models 🔥</li><li><a href="https://x.com/stalkermustang/status/1895196743391739987">Tweet from Igor Kotenkov (@stalkermustang)</a>: My 2 cents in the light of my predictions from today:— as expected, the model is worse than the reasoners: sometimes it even loses to o1 and o3-mini.— its agent skills (using tools) also fall short of...</li><li><a href="https://x.com/karpathy/status/1895213020982472863">Tweet from Andrej Karpathy (@karpathy)</a>: GPT 4.5 + interactive comparison :)Today marks the release of GPT4.5 by OpenAI. I&#39;ve been looking forward to this for ~2 years, ever since GPT4 was released, because this release offers a qualitat...</li><li><a href="https://www.youtube.com/watch?v=cfRYp0nItZ8"> - YouTube</a>: no description found</li><li><a href="https://github.com/Tencent/llm.hunyuan.turbo-s">GitHub - Tencent/llm.hunyuan.turbo-s</a>: Contribute to Tencent/llm.hunyuan.turbo-s development by creating an account on GitHub.</li><li><a href="https://x.com/Qusaismael/status/1895214415076811162">Tweet from Qusai Ismael (@Qusaismael)</a>: @TheXeophon so machine words are now more valuable than human&#39;s?</li><li><a href="https://x.com/arcprize/status/1895206472004591637">Tweet from ARC Prize (@arcprize)</a>: GPT-4.5 Results on ARC-AGISemi Private Set (100 hold out tasks):* Score: 10.33%* Average Cost per Task: $0.29</li><li><a href="https://simonwillison.net/2025/Feb/27/introducing-gpt-45/">Initial impressions of GPT-4.5</a>: GPT-4.5 is out today as a “research preview”—it’s available to OpenAI Pro ($200/month) customers and to developers with an API key. OpenAI also published a GPT-4.5 system card. I’ve started …</li><li><a href="https://fxtwitter.com/AIatMeta/status/1895187608969584660">Tweet from AI at Meta (@AIatMeta)</a>: Introducing Aria Gen 2, next generation glasses that we hope will enable researchers from industry and academia to unlock new work in machine perception, contextual AI, robotics and more.Aria Gen 2 de...</li><li><a href="https://x.com/jajazoon/status/1895216844610642080">Tweet from Gojozoon (@jajazoon)</a>: @GolerGkA @TheXeophon $30/$60 for normal GPT-4, $60/$120 for GPT-4 with 32K context</li><li><a href="https://tenor.com/view/this-is-fine-fire-house-burning-okay-gif-5263684">This Is Fine Fire GIF - This Is Fine Fire House - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://x.com/benhylak/status/1895212184092975276">Tweet from ben (@benhylak)</a>: compare the output for the same prompt to gpt 4o below. 4o is complete AI slop. it&#39;s not even close. it&#39;s not even in the same universe.this is the first time i&#39;ve ever thought ai writing ...</li><li><a href="https://x.com/karpathy/status/1895213028418920534">Tweet from Andrej Karpathy (@karpathy)</a>: Question 2</li><li><a href="https://x.com/taker_of_whizz/status/1894775460602540147?s=46&t=Y6KMaD0vAihdhw7S8bL5WQ">Tweet from whizz taker (@taker_of_whizz)</a>: GPT-4.5 tomorrow, MoE universal transformer with 1T active parameters, 120T tokens</li><li><a href="https://x.com/simonw/status/1895210413148803551?s=46&t=Y6KMaD0vAihdhw7S8bL5WQ">Tweet from Simon Willison (@simonw)</a>: GPT 4.5 just told me it has a training cut-off date of October 2023, is that true? https://github.com/simonw/llm/issues/795#issuecomment-2689038127It also made me this pelican</li><li><a href="https://x.com/benhylak/status/1895212181597397493">Tweet from ben (@benhylak)</a>: i&#39;ve been testing gpt 4.5 for the past few weeks.it&#39;s the first model that can actually write. this is literally the midjourney-moment for writing.(comparison to gpt 4o below)</li><li><a href="https://x.com/paulgauthier/status/1895221869844013108">Tweet from Paul Gauthier (@paulgauthier)</a>: GPT-4.5 Preview scored 45% on aider&#39;s polyglot coding benchmark.65% Sonnet 3.7, 32k think tokens (SOTA)60% Sonnet 3.7, no thinking48% DeepSeek V345% GPT 4.5 Preview27% ChatGPT-4o23% GPT-4ohttps://...</li><li><a href="https://x.com/sam_paech/status/1895220802376884445">Tweet from Sam Paech (@sam_paech)</a>: I&#39;ve been working on an EQ-Bench successor. Here&#39;s some preliminary results, including GPT-4.5-preview.This time around it&#39;s a LLM-judged task, where the task is to mediate conflict in var...</li><li><a href="https://x.com/_xjdr/status/1895184402281570450">Tweet from xjdr (@_xjdr)</a>: huge if it holds up in practice</li><li><a href="https://x.com/soldni/status/1895225893062381712">Tweet from Luca Soldaini 🎀 (@soldni)</a>: GPT 4.5 is SOTA on ButtBenchZitiert Luca Soldaini 🎀 (@soldni) ButtBench update: o1-preview though really hard and got SOTA; but we are still far from human performance</li><li><a href="https://x.com/bobmcgrewai/status/1895228291981943265">Tweet from Bob McGrew (@bobmcgrewai)</a>: That o1 is better than GPT-4.5 on most problems tells us that pre-training isn&#39;t the optimal place to spend compute in 2025. There&#39;s a lot of low-hanging fruit in reasoning still.But pre-train...</li><li><a href="https://www.cnbc.com/2025/02/27/meta-plans-to-release-a-standalone-meta-ai-app.html">Meta plans to release standalone Meta AI app in effort to compete with OpenAI&#x27;s ChatGPT</a>: Meta&#x27;s upcoming AI app advances CEO Mark Zuckerberg&#x27;s plans to make his company the leader in AI by the end of the year, people familiar with the matter said.</li><li><a href="https://azure.microsoft.com/en-us/blog/empowering-innovation-the-next-generation-of-the-phi-family/">Empowering innovation: The next generation of the Phi family | Microsoft Azure Blog</a>: We are excited to announce Phi-4-multimodal and Phi-4-mini, the newest models in Microsoft’s Phi family of small language models. Learn more.</li><li><a href="https://news.ycombinator.com/item?id=43198118">no title found</a>: no description found</li><li><a href="https://docs.google.com/spreadsheets/d/1foc98Jtbi0-GUsNySddvL0b2a7EuVQw8MoaQlWaDT-w">LLM capability, cost, &amp; throughput (www.harlanlewis.com)</a>: no description found
</li>
</ul>

</div>
  

---


### **Interconnects (Nathan Lambert) ▷ #[ml-drama](https://discord.com/channels/1179127597926469703/1181746144821387334/1344736973654331442)** (4 messages): 

> `Anthropic data collection, Alignment for monitoring` 


- **Anthropic Accused of Data Collection Shenanigans**: A user accused **Anthropic** of *sneaky* data collection from the Computer Use API, using it to train classifiers for corporate ethical guidelines, and updating their website to appear transparent, according to [this fxtwitter thread](https://fxtwitter.com/elder_plinius/status/1895177131576918200).
- **Alignment Monitoring's Data Origins Unclear**: It was inferred that **Anthropic** used user data based on their [summarization for monitoring blogpost](https://alignment.anthropic.com/2025/summarization-for-monitoring/); although, a user pointed out that the data source for training remains unspecified.



**Link mentioned**: <a href="https://fxtwitter.com/elder_plinius/status/1895177131576918200">Tweet from Pliny the Liberator 🐉󠅫󠄼󠄿󠅆󠄵󠄐󠅀󠄼󠄹󠄾󠅉󠅭 (@elder_plinius)</a>: sneaky sneaky, @AnthropicAIcollecting user data from everyone that used the Computer Use API without informed consent or an opt-out option is dirty workusing that data to then train a classifier to im...

  

---


### **Interconnects (Nathan Lambert) ▷ #[random](https://discord.com/channels/1179127597926469703/1183121795247779910/1344534452897976380)** (19 messages🔥): 

> `Claude Code access and potential uses, DeepEP analysis, AI competing on Pokemon Red%, Claude 3.7 Sonnet RL issues` 


- ****Claude Code** Craze & Obsidian Integration**: A member is curious about **Claude Code** access and is considering using it within their Obsidian vault, coupled with Google Calendar and Gmail MCPs, to [organize their life](https://mp.weixin.qq.com/s/1Cz7oQbVkPMam3eoKQWz0w).
- ****DeepEP** Deconstructed & Hardware Caveats**: A member shared an analysis of **DeepEP**, noting it as a valuable work with many details to learn from, but also pointing out [hardware limitations](https://mp.weixin.qq.com/s/1Cz7oQbVkPMam3eoKQWz0w) that are better understood in conjunction with suggestions from the DeepSeek-V3 paper.
- ****MissingNo** Mayhem & Model Misbehavior**: A member joked about AI companies competing on **Pokemon Red%**, predicting a model will exploit a bug like *MissingNo*, causing safety concerns due to widespread guides, even suggesting the possibility of China releasing such a model in real life.
   - This comment was followed by a link to a [depressing result from Claude](https://claude.ai/share/7962a1cc-ddb8-40db-8423-faa0dc826d10), where the implementation doesn't stick to the reasoning trace; another user noted that R1 can do this more reliably, with a [sample Claude output](https://claude.ai/share/daa388d6-5a77-4e75-ba51-769402e5bb8d).
- ****Sonnet 3.7** Stumbles & Rule Rejection**: A member shared their experience using **Claude 3.7 Sonnet** in Cursor, finding it over-confident and prone to ignoring rules, echoing [Catalin's sentiments](https://x.com/alex_peys/status/1895179492664156277) of the model being worse than 3.5 due to its addiction to the reward signal.
   - This was juxtaposed against the expectation of a higher-EQ *beeeg 4.5 model*, with a link to a [tweet celebrating teortaxes' victory](https://x.com/jakehalloran1/status/1895199906387955714).


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://claude.ai/share/7962a1cc-ddb8-40db-8423-faa0dc826d10">Claude</a>: Talk with Claude, an AI assistant from Anthropic</li><li><a href="https://claude.ai/share/daa388d6-5a77-4e75-ba51-769402e5bb8d">Claude</a>: Talk with Claude, an AI assistant from Anthropic</li><li><a href="https://x.com/alex_peys/status/1895179492664156277">Tweet from alex peysakhovich 🤖 (@alex_peys)</a>: all these heavily RL trained models (i assume sonnet 3.7 is heavy on the RL like o1/r1/etc...) are soooo addicted to the reward signal they&#39;ll keep trying ANYTHING to get tasks done, it&#39;s actu...</li><li><a href="https://x.com/jakehalloran1/status/1895199906387955714">Tweet from Jake Halloran (@jakehalloran1)</a>: total @teortaxesTex victory</li><li><a href="https://mp.weixin.qq.com/s/1Cz7oQbVkPMam3eoKQWz0w">分析一下EP并行和DeepSeek开源的DeepEP代码</a>: no description found
</li>
</ul>

</div>
  

---


### **Interconnects (Nathan Lambert) ▷ #[memes](https://discord.com/channels/1179127597926469703/1187551504995987576/1344551687934709770)** (10 messages🔥): 

> `GPT-4.5 release, DeepSeek r1, Claude Code ls node_modules, Gary Marcus GPT-4.5` 


- **OpenAI skips GPT-4.5 and goes to OpenAI Five**: Twitter user noted OpenAI skipped **GPT-4.5** and went straight to [**"OpenAI Five"**](https://x.com/NotTuxedoSam/status/1894942016750133483).
- **GPT 4.5 can hold your hand**: A user jokes about the **DeepSeek r1** release, claiming **Grok 3** beats every benchmark, and **GPT 4.5** *can hold my hand when I am scared* [according to this tweet](https://x.com/samsja19/status/1895193608350830885).
- **Claude code executes ls in node_modules**: A user shared that **Claude Code** decided to `ls` in `node_modules` [according to this tweet](https://x.com/andrew_n_carr/status/1895217760411754552).
- **GPT 4.5 is nothingburger says Gary Marcus**: **Gary Marcus** wrote a [Substack article](https://garymarcus.substack.com/p/hot-take-gpt-45-is-a-nothing-burger) claiming that **GPT-4.5** is a *nothing burger* and **GPT 5** is still a fantasy.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://x.com/andrew_n_carr/status/1895217760411754552">Tweet from Andrew Carr (e/🤸) (@andrew_n_carr)</a>: claude code decided to `ls` in `node_modules`</li><li><a href="https://garymarcus.substack.com/p/hot-take-gpt-45-is-a-nothing-burger">Hot take: GPT 4.5 is a nothing burger</a>: Pure scaling in shambles</li><li><a href="https://x.com/nabeelqu/status/1895205660029243860">Tweet from Nabeel S. Qureshi (@nabeelqu)</a>: For the confused, it&#39;s actually super easy:- GPT 4.5 is the new Claude 3.6 (aka 3.5)- Claude 3.7 is the new o3-mini-high- Claude Code is the new Cursor- Grok is the new Perplexity- o1 pro is the &...</li><li><a href="https://x.com/NotTuxedoSam/status/1894942016750133483">Tweet from tuxedo sam (@NotTuxedoSam)</a>: holy fuck they skipped GPT-4.5 and went straight to &#34;OpenAI Five&#34;</li><li><a href="https://x.com/samsja19/status/1895193608350830885">Tweet from samsja (@samsja19)</a>: deepseek r1 release: open source o1grok 3 release: beats every benchmarkgpt 4.5 release: Can hold my hand when I am scared
</li>
</ul>

</div>
  

---


### **Interconnects (Nathan Lambert) ▷ #[reads](https://discord.com/channels/1179127597926469703/1214764639397617695/1344717270374547499)** (3 messages): 

> `Alignment, Realism-grounded alignment` 


- **Anthropic Reveals Alignment Monitoring via Summarization**: Anthropic posts about [Alignment Monitoring via Summarization](https://alignment.anthropic.com/2025/summarization-for-monitoring/) for their alignment techniques.
- **Realism-Grounded Alignment Gets Thumbs Up**: A member expressed a preference for *realism-grounded* alignment approaches.


  

---


### **Interconnects (Nathan Lambert) ▷ #[posts](https://discord.com/channels/1179127597926469703/1228051082631188530/1344837063069012028)** (2 messages): 

> `olmOCR vs Top PDF tools, Pairwise judgments and Elo score` 


- **olmOCR Dominates PDF Processing**: **Allen AI's olmOCR** tool outperforms top **PDF processing tools** in human evaluations using [pairwise judgments](https://x.com/allen_ai/status/1894415494004035814).
- **Pairwise Ranking Decoded**: A member clarified that the y-axis on the linked chart likely represents an **Elo score**, inferred from the mention of *pairwise ranking* in the **olmOCR** comparison.



**Link mentioned**: <a href="https://x.com/allen_ai/status/1894415494004035814">Tweet from Ai2 (@allen_ai)</a>: olmOCR dominates the competition! Our human evaluation using pairwise judgments against top PDF processing tools show olmOCR&#39;s rating significantly above other tools. Don&#39;t take our word for i...

  

---


### **Latent Space ▷ #[ai-general-chat](https://discord.com/channels/822583790773862470/1075282825051385876/1344407651127132291)** (133 messages🔥🔥): 

> `Speak AI revenue graph, Hume AI's Octave text-to-speech LLM, Levelsio flying project, Perplexity Sonar API Deep Research, Firecrawl Deep Research API` 


- **Speak AI's Novel Exponential Revenue**: Paul Graham shared a [revenue graph](https://x.com/paulg/status/1894827577325560215) showing a novel variant of exponential growth, where a company selling a *new year's resolution* product sees sustained usage due to its effectiveness.
   - Swyx noted this observation, highlighting the company's unique growth pattern.
- **Hume AI Releases Octave Text-to-Speech LLM**: Hume AI launched [Octave](https://x.com/hume_ai/status/1894833497824481593), a new **LLM** for text-to-speech that can design voices with prompts and control emotion and delivery, with a creator studio for long-form content production.
   - It understands how meaning affects delivery to generate emotional, human-like speech, unlike traditional TTS systems.
- **Inception Labs releases Mercury dLLM**: Inception Labs introduced [Mercury](https://x.com/InceptionAILabs/status/1894847919624462794), the first commercial-grade **diffusion large language model (dLLM)**, which promises parallel, coarse-to-fine text generation.
   - Karpathy commented that this model has the potential to be different, and possibly showcase new, unique psychology, or new strengths and weaknesses, [encouraging people to try it out](https://x.com/karpathy/status/1894923254864978091).
- **MCP: Tool Calling Renaissance**: There are contrasting views on MCP's value prop, Greg Kamradt suggests developers *jump on the Anthropic MCP train and build*, while others find *the dev experience sucks*.
   - Members defined **MCP as a tool call with your own tools**, or potentially use tools other people have built without wanting to figure out their *underlying* API.
- **Karpathy Teaches LLMs**: Andrej Karpathy released a [2h11m YouTube video](https://x.com/karpathy/status/1895242932095209667) on *How I Use LLMs*, covering a practical guide to the **LLM ecosystem** with examples, including tool use, file uploads, audio/video I/O, memory, and custom GPTs.
   - Chapters include: ChatGPT interaction, tool use (internet search, deep research, Python interpreter), Claude Artifacts, Cursor Composer, Speech I/O, NotebookLM, and image/video I/O.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://x.com/InceptionAILabs/status/1894847919624462794">Tweet from Inception Labs (@InceptionAILabs)</a>: We are excited to introduce Mercury, the first commercial-grade diffusion large language model (dLLM)! dLLMs push the frontier of intelligence and speed with parallel, coarse-to-fine text generation.</li><li><a href="https://chat.inceptionlabs.ai">Mercury Coder</a>: no description found</li><li><a href="https://alterhq.com/docs#workspaces">Alter | AI For Your Entire Workday</a>: Alter: The seamless AI that supercharges your Mac. Skip the chat, execute instant actions across all apps. 10x your productivity with complete privacy control.</li><li><a href="https://x.com/hume_ai/status/1894833497824481593?s=46">Tweet from Hume (@hume_ai)</a>: Today, we’re releasing Octave: the first LLM built for text-to-speech.🎨Design any voice with a prompt🎬 Give acting instructions to control emotion and delivery (sarcasm, whispering, etc.)🛠️Produce ...</li><li><a href="https://x.com/firecrawl_dev/status/1895156300612603918">Tweet from Firecrawl (@firecrawl_dev)</a>: Announcing the Firecrawl Deep Research API 🔎A complete research API that allows you to easily build deep research into your own applications.Join the waitlist below!</li><li><a href="https://x.com/paulg/status/1894827577325560215?s=46">Tweet from Paul Graham (@paulg)</a>: Here&#39;s what happened to that startup&#39;s revenue graph in the next year (in blue).Quoting Paul Graham (@paulg) A novel variant of exponential revenue graph. This company is selling something use...</li><li><a href="https://x.com/levelsio/status/1894848949082825176?s=46">Tweet from @levelsio (@levelsio)</a>: I think 5000 people flying but I also see some bots 😅Quoting Thomas Slabbers (@Thomasslabbers) This is pure genius - look at how many people are flying right now! I also found Mars. Pieter this might...</li><li><a href="https://x.com/karpathy/status/1895242932095209667?s=46">Tweet from Andrej Karpathy (@karpathy)</a>: New 2h11m YouTube video: How I Use LLMsThis video continues my general audience series. The last one focused on how LLMs are trained, so I wanted to follow up with a more practical guide of the entire...</li><li><a href="https://www.reddit.com/r/ClaudeAI/comments/1h4yvep/mcp_filesystem_is_magic/">Reddit - Dive into anything</a>: no description found</li><li><a href="https://x.com/aravsrinivas/status/1894471526449385687?s=46">Tweet from Aravind Srinivas (@AravSrinivas)</a>: We’re making Deep Research available as an endpoint to all developers through the Perplexity Sonar API to help people build their custom research agents and workflows! Excited to see what people are g...</li><li><a href="https://x.com/addyosmani/status/1894814414102282747">Tweet from Addy Osmani (@addyosmani)</a>: Can you accurately transcribe fast speech? Tested @elevenlabsio&#39; new Speech-to-Text model (Scribe) with Eminem&#39;s &#34;Rap God&#34; (4.28 words/sec!) & it nailed it. Great quality and supports ...</li><li><a href="https://x.com/karpathy/status/1894923254864978091?s=46">Tweet from Andrej Karpathy (@karpathy)</a>: This is interesting as a first large diffusion-based LLM.Most of the LLMs you&#39;ve been seeing are ~clones as far as the core modeling approach goes. They&#39;re all trained &#34;autoregressively&#3...</li><li><a href="https://x.com/quintendf/status/1894868774534422953?s=46">Tweet from Quinten Farmer (@quintendf)</a>: I’m excited to announce Tolan, our first Embodied Companion.With no launch or press we’ve quietly hit 500,000+ downloads, over $1m in ARR, and a #1 app store category ranking.Today I’m also announcing...</li><li><a href="https://x.com/openai/status/1895134318835704245?s=46&t=b7l37rB6wtbyAh6ah1NpZQ">Tweet from OpenAI (@OpenAI)</a>: Livestream in 4.5 hours.</li><li><a href="https://x.com/aravsrinivas/status/1894471526449385687?">Tweet from Aravind Srinivas (@AravSrinivas)</a>: We’re making Deep Research available as an endpoint to all developers through the Perplexity Sonar API to help people build their custom research agents and workflows! Excited to see what people are g...</li><li><a href="https://x.com/nickfloats/status/1894460305507266736">Tweet from Nick St. Pierre (@nickfloats)</a>: In just the past few weeks:o1-pro was SOTADeepseek r1 was SOTAo3‑mini was SOTAGrok 3 was SOTAClaude 3.7 was SOTACan you feel the acceleration?</li><li><a href="https://github.com/go-go-golems/go-go-mcp/pull/9">Add update-ui tool with synchronous UI action handling by wesen · Pull Request #9 · go-go-golems/go-go-mcp</a>: This PR introduces a synchronous UI update system that waits for user actionsbefore completing requests, making it easier to build interactive applications.Key changes:Refactored UI handling in...</li><li><a href="https://x.com/GregKamradt/status/1894931237841838402">Tweet from Greg Kamradt (@GregKamradt)</a>: If you’re a dev looking for a career directionGo jump on the Anthropic MCP train and buildIt’s having a moment and there are 1M best practices to figure outThis is the sign you’ve been waiting for</li><li><a href="https://x.com/frantzfries/status/1895159782220181848">Tweet from Chris Frantz (@frantzfries)</a>: Could somebody please explain why MCP’s are valuableI tried setting up a few, the dev experience sucks and the GitHub’s repos are full of issues saying it sucks after trying themExisting API’s are fas...</li><li><a href="https://techcommunity.microsoft.com/blog/educatordeveloperblog/welcome-to-the-new-phi-4-models---microsoft-phi-4-mini--phi-4-multimodal/4386037">Welcome to the new Phi-4 models - Microsoft Phi-4-mini &amp; Phi-4-multimodal</a>: Phi-4-mini brings significant enhancements in multilingual support, reasoning, and mathematics, and now, the long-awaited function calling feature is finally...
</li>
</ul>

</div>
  

---


### **Latent Space ▷ #[ai-in-action-club](https://discord.com/channels/822583790773862470/1200548371715342479/1344759160453202061)** (166 messages🔥🔥): 

> `GPT 4.5, Claude 3.7 Sonnet, Model Scaling, Open Source, Every Hiring` 


- **GPT-4.5 Watch Party Rough Start**: Members experienced initial technical difficulties and struggled to hear the stream audio of the **GPT-4.5** launch, with some humorously suggesting the presenter was *roasted*.
   - Viewers generally felt the **GPT 4.5** launch stream was a disappointment, with descriptions such as *hostage video* and some saying *this stream is rough* and that the *vibe test failed*.
- **New Scaling Laws HOT SWAP**: OpenAI presentation introduces *new scaling laws*, indicating a change in the *ratio between data and param size* in the post-training stage.
   - They asked themselves during the presentation *are we hitting a wall*.
- **GPT-4.5 Skips API, aims for Therapy**: The new model doesn't have an API, and is focused on heavy-tail, real world edge cases like responding to angry texts, and better use cases.
   - Members were unimpressed with GPT-4.5's example use cases (*everyday queries including texts to send to your friends*).
- **Sonnet 3.7 Overconfident Ignoring Rules**: A member claimed that **Claude 3.7 Sonnet** is worse than **3.5**, as it is *over-confident*, *ignores rules*, and *unnecessarily does more than it needs to do and therefore breaks the code*.
   - They are going back to **3.5**.
- **Every Hires for Cora Calm Inbox**: Every is hiring a full-stack **AI engineer** for Cora, building a calm inbox with over **1,000 daily active users** and **10,000 on the waitlist**.
   - There are also openings for a **growth marketing lead** and a **full-stack designer** for their [website](https://every.to/chain-of-thought/gpt-4-5-won-t-blow-your-mind-it-might-befriend-it-instead).


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://x.com/alex_peys/status/1895179492664156277">Tweet from alex peysakhovich 🤖 (@alex_peys)</a>: all these heavily RL trained models (i assume sonnet 3.7 is heavy on the RL like o1/r1/etc...) are soooo addicted to the reward signal they&#39;ll keep trying ANYTHING to get tasks done, it&#39;s actu...</li><li><a href="https://every.to/chain-of-thought/gpt-4-5-won-t-blow-your-mind-it-might-befriend-it-instead">GPT-4.5 Won’t Blow Your Mind. It Might Befriend It Instead.</a>: We’ve been testing the latest model for a few days. Here’s what we found.</li><li><a href="https://x.com/polynoamial/status/1895205979962384438">Tweet from Noam Brown (@polynoamial)</a>: @swyx Scaling pretraining compute and scaling thinking compute are two different dimensions of improvement. They are complementary, not in competition.
</li>
</ul>

</div>
  

---


### **Nous Research AI ▷ #[general](https://discord.com/channels/1053877538025386074/1149866623109439599/1344407157910667306)** (280 messages🔥🔥): 

> `Apple Intelligence Underwhelming, Efficient CoT, GPT-4.5, MoE Models, Wan2.1 video model` 


- **Wan2.1 rises as Stable Diffusion moment for video**: The release of [Wan2.1](https://github.com/Wan-Video/Wan2.1), an open and advanced large-scale video generative model, has been hailed as *the stable diffusion moment* for video models.
- **Experiments on efficient CoT with Reward Models**: Members discussed methods to make long **Chain of Thought (CoT)** more efficient, including using another LLM to make CoTs more concise and defining a reward function that rewards efficient thoughts, but the consensus is that this is the *problem of the year*.
   - Suggestions included ideas such as **latent CoTs**, **MoE models**, and optimizing for the correct outcome while minimizing excess reasoning tokens; but everyone is noticing that Process Reward Models *kinda suck*.
- **MoE Models Prove Speedier on CPUs**: Members tested **Mixtral**, **Granite**, and **DeepSeek R1** against models such as **Llama 3.2** and **OLMoE**, showing that MoE models are faster and lose less performance when going to pure CPU execution.
   - One user notes that they *highly* recommend **OLMoE** to be thrown on smaller CPU only devices, like a **16GB Raspberry Pi** because there is still value in getting answers back effectively instantly.
- **GPT-4.5 release underwhelming**: **GPT-4.5** has been released, being described as a very large and compute-intensive model, making it more expensive than and not a replacement for **GPT-4o**, with Sam Altman stating that this model *feels like talking to a thoughtful person*.
   - Karpathy claims it has **10x more pretraining compute than GPT-4**, however its use case might be limited given it is overfit on the river crossing puzzle, and more geared towards creative use cases.
- **Apple Intelligence: Big Shift or Big Miss?**: Members discussed **Apple Intelligence**, with some believing it is underwhelming, and also a big shift away from the money coming from business API use over to the money coming from consumers, while one mentioned they're in an *edge-inference-first trap*.
   - Members note that Apple focused on use cases possible with on-device constraints, while everyone else just tried to make AI as good as possible, suggesting **Apple** should have been first on this, but *messed it up*.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://claude.ai/share/03b29290-bc0e-4425-b3b1-211785321b6e">Claude</a>: Talk with Claude, an AI assistant from Anthropic</li><li><a href="https://x.com/sama/status/1895203654103351462?t=In8IvAuxVWZFZOkOq392_Q&s=19">Tweet from Sam Altman (@sama)</a>: GPT-4.5 is ready!good news: it is the first model that feels like talking to a thoughtful person to me. i have had several moments where i&#39;ve sat back in my chair and been astonished at getting ac...</li><li><a href="https://fxtwitter.com/yacinemtb/status/1894601593904893984?s=46">Tweet from kache (@yacineMTB)</a>: the stable diffusion moment of video models is here</li><li><a href="https://x.com/ai_for_success/status/1895037373576290735">Tweet from AshutoshShrivastava (@ai_for_success)</a>: GPT-4.5 is rumored to have 45 googolplex parameters 😆</li><li><a href="https://www.youtube.com/watch?v=1DtJe7-4aas">Nvidia CEO Huang: DeepSeek incident underscored the substantial demand for AI compute power</a>: Jensen Huang, Nvidia CEO, joins CNBC&#39;s Jon Fortt for a special report following Nvidia&#39;s quarterly report.</li><li><a href="https://x.com/karpathy/status/1895213020982472863)">Tweet from Andrej Karpathy (@karpathy)</a>: GPT 4.5 + interactive comparison :)Today marks the release of GPT4.5 by OpenAI. I&#39;ve been looking forward to this for ~2 years, ever since GPT4 was released, because this release offers a qualitat...</li><li><a href="https://github.com/Wan-Video/Wan2.1">GitHub - Wan-Video/Wan2.1: Wan: Open and Advanced Large-Scale Video Generative Models</a>: Wan: Open and Advanced Large-Scale Video Generative Models - Wan-Video/Wan2.1
</li>
</ul>

</div>
  

---


### **Nous Research AI ▷ #[ask-about-llms](https://discord.com/channels/1053877538025386074/1154120232051408927/1344437204272943195)** (4 messages): 

> `AI Voice Commands, Reasoning in AI Models, Text-to-Speech AI, Elevenlabs, Cartesia` 


- **Reasoning Toggle for AI via Voice Commands**: A user inquired about toggling reasoning in an AI model via voice commands, aiming for **90% reasoning off** unless specifically prompted with phrases like *"use reasoning"*.
   - The user asked if they could add a system prompt to achieve this and whether it's possible to **finetune the reasoning** process and enable text-to-speech functionality.
- **Text-to-Speech AI models are being discussed**: A user planned to implement voice output using **Elevenlabs** or **Cartesia** text-to-speech, clarifying their intention after another user stated that the model cannot speak in voice.
   - The member pointed to [this YouTube video](https://www.youtube.com/watch?v=zoBwIi4ZiTA) as a demonstration of something similar to what they are trying to achieve with AI assistants.



**Link mentioned**: <a href="https://www.youtube.com/watch?v=zoBwIi4ZiTA">Deepseek AI Assistant: ALWAYS ON Python AI Agent for Engineers that SHIP</a>: 🔥 Is your Personal AI Assistant truly ALWAYS ON? Discover how Ada, powered by DeepSeek V3, is revolutionizing the way engineers ship code! 🚀🎥 Resources fo...

  

---


### **Nous Research AI ▷ #[research-papers](https://discord.com/channels/1053877538025386074/1104063238934626386/1344584159380901959)** (1 messages): 

> `Language Models, REFUTE benchmark, algorithmic problem solving` 


- **Language Models to accelerate science?**: Language Models (LMs) have the potential to accelerate scientific discovery by helping to falsify hypotheses and refine claims iteratively.
   - Current benchmarks for LMs assess their ability to generate solutions rather than challenge them.
- **Introducing REFUTE Benchmark for Algorithmic Problem Solving**: A new dynamically updating benchmark called **REFUTE** is introduced to assess LMs' ability to generate counterexamples for incorrect solutions in [algorithmic problem solving](https://huggingface.co/papers/2502.19414).
   - It includes recent problems and incorrect submissions from programming competitions, where human experts successfully identified counterexamples.
- **LMs struggle with verification on REFUTE**: Analysis of the **REFUTE** benchmark reveals that even the best reasoning agents succeed in finding counterexamples only **9%** of the time.
   - This suggests that verification can be significantly harder than generation for language models.



**Link mentioned**: <a href="https://huggingface.co/papers/2502.19414">Paper page - Can Language Models Falsify? Evaluating Algorithmic Reasoning with
  Counterexample Creation</a>: no description found

  

---


### **Nous Research AI ▷ #[interesting-links](https://discord.com/channels/1053877538025386074/1132352574750728192/1344581040706027622)** (3 messages): 

> `Diffusion LLMs, Mercury dLLM, LLaDA Release` 


- ****Mercury dLLM** Launches for Commercial Use!**: **Inception Labs** introduces **Mercury**, a new family of diffusion large language models (**dLLMs**), claiming it's up to **10x** faster than current speed-optimized LLMs, achieving over **1000 tokens/sec** on **NVIDIA H100s**; a code generation model, **Mercury Coder**, is available for testing in a [playground](https://chat.inceptionlabs.ai).
- ****LLaDA** Model Gets Official PyTorch Implementation!**: The group **ML-GSAI** released their model with an official PyTorch implementation for "Large Language Diffusion Models" available on [GitHub](https://github.com/ML-GSAI/LLaDA).


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://www.inceptionlabs.ai/news">Inception Labs</a>: We are leveraging diffusion technology to develop a new generation of LLMs. Our dLLMs are much faster and more efficient than traditional auto-regressive LLMs. And diffusion models are more accurate, ...</li><li><a href="https://github.com/ML-GSAI/LLaDA">GitHub - ML-GSAI/LLaDA: Official PyTorch implementation for &quot;Large Language Diffusion Models&quot;</a>: Official PyTorch implementation for &quot;Large Language Diffusion Models&quot; - ML-GSAI/LLaDA
</li>
</ul>

</div>
  

---


### **Nous Research AI ▷ #[research-papers](https://discord.com/channels/1053877538025386074/1104063238934626386/1344584159380901959)** (1 messages): 

> `Language Models, Scientific discovery, REFUTE Benchmark` 


- **Language Models fuel Scientific Discovery**: There is growing excitement about the potential of **Language Models (LMs)** to accelerate **scientific discovery**.
   - Current benchmarks for LMs predominantly assess their ability to generate solutions rather than challenge them.
- **REFUTE Benchmark introduced**: The **REFUTE** benchmark includes recent problems and incorrect submissions from programming competitions, where human experts successfully identified counterexamples.
   - Analysis shows that the best reasoning agents succeed only **9%** of the time, showing verification can be a lot harder than generation sometimes.



**Link mentioned**: <a href="https://huggingface.co/papers/2502.19414">Paper page - Can Language Models Falsify? Evaluating Algorithmic Reasoning with
  Counterexample Creation</a>: no description found

  

---


### **HuggingFace ▷ #[general](https://discord.com/channels/879548962464493619/879548962464493622/1344457413314740256)** (132 messages🔥🔥): 

> `HuggingFace Spaces licensing, Fal AI vs Deepinfra pricing, Lighteval MMLU-Pro support, LEFFA paper implementation, HuggingMod bot` 


- **Spaces License Snafu?**: A user inquired about needing a *license* to create a Space for a community bot, clarified as a software license for code publishing, not a permission for creating a Space.
   - Another user directed them to the [HuggingMod bot](https://huggingface.co/spaces/discord-community/HuggingMod) for code snippets and guidance.
- **Deepinfra Dominates Fal AI in Cost?**: While one user recommended **Fal AI** with *$50* free credit, another claimed **Deepinfra** is *100x* cheaper for character processing at *$0.8 per million characters* and offers free compute.
   - The first user also suggested **Kokoro TTS** as a cheap option.
- **Apple Silicon Sparks LLM Strides**: A user asked about running LLMs on **Apple's Neural Engine**, with another pointing to **Core ML** and [Apple's documentation](https://machinelearning.apple.com/research/core-ml-on-device-llama) on optimizing LLMs for Apple silicon.
   - Discussion indicated model conversion to the *.mlmodel* extension is necessary but can be complex.
- **Gemma Quantization Quandaries**: A user inquired about the size of **GemmaX2**, another user pointed to [this page](https://huggingface.co/Tonic/GemmaX2-28-2B-gguf/tree/main) and mentioned it varies between *1.5GB* and *5.3GB* depending on the quantization.
   - The same user also told the user how to check out the size, by clicking on *Use this model*.
- **Is OpenAI's generated text detectable?**: Users discussed **AI-generated text detection**, with one sharing that academic institutions may not check due to lack of definitive proof.
   - A user shared images of cover letters before and after AI improvement, noting that **OpenAI models fail horribly in terms of following patterns**.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://huggingface.co/spaces/discord-community/HuggingMod">HuggingMod - a Hugging Face Space by discord-community</a>: no description found</li><li><a href="https://tenor.com/view/drake-gif-21355539">Drake GIF - Drake - Discover &amp; Share GIFs</a>: Click to view the GIF</li><li><a href="https://huggingface.co/spaces/huggingchat/chat-ui/discussions/682">huggingchat/chat-ui · New Design Proposal for Hugging Face Chat</a>: no description found</li><li><a href="https://machinelearning.apple.com/research/core-ml-on-device-llama">On Device Llama 3.1 with Core ML</a>: Many app developers are interested in building on device experiences that integrate increasingly capable large language models (LLMs)…</li><li><a href="https://huggingface.co/Tonic/GemmaX2-28-2B-gguf/tree/main">Tonic/GemmaX2-28-2B-gguf at main</a>: no description found</li><li><a href="https://huggingface.co/spaces/discord-community/HuggingMod/blob/main/app.py">app.py · discord-community/HuggingMod at main</a>: no description found</li><li><a href="https://github.com/benchflow-ai/benchflow">GitHub - benchflow-ai/benchflow: AI benchmark runtime framework that allows you to integrate and evaluate AI tasks using Docker-based benchmarks.</a>: AI benchmark runtime framework that allows you to integrate and evaluate AI tasks using Docker-based benchmarks. - benchflow-ai/benchflow</li><li><a href="https://github.com/huggingface/smolagents/issues">huggingface/smolagents</a>: 🤗 smolagents: a barebones library for agents. Agents write python code to call tools and orchestrate other agents. - huggingface/smolagents</li><li><a href="https://huggingface.co/Tonic/">Tonic (Joseph [open/acc] Pollack)</a>: no description found
</li>
</ul>

</div>
  

---


### **HuggingFace ▷ #[today-im-learning](https://discord.com/channels/879548962464493619/898619964095860757/1344402717862268969)** (4 messages): 

> `Hiding vs Removing, F2 vs F12, Smol Agents Framework` 


- **Hiding is not Removing!**: A member inquired about the difference between **hiding** and **removing**, questioning the benefit of hiding.
   - They seemed confused after seeing [a pair of screenshots](https://cdn.discordapp.com/attachments/898619964095860757/1344402717518594068/SCR-20250226-qcsi.png?ex=67c21999&is=67c0c819&hm=b8bb0a85e03415adc0e67d6f889d0baab302156da75c906e7cc63cbc7c1e6a73&) contrasting hiding versus removing.
- **F2 is nothing like F12**: A member shared their TIL (today I learned) moment about the difference between the **F2** and **F12** keys.
   - No further context was provided.
- **Smol Agents Framework**: A member is learning how to build a basic agent using the **smol agents framework**.
   - They shared no further details about the agent they are building, or their experience.


  

---


### **HuggingFace ▷ #[i-made-this](https://discord.com/channels/879548962464493619/897390720388825149/1344400801635893260)** (8 messages🔥): 

> `LLM performance benchmark, Face similarity questionnaire, PyTorch library for 360° images, Phi-4 models` 


- ****LLM Benchmark** Unveiled to Evaluate Performance**: A member developed a small private benchmark to quickly check general **LLM performance** using previously unseen questions and estimate how far small local models are from the best online models, now including over **1000 models**.
   - The benchmark and models scores are available at [MoonRide's Hashnode blogpost](https://moonride.hashnode.dev/biased-test-of-gpt-4-era-llms-300-models-deepseek-r1-included) and on [HuggingFace](https://huggingface.co/datasets/MoonRide/MoonRide-LLM-Index-v7).
- ****Face Similarity** Preferences Needed for Master's Thesis**: A member is requesting participation in a questionnaire for their master's thesis, which focuses on determining which **faces look more similar** using a pipeline for generating faces.
   - The questionnaire, optimized for PC, takes around 5 minutes to complete and is available at [this link](https://1ka.arnes.si/a/70715279).
- ****PyTorch360Convert Library** Simplifies 360° Image Handling**: A member introduced a new, lightweight **PyTorch library** called **pytorch360convert** to simplify working with **360° images** for VR, AR, video games, and more, available via `pip install pytorch360convert`.
   - The library supports various image representations, including **equirectangular images** and **cubemaps**, and is **GPU/CPU compatible**, supporting **float32, float64, float16, and bfloat-16 precision types**, and is available on [GitHub](https://github.com/ProGamerGov/pytorch360convert).
- **Phi-4 Models Debut on HF Spaces**: A member shared a link to **phi 4 models** on Hugging Face Spaces, marking the availability of this project.
   - The project can be found at [Hugging Face Spaces](https://huggingface.co/spaces/merterbak/phi-4).


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://huggingface.co/spaces/merterbak/phi-4">Phi 4 - a Hugging Face Space by merterbak</a>: no description found</li><li><a href="https://1ka.arnes.si/a/70715279">User ranking based on similarity - 1KA | Web surveys</a>: no description found</li><li><a href="https://github.com/ProGamerGov/pytorch360convert">GitHub - ProGamerGov/pytorch360convert: PyTorch based image conversions between equirectangular, cubemap, and perspective. Based on py360convert</a>: PyTorch based image conversions between equirectangular, cubemap, and perspective. Based on py360convert - ProGamerGov/pytorch360convert</li><li><a href="https://moonride.hashnode.dev/biased-test-of-gpt-4-era-llms-300-models-deepseek-r1-included">Biased test of GPT-4 era LLMs (300+ models, DeepSeek-R1 included)</a>: IntroTime to time I was playing with various models I can run locally (on a 16GB VRAM GPU), checking out their conversational and reasoning capabilities. I don&#x27;t fully trust public benchmarks, as...</li><li><a href="https://huggingface.co/datasets/MoonRide/MoonRide-LLM-Index-v7">MoonRide/MoonRide-LLM-Index-v7 · Datasets at Hugging Face</a>: no description found
</li>
</ul>

</div>
  

---


### **HuggingFace ▷ #[reading-group](https://discord.com/channels/879548962464493619/1156269946427428974/1344576438346584085)** (2 messages): 

> `Language Models (LMs), REFUTE Benchmark, Reasoning Agents` 


- **Language Models Speeding Scientific Discovery**: A new paper highlights the potential of **Language Models (LMs)** to accelerate scientific discovery, emphasizing the importance of falsifying hypotheses.
   - The paper notes that current benchmarks predominantly assess the ability to generate solutions rather than challenge them, advocating for benchmarks that evaluate the inverse capability and linking to their paper [here](https://huggingface.co/papers/2502.19414).
- **Introducing the REFUTE Benchmark**: The authors introduce **REFUTE**, a dynamically updating benchmark that includes recent problems and incorrect submissions from programming competitions where human experts successfully identified counterexamples.
   - Analysis shows that even the best reasoning agents score low (9%) at falsifying incorrect algorithmic solutions, despite generating correct ones for 50% of the problems.
- **LLMs as Retrieval Engines**: A member commented on the scarcity of data showing that verification can be harder than generation, noting that generating the correct solution type of code dominates everywhere.
   - The member suggested that **LLMs** can't reason too much and are mainly a retrieval engine.



**Link mentioned**: <a href="https://huggingface.co/papers/2502.19414">Paper page - Can Language Models Falsify? Evaluating Algorithmic Reasoning with
  Counterexample Creation</a>: no description found

  

---


### **HuggingFace ▷ #[computer-vision](https://discord.com/channels/879548962464493619/922424143113232404/1344707878547882065)** (2 messages): 

> `` 


- **No Topics Discussed**: No significant topics were discussed in the provided messages.
- **Awaiting Next Session**: A member expressed their intention to join the next session.


  

---


### **HuggingFace ▷ #[gradio-announcements](https://discord.com/channels/879548962464493619/1014577787039924226/1344707377781411914)** (1 messages): 

> `FastRTC` 


- **FastRTC Category is LIVE!**: A member directs everyone to the **FastRTC** category for questions, discussions, and announcements.
   - The link to the specific channel is [here](https://discord.com/channels/879548962464493619/1344703220332756994).
- **Reminder to use FastRTC Category**: To keep the server organized, members are encouraged to use the **FastRTC** category for related discussions.
   - This helps ensure that relevant information is easily accessible and conversations remain focused.


  

---


### **HuggingFace ▷ #[smol-course](https://discord.com/channels/879548962464493619/1313889336907010110/1344439778833334283)** (9 messages🔥): 

> `Inference Engine Alternatives, Smolagents Quiz Iframe, Smolagents Quiz Failures, HfApiModel vs LiteLLMModel Confusion, SFT Trainer Loss Function` 


- **Inference Credits Exhausted**: A user inquired about discounts or alternative inference engines to continue the studio notebooks on Google Colab for the **smolagents course**, after exceeding Hugging Face's inference requests limit.
   - They expressed a desire to continue following along with the course.
- **Smolagents Quiz Display Issues**: A user reported that the **iframe** in the final quiz for unit 2.1 of the smolagents course is too small, making the feedback difficult to read even on a **32" 4k monitor**.
   - They suggested increasing the iframe size to **800x600** or **850x850** to improve readability.
- **Smolagents Quiz Validation is BSing User**: A user complained that the agent verifying answers in quiz 2.1 of the agent course is giving contradictory feedback regarding the id argument in **HfApiModel**, requiring it and then rejecting it.
   - The user argued that the **HfApiModel** class should default to the **Qwen** model, making the id argument optional, and requested more *mental elasticity* from the validation agent.
- **SFTTrainer Loss Elucidation**: A user sought clarification on the loss function used by **SFTTrainer**, questioning whether it's inferred from the model type (e.g., **crossentropy** for CLM).
   - It was also confirmed that the agent works the same with or without explicit imports.
- **Documentation Discrepancies Frustrate Quiz Takers**: A user expressed frustration with errors encountered in the second quiz, citing discrepancies between the quiz's security settings and current documentation.
   - The user also noted confusion regarding the model implementation with **HfApiModel** versus **LiteLLMModel**, stating that the documentation doesn't seem to indicate that **HfApiModel** has a model_id for **LiteLLMModel**.


  

---


### **HuggingFace ▷ #[agents-course](https://discord.com/channels/879548962464493619/1329142738440028273/1344405585461772318)** (129 messages🔥🔥): 

> `Chat templates, agent, and LLM interaction, NVIDIA AI Red Team Prompt Injection, CodeAgent's Python interpreter, Smolagents codeagents to set the system prompts, Agent Laboratory for research reports and code repositories` 


- **Agent's Prompt Template gets Populated**: A member was trying to verify their understanding of how **chat templates**, **agents**, and **LLMs** interact, noting that the `prompts.yaml` file defines the `system_prompt` and is populated with actual tools provided in the agent initialization.
   - Another member clarified that the **CodeAgent** actually has it's own **Python interpreter**.
- **NVIDIA AI Red Team Tackles Prompt Injection**: The [NVIDIA AI Red Team](https://developer.nvidia.com/blog/nvidia-ai-red-team-an-introduction/) identified vulnerabilities where **prompt injection** can be used to exploit three plug-ins included in the [LangChain](https://www.langchain.com/) library.
   - Prompt injection is a new attack technique specific to **large language models (LLMs)** that enables attackers to manipulate the output of the LLM, especially when LLMs are equipped with plug-ins.
- **Debugging Nightmares with SmolAgents**: A member reported running into an issue with the examples in **Unit 2**, stating that most of the sample code fails due to reaching the maximum number of steps.
   - Another member shared some concerns about deploying **Smolagents** to production, noting that *"because they don't run async I have to run them in threads"*.
- **Gemini is more Generous**: A member stated that they were facing **Payment Required** message.
   - Another member recommended switching to using **Gemini** with **LiteLLM** because *"Gemini has generous free tier with Google AI Studio"*.
- **Agent Laboratory helps you ideate**: [Agent Laboratory](https://agentlaboratory.github.io/) takes as input a human-produced research idea and outputs a research report and code repository.
   - It enables you *"to focus on ideation and critical thinking while automating repetitive and time-intensive tasks like coding and documentation"*, according to their GitHub page.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://huggingface.co/spaces/agents-course/unit_1_quiz">Unit 1 Quiz - AI Agent Fundementals - a Hugging Face Space by agents-course</a>: no description found</li><li><a href="https://huggingface.co/learn/agents-course/unit2/smolagents/why_use_smolagents">Why use smolagents - Hugging Face Agents Course</a>: no description found</li><li><a href="https://huggingface.co/spaces/agents-course/unit1-certification-app">Unit 1 Certification - AI Agent Fundamentals - a Hugging Face Space by agents-course</a>: no description found</li><li><a href="https://huggingface.co/docs/smolagents/main/en/reference/agents#smolagents.PromptTemplates">Agents</a>: no description found</li><li><a href="https://agentlaboratory.github.io/">Agent Laboratory: Using LLMs as Research Assistants</a>: by Samuel Schmidgall at JHU</li><li><a href="https://www.youtube.com/watch?v=2ky50XT0Nb0"> - YouTube</a>: no description found</li><li><a href="https://developer.nvidia.com/blog/securing-llm-systems-against-prompt-injection/">Securing LLM Systems Against Prompt Injection | NVIDIA Technical Blog</a>: This post explains prompt injection and shows how the NVIDIA AI Red Team identified vulnerabilities where prompt injection can be used to exploit three plug&#x2d;ins included in the LangChain library.
</li>
</ul>

</div>
  

---


### **Perplexity AI ▷ #[general](https://discord.com/channels/1047197230748151888/1047649527299055688/1344399549812445317)** (264 messages🔥🔥): 

> `Perplexity Pro Flair, New Voice Mode, Disable Web Search, Coding with Perplexity, Gemini Real Time Video Chat` 


- ****Voice Mode Vigorously Vouched for****: Members discussed the new **voice mode** feature, noting improvements in **UI**, the ability to **interrupt**, and changes to **voices**.
   - While some users found it impressive, others felt it didn't quite match the level of **Microsoft Copilot**, **Grok 3**, or **ChatGPT**.
- ****Writing Wonders Without Web Woes****: Members discussed the ability to **disable web search** in Perplexity, with one user suggesting the use of **writing focus** to achieve this.
   - However, some users reported that even in writing mode, **web sources** were still being used, while others claimed it worked fine for them.
- ****GPT-4.5 gossip grows galore****: Users discussed the potential integration of **GPT-4.5** into Perplexity, referencing a [YouTube demo](https://www.youtube.com/watch?v=cfRYp0nItZ8) and noting it as a model with *greater context* and *more human-like* responses.
   - A user shared a link from [Sam Altman on X](https://x.com/sama/status/1895203654103351462) mentioning that **GPT-4.5** is *the first model that feels like talking to a thoughtful person*.
- ****Model Mixing Mayhem in Spaces****: Users discussed issues with Spaces, where the system prompt tells it that *You are Perplexity, a helpful search assistant trained by Perplexity AI* even when using other models.
   - One user shared a [link to test this in spaces](https://www.perplexity.ai/search/what-ai-model-are-you-qsgHi_lOQNq1TSv7UKw2kw), and another suggested writing it in the space instructions.
- ****Pro or Grok: A Grandiose Gabfest****: Members debated the value of Perplexity Pro versus SuperGrok, with one user asking *What is the difference between the $50 dollar premium + plan vs Supergrok via there app?*
   - A user clarified that SuperGrok offers *more advanced reasoning* through a **Big Brain** mode not available in Premium+.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://x.com/sama/status/1895203654103351462">Tweet from Sam Altman (@sama)</a>: GPT-4.5 is ready!good news: it is the first model that feels like talking to a thoughtful person to me. i have had several moments where i&#39;ve sat back in my chair and been astonished at getting ac...</li><li><a href="https://en.wikipedia.org/wiki/I_know_that_I_know_nothing">I know that I know nothing - Wikipedia</a>: no description found</li><li><a href="https://www.youtube.com/watch?v=cfRYp0nItZ8">Introduction to GPT-4.5</a>: Mia Glaese, Rapha Gontijo Lopes, Youlong Cheng, Jason Teplitz, and Alex Paino introduce and demo GPT-4.5.</li><li><a href="https://youtu.be/RI-BxtCx32s?si=Mk2TDhRQ3YrjRl8n">Live demo of GPT-4o vision capabilities</a>: This was a live demo from our OpenAI Spring Update event.Read more about GPT-4o: https://www.openai.com/index/hello-gpt-4o/
</li>
</ul>

</div>
  

---


### **Perplexity AI ▷ #[sharing](https://discord.com/channels/1047197230748151888/1054944216876331118/1344465562218004500)** (17 messages🔥): 

> `Majorana-1 Quantum, AI Communication, Lab Mice First Aid, House Blueprint, Ransomware Leaks` 


- **Perplexity Users sharing many Perplexity Links**: Several users shared an array of **Perplexity AI** search and page links, spanning topics from [quantum computing](https://www.perplexity.ai/search/majorana-1-the-worlds-first-qu-GfQ6ey8KRHKJoZXASTx94w) to [AI communication](https://www.perplexity.ai/search/i-heard-about-two-ais-communic-2NNO3p7QQdac1IJ0TDAmjA) and [lab mice giving first aid](https://www.perplexity.ai/page/lab-mice-give-first-aid-Cr8kRPgoTLSBbXUWtl48AQ).
   - These links also included a [YouTube video](https://www.youtube.com/embed/gdiYF-UQ2K8) and discussions around [building a house](https://www.perplexity.ai/search/i-need-to-build-a-house-to-rep-OQoLSIjESviUYqwhCnA0uw), ransomware leaks, and AI-driven diagnoses.
- **Nvidia Stocks discussed on Perplexity AI**: Users shared links regarding the impact of [Nvidia's strong results](https://www.perplexity.ai/page/nvidia-s-strong-results-impact-bMt3pD7NTH2Tlk8QcsIo2Q) on the market.
   - There were also open invitations to discuss a *Z-a trading strategy*.
- **Deep Dive into Deep Sea Discussions**: A shared link points to discussions about the [deep sea](https://www.perplexity.ai/search/deep-sea-k-kxb5gKq4RyeIOuGppGtSKQ#0) on **Perplexity AI**.
- **SchellingPoint gets Poisoned Well Label**: A user mentions `$SchellingPointZEC` and `POISONED WELL` with link to article about [data centers and their health costs](https://www.perplexity.ai/page/data-centers-health-costs-43FGGYpDQV2U4NiA.8pBuQ).



**Link mentioned**: <a href="https://www.youtube.com/embed/gdiYF-UQ2K8">YouTube</a>: no description found

  

---


### **Perplexity AI ▷ #[pplx-api](https://discord.com/channels/1047197230748151888/1161802929053909012/1344564918183268385)** (4 messages): 

> `Perplexity Pro API credits, Obsidian Web Clipper configuration, sonar-deep-research model, Refunds for Perplexity API` 


- **Perplexity Pro Credits: How many APIs can I call?**: A user inquired about the number of API calls and searches possible with the **$5 API credit** included with **Perplexity Pro**, and how to pay if they exceed the given credit.
- **Troubles configuring Perplexity API in Obsidian Web Clipper**: A user is experiencing issues configuring the **Perplexity API** with the `sonar-deep-research` model in **Obsidian Web Clipper** despite setting the correct **Base URL** and **API Key**.
   - The user has provided [screenshots](https://cdn.discordapp.com/attachments/1161802929053909012/1344638496190627922/Image_27-2-25_at_12.42_PM.jpeg?ex=67c24c6f&is=67c0faef&hm=8e87be021f18ebec8872bb67c9635f61d713e54264e2613c300ca3564492218d&) of their configuration and the failure message, seeking assistance with troubleshooting.
- **Perplexity API refund process questioned**: A user asked about how to get a **refund** if the **API is recharged by mistake** and remains unused.


  

---


### **Stability.ai (Stable Diffusion) ▷ #[announcements](https://discord.com/channels/1002292111942635562/1002292398703001601/1344733055432134707)** (1 messages): 

> `Website Redesign Contest, Stable Diffusion 3.5, AI-generated artwork, US participants only` 


- **Stability AI Launches Website Redesign Contest**: Stability AI is inviting the **Stable Diffusion** community to showcase their best work in a **Website Redesign Contest** with winning images featured on **Stability AI’s official website**.
   - The contest seeks images that feel *fresh, impressive, and forward-thinking*, created using **Stable Diffusion 3.5** and conveying *innovation, beauty, and the future of creativity*.
- **Stable Diffusion 3.5 Base Required for Entries**: To enter the Website Redesign Contest, artwork must be created using **Stable Diffusion 3.5** as a base, but can incorporate **custom nodes, fine-tunes, or LoRAs**.
   - The guidelines explicitly prohibit **IP-infringing content, robots or apocalyptic themes, and NSFW material**.
- **US Participants Eligible for Stability AI Contest**: The Website Redesign Contest is open to **US participants only**, with submissions needing to be in **16:9 aspect ratio**.
   - Submissions close on **Friday, March 7th**, and selected artwork will gain **recognition and community showcase** on Stability AI's platforms.


  

---


### **Stability.ai (Stable Diffusion) ▷ #[general-chat](https://discord.com/channels/1002292111942635562/1002292112739549196/1344435747901669417)** (92 messages🔥🔥): 

> `ControlNet models for consistent characters, LLMs referencing real-time data, SDXL alternative with T5 CLIP, Inpaint Anything error, Selling ComfyUI workflows` 


- **Seeking ControlNet Character Consistency**: A member asked for recommendations for the best **ControlNet models** to maintain character consistency in **SDXL**.
   - They specifically requested a reference **U-Net model**, if available.
- **Gemini Real-Time Data Access?**: A member inquired about **LLMs** that can reference and update with **real-time data**, mentioning **Gemini** as a potential option.
   - Another member noted that most LLMs don't update in real-time but suggested enabling web search for more relevant information.
- **T5 CLIP Craze**: A member sought an **SDXL-like model** with **T5 CLIP** integration, saying they had a taste of **T5 prompt adherence in SD3.5**.
   - They found the **T5 adherence addictive** and was looking for an alternative.
- **"Inpaint Anything" shape mismatch error arises!**: A member reported a shape mismatch error in **Inpaint Anything**: *value tensor of shape [159, 256] cannot be broadcast to indexing result of shape [64, 256]*.
   - The member was using **Automatic1111** with the Inpaint Anything extension and asked how to resolve this error.
- **ComfyUI Remote Installs Sell**: A member mentioned selling **ComfyUI workflows** and remote installs to make them work for users, typically using **TeamViewer**.
   - They clarified that they charge for their time and knowledge, rather than the workflow itself.


  

---


### **Eleuther ▷ #[general](https://discord.com/channels/729741769192767510/729741769738158194/1344421450064593038)** (8 messages🔥): 

> `Hugging Face Deprecation, Best RAG Tool, LLM Pretraining Guide` 


- **HF Deprecation Discoveries**: A member inquired about marking a repo as deprecated on **Hugging Face** with a link to a newer version, but later realized that this feature only applies to **models**, not datasets.
- **RAG Tool Recommended for Personal Use**: A member asked *which RAG tool is now best for personal users*?
   - Another recommended **BM25**.
- **All-in-One LLM Training Guide Needed**: Someone asked if there is *a single self contained guide on pretraining and post training including SFT and RL for LLMs*.
- **LLM Prompt Relevance Triumphs RAG?**: One member suggested that for small corpora, prompting an **LLM** to check for relevance is better than tweaking embeddings and rerankers.
   - They added it's better to prompt than to tweak embeddings if you *don't mind some latency*.


  

---


### **Eleuther ▷ #[research](https://discord.com/channels/729741769192767510/747850033994662000/1344408590714146918)** (36 messages🔥): 

> `Data Mixing, DualPipe, DeepSeek, Gemini Flash Thinking, SWE-RL` 


- **Gradient Descent Mixes Data, Minimizes Compute**: A new [paper](https://arxiv.org/abs/2502.10510) introduces **MixMin**, a gradient-based approach for optimizing data mixtures, which improves mixtures with less than **0.2%** additional compute.
   - The method addresses the challenge of finding the optimal data mixture for machine learning pipelines by formalizing it as a convex bi-level objective.
- **DeepSeek Unveils DualPipe for Training**: **DeepSeek** released [DualPipe](https://github.com/deepseek-ai/DualPipe), a bidirectional pipeline parallelism algorithm designed to overlap computation and communication in V3/R1 training.
   - A user expressed hope that DeepSeek would release its entire pretraining framework, including core bits, on the final day.
- **Gemini's Flash Thinking Sparks Debate**: Members discussed [Gemini 2.0 Flash Thinking](https://deepmind.google/technologies/gemini/flash-thinking/), Google's enhanced reasoning model that *shows its thoughts* to improve performance and explainability, particularly in math and science.
   - Some suspect the model was benchmarked internally but not published due to underperformance compared to **O3 Mini**.
- **Scaling LLM Reasoning for Software Eng with SWE-RL**: A [paper](https://arxiv.org/abs/2502.18449) introduces **SWE-RL**, which scales RL-based LLM reasoning for real-world software engineering using a lightweight rule-based reward.
   - This approach enables LLMs to autonomously recover a developer's reasoning processes from open-source software evolution data, training on top of **Llama 3**.
- **SSL Methods for ResNet Training**: A user asked about cheap SSL methods to train a **ResNet** for decent linear probe performance on **CIFAR10** quickly.
   - Another user suggested that tuning hyperparameters/architecture might be more efficient than changing the loss function, since nothing may be significantly more efficient than **DINO**.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://arxiv.org/abs/2502.18779">arXiv reCAPTCHA</a>: no description found</li><li><a href="https://arxiv.org/abs/2502.10510">MixMin: Finding Data Mixtures via Convex Minimization</a>: Modern machine learning pipelines are increasingly combining and mixing data from diverse and disparate sources, e.g., pre-training large language models. Yet, finding the optimal data mixture is a ch...</li><li><a href="https://arxiv.org/abs/2502.19187">BIG-Bench Extra Hard</a>: Large language models (LLMs) are increasingly deployed in everyday applications, demanding robust general reasoning capabilities and diverse reasoning skillset. However, current LLM reasoning benchmar...</li><li><a href="https://deepmind.google/technologies/gemini/flash-thinking/">Gemini 2.0 Flash Thinking</a>: Gemini 2.0 Flash Thinking is our enhanced reasoning model, capable of showing its thoughts to improve performance and explainability.</li><li><a href="https://github.com/deepseek-ai/DualPipe">GitHub - deepseek-ai/DualPipe: A bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training.</a>: A bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training. - deepseek-ai/DualPipe</li><li><a href="https://arxiv.org/abs/2502.18449">SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution</a>: The recent DeepSeek-R1 release has demonstrated the immense potential of reinforcement learning (RL) in enhancing the general reasoning capabilities of large language models (LLMs). While DeepSeek-R1 ...
</li>
</ul>

</div>
  

---


### **Eleuther ▷ #[interpretability-general](https://discord.com/channels/729741769192767510/1052314805576400977/1344459755451715756)** (22 messages🔥): 

> `Jacobian Sparse Autoencoders, SmolLM2 Intermediate Checkpoints, Mechanistic Interpretability Resources, Saving Weights after Iteration, Open Problems in Mechanistic Interpretability` 


- **Jacobian Sparse Autoencoders Sparsify Computations**: A new paper introduces **Jacobian Sparse Autoencoders (JSAEs)**, a novel architecture designed to induce sparsity in both computations and representations within LLMs, aiming for a sparse computational graph that works on the full distribution of inputs. Read the [full paper here](https://arxiv.org/abs/2502.18147).
- **SmolLM2 Models get 50+ checkpoints**: 50+ intermediate checkpoints for ALL the **SmolLM2** models were released, in the hopes of helping people learn about interpretability. Check out the announcement [here](https://x.com/eliebakouch/status/1895136704077463768).
- **Neel Nanda's Comprehensive List of MI Resources**: A user shared a collection of resources for learning mechanistic interpretability, primarily linking to content created by **Neel Nanda**, including a ["getting started" guide](https://www.neelnanda.io/mechanistic-interpretability/getting-started) and a [list of good papers](https://www.neelnanda.io/mechanistic-interpretability) to read when getting into the field.
   - Also shared was Neel Nanda's updated (2024) list of favorite papers can be found [here](https://www.alignmentforum.org/posts/NfFST5Mio7BCAQHPA/an-extremely-opinionated-annotated-list-of-my-favourite).
- **Weight Saving Solutions Sought Post-Iteration**: A user inquired about research or tools for efficiently saving weights after each iteration during pretraining to observe fine-grain dynamics, also linking to an initial MVP [on GitHub](https://github.com/manncodes/interp-infra/blob/master/weight-trace.ipynb).
- **Mech Interp Groups Put Out Survey**: A large survey paper representing many of the major mech interp groups was shared, titled [open problems in mechanistic interpretability](https://arxiv.org/abs/2501.16496).


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://arxiv.org/abs/2501.16496)">arXiv reCAPTCHA</a>: no description found</li><li><a href="https://x.com/eliebakouch/status/1895136704077463768">Tweet from elie (@eliebakouch)</a>: LET&#39;S GOOO, we&#39;ve just release 50+ intermediate checkpoints for ALL the SmolLM2 models 🔥</li><li><a href="https://www.neelnanda.io/mechanistic-interpretability">Mechanistic Interpretability &mdash; Neel Nanda</a>: Blog posts about Mechanistic Interpretability Research</li><li><a href="https://www.neelnanda.io/mechanistic-interpretability/getting-started">Concrete Steps to Get Started in Transformer Mechanistic Interpretability &mdash; Neel Nanda</a>: Disclaimer   : This post mostly links to resources I've made. I feel somewhat bad about this, sorry! Transformer MI is a pretty young and small field and there just aren't many people making education...</li><li><a href="https://github.com/manncodes/interp-infra/blob/master/weight-trace.ipynb">interp-infra/weight-trace.ipynb at master · manncodes/interp-infra</a>: Contribute to manncodes/interp-infra development by creating an account on GitHub.</li><li><a href="https://www.lesswrong.com/posts/FrekePKc7ccQNEkgT/paper-jacobian-sparse-autoencoders-sparsify-computations-not">[PAPER] Jacobian Sparse Autoencoders: Sparsify Computations, Not Just Activations — LessWrong</a>: We just published a paper aimed at discovering “computational sparsity”, rather than just sparsity in the representations. In it, we propose a new ar…</li><li><a href="https://www.alignmentforum.org/posts/NfFST5Mio7BCAQHPA/an-extremely-opinionated-annotated-list-of-my-favourite">An Extremely Opinionated Annotated List of My Favourite Mechanistic Interpretability Papers v2 — AI Alignment Forum</a>: This post represents my personal hot takes, not the opinions of my team or employer. This is a massively updated version of a similar list I made two…
</li>
</ul>

</div>
  

---


### **Eleuther ▷ #[lm-thunderdome](https://discord.com/channels/729741769192767510/755950983669874798/1344477149750497341)** (17 messages🔥): 

> `QA Task Evaluation, ARC-Easy, ARC-hard, Mosaic's Eval Framework, GPQA Diamond COT Zero-Shot Evaluation` 


- **Evaluating QA Tasks with Harness Sparks Debate**: A member inquired about evaluating **QA tasks** like **ARC-Easy** and **ARC-hard** using a harness, questioning why the concatenation only includes *Question + Option* instead of *Question + Options + Answer* for each option.
   - Another member pointed to [Mosaic's eval framework](https://arxiv.org/pdf/2404.08382) and [Section 5.2](https://arxiv.org/pdf/2405.14782) for background on task structures and evaluation methods.
- **ARC Evaluation Relies on Loglikelihoods**: In response to a question about evaluation methods, a member clarified that they found **ARC-Challenge** and **ARC-Easy** follow the former approach (Question + Option) and that they can use *generate_until* instead of *Loglikelihoods* then perform exact match.
   - Another member confirmed that this approach aligns with the GPT-3 paper.
- **GPQA Diamond COT Zero-Shot Command Shared**: A member asked for the command used to run evaluations, noting that someone else reported getting less than 10% accuracy.
   - Another member shared the command `gpqa_diamond_cot_zeroshot` on the `thinktest` branch along with specific model arguments and parameters for parallelization, citing a [github.com/EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/arc/arc_challenge_chat.yaml).



**Link mentioned**: <a href="https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/arc/arc_challenge_chat.yaml">lm-evaluation-harness/lm_eval/tasks/arc/arc_challenge_chat.yaml at main · EleutherAI/lm-evaluation-harness</a>: A framework for few-shot evaluation of language models. - EleutherAI/lm-evaluation-harness

  

---


### **Yannick Kilcher ▷ #[general](https://discord.com/channels/714501525455634453/986699377257119794/1344398918708101232)** (58 messages🔥🔥): 

> `Microsoft's survival aided by governments, Deterministic manners of AI models, AI in programming, Agentic systems struggle, Small team build a better browser than Chrome` 


- **Microsoft's Dominance Debated**: A member asserted that **Microsoft** has never been a true innovator, but has been sustained by government support.
   - Another member countered that while money and power are important, they don't guarantee long-term success, pointing to **Yahoo** as an example of a company that lost its dominance despite having significant resources.
- **AI Models Generate Meaningful but Non-Deterministic Results**: A member questioned how non-deterministic AI models can exhibit deterministic behavior and converge.
   - Another member responded that while the exact results may vary, AI models generate outputs with the same meaning, citing the example of regenerated code in **Cursor** with only changes in comments and variable names.
- **AI Excels in Static Programming Tasks**: A member shared that AI models learn programming more easily than other tasks, focusing on the programming side, being proficient at static things but struggling with dynamic tasks which hurts agentic systems.
   - They pointed to the possibility of individuals threatening big companies since smaller teams can move faster and build better tools.
- **OpenAI Releases GPT-4.5 Research Preview**: Members discussed the release of **GPT-4.5**, noting that it focuses more on user preference and helpfulness rather than groundbreaking advancements as described in the [Introduction to GPT-4.5 YouTube video](https://www.youtube.com/watch?v=cfRYp0nItZ8).
   - Some felt **OpenAI** was pressured to release something due to competition from **Grok-3** and **Claude 3.7**, noting the increased pricing of **$75** per million input tokens and **$150** for output.
- **OpenAI's MoE Architecture Confirmed**: A member shared a more or less official confirmation that **OpenAI's** base models are all **MoE** (Mixture of Experts) as linked in this [YouTube video](https://youtu.be/pdfI9MuxWq8?si=d_x-6xvuLZ9ZybZ8&t=685).
   - The member stated that while this wasn't really news, as it was somewhat known already, this confirmation was not a rumor but pretty well founded.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://x.com/OpenAI/status/1895134318835704245">Tweet from OpenAI (@OpenAI)</a>: Livestream in 4.5 hours.</li><li><a href="https://fxtwitter.com/polynoamial/status/1895207166799401178">Tweet from Noam Brown (@polynoamial)</a>: Scaling pretraining and scaling thinking are two different dimensions of improvement. They are complementary, not in competition.</li><li><a href="https://www.youtube.com/watch?v=cfRYp0nItZ8">Introduction to GPT-4.5</a>: Mia Glaese, Rapha Gontijo Lopes, Youlong Cheng, Jason Teplitz, and Alex Paino introduce and demo GPT-4.5.</li><li><a href="https://www.reddit.com/r/singularity/comments/1izmg33/figure_launching_robots_into_the_home_alpha/">Reddit - Dive into anything</a>: no description found</li><li><a href="https://youtu.be/pdfI9MuxWq8?si=d_x-6xvuLZ9ZybZ8&t=685"> - YouTube</a>: no description found</li><li><a href="https://www.youtube.com/watch?v=2ky50XT0Nb0">ChatGPT Opens A Research Lab…For $2!</a>: ❤️ Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papersGuide for using DeepSeek on Lambda:https://docs.lambdalabs.com/educati...
</li>
</ul>

</div>
  

---


### **Yannick Kilcher ▷ #[paper-discussion](https://discord.com/channels/714501525455634453/1045297868136779846/1344503565980270652)** (7 messages): 

> `Hash Collisions, KV Similarity` 


- **Hash Collisions Intended**: Instead of eliminating **hash collisions**, the implementation aims to *induce collisions* when qkT_i is high.
   - The probability of hash collision, P(h(q) == h(k_i)), is leveraged, where *h* is a hash function.
- **KV Similarity via Hash Collisions**: Hash collisions are used as a metric to remove similar key-value pairs, as described in [arxiv.org/pdf/2502.03387](https://arxiv.org/pdf/2502.03387).
   - The discussion referenced a **pseudo truthmatteo.batelic** file, though its exact purpose wasn't specified.



**Link mentioned**: <a href="https://www.twitch.tv/claudeplayspokemon">ClaudePlaysPokemon - Twitch</a>: Claude Plays Pokemon - Debut Stream

  

---


### **Yannick Kilcher ▷ #[ml-news](https://discord.com/channels/714501525455634453/853983317044756510/1344416952914940005)** (15 messages🔥): 

> `Remarkable Alexa, GPT-4.5 Announcement, DeepSeek AI Open Infra Index` 


- **Amazon's Alexa to have Monthly Subscription?**: Rumors suggest that the new Alexa, codenamed **Remarkable**, might require a subscription fee ranging from **$5 to $10 per month** according to [tomsguide.com](https://www.tomsguide.com/ai/remarkable-alexa-with-ai-could-cost-dollar5-to-dollar10-a-month-heres-what-it-could-do).
   - The article highlights that it remains to be seen if consumers will pay for Alexa, given that **Google, Samsung, and Apple offer their AI services for free**.
- **DeepSeek AI Opens Infrastructure Index**: DeepSeek AI has released an open-source infrastructure index which can be found [here](https://github.com/deepseek-ai/open-infra-index).
- **OpenAI Teases GPT-4.5 Launch**: OpenAI teased the launch of **GPT-4.5** with a [livestream](https://x.com/OpenAI/status/1895134318835704245) and later released an [introductory YouTube video](https://www.youtube.com/live/cfRYp0nItZ8) featuring Mia Glaese, Rapha Gontijo Lopes, Youlong Cheng, Jason Teplitz, and Alex Paino.
   - The announcement was met with mixed reactions, with some criticizing the presentation and the scenarios showcased, such as *'Write an angry text because I am mad with the friend.'


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://x.com/OpenAI/status/1895134318835704245">Tweet from OpenAI (@OpenAI)</a>: Livestream in 4.5 hours.</li><li><a href="https://www.tomsguide.com/home/live/amazon-alexa-event-live-last-minute-amazon-devices-rumors-and-all-the-big-news-as-it-happens">Amazon Alexa Plus event &mdash; all the big announcements and new AI features</a>: The new Alexa is here</li><li><a href="https://www.youtube.com/live/cfRYp0nItZ8">Introduction to GPT-4.5</a>: Mia Glaese, Rapha Gontijo Lopes, Youlong Cheng, Jason Teplitz, and Alex Paino introduce and demo GPT-4.5.
</li>
</ul>

</div>
  

---


### **Cohere ▷ #[discussions](https://discord.com/channels/954421988141711382/954421988783444043/1344422166220902450)** (44 messages🔥): 

> `Cohere models in OpenAI SDK, Auto Subtitles, Command R+ update, R7B Arabic vs Fanar and ALLaM` 


- **Cohere Models can now use OpenAI SDK**: Members celebrated the ability to access **Cohere models** directly through the **OpenAI SDK**. A link to the [Quickstart Guide](https://docs.cohere.com/docs/compatibility-api) was shared, featuring demos for **Python, TS, & cURL**, plus streaming, tool calls, and structured outputs.
- **Community Seeks Auto Subtitle Solutions**: A user requested recommendations for AI APIs that generate auto subtitles similar to those on **TikTok** or **YouTube Shorts**.
   - Another user suggested using **Google STT**, noting that YouTube's auto subtitles are likely powered by **Google's** own tooling.
- **Command R+ Update Anticipation Builds**: Community members discussed and expressed their eagerness for an upcoming **Command R+** update, with one hoping it will surpass **Mistral Large 2411**.
   - Members highlighted that specific release details are unlikely to be shared due to **NDAs**, and advised against spreading unconfirmed information or rumors.
- **Arabic LLM Benchmarks**: There was interest in benchmarking **Cohere's R7B Arabic** model against **Qatar's Fanar model** and **Saudi's ALLaM**, with the suggestion to use the Arabic Balsam index.
   - A member also shared a link to the [GPT-4.5 system card](https://cdn.openai.com/gpt-4-5-system-card.pdf) which provides a great overview of the latest benchmarking methodology.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://x.com/itssandrakublik/status/1894791769117650998?s=46&t=r1mNPSgnb3pIcbR7vcCi-g">Tweet from Sandra Kublik (@itsSandraKublik)</a>: You can now access Cohere models directly through the OpenAI SDK :) Check out our Quickstart Guide for Python, TS, & cURL demos, plus streaming, tool calls, structured outputs, and more. Happy buildin...</li><li><a href="https://x.com/itssandrakublik/status/1894791769117650998?s=46&t=r1mNPSgnb3pIc">Tweet from Sandra Kublik (@itsSandraKublik)</a>: You can now access Cohere models directly through the OpenAI SDK :) Check out our Quickstart Guide for Python, TS, & cURL demos, plus streaming, tool calls, structured outputs, and more. Happy buildin...
</li>
</ul>

</div>
  

---


### **Cohere ▷ #[announcements](https://discord.com/channels/954421988141711382/996880279224451154/1344756676858875938)** (1 messages): 

> `Command R7B Arabic Model, Multilingual AI Model, Arabic Language Optimization` 


- **Arabic Command R7B model goes live!**: Cohere announces **Command R7B Arabic**, a variant of the **R7B model** optimized for **Arabic** performance while maintaining its performance in **English**.
   - It is now available on the [Cohere Platform](https://dashboard.cohere.com/playground/chat) via *command-r7b-arabic-02-2025* and on [Hugging Face](https://huggingface.co/CohereForAI/c4ai-command-r7b-arabic-02-2025) and will be on **Ollama** later today.
- **R7B Arabic excels at enterprise tasks**: The **Command R7B Arabic model** excels at tasks such as *instruction following, length control, RAG, and responding in the correct language*.
   - It has a context length of **128,000 tokens**.
- **Blog post goes live on Arabic language model**: A blog post introducing **Command R7B Arabic** is now live, detailing its optimization for **Arabic language capabilities** to support enterprises in the **MENA region**.
   - Read more in the [release notes](https://docs.cohere.com/v2/changelog/command-r7b-arabic).


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://huggingface.co/CohereForAI/c4ai-command-r7b-arabic-02-2025">CohereForAI/c4ai-command-r7b-arabic-02-2025 · Hugging Face</a>: no description found</li><li><a href="https://cohere.com/blog/command-r7b-arabic">Introducing Command R7B Arabic</a>: Our state-of-the-art lightweight multilingual AI model has been optimized for advanced Arabic language capabilities to support enterprises in the MENA region. </li><li><a href="https://docs.cohere.com/v2/changelog/command-r7b-arabic">Cohere Releases Arabic-Optimized Command Model! — Cohere</a>: Release announcement for the Command R7B Arabic model
</li>
</ul>

</div>
  

---


### **Cohere ▷ #[cmd-r-bot](https://discord.com/channels/954421988141711382/1168578374038470656/1344514004164542557)** (3 messages): 

> `Differential Transformers, World Without Coffee Essays` 


- **Differential Transformer Concepts Requested**: A user asked the bot, what is the main concept behind **Differential Transformers**.
   - No further discussion or details were provided about **Differential Transformers**.
- **Coffee Essay Prompt Triggered Bot**: A user asked the bot to write an essay about *a world without coffee*.
   - Another user repeated this prompt, suggesting interest in the bot's response to hypothetical scenarios.


  

---


### **Cohere ▷ #[projects](https://discord.com/channels/954421988141711382/1218409701339828245/1344592099613343836)** (9 messages🔥): 

> `Free auto caption APIs, Adobe Premiere auto transcription` 


- **Members seek free auto caption APIs**: One member inquired about free APIs for generating auto captions, wondering whether they needed to build one themselves.
   - Another member explained a linked tool *does auto subtitles/captions* for your video.
- **Adobe Premiere: Auto Transcription Revelation**: A member suggested that **Adobe Premiere** has an auto transcription feature.
   - Other members agreed and confirmed its existence and availability.


  

---


### **LlamaIndex ▷ #[blog](https://discord.com/channels/1059199217496772688/1187460979064324127/1344410350283657246)** (2 messages): 

> `LlamaIndex CentralReach, LlamaExtract Public Beta` 


- **LlamaIndex Transforms Autism and IDD Care**: [LlamaIndex is helping CentralReach](https://t.co/Y9Snu1KRho) transform autism and IDD care with AI.
   - AI's utility in medical fields lies in *boiling down mountains of research and paperwork into relevant insights and key points*, enhancing doctor efficiency.
- **LlamaExtract Enters Public Beta**: LlamaIndex's [LlamaExtract](https://twitter.com/llama_index/status/1895164615010722233) is now in public beta, simplifying structured data extraction from unstructured documents.
   - It enables users to **define and customize schemas for data extraction** programmatically.


  

---


### **LlamaIndex ▷ #[general](https://discord.com/channels/1059199217496772688/1059201661417037995/1344472071329157193)** (48 messages🔥): 

> `Data Leak in LlamaParse 0.6.2, Reloading pgvector Index Table, AgentWorkflow Custom Exception Handling, Elasticsearch Metadata Schema, LlamaExtract Documentation Outdated` 


- ****LlamaParse 0.6.2 Data Leak Debacle Unfolds!****: A user reported a significant data leak in **LlamaParse 0.6.2**, observing images and analyses from other users mixed into their own results, including sensitive information like **bank account details** and **transaction histories**.
   - The issue, confirmed as a mix-up with test/benchmark data, has been fixed in the backend API, with the reporter providing a list of [Job IDs](https://example.com/jobids) for investigation.
- ****pgvector Index Reloading: Index Deja Vu****: A user inquired about how to reload a previously created **pgvector index table** from the database, aiming to avoid re-creation.
   - Another user suggested using `index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model)` to reload the index from the vector store.
- ****AgentWorkflow's Custom Exception Conundrum****: A user asked if it's possible to allow **AgentWorkflow** to throw a custom exception, attempting to break the workflow and handle the exception outside of the tool's scope.
   - While not currently supported, a member suggested the team could add an option to **FunctionTool** to support this use case.
- ****LlamaExtract's Documentation: Lost in the Cloud****: A user found that the `create_agents` method was missing in **LlamaExtract 0.0.4**, indicating outdated documentation.
   - It was confirmed that the project has moved to [LlamaCloud Services](https://github.com/run-llama/llama_cloud_services), with the relevant code now located in the *llama_cloud_services* repo, and the documentation indeed being out of date.
- ****Searxng Search Engine: A Fresh Face?****: A user inquired about integrating **Searxng**, a free meta-search engine, into the framework.
   - A member responded that it was the *first time they've heard of it* but suggested using it with an agent by putting it in a FunctionTool.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://colab.research.google.com/drive/1ZCG36eLVaaZGA0XIjJH1M5EN8QhygkCC?usp=sharing">Google Colab</a>: no description found</li><li><a href="https://github.com/run-llama/llama_extract">GitHub - run-llama/llama_extract</a>: Contribute to run-llama/llama_extract development by creating an account on GitHub.</li><li><a href="https://github.com/run-llama/llama_cloud_services/blob/main/extract.md">llama_cloud_services/extract.md at main · run-llama/llama_cloud_services</a>: Knowledge Agents and Management in the Cloud. Contribute to run-llama/llama_cloud_services development by creating an account on GitHub.</li><li><a href="https://github.com/run-llama/llama_extract?tab=readme-ov-file#%EF%B8%8F-this-project-has-been-moved-to-llamacloud-services">GitHub - run-llama/llama_extract</a>: Contribute to run-llama/llama_extract development by creating an account on GitHub.</li><li><a href="https://github.com/run-llama/llama_index/blob/main/llama-index-integrations/vector_stores/llama-index-vector-stores-elasticsearch/llama_index/vector_stores/elasticsearch/base.py">llama_index/llama-index-integrations/vector_stores/llama-index-vector-stores-elasticsearch/llama_index/vector_stores/elasticsearch/base.py at main · run-llama/llama_index</a>: LlamaIndex is the leading framework for building LLM-powered agents over your data. - run-llama/llama_index
</li>
</ul>

</div>
  

---


### **DSPy ▷ #[show-and-tell](https://discord.com/channels/1161519468141355160/1202371242519441499/1344612751036907552)** (1 messages): 

> `Prompt Engineering Studio, AI-powered assistant, Reusable templates, Version control, Team collaboration` 


- **Portkey AI Launches Prompt Engineering Studio**: Portkey AI launched a **Prompt Engineering Studio**, an IDE for prompt engineers, that allows users to test across **1600+ models** with side-by-side comparison and offers instant improvements from an **AI-powered assistant**.
   - The studio enables the creation of **reusable templates** with mustache and partials, **version and deployment** of prompts with proper labeling, and **performance tracking** with real-time analytics.
- **Portkey Workshop to Demo New Studio**: Portkey AI will host a live workshop on **Monday, March 3rd, at 10:30 AM PST** to demo their Prompt Engineering Studio and host an AMA with their CEO Rohit, accessible via [Portkey's website](https://portkey.sh/promptworkshop).
   - The workshop will showcase how to test prompts, use the AI assistant, build reusable templates, implement version control, and collaborate with teams using shared prompt libraries.



**Link mentioned**: <a href="https://portkey.sh/promptworkshop">Demo: Prompt Engineering Studio · Zoom · Luma</a>: Join us for an exclusive first look at Portkey&#x27;s Prompt Engineering Studio - the most comprehensive toolkit for building, testing, and deploying AI prompts at…

  

---


### **DSPy ▷ #[general](https://discord.com/channels/1161519468141355160/1161519469319946286/1344406196731514880)** (37 messages🔥): 

> `ReAct Agent Integration, DSPy Release Bug, MIPROv2 Optimizer Error, Refine API Feedback, Community Engagement` 


- **ReAct Agent juggles external tools**: A user questioned how to integrate tools requiring external pings with **dspy.ReAct** for complex tasks like creating text and sending emails, especially concerning orchestration.
   - The challenge lies in ensuring the system understands the sequence of actions (text creation before email) when email function requires external function calls.
- **DSPy Release 2.6.7 bugs out, Imports vanish**: Users reported a **ModuleNotFoundError** in **dspy-ai==2.6.7**, with a [GitHub issue](https://github.com/stanfordnlp/dspy/issues/7867) detailing the import failure.
   - Downgrading to version **2.6.6** resolved the issue; the faulty release was quickly yanked, and **2.6.8** was released to address the import problems caused by a migration from setup.py to pyproject.toml.
- **MIPROv2 optimizer hits context limits**: A user encountered a **ContextWindowExceededError** with **MIPROv2**, even after ensuring conversations were under 1000 characters and using *light* mode.
   - It was suggested that the user reduce the number of demos in the optimizer or set `view_data_batch_size=3` in the `.compile()` call to address the token limit issue, this setting was required to reduce the data summary size.
- **Refine API evolves feedback loops**: A user inquired about how to control advice/feedback passed to the LLM on subsequent retries with **dspy.Refine**, compared to older assertion methods.
   - Feedback will be returned in the `reward_fn`, and that `dspy.Refine` should now participate in the compilation feedback mechanism, allowing for optimization of previously unoptimizable suggestions.
- **Community yearns for signal from noise**: Concerns were raised about getting quality feedback from a large Discord community to improve DSPy and avoid *too many knobs*.
   - The proposition of weekly open calls/meetings was floated, along with the idea of short posts or PRs offering feedback from production use, similar to examples in the Discord channels.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://www.kaggle.com/datasets/rtatman/ubuntu-dialogue-corpus">Ubuntu Dialogue Corpus</a>: 26 million turns from natural two-person dialogues</li><li><a href="https://github.com/stanfordnlp/dspy/issues/7867">[Bug] ModuleNotFoundError: No module named &#39;dspy.predict&#39; · Issue #7867 · stanfordnlp/dspy</a>: What happened? When you import dspy with dspy-ai==2.6.7 it just fails immediately with ModuleNotFoundError: No module named &#39;dspy.predict&#39; Steps to reproduce Here&#39;s my gist https://gist.gi...
</li>
</ul>

</div>
  

---


### **Torchtune ▷ #[general](https://discord.com/channels/1216353675241590815/1216353675744641096/)** (1 messages): 

yamashi: Gpt4.5 available on azure
  

---


### **Torchtune ▷ #[dev](https://discord.com/channels/1216353675241590815/1236040539409879170/1344431061135790202)** (26 messages🔥): 

> `CI troubles, Activation Offloading, Distributed Torch FL Code, DPO Integration Test` 


- **CI run requested for PR#2419**: A member requested someone to start CI for [PR#2419](https://github.com/pytorch/torchtune/pull/2419) without merging, as they are making a *last attempt for today*.
   - The PR in question regards *truncation and skipping*.
- **Activation Offloading and Checkpointing**: A member inquired whether there is a reason why **activation offloading** can only be used in conjunction with **activation checkpointing**.
   - Another member explained that activations require *waaaay more memory* than just the checkpoints, which in their case is just the input vector to the transformer block, so *offloading and loading them will throttle GPU* and make it unbearably slow.
- **Handling Merged Model Loading in Distributed FL**: A member sought advice on handling merged model loading in **distributed Federated Learning (FL)** code, particularly how to avoid downloading the merged model on all ranks.
   - They considered dumping the merged model to disk and having all ranks load from the disk, and was recommended to use **shared memory** instead.
- **Bullying pre-commit**: A member mentioned being *bullied by pre-commit again* while trying to implement Federated Learning. The relevant function in question resides [here](https://github.com/maximegmd/torchtune/blob/d5dc4e6027ec0de33f6ffdc2eb1eee2148a1fb69/torchtune/training/federation/_participant.py#L171).
   - The member expressed relief after managing to go through it: *pls dont 🥲*
- **DPO Integration Test Status**: A member asked about the status of the **DPO integration test**, wondering why there was a problem adding it.
   - Another member replied that there is currently one for the single device recipe, referencing [this file](https://github.com/pytorch/torchtune/blob/7cbac8173edecd7f801bbbe9ee67adf00d6261c6/tests/recipes/test_lora_dpo_single_device.py), clarifying there shouldn't be any issue adding for distributed recipe too.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://github.com/maximegmd/torchtune/blob/d5dc4e6027ec0de33f6ffdc2eb1eee2148a1fb69/torchtune/training/federation/_participant.py#L171>">torchtune/torchtune/training/federation/_participant.py at d5dc4e6027ec0de33f6ffdc2eb1eee2148a1fb69 · maximegmd/torchtune</a>: A Native-PyTorch Library for LLM Fine-tuning. Contribute to maximegmd/torchtune development by creating an account on GitHub.</li><li><a href="https://github.com/maximegmd/torchtune/blob/d5dc4e6027ec0de33f6ffdc2eb1eee2148a1fb69/torchtune/training/federation/_participant.py#L121>">torchtune/torchtune/training/federation/_participant.py at d5dc4e6027ec0de33f6ffdc2eb1eee2148a1fb69 · maximegmd/torchtune</a>: A Native-PyTorch Library for LLM Fine-tuning. Contribute to maximegmd/torchtune development by creating an account on GitHub.</li><li><a href="https://github.com/pytorch/torchtune/pull/2419">[RFC] truncation and skipping by krammnic · Pull Request #2419 · pytorch/torchtune</a>: #2344 Mention two important points related to our data loading and processing. This RFC works on both of these aspects.TruncationCurrently, we don&amp;#39;t support truncation in both right and left ....
</li>
</ul>

</div>
  

---


### **Torchtune ▷ #[papers](https://discord.com/channels/1216353675241590815/1293438210097025085/1344696167551864874)** (10 messages🔥): 

> `DeepSeek DualPipe, Federated Learning at Scale` 


- **DualPipe for Computation-Communication Overlap Surfaces**: A member shared a link to **DeepSeek's DualPipe** [GitHub repository](https://github.com/deepseek-ai/DualPipe/tree/main), which presents a *bidirectional pipeline parallelism algorithm* for computation-communication overlap in **V3/R1 training**.
- **Federated Learning faces Communication Bottlenecks**: A member expressed excitement about **DualPipe**, but noted its novelty and mentioned attempting to implement **federated learning (FL)** across **40 hospitals in Europe** using a **70B model**.
   - They humorously acknowledged that the communication overhead in their FL setup would likely dwarf the optimizations offered by **DualPipe**, but suggested it might be useful for gains between FL syncs.



**Link mentioned**: <a href="https://github.com/deepseek-ai/DualPipe/tree/main">GitHub - deepseek-ai/DualPipe: A bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training.</a>: A bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training. - deepseek-ai/DualPipe

  

---


### **Notebook LM ▷ #[use-cases](https://discord.com/channels/1124402182171672732/1124403655819415592/1344524774768119860)** (2 messages): 

> `` 


- **N/A**: N/A
- **N/A**: N/A


  

---


### **Notebook LM ▷ #[general](https://discord.com/channels/1124402182171672732/1124402182909857966/1344405926349639693)** (29 messages🔥): 

> `Notebook emoji changes, Arraying instructions with keywords, Sharing Notebooks with groups, Audio overview error, Public link to notebook` 


- **Users request Emoji Options for Notebooks**: Users requested the ability to change emojis on their notebooks, but the feature is currently unavailable, but it was suggested to support existing feature requests or create new ones. There are many strong options against OneNote, Obsidian, and Goodnotes.
   - One user pointed to a [tweet](https://x.com/signulll/status/1894806791172559355?t=M_rcWIE4NHsrLy8Ry3DzKA&s=19) lamenting **NotebookLM's** lack of momentum and mobile apps, blaming Google's pattern of stifling internal innovation.
- **Notebook Sharing Shenanigans**: Users are encountering issues sharing notebooks with groups, finding that simply handing over the link is insufficient, as they need to add users specifically to grant access.
   - It seems that users may need to have an account before they can access a shared notebook, and both adding the user via email and providing the link might be necessary.
- **Audio Overview Agony**: Users are frequently encountering an error saying *“There was an error fetching your conversation. Please try again.”* when trying to load the audio overview.
   - The issue seems intermittent, working sometimes but failing frequently, causing frustration among users who rely on this feature.
- **User reports 'Service Unavailable' Error**: A user reported receiving a *'Service unavailable'* error when logging into NotebookLM, with a message indicating that *'You tried to access a service that isn't available for your account'*, and linked to their [Google Account services page](https://accounts.google.com/info/servicerestricted).
   - A user suggested that the account may be defaulting to a school account instead of a personal one.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://accounts.google.com/info/servicerestricted">Service unavailable</a>: no description found</li><li><a href="https://x.com/signulll/status/1894806791172559355?t=M_rcWIE4NHsrLy8Ry3DzKA&s=19">Tweet from signüll (@signulll)</a>: notebooklm had insane potential, one of the best products google’s put out in years. but in classic google fashion, it seems like it lost all momentum & got left to die. no mobile apps, no meaningful ...
</li>
</ul>

</div>
  

---


### **Modular (Mojo 🔥) ▷ #[general](https://discord.com/channels/1087530497313357884/1098713601386233997/1344403286928789516)** (5 messages): 

> `Repo Structure Simplification, Mojo Prioritization, Chris Lattner's Blog Post` 


- **Modular Simplifies MAX and Mojo Repo Structure**: Modular aims to simplify their **MAX** and **Mojo** repo structure to ease contributions to documentation and the standard library, and to consolidate bug reports and feature requests, as detailed in [this forum thread](https://forum.modular.com/t/upcoming-changes-to-our-github-repositories/648).
- **Doubts Emerge on Mojo's Standalone Future**: A member questioned whether the repo simplification indicates a shift away from prioritizing **Mojo** as its own standalone language.
- **Chris Lattner's Blog Post Series**: A member found **Chris Lattner's** blog post series excellent and insightful, regretting not taking the **GPU programming course**.
   - The member mentioned being previously turned off by doing *trivial things in tensorflow* in introductory classes, noting more complex tasks seemed *locked away behind a pile of data*.



**Link mentioned**: <a href="https://forum.modular.com/t/upcoming-changes-to-our-github-repositories/648">Upcoming changes to our GitHub repositories</a>: Tomorrow (February 27), we’re streamlining our GitHub repositories! The max repo is merging into the mojo repo, bringing everything under one roof. A new subdirectory will house the Mojo standard libr...

  

---


### **Modular (Mojo 🔥) ▷ #[mojo](https://discord.com/channels/1087530497313357884/1151418092052815884/1344399712941248552)** (25 messages🔥): 

> `MLIR in stdlib, HyperLogLog in Mojo, MLIR Dialects in Mojo, MAX Graph Compiler, Unions in Mojo` 


- **Mojo Hyperlogs with Github!**: A member implemented the **HyperLogLog algorithm** in Mojo and shared it on [GitHub](https://github.com/axiomhq/mojo-hyperloglog), seeking suggestions for improvement.
   - They express enjoyment in using Mojo, describing it as *a more powerful Python*.
- **MAX uses undocumented MLIR**: Members discussed the use of inline **MLIR** in the stdlib, which is largely undocumented and intended for internal use by Modular and stdlib contributors.
   - It's implied that the in-house dialects `mo`, `moq`, `mogg`, `mef`, `mgp`, `grt`, `rmo` are not intended to be exposed to the general public.
- **Exploring Internal Mojo Dialects**: A member explored Mojo's internals using `nm` to discover and list details related to **dialects, types, and ops** within `libmof.so`.
   - This exploration revealed the `union` type, prompting discussion about its intended use and potential hazards due to poorly defined **aliasing and type-punning rules**.
- **MAX graph compiler uses mlir dialects**: A member clarified that specific MLIR dialects (like `mo`) are primarily used by the **MAX Graph Compiler** and are not part of Mojo's runtime.
   - These dialects are relevant for **graph compilation only**, with no current way to manually load them into Mojo's MlirContext.
- **Stability concerns for mojo's MLIR**: Stability and documentation efforts are reasons why some MLIR dialects aren't publicly available, as they include aspects critical to Modular's competitive advantage, and completely documenting them could dilute their value.
   - A member noted once Modular is more established, they can afford to open things up since it will be easier to use their system than to replicate it.



**Link mentioned**: <a href="https://github.com/axiomhq/mojo-hyperloglog">GitHub - axiomhq/mojo-hyperloglog</a>: Contribute to axiomhq/mojo-hyperloglog development by creating an account on GitHub.

  

---


### **MCP (Glama) ▷ #[general](https://discord.com/channels/1312302100125843476/1312302100125843479/1344400592914878636)** (18 messages🔥): 

> `MCP in production, Claude Code diff based editing, Official everything server SSE, Glama AI GitHub App, Claude Code Invite` 


- ****MCP** finds users in production**: Members confirmed that **MCP** can be used in production-level workflows.
   - One user noted utilizing it despite issues with line numbers changing, which they mitigate through prompting and resource inclusion.
- ****Claude Code** uses **Diff-Based Editing**, struggles with GO**: Users report **Claude Code** uses diff-based editing, which fails when editing Go due to space additions for readability.
   - A user mentioned that this issue is caused by *the way spaces get added to go code to improve readability*.
- ****Official Everything Server** has **SSE****: The official everything server has **SSE (Server-Sent Events)** functionality, which is suitable for tests.
   - A user found that **SSE** is *perfect* for testing purposes.
- ****GitHub App** helps scale **Glama AI****: The creator of **Glama AI** requested users to install a GitHub app to support the project and increase API rate limits.
   - One user encountered a `could_not_parse_params` error during installation, but the creator clarified that the installation registration is sufficient and no data collection occurs.
- ****MCP Server** has remote resource issue**: A user struggled to get their **MCP server** to work with resources for the life of me, including the subscribe_resource decorator.
   - It was discovered that users have to *manually add resources to context like adding a file from the filesystem for the client to be able to use the resource/read method?*.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://glama.ai/mcp">Open-Source MCP servers</a>: Enterprise-grade security, privacy, with features like agents, MCP, prompt templates, and more.</li><li><a href="https://github.com/apps/glama-ai">Build software better, together</a>: GitHub is where people build software. More than 150 million people use GitHub to discover, fork, and contribute to over 420 million projects.
</li>
</ul>

</div>
  

---


### **MCP (Glama) ▷ #[showcase](https://discord.com/channels/1312302100125843476/1315696461316358175/1344689347970203739)** (5 messages): 

> `Redmine MCP Server, Ableton Voice Control, tinylm library for running LLMs` 


- **MCP Redmine Lands with Great API Coverage**: A new [MCP Redmine server](https://github.com/runekaagaard/mcp-redmine) has been released, boasting coverage of nearly the entire **Redmine json API** in under 50 lines of code.
   - The server utilizes the **gh user d-yoshi OpenAPI specification**, according to reports.
- **Ableton Voice Control Dreams Surface**: A member expressed enthusiasm for the MCP Redmine and imagined controlling **Ableton** via voice commands, suggesting a workflow like *'Ok now lets record a new track using input7 with a bit of reverb added and routed to output 3+4.'*
   - Another member noted that while direct loading of devices isn't possible with **Ableton remote control scripts**, a **Whisper routine** paired with a custom **Ableton MCP client** could achieve this.
- **tinylm Powers Client-Side LLMs in Browser**: Version 0 of [tinylm](https://github.com/wizenheimer/tinylm) was released, a library for running **LLMs** and embedding models client-side in the browser or Node.js with **WebGPU acceleration**, supporting an OpenAI-compatible API.
   - tinylm touts **zero-cost inference**, complete privacy, and features like text generation, text embeddings, and real-time token streaming.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://github.com/runekaagaard/mcp-redmine">GitHub - runekaagaard/mcp-redmine: A redmine MCP server covering close to 100% of redmines API</a>: A redmine MCP server covering close to 100% of redmines API - runekaagaard/mcp-redmine</li><li><a href="https://tinylm.wizenheimer.dev/">tinylm - Run Models Locally with WebGPU</a>: no description found</li><li><a href="https://github.com/wizenheimer/tinylm">GitHub - wizenheimer/tinylm: Zero-cost client-side inference using WebGPU | OpenAI-compliant | NodeJS | Chrome</a>: Zero-cost client-side inference using WebGPU | OpenAI-compliant | NodeJS | Chrome - wizenheimer/tinylm
</li>
</ul>

</div>
  

---


### **Nomic.ai (GPT4All) ▷ #[general](https://discord.com/channels/1076964370942267462/1090427154141020190/1344417755591479306)** (18 messages🔥): 

> `Live Mode, Voice Assistant, GGUF models, Alltalk TTS` 


- **Request for LIVE mode feature**: A member requested a **LIVE mode** feature similar to **Google Gemini**, suggesting it would surpass Google's tools.
   - They proposed using **voice recognition (STT)** for input and **TTS** for output, linking a [YouTube video](https://www.youtube.com/watch?v=6zAk0KHmiGw) demonstrating a **GPT4ALL Voice Assistant** built in Python that utilizes **OpenAI Whisper** for offline voice detection.
- **Comprehending Chat Templates for GGUF Models**: A member inquired about the usage of **chat_template** with **GGUF models**, questioning if the template is read from the **.gguf** file on initial load and stored in **model3.json**.
   - They sought confirmation that changes made in the **GUI** are saved in **model3.json**, as observed with **gpt4all** and **Hugging Face** models.
- **Oobabooga implements Alltalk TTS**: A member mentioned that [Oobabooga](https://github.com/oobabooga/text-generation-webui) implements a **text-to-speech** extension called **alltalk_tts** that functions with **GGUF**, **AWQ**, and **GPTQ** models.
   - They noted the installation is somewhat tricky, involving a **Python installation** with a **BAT install**, but requires no coding.
- **Internet speed impacts installation time**: A member lamented their slow internet speed of **40 kbps**, which would make the [Oobabooga](https://github.com/oobabooga/text-generation-webui) installation take approximately **two days**.
   - The other member had said the install takes **one hour**.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://www.youtube.com/watch?v=6zAk0KHmiGw">Create a GPT4ALL Voice Assistant in 10 minutes</a>: Use Python to code a local GPT voice assistant. In this video we learn how to run OpenAI Whisper without internet connection, background voice detection in P...</li><li><a href="https://github.com/oobabooga/text-generation-webui">GitHub - oobabooga/text-generation-webui: A Gradio web UI for Large Language Models with support for multiple inference backends.</a>: A Gradio web UI for Large Language Models with support for multiple inference backends. - oobabooga/text-generation-webui
</li>
</ul>

</div>
  

---


### **tinygrad (George Hotz) ▷ #[general](https://discord.com/channels/1068976834382925865/1068976834928193609/1344447619362979931)** (12 messages🔥): 

> `GROUP operations AST changes, BEAM search strategies for OptOps, arange GROUP optimization failure, LLVM speed regression` 


- **GROUP AST changes hit performance **blocker****: Changes to the AST for **GROUP operations** have reached parity with PyTorch when summing (2048,2048) tensors but struggle with (4096,4096) tensors due to the need for **multiple successive OptOps**.
   - The author asks whether they should attempt to adjust **BEAM search** to find these OptOps or modify the **lowerer/expander** to output something different that will do **multiple accumulators**.
- **BEAM Search Stalls Out, **Frustrates Progress****: The author is facing challenges getting **BEAM search** to find the optimal sequence of **OptOps** needed for efficient summation of larger tensors (4096,4096).
   - They are considering modifying the **lowerer** or **expander** to generate alternative ASTs that could better utilize multiple accumulators and horizontal add swizzles but express uncertainty about guaranteeing performance improvements.
- **arange GROUP Optimization, **Breaks CI****: The author reports that the `arange` **GROUP optimization** is not being applied, leading to an extra inner loop in arange operations and broken CI.
   - They rebased onto master and tests are passing, with successful matching of pytorch, asking for any advice there might be on the `arange` **GROUP optimization**.
- **Speed Test BEAM=2 **Times Out****: A member noticed that "Speed Test BEAM=2" is timing out [on GitHub Actions](https://github.com/tinygrad/tinygrad/actions/runs/13555381099/job/37888418102?pr=9190).
   - The author fixed this issue by trimming some of the added OptOps and also reported that adding **GROUP** and **GROUPTOP** slowed the **BEAM search** due to greatly increased number of kernels tried.
- **Tests still failing on **Pull Request****: A member said that the tests are still failing on the pull request and the code is also a lot slower on **LLVM** speed with **0 gain**.
   - The author clarified that they were not asking for a review yet, but wanted to know if the arange tests failing on **GROUP OptOps** was a known issue.


<div class="linksMentioned">

<strong>Links mentioned</strong>:

<ul>
<li>
<a href="https://github.com/tinygrad/tinygrad/pull/9190/files">[Bounty] Made TestSpeed.test_sum yellow on Macs with LLVM by josephsweeney · Pull Request #9190 · tinygrad/tinygrad</a>: To make this happen, I enabled GROUP OptOps&amp;#39;s on devices without local variables (CLANG and LLVM), by just adding an extra reduce instead on emitting locals. The other necessary changes came d...</li><li><a href="https://github.com/tinygrad/tinygrad/pull/9190">[Bounty] Made TestSpeed.test_sum yellow on Macs with LLVM by josephsweeney · Pull Request #9190 · tinygrad/tinygrad</a>: To make this happen, I enabled GROUP OptOps&amp;#39;s on devices without local variables (CLANG and LLVM), by just adding an extra reduce instead on emitting locals. The other necessary changes came d...</li><li><a href="https://github.com/tinygrad/tinygrad/actions/runs/13555381099/job/37888418102?pr=9190">[Bounty] Made TestSpeed.test_sum yellow on Macs with LLVM · tinygrad/tinygrad@fd63dd6</a>: You like pytorch? You like micrograd? You love tinygrad! ❤️  - [Bounty] Made TestSpeed.test_sum yellow on Macs with LLVM · tinygrad/tinygrad@fd63dd6
</li>
</ul>

</div>
  

---


### **tinygrad (George Hotz) ▷ #[learn-tinygrad](https://discord.com/channels/1068976834382925865/1070745817025106080/1344553935016431646)** (1 messages): 

> `` 


- **User Embarks on Code Expedition**: A user expressed gratitude and indicated they would explore the code to answer their questions.
- **Self-Reliance in Problem-Solving**: The user decided to investigate the codebase independently to resolve their inquiries.


  

---


### **LLM Agents (Berkeley MOOC) ▷ #[mooc-questions](https://discord.com/channels/1280234300012494859/1280370030609170494/1344574285179781161)** (2 messages): 

> `Research Plans Announcement, Discord Server Recruitment` 


- **Research Plans Announcement Via Discord!**: A member shared a Discord invite link ([https://discord.gg/5MbT7ce9](https://discord.gg/5MbT7ce9)) for a *more detailed announcement about their research plans*.
   - They encouraged interested parties to DM them for more information or join the Discord server directly.
- **Discord Server Seeks New Recruits!**: An enthusiastic member extended an invitation to join their Discord server to learn about their research plans, as well as engage directly via DMs.
   - The provided Discord invite link ([https://discord.gg/5MbT7ce9](https://discord.gg/5MbT7ce9)) promises a *more detailed announcement* regarding their ongoing projects and collaborative opportunities.


  

---


### **LLM Agents (Berkeley MOOC) ▷ #[mooc-lecture-discussion](https://discord.com/channels/1280234300012494859/1282734248112947210/1344780853070397472)** (1 messages): 

> `Research Track, Predictive Decision Making, Long Term Memory in Agents` 


- **Research Track Launches Subgroups for Focused Study**: A research track is forming, focusing on **predictive decision making** and **long-term memory** in agents.
   - The group will hold regular sync meetings to discuss lectures and foster collaboration; interested members can join via [this Discord invite](https://discord.gg/5MbT7ce9).
- **Predictive Decision Making Subgroup Kicks Off**: A new subgroup will concentrate on **predictive decision-making** strategies within AI agents.
   - This subgroup aims to explore methods for enhancing agents' abilities to anticipate future outcomes and make informed choices.


  

---


### **MLOps @Chipro ▷ #[general-ml](https://discord.com/channels/814557108065534033/828325357102432327/1344809582878396538)** (1 messages): 

> `tinylm, WebGPU, OpenAI SDK, client-side LLMs` 


- **tinylm v0 released**: A library for running **LLMs** and embedding models client-side in a browser or **Node.js** with **WebGPU** acceleration has been released, called [tinylm](https://tinylm.wizenheimer.dev/).
   - It supports **OpenAI SDK** like text generation and embeddings generation with text-to-speech and speech-to-text coming soon, with no servers needed.
- **tinylm features OpenAI-compatible API**: [tinylm](https://tinylm.wizenheimer.dev/) provides an **OpenAI-compatible API** for running language models directly in your browser or Node.js application using **WebGPU** acceleration.
   - Features include **zero-cost inference**, **client-side processing**, **text generation**, **text embeddings**, **cross-platform compatibility**, **true streaming**, and **detailed progress tracking**.



**Link mentioned**: <a href="https://tinylm.wizenheimer.dev/">tinylm - Run Models Locally with WebGPU</a>: no description found

  

---


---


{% else %}


> The full channel by channel breakdowns have been truncated for email. 
> 
> If you want the full breakdown, please visit the web version of this email: [{{ email.subject }}]({{ email_url }})!
>
> If you enjoyed AInews, please [share with a friend](https://buttondown.email/ainews)! Thanks in advance!

{% endif %}
