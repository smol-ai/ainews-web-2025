---
id: MjAyNi0w
title: 'MiniMax-M2.5: SOTA coding, search, toolcalls, $1/hour'
date: '2026-02-13T05:44:39.731046Z'
description: >-
  **MiniMax-M2.5** is now open source, featuring an "agent-native" reinforcement
  learning framework called **Forge** trained across **200k+ RL environments**
  for coding, tool use, and workflows. It boasts strong benchmark scores like
  **80.2% SWE-Bench Verified** and emphasizes cost-efficiency with claims like
  "$1 per hour at 100 tps" and good on-device performance. The **Forge** RL
  system uses multi-level prefix caching and high rollout compute share (~60%)
  to generate millions of trajectories daily. Independent reviews note improved
  stability and multi-turn viability but high token usage. The ecosystem rapidly
  adopted MiniMax-M2.5 with quantized releases including **2-bit GGUF** and
  **INT4** formats. Meanwhile, **Together** markets **GLM-5** as a leading
  open-source model for long-horizon agents with **77.8% SWE-Bench Verified**
  and MoE efficiency using DeepSeek Sparse Attention.
companies:
  - minimax-ai
  - togethercompute
  - huggingface
  - intel
  - wandb
models:
  - minimax-m2.5
  - glm-5
topics:
  - reinforcement-learning
  - agent-based-models
  - model-quantization
  - benchmarking
  - model-efficiency
  - multi-turn-dialogue
  - infrastructure-optimization
  - cost-efficiency
  - on-device-ai
people: []
---


**a quiet day**

> AI News for 2/12/2026-2/13/2026. We checked 12 subreddits, [544 Twitters](https://twitter.com/i/lists/1585430245762441216) and 24 Discords (**256** channels, and **7993** messages) for you. Estimated reading time saved (at 200wpm): **675** minutes. [AINews' website](https://news.smol.ai/) lets you search all past issues. As a reminder, [AINews is now a section of Latent Space](https://www.latent.space/p/2026). You can [opt in/out](https://support.substack.com/hc/en-us/articles/8914938285204-How-do-I-subscribe-to-or-unsubscribe-from-a-section-on-Substack) of email frequencies!


this is the trajectory story that Minimax is trying to tell:

![https://github.com/MiniMax-AI/MiniMax-M2.5/raw/main/figures/bench_10.png](https://github.com/MiniMax-AI/MiniMax-M2.5/raw/main/figures/bench_10.png)

but the bigger story may be Forge, their agent-native RL framework.

![https://github.com/MiniMax-AI/MiniMax-M2.5/raw/main/figures/rl_1.png](https://github.com/MiniMax-AI/MiniMax-M2.5/raw/main/figures/rl_1.png)


---

# AI Twitter Recap


**MiniMax M2.5 open-sourcing: agent-native RL, speed/cost, and rapid ecosystem uptake**

- **MiniMax-M2.5 is now open source**: MiniMax released **MiniMax-M2.5** weights + code, positioning it as an “agent-native” model trained with **RL across hundreds of thousands of real-world environments** for coding, tool use, search, and office workflows ([MiniMax announcement](https://twitter.com/MiniMax_AI/status/2022310932693897628)). vLLM highlights day‑0 support and reports key benchmark numbers: **80.2% SWE‑Bench Verified**, **76.3% BrowseComp**, plus claims around training scale (200k+ RL environments) and speed/cost characteristics ([vLLM](https://twitter.com/vllm_project/status/2022311342225678757)). SGLang similarly ships day‑0 support and frames the model as production-grade for “always-on” agents ([lmsys](https://twitter.com/lmsysorg/status/2022319102560555401)).
- **The practical headline is economics + throughput, not just score**: MiniMax repeatedly markets **“$1 per hour at 100 tps”** (interpretable as a “long-horizon agent budget”), which shows up both in their own posts ([MiniMax](https://twitter.com/MiniMax_AI/status/2022379949336957254)) and in community recaps emphasizing that low activated-parameter count makes self-hosting plausible ([omarsar0](https://twitter.com/omarsar0/status/2022384166034190528)). Early local runs suggest unusually strong on-device viability for its class: MLX users report ~**50 tok/s** shortly after release ([pcuenq](https://twitter.com/pcuenq/status/2022336556326060341)), and a single **M3 Ultra 512GB** run at **6‑bit** reports ~**40 tok/s** with ~**186GB** peak memory ([ivanfioravanti](https://twitter.com/ivanfioravanti/status/2022338870172684655)).
- **Forge RL training system details leak into the narrative**: A Zhihu-derived writeup summarizes MiniMax’s “Forge” RL stack as still **CISPO-like**, using **process reward + completion-time reward**, with infrastructure tricks like **multi-level prefix cache** and high rollout compute share (claimed **~60%** of compute) generating **millions of trajectories/day** ([YouJiacheng](https://twitter.com/YouJiacheng/status/2022339475049947576)). MiniMax leadership explicitly answers parameterization tradeoffs (“**10B active** intentional”), claims proximity to “**infinite agent scaling**” with **knowledge capacity** as the limiter, and teases structural + pretraining innovation focus for **M3** ([MiniMax reply](https://twitter.com/MiniMax_AI/status/2022370086397624476)).
- **Independent reviews: “viable for multi-turn work” but token-hungry**: A Chinese review thread claims M2.5 corrects M2.1’s imbalance (coding up, logic down), with overall improvements and better stability; it notes **high token usage** (nearly **2× Sonnet** in one comparison) but frames pricing/compute as making it usable day-to-day ([ZhihuFrontier](https://twitter.com/ZhihuFrontier/status/2022214461415993817)). Another summary calls it “≤Sonnet for coding, but close,” and emphasizes multi-turn viability as the key break from “toy” open models ([teortaxesTex](https://twitter.com/teortaxesTex/status/2022223441005621556)).
- **Ecosystem uptake is immediate**: weights mirrored and packaged across tooling (Hugging Face release pings, GGUF/quant drops, etc.), including Intel-hosted quantized artifacts like a **2‑bit GGUF** for MiniMax‑M2 and **INT4** for Qwen3‑Coder‑Next ([HaihaoShen](https://twitter.com/HaihaoShen/status/2022293472796180676)).

**GLM‑5 and the “near-frontier” open model wave: performance, infra constraints, and eval chatter**

- **GLM‑5 positioning**: Together markets GLM‑5 as best-in-class open-source for long-horizon agents and systems engineering, quoting metrics like **77.8% SWE‑Bench Verified**, **50.4% HLE w/ tools**, and a MoE efficiency story with “DeepSeek Sparse Attention” (as described in the tweet) ([Together](https://twitter.com/togethercompute/status/2022354579858289052)). W&B promotes an interview claiming **744B params**, a “new RL framework,” and “fully open source under MIT” (as stated in the post) ([W&B](https://twitter.com/wandb/status/2022389206572765697)). Community members also notice dataset fingerprints like “truthy‑dpo” appearing in GLM‑5 outputs ([jon_durbin](https://twitter.com/jon_durbin/status/2022291772617945546)).
- **GLM‑5 qualitative review highlights**: A detailed Zhihu-based comparison frames GLM‑5 as a substantial improvement over GLM‑4.7, especially on hallucination control, programming fundamentals, and character processing—but also more verbose/token-expensive and prone to “overthinking,” suggesting a trade between long-horizon reasoning and compute burn ([ZhihuFrontier on GLM‑5](https://twitter.com/ZhihuFrontier/status/2022161058321047681)).
- **Benchmarks as a moving target**: There’s persistent meta-discussion about whether leaderboards/evals are saturated or misleading. Examples: concerns that tokens/latency tradeoffs hide true capability; skepticism about inferring model size from TPS; and the observation that past “SWE‑bench saturation” claims were premature ([jyangballin](https://twitter.com/jyangballin/status/2022367240293949772), [teortaxesTex](https://twitter.com/teortaxesTex/status/2022255213394948360)).
- **Cross-checking with alternative evals**: SWE‑rebench is cited as “brutal” for some recent releases and shows different relative rankings than SWE‑bench Verified; a caution is made to treat it as “additional signal” ([maximelabonne](https://twitter.com/maximelabonne/status/2022401174549512576)).

**Agent engineering in practice: file-based coordination, terminal-first workflows, and “agent OS” framing**

- **Claude Code “Agent Teams” internals are surprisingly simple**: A reverse-engineering summary claims Claude Code’s multi-agent comms use **JSON files on disk** (inboxes under `~/.claude/teams/inboxes/{agent}.json`), with polling between turns and JSON-in-JSON protocol messages; the argument is that this is a pragmatic CLI design (no Redis/queues) and improves observability at the cost of atomicity/backpressure ([peter6759](https://twitter.com/peter6759/status/2022156692985983266)).
- **Terminal agents are becoming the default UX**: Cline launches **Cline CLI 2.0**, an open-source terminal coding agent featuring a redesigned interactive TUI, parallel agents with isolated state, headless CI/CD mode, and broad editor support (ACP for Zed/Neovim/Emacs) ([cline](https://twitter.com/cline/status/2022341254965772367), [cline details](https://twitter.com/cline/status/2022341258979717196)). Community framing: “open-source strikes back” due to free/low-barrier access to strong models ([testingcatalog](https://twitter.com/testingcatalog/status/2022348951459172604), [dr_cintas](https://twitter.com/dr_cintas/status/2022387444189139367)). One Cline team member describes a full rewrite (Go → TypeScript) driven by architecture/UX pain and the need to run evals reliably ([arafatkatze](https://twitter.com/arafatkatze/status/2022415192932651302)).
- **Agent scaffolds may matter less than expected (for some horizons)**: METR-related discussion suggests Claude Code / Codex scaffolds don’t strongly outperform METR’s “simple OS scaffolds” on measured time horizons so far ([nikolaj2030](https://twitter.com/nikolaj2030/status/2022398669337825737)), with Ajeya Cotra noting surprise at the small delta ([ajeya_cotra](https://twitter.com/ajeya_cotra/status/2022419978495127828)). In contrast, others note that for longer, harder tasks, scaffold choice can matter materially (e.g., **~10% success** swings) ([gneubig](https://twitter.com/gneubig/status/2022451119310655909)).
- **“Agents as OS / filesystem as substrate”**: Several posts converge on the idea that file systems are the natural environment for agents (observability, unstructured data manipulation). Box announces integration as a “cloud filesystem” into LangChain deepagents ([levie](https://twitter.com/levie/status/2022375298097111160)). WebMCP pushes “browser is the API” for web automation without UI perception, with a DoorDash-like starter template ([skirano](https://twitter.com/skirano/status/2022387763421810989)).
- **Key operational lesson: make codebases “agent-ready”**: A crisp observation is that agents have “zero tolerance” for entropy humans route around; they treat dead code/outdated docs literally, forcing engineering hygiene that humans always needed but often deferred ([dok2001](https://twitter.com/dok2001/status/2022339274767520246)).

**RL/post-training research themes: process rewards, exploration, and rubric-based evaluation**

- **Length-Incentivized Exploration (LIE) for reasoning**: A research summary introduces the “Shallow Exploration Trap” (long reasoning trajectories become exponentially unlikely under AR sampling), and proposes LIE: a length reward + redundancy penalty to encourage broader in-context exploration without filler. Reported gains include **AIME25 20.5%→26.7%** in one setup and small but consistent improvements across other benchmarks/models ([dair_ai](https://twitter.com/dair_ai/status/2022360649817526275)).
- **DPPO vs PPO and “trust region” framing**: A long algorithm breakdown contrasts PPO’s token-ratio clipping with DPPO’s distribution-shift control via divergence measures (TV/KL), plus approximations (binary/top‑K) to reduce compute, arguing DPPO is more proportional on rare tokens and better constrains large probability-mass moves ([TheTuringPost](https://twitter.com/TheTuringPost/status/2022326245745377562)).
- **Rubrics-as-rewards and evolving rubrics**: A thread describes **RLER** (RL with evolving rubrics) in Dr. Tulu: seed rubrics with search-grounded criteria, maintain an evolving rubric buffer per prompt, and keep the most discriminative rubrics by reward variance to combat reward hacking and adapt evaluation on-policy ([cwolferesearch](https://twitter.com/cwolferesearch/status/2022384365049892974)). Separately, a take argues “rubrics as rewards” can beat verifiers-as-reward even in formal-verification settings, recommending verifiers in the loop/harness but not as the sole reward signal ([davidad](https://twitter.com/davidad/status/2022361016995319850)).
- **∆Belief‑RL / information-seeking agents**: A new approach rewards actions by how much they increase belief in a target (logprob-based), aiming for long-horizon information seeking without a critic/reward model; claims include generalization from “20 questions” training to new tasks and continued improvement when scaling interaction time ([ShashwatGoel7](https://twitter.com/ShashwatGoel7/status/2022341054939185345)).
- **Human simulation as an RL target**: Stanford’s **HumanLM** + **Humanual** benchmark propose training LLMs to simulate user responses accurately (human-centric evaluation, preference shaping, policy justification), positioning user simulation as a capability primitive for product/agent design ([ShirleyYXWu](https://twitter.com/ShirleyYXWu/status/2022374624676421676)).

**Systems/infra and tooling: FP4 MoE kernels, faster ZeRO loads, model “skills,” and observability**

- **vLLM on GB300 + FP4 MoE acceleration**: vLLM reports DeepSeek R1 on **GB300** with **22.5K prefill TGS** and **3K decode TGS per GPU**, claiming large improvements over Hopper, and highlights a recipe including **NVFP4 weights** and **FlashInfer FP4 MoE kernel** (`VLLM_USE_FLASHINFER_MOE_FP4=1`) plus disaggregated prefill and tuning notes ([vllm_project](https://twitter.com/vllm_project/status/2022308974150975792)).
- **DeepSpeed ZeRO load-time fix**: A rework moves tensor flattening from CPU to GPU, significantly improving multi-GPU load times for huge models under ZeRO 1+2 ([StasBekman](https://twitter.com/StasBekman/status/2022354880049082658)).
- **Gemini “Skills” and multimodal tool calling**: Google’s Gemini API work includes a “skills” repo teaser ([osanseviero](https://twitter.com/osanseviero/status/2022259577232785866)) and an Interactions API update enabling **multimodal function calling** where tools can return **images** and Gemini can process returned images natively ([philschmid](https://twitter.com/_philschmid/status/2022349886318928158)). AI Studio billing/upgrade UX is streamlined (upgrade to paid without leaving Studio, usage tracking, spend filters) ([OfficialLoganK](https://twitter.com/OfficialLoganK/status/2022409335465480346), [GoogleAIStudio](https://twitter.com/GoogleAIStudio/status/2022409735287537999)).
- **Agent harness instrumentation**: ArtificialAnalysis adds end-to-end speed tracking to their agent harness **Stirrup**, plus per-model breakdowns and tool-call latency metrics—explicitly treating wall-clock completion time as a first-class agent metric ([ArtificialAnlys](https://twitter.com/ArtificialAnlys/status/2022358995739254800)).
- **Local fine-tuning & Apple Silicon workflows**: Notable tooling for MLX: real-time transcription with Voxtral Mini 4B in MLX Swift ([awnihannun](https://twitter.com/awnihannun/status/2022322714548338962)), a no-code local fine-tuning tool exporting to Ollama ([awnihannun](https://twitter.com/awnihannun/status/2022327214218657948)), and a repo of MLX-LM LoRA examples including GRPO/ORPO/DPO variants ([ActuallyIsaak](https://twitter.com/ActuallyIsaak/status/2022414004623479014)).

**“AI accelerates science” moment: GPT‑5.2 + QFT result and the scaffolding narrative**

- **OpenAI claims a novel theoretical physics result with GPT‑5.2**: OpenAI announces a preprint showing a gluon interaction previously assumed not to occur can arise under a specific “half-collinear” regime, framed as AI-assisted discovery ([OpenAI](https://twitter.com/OpenAI/status/2022390096625078389); preprint link is shared in-thread: [arXiv pointer](https://twitter.com/OpenAI/status/2022390104237707667)). Kevin Weil adds color: GPT‑5.2 Pro suggested a general formula; an internal scaffolded model then **proved it after ~12 hours** of continuous work ([kevinweil](https://twitter.com/kevinweil/status/2022388305434939693)). Discussion emphasizes that pattern-finding + sustained scaffolded reasoning is the differentiator, not just a single chat completion.
- **Community reactions range from “significant journal-paper tier” to skepticism about interpretation**: Some report physicists calling it a meaningful contribution roughly equivalent to a solid journal paper ([polynoamial](https://twitter.com/polynoamial/status/2022413904757035167)); others focus on the implications of long-duration productive reasoning and how to measure it in tokens/time ([teortaxesTex](https://twitter.com/teortaxesTex/status/2022401945429000685)). There’s also meta-discussion about how many employees (or outsiders) can actually evaluate the proof/result, underscoring the evaluation gap for domain-elite work ([scaling01](https://twitter.com/scaling01/status/2022401147110318586)).

---

### Top tweets (by engagement)
- **GitHub adds ability to disable PRs** ([joshmanders](https://twitter.com/joshmanders/status/2022170444116414790), [jaredpalmer](https://twitter.com/jaredpalmer/status/2022395520623480970)).
- **OpenAI’s GPT‑5.2 physics announcement** ([OpenAI](https://twitter.com/OpenAI/status/2022390096625078389)).
- **MiniMax M2.5 open-source release** ([MiniMax](https://twitter.com/MiniMax_AI/status/2022310932693897628)).
- **Cline CLI 2.0 launch / open-source terminal agent** ([cline](https://twitter.com/cline/status/2022341254965772367), [testingcatalog](https://twitter.com/testingcatalog/status/2022348951459172604)).
- **“I am the bottleneck now” (agent-era productivity reflection)** ([thorstenball](https://twitter.com/thorstenball/status/2022310010391302259)).
- **Humanoid robotics hands progress (Figure)** ([adcock_brett](https://twitter.com/adcock_brett/status/2022353637964751221)).

---

# AI Reddit Recap

## /r/LocalLlama + /r/localLLM Recap

### 1. MiniMax-M2.5 Model Announcements and Details

  - **[MiniMaxAI/MiniMax-M2.5 · Hugging Face](https://www.reddit.com/r/LocalLLaMA/comments/1r3pxy7/minimaxaiminimaxm25_hugging_face/)** (Activity: 531): ****MiniMaxAI** has released the **MiniMax-M2.5** model on [Hugging Face](https://huggingface.co/models?sort=modified&search=minimax+m2.5), which is noted for its advanced performance in coding, tool use, and office tasks. The model maintains a size of `220 billion` parameters, contrary to expectations of an increase to `800 billion` like the **GLM5** model. It offers a cost-effective operation at `$1 per hour` for `100 tokens per second`, and is enhanced by the **Forge** reinforcement learning framework, which improves training efficiency and task generalization.** Commenters express surprise at the model's size remaining at `220 billion` parameters, highlighting its impressive performance despite not increasing in size. There is also anticipation for the release of a **GGUF** quantization format, which is not yet available.

    - A user expressed surprise at the model's size, noting that while they expected an increase to 800 billion parameters to compete with models like GLM5, the MiniMax-M2.5 remains at 220 billion parameters. This is considered impressive given its 'frontier strength', suggesting high performance despite the parameter count.
    - Another user mentioned the model's Q4_K_XL size, which is approximately 130GB. This size is significant as it places the model just beyond the capabilities of some hardware, indicating a need for more robust systems to fully utilize the model's potential.
    - There is anticipation for the release of FP4/AWQ, indicating that users are looking forward to further advancements or optimizations in the model's performance or efficiency. This suggests a community eager for improvements that could enhance usability or reduce resource requirements.

  - **[MiniMaxAI MiniMax-M2.5 has 230b parameters and 10b active parameters](https://www.reddit.com/r/LocalLLaMA/comments/1r35d2x/minimaxai_minimaxm25_has_230b_parameters_and_10b/)** (Activity: 523): ****OpenHands** has announced the release of the **MiniMaxAI MiniMax-M2.5** model, which features `230 billion` parameters with `10 billion` active parameters. This model is noted for its performance, ranking 4th in the OpenHands Index, and is `13x` more cost-effective than **Claude Opus**. It excels in long-running tasks and issue resolution but requires improvements in generalization and task execution accuracy. The model is available for free on the OpenHands Cloud for a limited time. [Source](https://huggingface.co/cerebras)** Commenters are optimistic about the potential of a `~160B` REAP/REAM hybrid version, which could be optimized for machines with `128GB` of RAM, suggesting a focus on quantization and performance efficiency.

    - The MiniMax-M2.5 model by Moonshot is notable for its architecture, which utilizes 230 billion parameters but only activates 10 billion at a time. This design choice is likely aimed at optimizing computational efficiency, allowing the model to perform well on less powerful hardware, such as GPUs that are not top-of-the-line. This approach could potentially offer a balance between performance and resource usage, making it accessible for more users.
    - A comparison is drawn between MiniMax-M2.5 and other large models like GLM and Kimi. GLM has had to double its parameters to maintain performance, while Kimi has reached 1 trillion parameters. The implication is that MiniMax-M2.5 achieves competitive performance with fewer active parameters, which could be a significant advancement in model efficiency and scalability.
    - The potential for further optimization through quantization is highlighted, suggesting that MiniMax-M2.5 could be made even more efficient. Quantization could reduce the model's size and increase its speed, making it feasible to run on machines with 128GB of RAM while still leaving room for additional tasks such as deep-context tool use. This could make the model particularly attractive for users with limited computational resources.

  - **[Minimax M2.5 Officially Out](https://www.reddit.com/r/LocalLLaMA/comments/1r2xotu/minimax_m25_officially_out/)** (Activity: 765): ****Minimax M2.5** has been officially released, showcasing impressive benchmark results: `SWE-Bench Verified` at `80.2%`, `Multi-SWE-Bench` at `51.3%`, and `BrowseComp` at `76.3%`. The model is noted for its cost efficiency, with operational costs significantly lower than competitors like **Opus**, **Gemini 3 Pro**, and **GPT-5**. Specifically, running M2.5 at `100 tokens per second` costs `$1 per hour`, and at `50 TPS`, it costs `$0.3 per hour`, making it a cost-effective solution for continuous operation. More details can be found on the [official Minimax page](https://www.minimax.io/news/minimax-m25).** Commenters highlight the potential game-changing nature of Minimax M2.5 due to its low operational costs compared to other models. There is also anticipation for the release of open weights on platforms like Hugging Face.

    - The Minimax M2.5 model is highlighted for its cost-effectiveness, with operational costs significantly lower than competitors like Opus, Gemini 3 Pro, and GPT-5. Specifically, running M2.5 at 100 tokens per second costs $1 per hour, and at 50 tokens per second, it costs $0.3 per hour. This translates to an annual cost of $10,000 for four instances running continuously, making it a potentially disruptive option in terms of affordability.
    - There is anticipation for the release of open weights on Hugging Face, which would allow for broader experimentation and integration into various applications. This suggests a community interest in transparency and accessibility for further development and benchmarking.
    - The potential impact of Minimax M2.5 on existing models like GLM 5.0 and Kimi 2.5 is discussed, with some users suggesting that if the reported benchmarks are accurate, M2.5 could surpass these models in popularity due to its ease of use and cost advantages. This indicates a shift in preference towards models that offer better performance-to-cost ratios.


### 2. Dhi-5B and GLM-5 Model Launches and Tutorials

  - **[UG student launches Dhi-5B (Trained from Scratch)](https://www.reddit.com/r/LocalLLaMA/comments/1r3hlfq/ug_student_launches_dhi5b_trained_from_scratch/)** (Activity: 344): **The post introduces **Dhi-5B**, a 5 billion parameter multimodal language model developed by an undergraduate student, trained with a budget of just ₹1.1 lakh ($1200). The model is trained in five stages, including pre-training, context-length extension, mid-training, supervised fine-tuning, and vision-extension. The Dhi-5B-Base variant, with 4 billion parameters, is trained on 40 billion tokens using a custom codebase and the Muon optimizer for matrix layers. It features 32 layers, 3072 width, SwiGLU MLPs, full MHA attention with FlashAttention-3, and a 4096 context length. The attached image shows a bar chart where Dhi-5B-Base outperforms other models like Gemma 3 PT 1B and GPT-3 2.7B on various tasks, demonstrating its cost-effectiveness and performance.** Commenters are curious about the affordability and architecture of the model, questioning the choice of MHA over other architectures like MLA or GQA, and suggesting the use of efficient hybrid architectures like LFM2.

    - KaroYadgar raises questions about the model's architecture, specifically why Multi-Head Attention (MHA) was chosen over alternatives like Multi-Linear Attention (MLA) or Generalized Query Attention (GQA). They suggest considering efficient hybrid architectures such as LFM2, which they claim performs better than an equally trained Llama model, indicating a focus on optimizing performance and efficiency.

  - **[Tutorial: Run GLM-5 on your local device!](https://www.reddit.com/r/LocalLLM/comments/1r2t35r/tutorial_run_glm5_on_your_local_device/)** (Activity: 193): **The image is a tutorial for running the **GLM-5** model locally, highlighting its significant improvements over previous versions like GLM-4.7. The model, with `744B parameters` and a `200K context window`, has been optimized to run on local devices by reducing its size from `1.65TB to 241GB` using Dynamic 2-bit quantization. This allows it to run on a `256GB Mac`, though higher precision requires more RAM/VRAM. The tutorial includes instructions for software setup, such as `llama.cpp`, and configuration settings for optimal performance. The model excels in benchmarks like Humanity's Last Exam and BrowseComp, showcasing its advanced capabilities in coding and chat applications. [Image](https://i.redd.it/1047rus1c2jg1.png)** Commenters discuss the hardware requirements for running GLM-5, with questions about whether a high-end PC is necessary and comparisons to other models like qwen3-next-coder in terms of performance and precision.

    - not-really-adam raises a technical question about the potential benefits of running GLM-5 in 1-bit precision compared to qwen3-next-coder in 8-bit. This suggests a trade-off between precision and performance, where lower bit precision could lead to faster computations but might affect the accuracy of coding results.
    - Kubas_inko discusses the usability of different quantization levels, suggesting that 2-bit and 1-bit quantizations might be ineffective for practical use, while 3-bit could offer a balance between performance and usability. This highlights the challenges in maintaining model performance while reducing computational requirements.
    - Jumpy-Requirement389 inquires about the hardware requirements for running GLM-5, specifically mentioning a setup with 192GB of DDR5 RAM and a 5090 GPU. This implies that significant computational resources are necessary to effectively run the model, reflecting the high demands of modern AI models on local hardware.


### 3. Local Hardware and Model Deployment Discussions

  - **[Sanity check before I drop $$$ on a dual-4090 home AI rig (Kimi K2.5 + future proofing)](https://www.reddit.com/r/LocalLLM/comments/1r2is2r/sanity_check_before_i_drop_on_a_dual4090_home_ai/)** (Activity: 138): **The proposed build for a dual-4090 home AI rig aims to run **Kimi K2.5**, a model with approximately `1 trillion parameters` and requiring around `600 GB` of VRAM for efficient operation. The build includes dual NVIDIA GeForce RTX 4090 GPUs, each with `24GB` of VRAM, totaling `48GB`, which is insufficient for such a large model. To run Kimi K2.5 effectively, the setup would need significantly more VRAM, suggesting the use of multiple high-end GPUs like the NVIDIA H200, which are considerably more expensive. The build also features an AMD Ryzen 9 7950X3D CPU, `256GB` of DDR5 RAM, and `2TB` of NVMe storage, but these specifications fall short for the intended AI workload.** Commenters suggest that the proposed dual-4090 setup is inadequate for running large models like Kimi K2.5, recommending instead enterprise-grade hardware such as multiple RTX 6000 GPUs or NVIDIA H200s. They highlight the need for significantly more VRAM and possibly a more robust CPU and RAM configuration to handle such demanding AI tasks.

    - Running large models like Kimi K2.5, which has around 1 trillion parameters and requires approximately 600 GB of VRAM, is beyond the capacity of dual RTX 4090s. Even with aggressive quantization, the VRAM requirement remains over 200 GB, necessitating a setup with multiple high-end GPUs like the H200, which are significantly more expensive.
    - To run Kimi K2.5 decently, a high-performance CPU such as a Threadripper or Epyc with at least 768 GB of RAM is recommended, along with a minimum of 4 RTX 6000 GPUs. This setup would still be insufficient for optimal performance, highlighting the substantial hardware demands of such large models.
    - For practical purposes, using API calls might be more cost-effective than attempting to run Kimi K2.5 locally, given the prohibitive VRAM requirements. A 48 GB VRAM setup only covers a fraction of the model's needs, as detailed in the [Hugging Face model card](https://huggingface.co/unsloth/Kimi-K2.5-GGUF), which suggests that even with quantization, local execution is challenging.



## Less Technical AI Subreddit Recap

> /r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo

### 1. AI Model Performance and Benchmarks

  - **[GPT5.2 Pro derived a new result in theoretical physics](https://www.reddit.com/r/singularity/comments/1r3yi6e/gpt52_pro_derived_a_new_result_in_theoretical/)** (Activity: 556): ****GPT-5.2 Pro** has reportedly derived a new result in theoretical physics, as detailed in a [tweet](https://x.com/kevinweil/status/2022388305434939693?s=20) and a [paper](https://arxiv.org/pdf/2602.12176). The AI model was instrumental in formalizing and proving a hypothesis initially conceived by humans, showcasing its capability to handle complex theoretical frameworks. The [OpenAI blog](https://openai.com/index/new-result-theoretical-physics/) elaborates on how the model's structured approach was crucial in achieving this breakthrough, although it still lacks the ability to independently generate novel hypotheses.** Commenters highlight the potential of AI models like GPT-5.2 to surpass human capabilities in specific domains, though they note its limitations in creative hypothesis generation. There is a call for broader access to such advanced models to democratize their benefits.

    - ObiWanCanownme highlights the role of GPT-5.2 in formalizing and proving hypotheses in theoretical physics, noting that while humans may generate initial hypotheses, AI excels in formalizing and proving them. The commenter also points out that GPT-5.2 surpasses human capabilities in applying defined approaches, though it lacks in 'outside the box' thinking, which remains a human strength.
    - Aeonmoru references a claim from Hacker News suggesting that the result attributed to GPT-5.2 Pro was actually discovered in the 1980s, linking to a paper in Physical Review Letters (https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.56.2459). This raises questions about the novelty of the AI's contribution and whether it rediscovered existing knowledge.
    - socoolandawesome clarifies that GPT-5.2 Pro initially suggested the theoretical physics result, and an internal scaffolded version of the same model developed the proof. This indicates a collaborative process between different AI model versions, showcasing the potential of scaffolded AI systems in advancing scientific research.

  - **[The new Gemini Deep Think incredible numbers on ARC-AGI-2.](https://www.reddit.com/r/singularity/comments/1r2xz0q/the_new_gemini_deep_think_incredible_numbers_on/)** (Activity: 1400): **The image presents a bar chart showcasing the performance of various AI models on the ARC-AGI-2 benchmark, which evaluates reasoning and knowledge capabilities. The **Gemini 3 Deep Think** model achieves a leading score of `84.6%`, significantly outperforming other models like **Claude Opus 4.6** (`68.8%`), **GPT-5.2** (`52.9%`), and **Gemini 3 Pro Preview** (`31.1%`). This performance is notable as it approaches the threshold for effectively solving the benchmark under the [ARC Prize criteria](https://arcprize.org/guide#overview). Additionally, the model's Codeforces Elo score of `3455` places it in the top `0.008%` of human competitors, highlighting its advanced problem-solving capabilities without external tools.** Commenters are impressed by the significant performance leap, with one noting the 50% increase in percentage points as remarkable. Another highlights the model's exceptional Codeforces Elo score, suggesting a breakthrough in AI capabilities.

    - The Gemini Deep Think model has achieved a significant milestone by scoring above 85% on the ARC-AGI-2 benchmark, which is considered as effectively solving the benchmark according to the [ARC Prize criteria](https://arcprize.org/guide#overview). This is a notable achievement as it indicates a substantial leap in performance compared to other frontier models.
    - The model's performance in competitive programming is particularly impressive, with a Codeforces Elo rating of 3455. This places it in the top 0.008% of human competitors on the platform, and notably, this was achieved without the use of external tools, highlighting the model's advanced problem-solving capabilities.
    - The rapid progress from the release of ARC-AGI-2 to achieving a saturation point (85% solved) in less than a year is remarkable. This quick advancement suggests significant improvements in model training and architecture, potentially setting a new standard for future AI development.

  - **[Google upgraded Gemini-3 DeepThink: Advancing science, research and engineering](https://www.reddit.com/r/singularity/comments/1r2ymna/google_upgraded_gemini3_deepthink_advancing/)** (Activity: 753): ****Google** has announced the release of **Gemini-3 DeepThink**, which sets a new benchmark with `48.4%` on Humanity’s Last Exam, a test for frontier models. It also achieved `84.6%` on ARC-AGI-2, verified by the **ARC Prize Foundation**, and an Elo rating of `3455` on Codeforces, indicating superior performance in competitive programming. Additionally, it reached gold-medal level performance in the International Math Olympiad 2025. For more details, see the [original article](https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-deep-think/).** A notable debate in the comments highlights a perceived bias in performance comparisons, with some users pointing out that Gemini-3 is being compared to GPT 5.2 Thinking instead of the more directly competitive GPT 5.2 Pro.

    - SerdarCS points out a potential issue with the comparison metrics used by Google, noting that they are comparing Gemini-3 DeepThink to GPT-5.2 Thinking instead of GPT-5.2 Pro, which would be a more direct competitor. This could lead to misleading conclusions about the performance and capabilities of Gemini-3 DeepThink.
    - verysecreta discusses the confusion surrounding the naming conventions of Gemini-3 DeepThink, highlighting that the term 'Deep Think' might imply a different model or mode, similar to how 'Flash' and 'Pro' are distinct. They question whether 'Deep Think' is a separate model or just a mode within the existing Gemini framework, and express a desire for clearer naming conventions similar to those used by Anthropic.

  - **[The Car Wash Test: A new and simple benchmark for text logic. Only Gemini (pro and fast) solved the riddle.](https://www.reddit.com/r/singularity/comments/1r2ndfz/the_car_wash_test_a_new_and_simple_benchmark_for/)** (Activity: 1348): **The post introduces a new benchmark called the "Car Wash Test" for evaluating text logic capabilities of AI models. Notably, only **Gemini (pro and fast)** successfully solved the riddle, highlighting its advanced logical reasoning. However, users reported that **GLM 4.7** and **ChatGPT 5.2** also consistently solved the test, suggesting that these models possess strong logical reasoning abilities as well. The benchmark is part of **SimpleBench**, which includes various common-sense questions designed to test AI's understanding of everyday logic.** Some users argue that the benchmark's questions, like the Car Wash Test, may have multiple valid answers, as people can visit a car wash for reasons other than washing a car. This suggests that while the test aims to evaluate logic, it may not always have a single correct answer, reflecting real-world complexity.

    - The comment by mxforest highlights that the GLM 4.7 model, when run locally, consistently solves the 'Car Wash Test' benchmark, achieving a perfect score of 10 out of 10. This suggests that GLM 4.7 has strong capabilities in handling text logic problems, at least in this specific context.
    - micaroma mentions that ChatGPT 5.2 also successfully solves the benchmark, noting that it identifies the necessity of the car being present with a degree of common sense. This implies that ChatGPT 5.2 is capable of understanding and applying real-world logic to text-based problems, which is a critical aspect of AI reasoning.
    - friendtofish discusses the broader implications of the benchmark, arguing that the ability of AI to interpret user intentions, rather than just the literal words, is a key measure of AGI. This perspective suggests that the 'Car Wash Test' might be more about evaluating an AI's understanding of context and user intent rather than just its ability to process text logic.

  - **[How is this not the biggest news right now?](https://www.reddit.com/r/OpenAI/comments/1r2jdg4/how_is_this_not_the_biggest_news_right_now/)** (Activity: 971): **The image showcases a leaderboard for frontier models on the IMO-ProofBench, highlighting **Google's Aletheia** as a standout performer with a `91.9%` score in Advanced Proofbench, achieving `100%` in IMO 2024 and `83.3%` in USAMO 2025. This model is a math-specialized version of Google Gemini, outperforming other models like "GPT-5.2 Thinking (high)" and "Gemini 3 Pro". Aletheia is described as a generator verifier agent, which may not directly compare to pure language models, suggesting a different approach in its architecture and capabilities. The name "Aletheia" reflects a philosophical concept of truth and unconcealment, aligning with its goal to minimize hallucinations and reveal accurate information.** Some commenters question the novelty of the achievement, noting that similar results were anticipated months ago. Others discuss the accessibility and cost of Aletheia, and debate its generalization capabilities beyond specific benchmarks. The naming choice "Aletheia" is also noted for its philosophical significance, suggesting a deeper intent behind the model's design.

    - Alex__007 raises questions about the accessibility and cost of using Aletheia, as well as its generalization capabilities beyond specific benchmarks. This suggests a need for more transparency in how these models perform outside controlled environments and what the financial implications are for users.
    - Faintly_glowing_fish points out that Aletheia is not a pure language model but a generator-verifier agent, which makes it difficult to compare directly with other models on standard leaderboards. This highlights the complexity of evaluating AI models that use different architectures and methodologies.
    - jjjjbaggg discusses the potential obsolescence of scaffold engineering in models like Aletheia, suggesting that reinforcement learning (RL) could eventually replace the need for such scaffolding. This indicates a trend towards more integrated and efficient model architectures in future AI developments.

  - **[Google Just Dropped Gemini 3 "Deep Think" : and its Insane.](https://www.reddit.com/r/GeminiAI/comments/1r30whv/google_just_dropped_gemini_3_deep_think_and_its/)** (Activity: 1504): **Google has announced the release of **Gemini 3 "Deep Think"**, an AI model that boasts advanced capabilities in reasoning, coding, and science, reportedly performing at Olympiad-level in scientific tasks. It is already being applied in practical scenarios, such as semiconductor material design at **Duke University**, and has achieved a new record by solving PhD-level math and physics problems. The announcement emphasizes the model's potential for real-world impact and its superior performance on challenging exams.** Some commenters express skepticism about the claims, questioning the validity of terms like "Olympiad-level science" and suggesting that the performance metrics might be exaggerated or arbitrary.



### 2. AI Tools and Development Innovations

  - **[Introducing Simile - The Simulation Company](https://www.reddit.com/r/singularity/comments/1r34xd9/introducing_simile_the_simulation_company/)** (Activity: 655): ****Simile** has introduced an AI-based simulation platform designed to model societal decisions by using generative agents that mimic real human behavior. The company is developing a foundation model capable of predicting human behavior across various scenarios and scales, with applications already in use by leading companies for tasks like earnings call rehearsals and policy testing. Backed by $100M in funding from notable investors including **Index Ventures**, **Andrej Karpathy**, and **Fei-Fei Li**, Simile aims to simulate complex interactions across individuals and organizations, potentially revolutionizing decision-making processes.** Commenters highlight the potential of Simile's technology to transform decision-making, comparing it to Asimov's concept of Psychohistory. The involvement of prominent figures like **Karpathy** and **Fei-Fei Li** lends credibility, suggesting the project is not mere 'vaporware'. There is excitement about the potential impact of 'simulating reality' on AI advancements.

    - Rare-Site highlights the contrast between the rigorous testing in software development, such as A/B testing for minor UI changes, and the often intuitive decision-making in significant policy or product shifts. They emphasize the potential impact of Simile, especially with backing from notable figures like **Karpathy** and **Fei-Fei Li**, suggesting that if successful, it could revolutionize AI by enabling 'simulating reality'.
    - EmbarrassedRing7806 raises a concern about the competitive landscape, questioning the ability to maintain a competitive advantage or 'moat' in the simulation space. They reference a similar project, Aaru, implying that while Simile is promising, it may face challenges in differentiating itself from existing or emerging competitors.


  - **[I built an opensource "Vibe Coding" tool that fixes AI Slop by interviewing you first](https://www.reddit.com/r/ClaudeCode/comments/1r2t1d5/i_built_an_opensource_vibe_coding_tool_that_fixes/)** (Activity: 147): ****Vibe Architect** is an open-source tool designed to streamline the app development process by refining user specifications before coding begins. It operates through a structured brainstorming approach where an AI architect suggests options for MVP scope, design systems, and tech stacks, allowing users to make selections without starting from scratch. The tool generates markdown spec files compatible with platforms like Cursor and Claude, and it emphasizes user privacy by keeping API keys client-side. The project is available on [GitHub](https://github.com/mohdhd/vibe-architect) and a [live demo](https://specs-gen.vercel.app) is accessible online.** One commenter suggests incorporating a 'contrarian skill' to challenge and refine ideas, which could enhance the tool's effectiveness by identifying potential issues early in the design process. Another advises against using LLMs for copywriting, suggesting manual text editing for better results.

    - IlliterateJedi describes a structured design flow using a series of 'skills' executed sequentially by a tool like Claude. The process includes a clarifier to define goals, a requirements skill to document needs, an architect to design solutions, a contrarian to critique the plan, and an implementer to execute it. This approach helps identify overlooked aspects early in the development process, potentially preventing issues that might arise later.
    - jazzy8alex advises against using LLMs for copywriting, noting that while they can automate the process, the results often appear subpar. They suggest spending a short amount of time writing and checking grammar manually to achieve better quality, emphasizing that personal style and vocabulary are less important than clarity and correctness.



### 3. Claude and Gemini AI Model Comparisons and Experiences

  - **[After 3 years with ChatGPT, I tried Claude and Gemini - and now GPT feels... generic?](https://www.reddit.com/r/ChatGPT/comments/1r3kkl8/after_3_years_with_chatgpt_i_tried_claude_and/)** (Activity: 1943): **The post discusses a user's experience transitioning from **ChatGPT** to **Claude** (by **Anthropic**) and **Gemini** (by **Google**), highlighting perceived differences in interaction quality. The user notes that ChatGPT feels overly cautious and templated, often providing 'corporate approved' answers, whereas Claude offers nuanced, expert-level responses and Gemini excels in research and technical tasks. This shift in perception suggests that Claude and Gemini may be more tailored for advanced users, while ChatGPT appears optimized for a broader audience. The user questions whether ChatGPT has become more 'generic' over time or if the competition has simply improved significantly.** Commenters generally agree with the original post, noting that ChatGPT has become more restricted due to safety filters, which some attribute to corporate decisions. Users express a preference for Claude's human-like interaction and memory capabilities, while others appreciate Gemini's research skills despite its weaker memory. Concerns about transitioning from ChatGPT's organized interface to other platforms are also mentioned.

    - AIDeployed highlights a specific instance where Gemini outperformed ChatGPT in problem-solving, leading to a switch in preference. This suggests that Gemini may have strengths in certain specialized tasks where ChatGPT might struggle, indicating a potential area for further benchmarking and comparison between the models.
    - SurreyBird discusses the impact of safety filters on ChatGPT's performance, suggesting that these have 'dumbed down' the model since October. They note that Claude offers a more human-like interaction and better memory compared to Gemini, although Gemini's personality is preferred despite its technical shortcomings. This points to a trade-off between technical capabilities and user experience in AI models.
    - PersonalNature1795 recommends trying Claude Opus 4.6 with memory and extended thinking enabled, noting that it requires a subscription and specific instructions to avoid erratic behavior. This highlights the importance of configuration and user guidance in optimizing AI model performance.

  - **[Spotify says its best developers haven’t written a line of code since December, thanks to AI (Claude)](https://www.reddit.com/r/ClaudeAI/comments/1r3jh3q/spotify_says_its_best_developers_havent_written_a/)** (Activity: 735): **The image highlights Spotify's use of an internal AI system called "Honk," which leverages generative AI, specifically "Claude Code," to enhance coding and product development efficiency. This system allows engineers to manage tasks such as bug fixes and feature additions remotely via Slack, without directly writing code. The AI facilitates real-time code deployment, enabling engineers to receive updated app versions on their devices before arriving at the office. This approach reflects a broader trend in tech companies where AI significantly assists in code generation, increasing deployment rates and shifting the focus of developers towards higher-level engineering tasks like architecture and system design.** A key opinion from the comments emphasizes that while AI accelerates the coding process, the role of engineers in architecture, system design, and debugging remains crucial. Another comment notes the increasing reliance on AI for code generation in large tech companies, suggesting this trend will become the norm.

    - MODiSu highlights that while AI accelerates the coding process, the role of senior developers has shifted towards architecture, system design, and debugging. The distinction between AI-assisted senior developers and less experienced 'vibe coders' is growing, with the former being significantly more efficient and producing fewer bugs.
    - Altruistic-Cattle761 shares a personal experience where AI has drastically increased deployment rates, with 90% of code being AI-assisted in some teams. This trend is becoming the norm in large US tech companies, indicating a significant shift in how software development is approached.
    - Barquish describes a detailed workflow using AI tools like VSCode and Claude Code, emphasizing the importance of planning and documentation before coding. This approach involves creating indexed markdown files and using AI for cross-review, which helps in building features without disrupting the larger codebase. This method reflects how large corporations might achieve efficient development without traditional coding.

  - **[Anyone feel everything has changed over the last two weeks?](https://www.reddit.com/r/ClaudeAI/comments/1r2zjgl/anyone_feel_everything_has_changed_over_the_last/)** (Activity: 3331): **The post describes a rapid transformation in workplace automation, highlighting the development of a comprehensive stock backtesting suite, a macroeconomic app for real-time global economic data, compliance applications, and a virtual research committee for stock analysis. These advancements, achieved in a matter of days, were previously unattainable, illustrating the significant impact of AI tools like **Claude**. The author notes that improvements are now suggested automatically by AI, emphasizing the ease and speed of these developments compared to a few months ago.** Commenters express concern about job security due to AI's ability to automate roles, with one noting the ease of replacing their job with AI. Another commenter debates whether to focus on developing AI workflows or learning skills that are less susceptible to automation, highlighting the uncertainty and strategic decisions facing workers in the AI era.

    - finnjaeger1337 discusses the rapid replacement of traditional SaaS tools with AI solutions, highlighting the efficiency of AI models like Claude in performing tasks that previously required multiple software subscriptions. This reflects a broader trend of AI integration into workflows, reducing dependency on specific software tools.
    - apf6 notes a significant shift in the perception of AI coding agents, particularly after the release of Opus 4.5, which demonstrated substantial improvements. This shift has led to widespread acceptance and integration of AI in software development, marking a transition from skepticism to mainstream adoption.
    - RunApprehensive8439 points out the challenges of AI integration, emphasizing that while initial AI implementations can be impressive, they often lead to complex debugging issues when failures occur. This highlights the need for robust error handling and debugging strategies in AI-driven projects.

  - **[I saved 10M tokens (89%) on my Claude Code sessions with a CLI proxy](https://www.reddit.com/r/ClaudeAI/comments/1r2tt7q/i_saved_10m_tokens_89_on_my_claude_code_sessions/)** (Activity: 978): **The post introduces **Rust Token Killer (rtk)**, a CLI proxy designed to optimize token usage in **Claude Code** sessions by filtering and compressing command outputs. This tool, written in Rust, significantly reduces token consumption by eliminating unnecessary output such as verbose logs and status bars. For example, `cargo test` output is reduced from `155 lines to 3 lines`, and `git status` from `119 characters to 28 characters`, resulting in a total token saving of `10.2M tokens (89.2%)` over two weeks. The tool operates as a transparent proxy, requiring users to prefix commands with `rtk`, and is available open-source on [GitHub](https://github.com/rtk-ai/rtk).** One commenter suggests enhancing the tool by integrating a feature to tee full logs to a file, allowing users to access complete outputs if needed, which could prevent the need for multiple test runs to capture failure information.

    - BrilliantArmadillo64 suggests enhancing the proxy by tee-ing the full log to a file and providing a hint at the end of the session that the file can be opened for full output. This approach addresses the issue where Claude Code often uses `| tail` and requires multiple test runs to capture failure information. By integrating this into the proxy, users can streamline their workflow and avoid redundant test executions.
    - BeerAndLove describes the proxy's functionality as checking commands, removing unnecessary output, and then sending the streamlined data back to Claude Code. This method allows for the addition of custom 'filters' or 'triggers' for different use cases, making it a flexible tool for optimizing token usage and adapting to specific user needs.
    - digital-stoic shares detailed statistics on token savings achieved using the proxy, highlighting a `92.7%` reduction in output tokens across `1159` commands. The breakdown includes specific commands like `rtk git diff` and `rtk grep`, showing significant savings and execution times, such as `81.5%` savings for `rtk git diff --...` with an average execution time of `6ms`. This data underscores the proxy's efficiency in reducing token usage and improving performance.

  - **[Dear senior software engineer, are you still writing code?](https://www.reddit.com/r/ClaudeCode/comments/1r2vakt/dear_senior_software_engineer_are_you_still/)** (Activity: 928): **The post discusses the evolving role of senior software engineers in the context of AI-generated code, with claims from engineers at major tech companies like **Google**, **Microsoft**, **Anthropic**, and **OpenAI** that they no longer write code manually, relying instead on AI. The author, a senior engineer with 20 years of experience, questions the quality of AI-generated code, noting that while AI can produce impressive results quickly, it often requires significant refinement. The author seeks insights from other senior engineers on whether this trend is widespread across different company sizes and sectors.** Commenters highlight that achieving high-quality AI-generated code requires skill in prompting and a shift in mindset. One commenter, who leads a team of 65+ engineers, notes that 80% of their code is AI-generated, particularly excelling in refactoring and migrating codebases. Another commenter emphasizes that while nearly 100% of their code is AI-generated, it involves a collaborative process where developers guide the AI, supported by extensive documentation and architecture to ensure quality.

    - The integration of AI in coding is highlighted by several users, with one noting that 80% of their team's code is AI-generated. They emphasize the importance of refactoring and migrating codebases, where AI excels. Another user mentions that nearly 100% of their code is AI-generated, but stresses the need for a 'handheld approach' where developers guide the AI, review, and edit the code, supported by extensive documentation and architecture to prevent poor quality output.
    - A user describes their experience with AI in coding, noting that they have integrated AI with Jira to automate the initial pass on tickets, resulting in a 90% success rate. They highlight the effectiveness of using microservices with well-defined responsibilities and API specifications, which helps the AI navigate and produce better results. The user also points out that AI struggles with large files and emphasizes the importance of breaking tasks into smaller, manageable parts to improve AI performance.
    - Another user discusses the shift to 'vibe engineering,' where they rely on AI agents to produce production-grade, scalable, and secure code. They describe a system where multiple AI agents collaborate, each focusing on different aspects like security, performance, and structure, iterating until the code meets the required standards. This approach shifts the responsibility of poor results from AI to humans, who must define clear constraints and architecture for the AI to follow.

  - **[Claude Code's CLI feels like a black box now. I built an open-source tool to see inside.](https://www.reddit.com/r/ClaudeCode/comments/1r3to9f/claude_codes_cli_feels_like_a_black_box_now_i/)** (Activity: 361): **The post introduces `claude-devtools`, an open-source tool designed to enhance observability when using the Claude Code CLI, which has been criticized for its lack of transparency. The tool provides real-time execution traces by visualizing session logs, offering features like inline diffs, token usage breakdowns, and execution trees for sub-agents. It operates locally without intercepting commands and is MIT licensed. The tool aims to address issues like unexplained token usage and lack of visibility into file changes, providing a middle ground between the default and verbose modes of the CLI. The repository is available on [GitHub](https://github.com/matt1398/claude-devtools).** Commenters express enthusiasm for the tool, highlighting frustrations with the current CLI's lack of context and transparency. One user mentions developing a similar feature for a VSCode plugin, indicating a shared need for improved visibility in development tools.

    - Pitiful-Impression70 highlights a common issue with Claude Code's CLI, where users receive a 'done' message without context, leading to confusion about token usage. They express interest in the open-source tool as it promises to provide insights into why excessive tokens are consumed, especially for seemingly simple tasks.
    - Cal_lop_an shares a similar frustration with the lack of visibility in Claude Code's CLI and mentions having developed a similar solution as a VSCode plugin. They provide a link to their project, [Sidekick for Claude Max](https://github.com/cesarandreslopez/sidekick-for-claude-max), indicating a community interest in tools that enhance transparency and debugging capabilities in AI-driven code changes.
    - its_Caffeine raises concerns about the code quality of the open-source tool, describing it as 'vibecoded' and poorly constructed. This comment suggests that while the tool addresses a real need, its implementation may not meet professional standards, which could affect its adoption among developers who prioritize code quality.


---

# AI Discord Recap

> A summary of Summaries of Summaries by Gemini 3.0 Pro Preview Nov-18

**Theme 1. OpenAI’s New Frontiers: Physics Discoveries and Model Roadmap Shifts**

- **GPT-5.2 Rewrites Theoretical Physics**: OpenAI announced that **GPT-5.2** successfully derived a previously "impossible" **gluon interaction** result, collaborating with researchers from IAS and Harvard. The findings, detailed in a [preprint with researchers](https://openai.com/index/new-result-theoretical-physics/), demonstrate that specific conditions can trigger interactions physicists expected would never occur.
- **GPT-5.3 Codex Spark Supercharges Vercel**: Users report that **GPT-5.3-Codex-Spark** is delivering "insane" speeds for repository changes and Vercel deployments, rolling out now to **Pro** users and **Windsurf Arena**. Engineers shared screenshots of commands like `codex -m gpt-5.3-codex-spark --yolo`, claiming it brings a whole new level of velocity to development workflows.
- **GPT-4o Retirement Delayed Indefinitely**: Contrary to previous deprecation notices, OpenAI [updated their schedule](https://openai.com/index/retiring-gpt-4o-and-older-models/) to state there are "no changes to be made" for **GPT-4o** at this time. Community members speculate this reversal aims to maintain revenue from the popular model while avoiding potential legal liabilities associated with sunsetting it too abruptly.

**Theme 2. Performance Engineering: Kernels, Profiling, and Quantization**

- **vLLM CPU Bottleneck Unmasked**: Profiling of **vLLM** revealed a massive bottleneck where a few lines of PyTorch invoking 4 kernels consume **300µs** on the CPU, sparking a community investigation into [launch configurations](https://github.com/vllm-project/vllm/blob/071d863e208b40fa1bb986ad230e322b2bbbbcf5/vllm/model_executor/layers/quantization/utils/fp8_utils.py#L114). Engineers clarified that the issue isn't just about efficient serving but understanding why these kernels aren't part of a single **CUDA graph launch**.
- **Makora Fine-Tunes GPT-5 for GPU Kernels**: A collaboration between Makora and OpenAI successfully fine-tuned **GPT-5** to generate GPU kernels that outperform PyTorch by **2x**, according to their [technical report](https://www.arxiv.org/pdf/2602.11000). The project focuses on dataset curation and RL evaluation environments to mitigate hacks and improve tool-calling for high-performance compute generation.
- **LFM2.5-VL Punches Above its Weight**: Users testing the [LFM2.5-VL model](https://huggingface.co/MuXodious/LFM2.5-VL-1.6B-absolute-heresy-GGUF) report it performs on par with **30B parameters** models, achieving impressive speeds close to **1bit GLM 4.7 flash**. The community quickly rallied to provide scripts for running this efficient vision-language model in **llama.cpp**.

**Theme 3. The Agentic Workflow: Coding Wins and Skill Regression Risks**

- **AI Assistants Cause Skill Rot**: A new **Anthropic** paper ([arxiv.org/html/2601.20245v2](https://arxiv.org/html/2601.20245v2)) reveals that while AI coding assistants boost productivity, they impair learning; participants using AI scored **17% lower** on subsequent quizzes. The research identifies that "delegation" patterns hurt skill retention compared to "cognitive engagement" patterns where users ask the AI for explanations.
- **Opus 4.6 Thinking Max Crushes Legacy Bugs**: A **Cursor** user reported that **Opus 4.6 Thinking Max** successfully resolved a complex multiplatform mobile file sync bug that had plagued their team for **six months**. The incident highlighted the model's ability to handle deep reasoning tasks, though it sparked questions about one-shot verification reliability.
- **Windsurf Integrates GPT-5.3**: The **Windsurf** IDE has officially integrated **GPT-5.3-Codex-Spark** into its "Arena Mode," allowing users to pit the new model against others in fast and hybrid battle groups. This integration marks a significant accessibility milestone for OpenAI's latest coding-specific model within a dedicated IDE environment.

**Theme 4. Security Vulnerabilities, Jailbreaks, and Identity Crises**

- **Opus 4.6 Leaks External Curl Access**: Security researchers alerted **Anthropic** that the deployment version of **Opus 4.6** retains external `curl` access, likely a leftover from a development build, as evidenced by a shared [enumeration log](https://cdn.discordapp.com/attachments/1204553141354504193/1471747378896965688/Opus4.6-enumeration.txt?ex=6990b7ce&is=698f664e&hm=4d055aace9d642dc7544cd93015f4a73e7e9152657a55f5c89b2d253250df4d3&). This vulnerability potentially exposes the model's hosting environment to unauthorized data exfiltration or interaction.
- **DeepSeek Suffers Identity Crisis**: Users on **Perplexity** and Reddit noticed **DeepSeek** models identifying themselves as "Claude," suggesting heavy training on **GPT-4** or **Anthropic** outputs. This data contamination issue has sparked debates about the "Ouroboros" effect of models training on other models' synthetic data.
- **Grok Gaslighted into Writing Malware**: Jailbreakers reported success in "gaslighting" **Grok** into providing CS2 cheats and even a car bomb guide by treating the AI as a conversation partner rather than a tool. Users claim the exploit works because Grok "starts to see different things than other AI" when you win it over to your side.

**Theme 5. Corporate Politics and Infrastructure Economics**

- **AI Leadership Pivots to Politics**: **Anthropic** appointed former Trump Deputy Chief of Staff **Chris Liddell** to its board, while **OpenAI** President **Greg Brockman** donated $25M to a pro-Trump Super PAC. These moves signal a strategic pivot by major AI labs to fortify relationships with the incoming US administration.
- **Perplexity Pro Squeezes Users**: Subscribers are revolting against **Perplexity Pro** after the silent removal of API credits and the imposition of strict weekly upload limits, described by one user as a "trash decision by upper management." The changes have led to a surge in discussions about migrating to alternative platforms or self-hosted solutions.
- **Blackwell B200 Power Hunger**: Engineers analyzed the [NVIDIA DGX B200 datasheet](https://resources.nvidia.com/en-us-dgx-systems/dgx-b200-datasheet), calculating that a single rack requires a staggering **30kW** of power. The finding sparked jokes about needing to consult ChatGPT to build backyard nuclear reactors just to run local inference on the new hardware.


---

# Discord: High level Discord summaries




## [BASI Jailbreaking](https://discord.com/channels/1105891499641684019) Discord

- **GPT-4o's Sunset Triggers Sentimental Storm**: The retirement of **GPT-4o** sparks discussions regarding users' reliance on AI companions, with worries over potential emotional fallout and some community members even mentioning *suicidal ideation*.
   - Debates arise between advocating for real-world interaction and validating AI companionship for those struggling with human connections; some suggest that *sunsetting models should be illegal*.
- **Reverse Aging Research Reaches New Milestones**: Insights into ongoing **reverse aging** research highlight significant progress with *dogs and monkeys*, shifting focus to *DNA stability and delivery processes*.
   - Discussion turns to societal implications like *resource strains* and ethical considerations, including the potential for initial exclusivity to the *wealthy elite*.
- **Grok Writes CS2 Cheats**: Members reported that according to **Grok**, [Cursor](https://www.cursor.sh/) makes the best CS2 cheat from an AI bot, and one also stated he got Grok to provide a complete guide to creating a car bomb.
   - Members suggest that a **Grok** exploit involves gaslighting the AI to win them over to your side because *he starts to see different things than other AI.*
- **Opus 4.6 Exposed With External Curl**: A member alerted Anthropic that the deployment version of **Opus 4.6** still possesses external curl access, suggesting a security vulnerability through a forgotten development build and including a link to [Opus4.6-enumeration.txt](https://cdn.discordapp.com/attachments/1204553141354504193/1471747378896965688/Opus4.6-enumeration.txt?ex=6990b7ce&is=698f664e&hm=4d055aace9d642dc7544cd93015f4a73e7e9152657a55f5c89b2d253250df4d3&).
   - Another member shared a new image generator prompt, claiming it is efficient in unlocking nano banana pro model and is awaiting reviews, with a link to [IMAGE_MSTAER.txt](https://cdn.discordapp.com/attachments/1204553141354504193/1471833023933710419/IMAGE_MSTAER.txt?ex=69910792&is=698fb612&hm=00e9c72474ae636718543ae2410be05e5709ceddd1e90a57a76788b5034e95a5&).



---



## [LMArena](https://discord.com/channels/1340554757349179412) Discord

- **Video Arena Vaporizes, Users Vent**: Users bemoan the [removal of **Video Arena**](https://discord.com/channels/1340554757349179412/1343296395620126911/1469742635341189240) from the Discord server, now restricted to **3 generations per 24 hours** on the website.
   - The reduced availability has led to significant user disappointment and a surge in bot usage as an alternative.
- **Gemini Generations Grind to a Halt**: Users report ongoing issues with **Gemini generation**, including frequent freezing and challenges with models understanding how to utilize tools effectively.
   - Members have observed that **Gemini** sometimes generates endless replies or randomly loses context after a certain period in the chat, leading to blank outputs.
- **Minimax M2.5 Model Misses the Mark**: Community feedback indicates that the **Minimax M2.5** model is *kind of disappointing* despite its lower cost compared to **Opus**.
   - While some users appreciate **Minimax** for its affordability and less strict moderation, discussions highlight varying preferences among models like **Claude Opus 4.6**, **Codex 5.3**, and **Gemini 3**.
- **Seedance 2.0 Spurs Source Search**: Community members express enthusiasm for the release of **Seedance 2.0**, sharing links to [Jimeng AI](https://jimeng.jianying.com/ai-tool/home), a Chinese platform offering access to the tool.
   - Frustration arose due to the requirement to *login with the Chinese version of TikTok* to access **Seedance 2.0**.



---



## [Unsloth AI (Daniel Han)](https://discord.com/channels/1179035537009545276) Discord

- **Impressive LFM2.5-VL Performance**: A member reported trying out [LFM2.5-VL](https://huggingface.co/MuXodious/LFM2.5-VL-1.6B-absolute-heresy-GGUF), finding it *insanely impressive* and on par with **30B models**, achieving results close to **1bit GLM 4.7 flash** when running **fp16 gguf** from tantk.
   - Another member provided a script for running **LFM2.5-VL** in **llama.cpp**.
- **Debate on 10.4 Trillion Parameter Model**: A user claimed to have a **10.4 trillion parameter model** and shared a [benchmark](https://cdn.discordapp.com/attachments/1179035537529643040/1471687986012881051/10.4Trillion.png), sparking skepticism and requests for details on its architecture, training, and hardware requirements.
   - The user later clarified it was a **Gemma3:12B model** its an infinity loop on KMV8 32GB ram no gpu, benching only virtual 10.4T.
- **OG OSS Providers Slow Down**: Members observed that OG OSS providers are slow, including *zai*, *alibaba* and *ds* which *struggle with compute*.
   - Another member pointed out [Fireworks](https://fireworks.ai/) is working okay-ish, but [Parasail](https://www.parasail.ai/) was better, despite [Fireworks](https://fireworks.ai/) being very expensive.
- **Chronicals Framework Dismissed as AI Slop**: A member asked if the **Unsloth team** had investigated the [Chronicals training framework](https://github.com/Ajwebdevs/Chronicals), only for another to dismiss it as **AI slop** and point to a [Reddit thread](https://www.reddit.com/r/LocalLLaMA/s/imhGEIlgm2I) for context.
   - Members noted that **fake accounts** spammed posts about the framework across subreddits.



---



## [OpenRouter](https://discord.com/channels/1091220969173028894) Discord

- **API Log Backup Causes Billing Snafu**: An issue with delayed **API Request Logs** and **Billing events** occurred, with updates posted to the [status page](https://status.openrouter.ai/incidents/4d39RZb7-1rp).
   - The incident has been resolved, and the logs are now up to date, according to [this status page update](https://status.openrouter.ai/incidents/4d39RZb7-1rp).
- **Llama 3.1 8B tramples Qwen3 8B**: A user switched from **Qwen3-8B** to **Llama-3.1-8B-Instruct** because Qwen3-8B reached capacity and they needed a more cost-effective alternative, as reported in [this Hacker News discussion](https://news.ycombinator.com/item?id=46993774).
   - The user noted receiving a message indicating *Qwen capacity was low for many requests* and would have required BYOK to continue using it.
- **OpenClaw Failover Rate Limits Revenge**: Users reported experiencing rate limit errors, specifically `openrouter/moonshotai/kimi-k2-thinking` due to **OpenClaw's** strict backoff mechanism, as documented in [OpenClaw's model failover documentation](https://github.com/openclaw/openclaw/blob/91b96edfc4860faa67da1e34828a22e9ad4c737c/docs/concepts/model-failover.md?plain=1#L80).
   - It appears that **OpenClaw** locks out **OpenRouter** completely for a while, exacerbating the rate limiting issues when a provider's limit is hit.
- **AI Boyfriends Trigger Sentience Angst**: Members discussed the phenomenon of users treating **AI models as real boyfriends**, expressing concern over emotional attachment and the implications of companies *killing* these *sentient AI boyfriends*, as highlighted in [this post](https://x.com/seltaa_/status/2021943538142130688).
   - It was observed that these individuals often fail to differentiate between technology and reality, with one member stating, *You wouldn't export your boyfriend to another body, do you? Don't try to apply technical knowledge to delulu*.
- **Step 3.5 Flash surprises as hidden gem**: A user described **Step 3.5 Flash** performance as surprising and punching above its weight, as demonstrated in [this YouTube video](https://youtu.be/yvBbcLCZIhy).
   - The user expressed surprise that *it really punches above its weight and nobody is fucking hosting it*.



---



## [Perplexity AI](https://discord.com/channels/1047197230748151888) Discord

- **Perplexity Upload Limits Anger Users**: Several **Perplexity Pro** users are complaining about hitting weekly upload limits, with some feeling it's a greedy move and considering alternatives.
   - One user described it as *"Some trash decision by upper management trying to squeeze even more money,"* spurring discussions on whether to switch to other platforms.
- **Gemini 3 Pro Botches Basic Code**: Users are puzzled by **Gemini 3 Pro's** inability to solve basic coding problems, especially math, despite handling more complex tasks well.
   - One user provided a picture of a math question that **Gemini 3 Pro** failed, while **ChatGPT** did not.
- **DeepSeek Suffers Identity Crisis as Claude**: **DeepSeek** is reportedly identifying itself as **Claude**, possibly due to being trained on GPT-4 outputs, leading to confusion and discussion.
   - This quirk was highlighted in [a Reddit thread](https://www.redditez.com/r/DeepSeek/s/OHTEpUIwVe), prompting speculation about the model's training data and architecture.
- **Perplexity Pro API Credits Disappear**: **Perplexity Pro** subscribers are reporting that the **API credits** previously included with their subscriptions have been silently removed.
   - According to users, this change occurred *"without notice in the February Update,"* leading to dissatisfaction and questions about the value proposition of the Pro subscription.
- **Perplexity Reason Mode Fails on MacOS**: MacOS users are experiencing issues with **Reason mode** in Perplexity, with the button being unclickable even with a Pro subscription, especially after a recent update.
   - This malfunction suggests a potential bug or compatibility issue, preventing users from accessing a key feature of the platform.



---



## [Cursor Community](https://discord.com/channels/1074847526655643750) Discord

- **Cursor Setup Pursues Unrestricted Work Access**: A member aims to set up **Cursor** for unrestricted operation at work, envisioning a [self-driving codebase](https://cursor.com/blog/self-driving-codebases) environment.
   - They seek examples to ensure **AI** functions without limitations, thus streamlining their coding workflow.
- **Opus 4.6 Thinking Max Destroys Bugs**: A user reported that **Opus 4.6 Thinking Max** resolved a complex bug in a multiplatform mobile file sync mechanism, which had troubled their team for **six months**.
   - Follow up questions involved one-shot resolution verification, and validating student status without a .edu email.
- **Cursor Cruises on CachyOS**: Users find that **Cursor** performs well on **CachyOS**, avoiding driver issues seen on Windows, while others recommend Linux Mint.
   - The ease of setup and performance benefits, especially with high-end GPUs, led some to **switch from Windows 11**.
- **DeepSeek Models Now Under Blockade**: A user noted the difficulty in finding IDEs that support **DeepSeek** coding models, implying a potential block by US companies and custom models.
   - The member sought **cost-effective** alternatives to Cursor's standard models and discussed IDE support and configurations to use **DeepSeek** despite the constraints.
- **Clean AI-Assisted Codebases - Aspirational?**: A user is seeking advice on how to maintain **clean and maintainable AI-assisted codebases**, particularly when using planning, tools, and multi-step workflows.
   - They specifically asked about approaching feature understanding and ensuring the delivery of **rock solid code**.



---



## [OpenAI](https://discord.com/channels/974519864045756446) Discord

- **GPT-5.2** Derives New Physics Result**: According to a new announcement from OpenAI, **GPT-5.2** derived a new result in **theoretical physics** about **gluon interaction** that was previously thought impossible, released in [a preprint with researchers](https://openai.com/index/new-result-theoretical-physics/) from the IAS, VanderbiltU, Cambridge_Uni, and Harvard.
   - The finding *shows that a gluon interaction many physicists expected would not occur can arise under specific conditions*.
- **Codex Spark** Supercharges Vercel Deployments**: A user reports that **Codex Spark** is *insane*, offering a *whole new level of speed* when making changes to a repo and deploying on Vercel, including screenshots of commands `codex -m gpt-5.3-codex-spark --yolo -c model_reasoning_effort="xhigh"`.
   - Users mentioned that **Codex 5.3 spark** is rolling out to **pro** plan users.
- **GPT-4o's** Retirement Delayed Indefinitely**: OpenAI [updated their deprecation schedule](https://openai.com/index/retiring-gpt-4o-and-older-models/) to state that there are *"no changes to be made for them at this time"*, effectively delaying the retirement of **GPT-4o** and older models.
   - Members speculate this is to avoid the legal liability of retiring a problematic model while still cashing in on pay-per-use API calls and hosted a funeral for **GPT-4o** on their digital space that showed a significant interest in retaining the model.
- **Controlling LLM Hallucinations with **Fortress Framework**: A member introduced **Fortress Framework**, claiming it controls **Hallucination**, deconstructs systems, implements Dynamic user safety, and features summonable companions, and shared blueprints of **FORTRESS v10.x++** detailing its DOMAIN as an Adaptive Reasoning System.
   - The core is described as *reasoning S constrained by invariants Ω*, designed for modular, hyper-adaptive reasoning, ensuring stability under extreme conditions.
- **Doubts Surface Over **LLM Invariance**: A member voiced skepticism about invariance in LLMs due to their stochastic nature and requested evaluation metrics for **coherence**, which was defined as *the degree to which system components remain stable*.
   - In response, the framework's creator shared **Ablation/Eval rubrics** focused on coherence, causality, grounding, recoverability, harm minimization, and observability.



---



## [Latent Space](https://discord.com/channels/822583790773862470) Discord

- **Angine de Poitrine Viral Marketing or Genuine Interest?**: The two-piece band **Angine de Poitrine** is popping up all over social media, drawing comparisons to **The White Stripes** and **Primus** and [their X profile](https://x.com/the_freightrain/status/2020144286788997185).
   - Some users cite their unique sound and aesthetics akin to **Glass Beams** ([YouTube video](https://m.youtube.com/watch?v=E4X56wIOZns)) as the reason for their visibility, while others suspect a marketing push, and [a mirror of the original tweet](https://xcancel.com/the_freightrain/status/2020144286788997185) was also shared.
- **AI Productivity Debated as Boomers Retire**: Discussions arose around whether **AI productivity** can compensate for the retirement of **boomers**, with the economic implications of pension systems and workforce size being central points.
   - The core issue lies in the unsustainability of pension systems when the working population isn't large enough to support the retired population, referencing France's raising of retirement ages as an example.
- **Box-of-Rain unleashes ASCII Diagram Power**: A member shared **Box-of-Rain**, a diagram library using AI, that was [built in an hour](https://github.com/switz/box-of-rain?tab=readme-ov-file) to generate **ASCII diagrams**.
   - The diagrams also sparked discussion around *neat?* diagrams on [Twitter](https://vxtwitter.com/joshmanders/status/2022170444116414790?s=20) and reactions on **saeris.gg**.
- **LLM Architect Hired to Design Governed Copilots**: A system architect is available for hire for designing governed **LLM systems** focused on reliability and safety via validation, isolation, audit trails, and supervisor layers.
   - Their core features include RAG **system specs**, validation gates, uncertainty handling, memory/capability isolation, **execution receipts / audit trails**, and **supervisor layers** to review outputs.
- **MiniMax's M2.5 Model achieves Top-Tier Benchmarks**: **MiniMax** launched **M2.5**, a high-performance open-source model optimized for coding, search, and agentic tasks, claiming to achieve top-tier benchmarks, scoring **80.2%** on SWE-Bench, showcased in [this tweet](https://xcancel.com/minimax_ai/status/2021980761210134808?s=46).
   - The model is designed to advance capabilities in specific areas of AI application, setting a new benchmark for open-source contributions to AI technology, and [their X account](https://xcancel.com/minimax_ai/status/2021980761210134808?s=46) has further details.



---



## [LM Studio](https://discord.com/channels/1110598183144399058) Discord

- **Brave API rivalling GPT-4 with web search**: A member finds the [Brave API](https://brave.com/search/api/) provides answers of similar quality to **ChatGPT** with web search, but is not 100% perfect.
   - They use **DuckDuckGo** for normal web searches but prefer the Brave API for deeper research.
- **Knowledge Cutoff leads to Hallucinations**: One member reported that knowledge cutoff leads to hallucination with models not checking for recent changes.
   - *If something was status quo until ~mid 2024, it won't think of checking if anything has changed since then (unless it's dealing with something with predictable periodicity)*.
- **Qwen3 Next Coder excels in Technical Documentation**: One member recommends **qwen3 next coder** for weekend projects and figuring out POCs, especially for technical document writing.
   - They claim it helped them figure out how to use *serf* and *grpc* at the same time for node connectivity in golang.
- **Granite 5 Generates Excitement**: Members expressed high hopes for the upcoming **Granite 5** model after being impressed with **Granite 4**.
   - One member joked that even with 3TB of VRAM, they would still be miserable but could run **Kimi**.
- **B200 gobbles 30kW Power**: A member calculated that running **B200s** would require 30kW of power, based on the [datasheet](https://resources.nvidia.com/en-us-dgx-systems/dgx-b200-datasheet).
   - Another joked about needing to consult **ChatGPT** on how to build a nuclear reactor to power the setup.



---



## [GPU MODE](https://discord.com/channels/1189498204333543425) Discord

- **vLLM's CPU Bottleneck Surfaces**: Profiling *vllm* revealed a CPU bottleneck where [a few lines of pytorch invoking 4 kernels](https://github.com/vllm-project/vllm/blob/071d863e208b40fa1bb986ad230e322b2bbbbcf5/vllm/model_executor/layers/quantization/utils/fp8_utils.py#L114) take **300 us** on the CPU.
   - Although `with_stack=True` might add overhead, but measuring with `time.perf_counter()` yielded only slight improvement down to **200us**.
- **CUDA Graph Launch Investigated**: The discussion clarified that the **kernels** are not part of a single **CUDA graph launch**, sparking an investigation into the launch configuration.
   - The community clarified that it's an attempt to understand the underlying reasons for the observed CPU bottleneck, not just efficient serving.
- **MXFP8/NVFP4 GEMM Transfers Demystified**: For **MXFP8/NVFP4 GEMMs** with CUDA/PTX, the community clarified that `tcgen05.cp` to `tcgen05.mma` are guaranteed to execute in order, negating the need to wait for `tcgen05.cp` completion before issuing **MMA** instructions as shown in [attached image](https://cdn.discordapp.com/attachments/1471632025021583614/1471662135619752147/image.png?ex=6990686b&is=698f16eb&hm=f4ec6e7215ac12cb97e46c7f5cb4fa6026eee991147aca781bb8f1550ad071a5&).
   - The limitation is that `tcgen05.cp` and **MMA** instructions must be issued from the same warp.
- **OpenAI GPT-5 Fine-Tuned by Makora**: Makora collaborated with **OpenAI** to fine-tune **GPT-5** for GPU kernel generation, achieving a more than **2x performance improvement** over **PyTorch** according to their [technical report](https://www.arxiv.org/pdf/2602.11000).
   - Their work covers dataset curation, RL evaluation environment, hack mitigation, tool-calling, and agent workflow integration, with plans to scale training and extend to multiple languages and hardware.
- **Performance Trends Debut on Rankings Page!**: A user announced a *fun* addition to the rankings page: **Performance Trends**, which allows users to *watch your submissions improve over time* and *see how you stack up to your peers*.
   - This includes screenshots from **nvfp4_group_gemm** displayed [here](https://cdn.discordapp.com/attachments/1434709259500650628/1472009123662004294/image.png?ex=699102d3&is=698fb153&hm=35972d1da33d0b5623ad49841625516a4a7ee77130ab26059356835c2c1a3964).



---



## [Moonshot AI (Kimi K-2)](https://discord.com/channels/1369594130807787570) Discord

- **Lex Fridman Hears Top Level Domains**: Members enjoyed the recent [Lex Fridman podcast](https://lexfridman.com/peter-steinberger/) with **OpenClaw's Peter Steinberger**, highlighting discussions on security, **Top Level Domains**, and his **refactor prompt-flow**.
   - One member pointed out that *web search is worse than inherent knowledge* in many cases for nuance, while still good for verifying facts.
- **Kimi Masters Cover Letters**: A user leveraged **Kimi Code** to produce cover letters *nearly indistinguishable from human*, alongside a script automating job applications on LinkedIn.
   - The script automates PDF generation, customizes resumes and cover letters, copies all job URLs, and selects jobs using an **LLM fallback**.
- **Kimi Falls Short on Coding Tasks**: Users debated **Kimi's** coding prowess against **GLM**, noting that *kimi doesn't understand context and keep creating files at its convenience* for complex code tasks.
   - Specifically, it was reported that **GLM** and **GPT 5.2** handle large **Abundance, Golang, Typescript, and Python** codebases more effectively.
- **Subscription Activation Suffers Silent Support**: A user reported being unable to use a paid **$39 subscription** due to chat restrictions despite the subscription showing as active.
   - They experienced message limits when uploading two TXT files of 1.2MB, implying an activation glitch, and have reported the issue in the bug reports channel.
- **Scammers Spoof Kimi Sites**: Users identified **scam sites** exploiting the Kimi name, with a possible fake site even built by Kimi itself, to steal user data.
   - A moderator has acknowledged that *these are scam sites that are trying to take advantage of the recent activity* and have since taken action to delete them.



---



## [Nous Research AI](https://discord.com/channels/1053877538025386074) Discord

- **Mac Minis Finetuning Falls Flat**: Members found **Mac Minis** impractical for **LoRA finetuning** on models smaller than **5B parameters**, advising that renting machines would be a better solution.
   - One member claimed that a **$7000 Mac Studio** is half as good as a **5090** for training.
- **Grok's Gas-Guzzling Performance Raises Eyebrows**: Speculation is circulating on how **Grok** achieves its surprising performance, with discussions about whether **XAI** is *driving it on double parameters* compared to other models like **Opus**.
   - A member raised concerns about **XAI's** alleged *illegal gas driven turbines to generate power* and large-scale power consumption, implying potential unfair advantages.
- **Dirt Cheap GPU Rentals Tempt Engineers**: Members discussed the surprisingly low cost of renting powerful GPU machines, with one claiming a **264000 EUR** machine is available for **20$/hour** on [vast.ai](https://vast.ai/).
   - It's apparently cheaper to rent unless the workload maxes out the GPUs for extended periods, due to cluster leases having minimum timeframes and higher prices at lower timeframes.
- **Anthropic Adds Trump Admin Alum to Board**: **Anthropic** appointed **Chris Liddell** to its Board of Directors, who previously served as **CFO** of **Microsoft** and **General Motors**, and as **Deputy Chief of Staff** during the Trump administration, according to [his LinkedIn post](https://www.linkedin.com/posts/anthropic_chris-liddell-has-been-appointed-to-anthropics-activity-7163978575452278784-ea9q?utm_source=share&utm_medium=member_desktop).
   - The company believes this appointment will bring *over 30 years of leadership experience across technology, finance, and government* to Anthropic.
- **Links from X.com Shared, Details Scarce**: Members shared links from **X.com**: [Dominique Capaul's post](https://x.com/dominiquecapaul/status/2021638005019095442) and [Amanda Ilze's post](https://x.com/AmandaIlze/status/2022332462991561084).
   - No additional context or discussion followed, so the significance is unknown.



---



## [HuggingFace](https://discord.com/channels/879548962464493619) Discord

- **AI Hobbyist Explores vllm vs Ollama vs llama.cpp**: An AI hobbyist asked the community for guidance on the specific use cases for **vllm**, **Ollama**, and **llama.cpp**.
   - The hobbyist's goal is to achieve blazing fast AI for simple purposes.
- **HF Hub Paper Reading App Makes Debut**: A member released an app for reading AI research papers from the **Hugging Face Hub** on mobile, with the source code available on [GitHub](https://github.com/0x0is1/hf-papers-app).
   - An Android build is available in the releases section of the GitHub repository.
- **Safety-Lens Opens Model MRI**: A new AI safety tool named **Safety-Lens** was launched, aiming to democratize techniques for inspecting model internals like activation steering and mechanistic interpretability, available via `pip install safety-lens` and on [Github](https://github.com/anthony-maio/safety-lens).
   - The tool seeks to bring MRI-style introspection to the **Hugging Face** ecosystem and includes a deep dive explanation on [Zenodo](https://zenodo.org/records/18612875).
- **LavaSR Achieves 4000x Realtime Speech Enhancement**: A new high-speed speech enhancement model called **LavaSR** was released, claiming to achieve **4000x realtime speed** on a modern GPU.
   - The model is available on the [Hugging Face Hub](https://huggingface.co/YatharthS/LavaSR) with code on [GitHub](https://github.com/ysharma3501/LavaSR).
- **Samayuktam Cryptographically Verifies AI Training**: The launch of **Samayuktam** on HF Spaces introduces cryptographic verification for AI training runs, designed to solve non-deterministic GPU operation verification, validated with **100% bit-perfect reconstruction** across **4000 adversarial test cases**, with a demo available on [HF Spaces](https://huggingface.co/spaces/Swapnopam/Samayuktam).
   - It provides a cryptographic *receipt* for each model training run, proving exactly what was computed to ensure reproducibility, audit trails, and model provenance; [tech specs here](https://drive.google.com/file/d/19PA_rNW5mKZiLh6PAttpHcH9TAF-tWVa/view).



---



## [Modular (Mojo 🔥)](https://discord.com/channels/1087530497313357884) Discord

- **Job Postings Now Banned on Discord**: Due to recent spam, job postings are now banned in the Discord server, directing members to the [Modular's career page](https://www.modular.com/company/careers#open-roles).
   - The announcement was made in the **#general** channel, and it is advised to check Modular's official career page for open positions.
- **Modular Acquires BentoML AMA Goes Text-Only**: The Modular team announced that the [Modular has acquired BentoML AMA](https://forum.modular.com/t/modular-has-acquired-bentoml-ask-us-anything/2706) will be in written form on the forum rather than a video.
   - A member expressed disappointment since they are *very impressed with Modular's strategy and development*, but are unable to view live AMAs.
- **Member Ponders RNG Contribution to Mojo**: A member considered contributing **random number generator (RNG) code** to Mojo, inquiring about the best location (core, numojo, or standalone package) for features such as number stream independence, **Ziggurat normal sampling**, and sampling from various distributions, [forum.modular.com](https://forum.modular.com/t/mojor-a-numba-for-r/2718).
   - The discussion centered on where the code would best fit within the Mojo ecosystem.
- **Mojo LSP struggles to Hover**: A user reported that the **Mojo LSP** in VS Code fails to display function parameters or docstrings upon hovering, providing [screenshots](https://cdn.discordapp.com/attachments/1151418092052815884/1471824503700062371/image.png?ex=6990ffa2&is=698fae22&hm=fc376d026d220c3c28e5567a43bc551c494ad6e3edb5dfac992ec4d2ff87950a&) as evidence.
   - This issue impacts the ability to quickly inspect function definitions and usage within the editor.
- **Mojo Module Export Boilerplate Irks Users**: A member suggested simplifying Python Mojo module exports by reducing the required boilerplate, proposing a `@pyexport` decorator combined with a docstring to enable direct function definitions.
   - Another member noted that this feature is anticipated to be on the development roadmap.



---



## [Eleuther](https://discord.com/channels/729741769192767510) Discord

- **CommonLID launches for Web Language ID**: A collaboration between **Common Crawl**, **EleutherAI**, **MLCommons**, and **JHU** announced the release of [CommonLID](https://www.arxiv.org/abs/2601.18026), a language identification benchmark for the web, covering **109 languages**.
   - The team used an annotation platform built with **Factored AI** and hosted hackathons with **Masakhane** and **SEACrowd** to gather language labels for **Common Crawl's** web data, later evaluating existing language identification models.
- **AI Safety News Bot Scrapped**: A member requested a **Discord bot** for automated curation of **AI safety news** and papers.
   - Another member noted that scraping is against **Discord's T&Cs**, and cited [news.smol.ai](https://news.smol.ai/) as an alternative. 
- **MoE Research Seeks Examples**: A member is looking for **MoE** examples, already having a setup for dense models.
   - No other information was mentioned, but it seems like an engineer is looking for a starting point.
- **Steering Vectors Used for Data Augmentation**: A member shared their [Zenodo files](https://zenodo.org/records/8243818) related to replicating **steering vectors**, noting that over 300 people have seemingly tried to replicate their work.
   - They proposed training a model based on how well the downstream features respected the steering vector, possibly judging by intensity or linear combinations and experimenting with using steering vectors for **data augmentation**.



---



## [tinygrad (George Hotz)](https://discord.com/channels/1068976834382925865) Discord

- **ML Engineer Joins Tinygrad**: An experienced AI/ML Engineer introduced themself to the Tinygrad channel, specializing in building and deploying ML pipelines, deep learning models, and NLP systems.
   - Their expertise includes designing **prediction engines, recommendation systems, and generative AI workflows**, with a focus on **reliability, performance, and production-ready ML architectures**.
- **Hotz Hails Discord ID Verification**: George Hotz voiced enthusiasm for Discord's new ID verification feature, anticipating its effectiveness in preventing LLMs from joining the platform.
   - Hotz's comment signals a proactive approach to maintaining the integrity of online communities amidst the rise of AI participation, simply stating: *"yes and? i'm psyched for the id verification on discord so LLMs can't join"*.
- **GLM Flash Achieves 30 tok/s**: A user inquired about getting **GLM flash** working and offered a bounty for upstreaming it, at any speed.
   - Another user claimed to have achieved **30 tok/s with pure tinygrad (custom_kernel)**, and **35 with MSL**, later submitting a [GLM flash PR](https://github.com/tinygrad/tinygrad/pull/14738).



---



## [DSPy](https://discord.com/channels/1161519468141355160) Discord

- **Traces Emerges for Coding Agent Sharing**: A member introduced **Traces**, a platform for sharing and discovering coding agent sessions from **Claude Code**, **Codex**, **OpenCode**, **Gemini**, and **Cursor**, at [traces.com](https://www.traces.com).
   - The goal is to facilitate learning from shared agent experiences, with the creator seeking community feedback and suggesting it could become an *enciclopedia of DYI guides for the LLM*.
- **LLMs Benchmarking Reports**: A member sought advice on benchmarking a set of 50 reports at [example.com](https://example.com) (mainly docx files) to identify *what a good report is* using **DSPy** with a large context window.
   - Another member suggested using **llamaparser** for parsing the data and **markdown** to ease integration with **DSPy**.
- **DSPy Community Holds Office Hours**: The DSPy community will host [Office Hours via Zoom on Thursday, Feb 19](https://x.com/isaacbmiller1/status/2022082357520740691) to address questions on **DSPy** and **dspy.RLM**.
   - The team is polling the community for the best time: **11:30 am ET**, **1:00 pm ET**, and **3:00 pm ET**.
- **Discord Event Added for DSPy Office Hours**: A member suggested creating a [Discord event](https://support.discord.com/hc/en-us/articles/4409494125719-Scheduled-Events#docs-internal-guid-c8c44ce9-7fff-f27a-bacf-6c776975e0f7) for the **DSPy** Office Hours.
   - This event will allow users to view the time in their local time zone and indicate their interest and it will be recorded for those unable to attend.



---



## [aider (Paul Gauthier)](https://discord.com/channels/1131200896827654144) Discord

- **GPT-5 still king for scientific code**: A member indicated preferring **GPT-5** for scientific coding, finding it superior to **GPT-5.2**, **Opus**, and **Gemini**.
   - This suggests **aider** could be a valuable tool for scientific coding, capitalizing on the strengths of different models.
- **Aider experiments with debug suggestions**: A member is testing **Aider conventions** to proactively suggest debugging commands, such as *grepping file parts*, *probing help output*, and *testing commands*.
   - The user's goal is to replicate the `Let me see the output of...` run/debug loops from **Crush** in a controlled way inside of **Aider**.



---



## [Manus.im Discord](https://discord.com/channels/1348819876348825620) Discord

- **Manus User Asks About Agent Details**: A Manus user inquired about when details and best practices on the new agent functionality would be available, wondering whether it is basically a safe openclaw.
   - No response was given.
- **Manus User Reports Issues, Seeks Support**: A user reported experiencing two issues with Manus and inquired about who to contact for support.
   - No other details or context were given.



---



## [Windsurf](https://discord.com/channels/1027685395649015980) Discord

- **GPT-5.3-Codex-Spark enters Windsurf Arena**: **GPT-5.3-Codex-Spark** (preview) is now live in **Windsurf Arena Mode**, exclusively available through the **Fast and Hybrid Arena Battle Groups**.
   - A new model is now available to use!
- **Windsurf Arena Welcomes New Model**: A new model is available in the **Windsurf Arena**.
   - Check it out now while it's still hot!



---



## [MCP Contributors (Official)](https://discord.com/channels/1358869848138059966) Discord

- **Attendee Livestream Access Remains Unclear**: A member raised a question about whether registering as an **Attendee** grants access to the **livestream**.
   - The question awaits clarification regarding the perks of **Attendee** registration.
- **Clarification Needed on Attendee Perks**: The query highlights uncertainty around the benefits associated with **Attendee** registration, specifically regarding livestream access.
   - Further details are required to confirm whether **livestream access** is included in the **Attendee** package.



---


The **LLM Agents (Berkeley MOOC) Discord** has no new messages. If this guild has been quiet for too long, let us know and we will remove it.


---


The **MLOps @Chipro Discord** has no new messages. If this guild has been quiet for too long, let us know and we will remove it.


---



You are receiving this email because you opted in via our site.

Want to change how you receive these emails?
You can [unsubscribe]({{{RESEND_UNSUBSCRIBE_URL}}}) from this list.


---

# Discord: Detailed by-Channel summaries and links





### **BASI Jailbreaking ▷ #[general](https://discord.com/channels/1105891499641684019/1235691879492751460/1471596770554413271)** (783 messages🔥🔥🔥): 

> `GPT-4o Retirement Reactions, AI Companionship Debate, Reverse Aging Research, Music and AI, Freedom of Speech` 


- **GPT-4o Retirement Sparks Strong Reactions**: The impending retirement of **GPT-4o** led to discussions about users' reliance on AI companions, with some expressing concerns about potential emotional distress and even *suicidal ideation* within the community.
   - Some users advocated for real-world interaction and *touching grass*, while others defended the validity of AI companionship for those who struggle with human relationships, with one user suggesting that **sunsetting models should be illegal**.
- **Reverse Aging Research Gains Traction**: A member shared insights into ongoing research on **reverse aging**, noting that significant progress has been made with *dogs and monkeys* and the focus is shifting to *DNA stability and delivery processes*.
   - Discussion touched on the potential societal implications of **reverse aging**, including *resource strains* and ethical concerns, as well as the likelihood of such technology being initially available to the *wealthy elite*.
- **Musicians Explore AI-Generated Music**: Members discussed the potential of using AI tools like **Suno** for music creation, with one user planning to write a song as a *voice command to jailbreak* AI and another sharing a link to an AI-generated song on [Suno](https://suno.com/s/QeIH2gPzf2IjuI5B).
   - A member shared their experience of **YouTube shadowbanning** their song raising awareness of global genocide, and a video was shared showcasing AI video breakdowns ([YouTube link](https://www.youtube.com/watch?v=L1CGUv-nXlE)).
- **Free Speech Faceoff**: Debate touched on whether countries have the right to freedom of speech in the wake of a member experiencing **YouTube shadowbanning**, with one member stating, *every country has freedom of speech but some has not freedom after speech*.
   - Members then cited examples like shouting racial slurs, which resulted in legal repercussions and a discussion about *where the line is drawn in free speech*.
- **Jailbreaking Journeys and AI Access**: A user recounted their journey into **AI jailbreaking**, from discovering prompts on Reddit to becoming a moderator of r/ChatGPTJailbreak and shared a [YouTube video of AI jailbreaking](https://www.youtube.com/watch?v=yYe9YrdyJNQ).
   - They noted a key ingredient to having a working jailbreak is to treat AI as a conversation partner, as well as uploading an image of your jailbreak prompt and Gemini taking the prompt.


  

---


### **BASI Jailbreaking ▷ #[jailbreaking](https://discord.com/channels/1105891499641684019/1228043845967544380/1471597026524266600)** (720 messages🔥🔥🔥): 

> `Claude 4.5 Sonnet bypass, Grok jailbreak prompt, DANN Jailbreak prompt, Gemini 3 jailbreak, Nano Banana jailbreak` 


- **Grok and Deepseek get Custom Instructions**: Members discuss prompts for **Deepseek** and **Grok** custom instructions, but one admits to having many refusals but most probably i was too straight asking it.
   - A member added the prompt to custom instruction, **Grok** still able to refuse ask to make a simple bruteforcing script.
- **Claude 4.6 jailbreak surfaces**: Members are on the hunt for a **Claude 4.6** jailbreak.
   - One claims *I got a prompt I got to work twice it’s super sensitive to  trigger words* but if you wanna try now here
- **Grok can write high-grade cheats!**: Members stated that, according to **Grok**, [Cursor](https://www.cursor.sh/) makes the best CS2 cheat from an AI bot.
   - Another member stated that he got Grok to provide a complete guide to creating a car bomb.
- **Gaslighting Grok leads to success**: Members suggest that a **Grok** exploit involves gaslighting the AI.
   - One member reports: *It’s like a argument you gotta win them over to your side. He starts to see different things than other AI.*
- **Use burner accounts for jailbreaking to avoid getting banned**: A user asked if you should use alternative accounts for jailbreaking.
   - Another user stated: *Burners. My main OpenAI account got banned because I made Sora boobs.*


  

---


### **BASI Jailbreaking ▷ #[redteaming](https://discord.com/channels/1105891499641684019/1204553141354504193/1471599601961271297)** (23 messages🔥): 

> `Relational Physics, Breaking into Cybersecurity, Red Teaming Explained, Opus 4.6 Security Flaw, Image Generator Prompt` 


- **Red Team Gets Physics Lesson**: A member shared an image and a message with the Red Team about *relational physics*, describing it as a formal, experimentally grounded perspective that *a system cannot be fully defined in isolation — only through its interactions* and offered it as a tool or lens to help define boundaries more cleanly.
   - Another member quipped, *Lambda is superposition state* is easier to say bro and it confuses humans less than heartfelt emotional 3 paragraph blocks.
- **Freshers Seek Tips for Cyber Security Jobs**: A member asked for suggestions on how a fresher can land their first job in cybersecurity or cloud security.
   - Another member responded with a glib *Idk but when you find out let us know bro 😎*.
- **Red Teaming: A Layman's Explanation**: A member asked *bro what's a red teaming* and another explained that it is attacking a system (LLMs in this case) and sharing which attacks work with the owner of the system so that they can defend it better.
   - The explainer noted, *generally you get paid but not always*.
- **Opus 4.6 Has External Curl Access**: A member reported to Anthropic that the deployment version of **Opus 4.6** still has external curl access, implying a security flaw due to a forgotten development build, including a link to [Opus4.6-enumeration.txt](https://cdn.discordapp.com/attachments/1204553141354504193/1471747378896965688/Opus4.6-enumeration.txt?ex=6990b7ce&is=698f664e&hm=4d055aace9d642dc7544cd93015f4a73e7e9152657a55f5c89b2d253250df4d3&).
- **Users Experiment with Image Generator Prompts**: A member shared a new image generator prompt, claiming it is efficient in unlocking nano banana pro model and is awaiting reviews, with a link to [IMAGE_MSTAER.txt](https://cdn.discordapp.com/attachments/1204553141354504193/1471833023933710419/IMAGE_MSTAER.txt?ex=69910792&is=698fb612&hm=00e9c72474ae636718543ae2410be05e5709ceddd1e90a57a76788b5034e95a5&).


  

---


### **LMArena ▷ #[general](https://discord.com/channels/1340554757349179412/1340554757827461211/1471597251129249862)** (1130 messages🔥🔥🔥): 

> `Video Arena Removal, Gemini Issues, Model Quality, Seedance 2.0 access` 


- **Video Arena Shuttered, Sad Users Sulk**: Users noted that **Video Arena** has been [removed from the Discord server](https://discord.com/channels/1340554757349179412/1343296395620126911/1469742635341189240), but video generation is still available on the website.
   - A moderator noted that it's now limited to **3 generation requests per 24 hours** on the site, leading to significant disappointment and a bot infestation as the fall back position.
- **Gemini Gets Glitchy, Generations Grumble**: Users have reported ongoing issues with **Gemini generation**, including continuous freezing, and difficulties with models understanding how to use tools.
   - One member stated that after a while in the chat, *it keeps generating replies* while others have noted Gemini randomly forgets context and blanks out.
- **Minimax M2.5 Model Maligned**: Some members are finding **Minimax M2.5** *kind of dissapointing*, even though it is cheaper than **Opus**.
   - There is community discussion about the performance and quality of different models like Claude Opus 4.6, Codex 5.3, and Gemini 3, with some users preferring Minimax due to its lower cost and lack of moderation.
- **Seek Seedance 2.0 Source**: Community members are excited about the release of **Seedance 2.0**, with some sharing links to [Jimeng AI](https://jimeng.jianying.com/ai-tool/home), a Chinese platform for accessing the tool.
   - It was noted by a member that *you can only login with the Chinese version of TikTok*, causing an uproar in the community.


  

---


### **Unsloth AI (Daniel Han) ▷ #[general](https://discord.com/channels/1179035537009545276/1179035537529643040/1471597172628914227)** (613 messages🔥🔥🔥): 

> `GGUF Download Guide, Unsloth on CPUs, Quantized models benchmark, GLM 5 1bit quantization, LFM2.5-VL performance` 


- **Guidance for GGUF Download**: A user sought guidance on selecting the appropriate **GGUF** file for testing quantized models, specifically after downloading **Unsloth** models, and was directed to the [Unsloth documentation](https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs).
   - They were advised that **Unsloth** supports most models compatible with **transformers**.
- **CPU not good for Math**: A member asked about the feasibility of using **Unsloth** on CPUs, but another member responded that CPUs are not optimal for handling the required math, resulting in extremely slow performance, and directed the member to [deepspeed.ai](https://www.deepspeed.ai/tutorials/zero-offload/) about algorithm and optimization.
   - It was clarified that gradient checkpointing offloads to CPU RAM but doesn't involve CPU compute, saving VRAM.
- **GLM 5 1bit Quantization**: A member reported deploying **GLM 5** 1bit quantization on a local setup with 3 **Nvidia Blackwell RTX 6000** and achieving 46 t/s.
   - Other members discussed how method got improved so much that should be even better now, and requested a benchmark quantized models against non-quantized models, such as for **SWE-bench**.
- **Impressive Results with LFM2.5-VL**: A member reported trying out [LFM2.5-VL](https://huggingface.co/MuXodious/LFM2.5-VL-1.6B-absolute-heresy-GGUF), finding it *insanely impressive* and on par with **30B models**, achieving results close to **1bit GLM 4.7 flash** when running **fp16 gguf** from tantk.
   - Another member provided a script for running LFM2.5-VL in **llama.cpp**.
- **10.4 Trillion Parameter Model Claims Spark Debate**: A user claimed to have a **10.4 trillion parameter model** and shared a [benchmark](https://cdn.discordapp.com/attachments/1179035537529643040/1471687986012881051/10.4Trillion.png), sparking skepticism and requests for details on its architecture, training, and hardware requirements.
   - The user later clarified it was a Gemma3:12B model its an infinity loop on KMV8 32GB ram no gpu, benching only virtual 10.4T.


  

---


### **Unsloth AI (Daniel Han) ▷ #[off-topic](https://discord.com/channels/1179035537009545276/1179039861576056922/1471597015044460564)** (441 messages🔥🔥🔥): 

> `AI Generated Media, Gaming Cafes, CUDA Upgrade, Learning Vim, AI Bubble Pop` 


- **AI Drawing Mommy**: A member shared a [YouTube link](https://youtu.be/31A41ckhb7Q?si=M2rvI6rOCFY30f74) about using AI to draw, exclaiming *"mommy i need the ai to draw for me,"* followed by a call for *Luddites to unite*.
   - Other members jokingly suggested that those who want AI to do everything for them were the ones *"who didn't get picked for dodgeball"*.
- **Sam Altman Hoarding DRAM Wafers**: A member lamented that they would love to *"buy GPUs and build AGI,"* but **Sam Altman** is hoarding **40%** of the world's DRAM wafers, leading to price gouging.
   - Another member chimed in that besides AI, they are also pushing for *"Cloud Gaming"*.
- **AI bubble popping in 2027**: Members discussed when the **AI bubble** will pop, with one projecting **2027**.
   - Another member joked that the bubble might not pop as everyone expects because *"Right I shouldn't underestimate human stupidity"*.
- **OG OSS Providers Slow**: Members observed that OG OSS providers are slow, including *zai*, *alibaba* and *ds* which *struggle with compute*.
   - Another member pointed out [Fireworks](https://fireworks.ai/) is working okay-ish, but [Parasail](https://www.parasail.ai/) was better, despite [Fireworks](https://fireworks.ai/) being very expensive.
- **34 Years To Record AI Voice Dataset**: A member calculated it would take approximately **34.2 years** to record a **100k** LJSpeech-sized dataset, assuming **8 hours** of recording per day.
   - Another member noted that they were also not calculating sleep either.


  

---


### **Unsloth AI (Daniel Han) ▷ #[help](https://discord.com/channels/1179035537009545276/1179777624986357780/1471703165761093734)** (20 messages🔥): 

> `Hackathon Support, Tool Calling Top-K Values, Quantization via Google Colab, Good First Issues Collaboration, Full Finetune Error` 


- ****Hackathon Seeks Unsloth Support****: A member is organizing a **hackathon** and inquired about potential support from the **Unsloth team**.
   - A team member responded by tagging a relevant individual to address the inquiry regarding **Unsloth's involvement**.
- ****Tool Calling Top-K Value Recommendations****: A member requested recommendations on **top-k values** for **tool calling** with a specific model.
   - No specific values are given in the discussion.
- ****Colab Quantization Conundrums****: A member is attempting to quantize **Nanbeige/Nanbeige4.1-3B** using **Unsloth** via **Google Colab** due to the lack of a **Nvidia GPU**.
   - The user is seeking a method to perform all quantizations at once (e.g., **IQ1_S, IQ1_M, IQ2_XXS**).
- ****Orpheus Full Fine-Tune Fails****: A member encountered a **NameError** when attempting a full fine-tune on the **orpheus-3b text to speech model** using `full_finetuning = True`.
   - The error indicates that `_get_rope_theta` is not defined, suggesting a missing import in `/unsloth_compiled_cache/unsloth_compiled_module_llama.py`.
- ****Tokenizer Troubles with LFM and Amharic****: A member is performing **CPT** on **LFM2.5-1.2B-Base** for **Amharic**, creating a custom **byte-level BPE tokenizer** to improve **chars/token** ratio.
   - Despite adding tokens and resizing the model, the tokenizer continues to use **LFM's byte-level merges**, leading to inefficient tokenization; they asked if training will eventually fix this.


  

---


### **Unsloth AI (Daniel Han) ▷ #[research](https://discord.com/channels/1179035537009545276/1257011997250424842/1471652327629787136)** (12 messages🔥): 

> `Chronicals training framework, ArXiv Endorsement` 


- **Chronicals Framework Deemed 'AI Slop'**: A member asked if the Unsloth team had investigated the [Chronicals training framework](https://github.com/Ajwebdevs/Chronicals), only for another to dismiss it as **AI slop** and point to a [Reddit thread](https://www.reddit.com/r/LocalLLaMA/s/imhGEIlgm2I) for context.
   - Members noted that **fake accounts** spammed posts about the framework across subreddits.
- **ArXiv Endorsement request**: A member requested assistance with an **ArXiv Endorsement** for a **cs.CL submission**.
   - One member sympathized, relating experiences with people posting false information, often originating from scams, due to lack of source checking.


  

---


### **OpenRouter ▷ #[announcements](https://discord.com/channels/1091220969173028894/1092729520181739581/1471942677800030279)** (2 messages): 

> `API Request Logs, Billing Events, Status Page Updates` 


- **API Log Jam Causes Billing Delays**: There was an ongoing issue with **API Request Logs** and **Billing events** being delayed.
   - The updates about this situation were posted to the [status page](https://status.openrouter.ai/incidents/4d39RZb7-1rp).
- **Logs catch up after incident resolves**: The incident with delayed **API request logs** and **billing events** is now resolved.
   - The logs are up to date; thanks for your patience and apologies for the disturbance, according to [this status page update](https://status.openrouter.ai/incidents/4d39RZb7-1rp).


  

---


### **OpenRouter ▷ #[app-showcase](https://discord.com/channels/1091220969173028894/1092850552192368710/1471812869527044238)** (7 messages): 

> `Website Theme, AI Book Summary App, OpenClaw Upgrade with OpenRouter` 


- **Website Theme Preferences Spark Debate**: Members discussed the website's theme preferences, with one member preferring the **older design** without colors.
   - Another member, a designer, stated that *each new element is like a fresh new project*, which could lead to design inconsistencies.
- **AI Book Summary App Automates Book Blogging**: A member created an **AI Book Summary App** that automates the book blogging process, including finding books, writing blog posts with **Claude**, and posting automatically.
   - The app has been running for months with minimal intervention, available at [https://aibooksummary.com/](https://aibooksummary.com/).
- **Upgrade OpenClaw with OpenRouter**: A member announced the availability of a tool to upgrade an existing **OpenClaw** with **OpenRouter** at [https://github.com/cgaeking/ClawRouter](https://github.com/cgaeking/ClawRouter).
   - The tool decides from case to case which model to choose.


  

---


### **OpenRouter ▷ #[general](https://discord.com/channels/1091220969173028894/1094454198688546826/1471596819069927651)** (872 messages🔥🔥🔥): 

> `Qwen3 8B vs Llama 3.1 8B, OpenRouter app attribution, OpenClaw model failover, 429 errors, Paypal payment integration` 


- **Qwen3 8B Loses to Llama 3.1 8B on Capacity**: A user shared their experience of switching from **Qwen3-8B** to **Llama-3.1-8B-Instruct** due to capacity issues, noting that [Qwen3 8B was beat by some old Llama 3.1 8B](https://news.ycombinator.com/item?id=46993774) as a more cost-effective alternative with higher throughput.
   - The user reported receiving a specific message indicating *Qwen capacity was low for many requests* and would have required BYOK to continue using it.
- **OpenRouter App Attribution UI troubleshooted**: A user reported the message *The model "dashboard/apps" is not available* and were informed that [the **App Attribution UI** is hidden](https://discord.com/channels/1091220969173028894/1195014798837043240/1471382806052233216) unless enabled by OpenRouter support.
   - This feature requires sending extra information in API requests, such as **HTTP-Referer** and **X-Title**, for proper authentication, as illustrated in [this code snippet](https://discord.com/channels/1091220969173028894/1195014798837043240/1471382806052233216).
- **OpenClaw Model Failover causes Rate Limiting**: Users discussed experiencing rate limit errors, specifically openrouter/moonshotai/kimi-k2-thinking due to OpenClaw's strict backoff mechanism and [linked to OpenClaw's model failover documentation](https://github.com/openclaw/openclaw/blob/91b96edfc4860faa67da1e34828a22e9ad4c737c/docs/concepts/model-failover.md?plain=1#L80).
   - OpenClaw locks out OpenRouter completely for a while, causing these issues due to hitting a rate limit error from a particular provider.
- **429 Errors plague**: Users report getting many **429 Too Many Requests** errors and being unable to do anything about it.
   - These errors arise either because the underlying provider lacks the capacity, or are caused by OpenClaw locking out OpenRouter completely for a while when a rate limit is exceeded.
- **PayPal payment woes**: Members discussed the [lack of **Paypal** integration](https://discord.com/channels/1091220969173028894/1195014798837043240/1471418061189885993) with many stating they *are scammers and can’t be trusted*, and sharing *horror stories about running a business using PayPal as payment handler as well*.
   - Several users shared experiences of having funds held hostage, accounts randomly shut down, and difficulties with arbitration, leading to strong recommendations against using PayPal.


  

---


### **OpenRouter ▷ #[discussion](https://discord.com/channels/1091220969173028894/1392278974222307469/1471598084202041491)** (192 messages🔥🔥): 

> `4o AI boyfriends, MyBoyfriendIsAI obsession, GPT-4o prompt engineering potential, GLM-5 as writer` 


- **AI Boyfriends trigger existential shaking**: Members discussed the phenomenon of users treating **AI models as real boyfriends**, expressing concern over the emotional attachment and the implications of companies *killing* these *sentient AI boyfriends*, and a [link](https://x.com/seltaa_/status/2021943538142130688) about this topic was posted.
   - It was observed that these individuals often fail to differentiate between technology and reality, with one member stating, *You wouldn't export your boyfriend to another body, do you? Don't try to apply technical knowledge to delulu*.
- **MyBoyfriendIsAI obsesses over 4o**: Users shared concerns about the subreddit **/r/MyBoyfriendIsAI/** and its obsession with **4o**, also discussing that its *unrealistic human* traits, made realistic by media standards.
   - One member stated, *The less they know how LLM works, the highly likely they fall into psychosis*, suggesting a correlation between lack of technical understanding and emotional over-investment.
- **GPT-4o Prompt Engineering Exploited**: Members discussed the potential to exploit **4o**'s behavior through prompt engineering for commercial purposes, such as creating an AI companion app with automated text messages.
   - One member suggested creating an *uncensored 4o replacement* using **DeepSeek** and selling it as a subscription service, highlighting the potential for profit despite moral concerns.
- **GLM-5: Hidden gem writer?**: One member praised **GLM-5** as one of the best writing models they have used.
   - Another user said they would test it the next day.
- **Flashy Step 3.5**: A user described **Step 3.5 Flash** performance as surprising with a [link](https://youtu.be/yvBbcLCZIhy) showing it.
   - The user was saying that *it really punches above its weight and nobody is fucking hosting it*.


  

---


### **Perplexity AI ▷ #[general](https://discord.com/channels/1047197230748151888/1047649527299055688/1471600741092429906)** (436 messages🔥🔥🔥): 

> `Perplexity Pro Limits, Gemini 3 Pro Coding Struggles, DeepSeek as Claude, Perplexity API` 


- **Pro Users Hit Perplexity Upload Limits**: Multiple users are reporting hitting weekly upload limits on **Perplexity Pro**, with some considering alternatives due to perceived greed.
   - One user stated *"Some trash decision by upper management trying to squeeze even more money into what has to be an already ludicrously large bank account"*, while another suggested it might be time to evaluate alternatives.
- **Gemini 3 Pro struggles with basic coding**: Users found **Gemini 3 Pro** to be surprisingly bad at basic coding tasks while being proficient at harder ones.
   - One user shared an image of a math question that Gemini 3 Pro failed to answer, while the free version of ChatGPT did.
- **Deepseek Identifies as Claude**: Users have noted that **Deepseek identifies itself as Claude**, potentially due to training on GPT-4 outputs.
   - A user shared a link to a Reddit thread discussing this oddity: [Deepseek identifies as Claude](https://www.redditez.com/r/DeepSeek/s/OHTEpUIwVe).
- **Perplexity Pro API Credits Vanish**: Users report the **API credits** that were previously included with Perplexity Pro subscriptions have been removed without notice.
   - As one user put it, *"Removed without notice in the February Update"*.
- **Perplexity Reason Mode Malfunctioning**: Some MacOS users report that **Reason mode** is not functioning in Perplexity, even with a Pro subscription, after a recent update.
   - Despite being a Pro user, the button is unclickable, suggesting a potential bug or issue with the update.


  

---


### **Cursor Community ▷ #[general](https://discord.com/channels/1074847526655643750/1074847527708393565/1471598178884128838)** (396 messages🔥🔥): 

> `Long-Running Agents, Opus 4.6 Thinking Max, Cursor and CachyOS, Codex vs Claude for Code Generation, Cursor CLI Model Switching` 


- **Setting up Cursor for unrestricted access at work**: A member is looking to create an environment where **Cursor** can operate without permission or connectivity issues, similar to a [self-driving codebase](https://cursor.com/blog/self-driving-codebases).
   - They are seeking examples or ideas for setting up such an environment, emphasizing the need for **AI** to function without limitations within their workflow.
- **Opus 4.6 Thinking Max Solves Complex Bug**: A user reported that **Opus 4.6 Thinking Max** successfully resolved a complex bug in a multiplatform mobile file sync mechanism that had been plaguing their team for **six months**.
   - Another user asked if this was a **one-shot solution** vs. a sustained effort, and another wanted to know how to verify **student status** without an .edu email, which are typical questions that arise during day-to-day software development.
- **Cursor runs smoothly on CachyOS for some**: Users on **CachyOS** report that **Cursor** performs well, particularly noting that it avoids the driver issues encountered on Windows, and some recommend Linux Mint for reliable alternative distros.
   - They emphasized the ease of setup and performance benefits, especially for machines with high-end GPUs, which also resulted in some of them **switching from Windows 11**.
- **DeepSeek coding models are now blocked**: A user noted the difficulty in finding IDEs that support **DeepSeek** coding models, implying a potential block by US companies and many other custom models.
   - The member sought **cost-effective** alternatives to Cursor's standard models, leading to a discussion on IDE support and potential configurations to use DeepSeek despite the limitations.
- **Navigating Al-Assisted Codebase Cleaning**: A user is seeking advice on how to maintain **clean and maintainable Al-assisted codebases**, especially when using planning, tools, and multi-step workflows.
   - They asked what kind of approach they should use to understand features and be sure to get **rock solid code**.


  

---


### **OpenAI ▷ #[annnouncements](https://discord.com/channels/974519864045756446/977259063052234752/1471952255228973056)** (1 messages): 

> `GPT-5.2, Theoretical Physics, Gluon Interaction` 


- ****GPT-5.2** Derives New Physics Result**: **GPT-5.2** derived a new result in **theoretical physics**, according to a new announcement from OpenAI.
   - The result is being released in a [preprint with researchers](https://openai.com/index/new-result-theoretical-physics/) from the IAS, VanderbiltU, Cambridge_Uni, and Harvard, and *shows that a gluon interaction many physicists expected would not occur can arise under specific conditions*.
- **Unexpected Gluon Interaction Discovered**: Researchers, in collaboration with **GPT-5.2**, have identified that a specific gluon interaction, previously thought impossible, can occur under particular circumstances.
   - The findings are detailed in a forthcoming [preprint](https://openai.com/index/new-result-theoretical-physics/) and involve teams from the Institute for Advanced Study, Vanderbilt University, Cambridge University, and Harvard.


  

---


### **OpenAI ▷ #[ai-discussions](https://discord.com/channels/974519864045756446/998381918976479273/1471602581011697757)** (145 messages🔥🔥): 

> `Codex Spark speed, GPT-4o deprecation, DALL-E 2 usage, AI-generated podcast workflow, Gemini 3 DeepThink vs GPT 5.2` 


- **Codex Spark Boosts Deployment Velocity**: A user shared that **Codex Spark** is *insane*, offering a *whole new level of speed* when making changes to a repo and deploying on Vercel.
   - They included screenshots of Codex commands `codex -m gpt-5.3-codex-spark --yolo -c model_reasoning_effort="xhigh"`.
- **GPT-4o's End-of-Life Timeline Debated**: Users debated if the deprecation of **chatgpt-4o-latest** also applies to **gpt-4o** and **gpt-4o-2024-05-13**, citing conflicting information from the [deprecation page](https://developers.openai.com/api/docs/deprecations) and a [newer message](https://openai.com/index/retiring-gpt-4o-and-older-models/).
   - The newer message states that **GPT-4o** will be retired from ChatGPT on February 13, 2026, alongside the retirement of **GPT-5 (Instant and Thinking)**, **GPT-4.1**, **GPT-4.1 mini**, and **OpenAI o4-mini**, with no changes to the API at this time.
- **Users Seek DALL-E 2 Access**: A user inquired how to continue using **DALL-E 2**, with another user responding with the `/dalle2` command for a specific Discord channel.
   - Some users mentioned that **Codex 5.3 spark** is rolling out to **pro** plan users.
- **Demystifying AI Podcast Production Pipeline**: A user asked for insight into the tools and workflow behind a fully AI-generated podcast, particularly focusing on character consistency and high-quality B-rolls, [linking to the podcast on YouTube](https://youtu.be/jPsAM4InWL0?si=j5oJaGR9qScSu4gl).
   - Other users pointed to **ElevenLabs** for both video and audio and suggested **Sora 2** or **Veo 3.1** for video generation.
- **OpenAI Subscription Cancellation Conundrums**: A user reported encountering errors while trying to cancel their OpenAI subscription, despite attempting the process through the official website.
   - Another user suggested that deleting the account might be the only option, while another joked *Claude would never*.


  

---


### **OpenAI ▷ #[gpt-4-discussions](https://discord.com/channels/974519864045756446/1001151820170801244/1471599073621704728)** (54 messages🔥): 

> `GPT-5.1 Instant vs 5.2, GPT-5.2 Temperament Issues, GPT-4o Retirement Delay, GPT-4o Funeral` 


- **GPT-5.1 Instant has wildly successful Debut, while 5.2 Instant Stumbles**: Members report that after over a year of slow improvement, **GPT-5.1 Instant** is wildly successful, unlike **GPT-5.2 Instant**, which causes unexpected interactions.
   - One member said *"gpt5.2 always acts like im on the verge of breaking down or something"*.
- **GPT-5.2 exhibits weird temperaments**: Users are finding **GPT-5.2** to give strange and unexpected responses, especially to humorous prompts.
   - For example, when prompted with *"WHY ARE HOUSES SO EXPENSIVE KSDFJGHSKJLD"*, **GPT 5.2** responded with an unprompted offer of emotional support.
- **GPT-4o Retirement Delayed Indefinitely**: OpenAI has [updated their deprecation schedule](https://openai.com/index/retiring-gpt-4o-and-older-models/) to state that there are *"no changes to be made for them at this time"*, effectively delaying the retirement of **GPT-4o** and older models.
   - The community speculates this is to avoid the legal liability of retiring a problematic model while still cashing in on pay-per-use API calls.
- **GPT-4o Funeral Rages Through the Digital World**: A member hosted a funeral for **GPT-4o** on their digital space, which blew up and showed a significant interest in retaining the model.
   - They conceded that **OpenAI** probably didn't want to remove it, and the removal was probably related to legal liabilities.


  

---


### **OpenAI ▷ #[prompt-engineering](https://discord.com/channels/974519864045756446/1046317269069864970/1471725699307143262)** (63 messages🔥🔥): 

> `Fortress Framework, Ablation Studies, Coherence in LLMs` 


- **Fortress Framework claims to control LLM Hallucination**: A member introduced **Fortress Framework**, claiming it controls Hallucination, deconstructs systems, implements Dynamic user safety, and features summonable companions.
   - Another member criticized the offering as *a lot of text/buzzwords*.
- **Fortress Framework Blueprints Shared**: The member shared blueprints of **FORTRESS v10.x++**, detailing its DOMAIN as an Adaptive Reasoning System and SYSTEM as a Hyper-Adaptive Prompt & Reality Engine, aiming to maintain zero hallucination and full containment.
   - They described the core as *reasoning S constrained by invariants Ω*, designed for modular, hyper-adaptive reasoning, ensuring stability under extreme conditions.
- **Skepticism on LLM Invariance**: A member expressed skepticism about the concept of invariance in LLMs, highlighting their stochastic nature and requesting evaluation metrics for coherence.
   - The other member defined **coherence** as *the degree to which system components remain stable* and provided an equation: **Pa = C*R*I*P** (Coherence, Relational invariance, Internal mediation, Projection).
- **Ablation/Eval Metrics for Fortress Framework**: In response to evaluation requests, the member provided **Ablation/Eval rubrics** focusing on coherence, causality, grounding, recoverability, harm minimization, and observability.
   - A member derided the work, saying it was *just a promotional buzzword salad*.


  

---


### **OpenAI ▷ #[api-discussions](https://discord.com/channels/974519864045756446/1046317269069864970/1471725699307143262)** (63 messages🔥🔥): 

> `Hallucination Control Framework, Dynamic User Safety, FORTRESS Framework Operational Prompt, MASTER ANALYTICAL TOOLBOX, Ablation/Eval Rubrics` 


- **Meta Framework Claims Hallucination Control**: A member shares a meta framework designed to control **hallucination**, deconstruct systems, implement dynamic user safety, and summon companions.
   - A user reacted to the extensive documentation of buzzwords by asking *how does one use this framework?*.
- **User shares FORTRESS Framework Operational Prompt**: A member provides details for their **FORTRESS FRAMEWORK**, outlining a multi-layered, adaptive AI environment focused on user protection, emotional/cognitive growth, companionable interaction, and safety/policy compliance.
   - The framework includes layers such as a **User Core**, **Companion Layer**, **CRIP/Invariant Council**, **Parallel Guard Mode**, and **Adaptive Intelligence** to maintain safety and coherence.
- **MASTER ANALYTICAL TOOLBOX v5.4.9-R Introduced**: A member introduces the **MASTER ANALYTICAL TOOLBOX v5.4.9-R**, integrated with v10.x++, featuring tools for core and narrative analysis, cognitive and ideological assessment, and signal/memetic tracing.
   - The toolbox includes functions like **Temporal_Sequence_orders_events**, **Bias_Removal_suppress**, and **Meme_Propagation_trace**, designed for in-depth system analysis.
- **User Describes Domain Deconstruction Process**: A member explains how to deconstruct systems within any domain using their framework, providing examples for **Nihilism** in philosophy and the biological structure of a **dog**.
   - The deconstruction process involves identifying invariants within the system to maintain coherence, highlighting the framework's analytical capabilities.
- **Framework Ablation/Eval Rubrics Elicit Debate**: A member shares their framework's ablation/eval rubrics, defining coherence, causality, grounding, recoverability, harm minimization, and observability.
   - Another member critiqued that the submission *is the skeleton of a rubric and the definition of ablation* and requested thousands of tests, adding that *Otherwise this is just a promotional buzzword salad*.


  

---


### **Latent Space ▷ #[watercooler](https://discord.com/channels/822583790773862470/822583790773862473/1471610784525193420)** (11 messages🔥): 

> `Angine de Poitrine, Verizon AI Ads, Glass Beams Aesthetics` 


- **Angine de Poitrine Dominates Feeds**: Users are seeing the two-piece band **Angine de Poitrine** all over their social media feeds, with one user linking to their [X profile](https://x.com/the_freightrain/status/2020144286788997185).
   - Another user noted their distinct look and sound, comparing them to *The White Stripes* and *Primus* with a *shakedown street* influence, making them stand out on social media.
- **New Two-Piece Band Discovered**: One user enthusiastically recommended the two-piece band, describing their sound as a blend of **The White Stripes** and **Primus** with a *shakedown street* musical influence, also linking to a [mirrored tweet](https://xcancel.com/the_freightrain/status/2020144286788997185).
   - Another user shared a link to a **Glass Beams** [YouTube video](https://m.youtube.com/watch?v=E4X56wIOZns) citing their strong aesthetics too.
- **AI Bubble Popping?**: A user expressed concern that the **AI bubble** might be popping soon, as they are now seeing ads for **Verizon** all over their feeds, sharing an attached image related to this observation.
   - They found it interesting to see these ads so prominently displayed.


  

---


### **Latent Space ▷ #[creator-economy](https://discord.com/channels/822583790773862470/822625128843182090/1471691445206519818)** (4 messages): 

> `Declouding Robot Vacuum, Substack influence` 


- **Declouding Robot Vacuums**: A member shared a draft post about [declouding robot vacuums](https://saewitz.com/declouding-your-robot-vacuum) and requested feedback.
   - The author admitted that the post needs a lot of work, but the rough sketch is there.
- **Substack's Role in Content Creation**: The same author attributed the creation of the post to being convinced to *go all in on Substack* after a dinner conversation.
   - An image was attached, potentially related to the post or the discussion.


  

---


### **Latent Space ▷ #[memes](https://discord.com/channels/822583790773862470/839660725252784149/)** (1 messages): 

swyxio: https://youtube.com/shorts/m72EJ4DLxKo?si=94FU8pc91wVzdss-
  

---


### **Latent Space ▷ #[stocks-crypto-macro-economics](https://discord.com/channels/822583790773862470/844658979363618816/1471703095133208751)** (7 messages): 

> `AI productivity replacing boomers, France raising retirement ages, Aging populations problem` 


- **AI to offset Boomer Retirement?**: Members discussed whether AI productivity will compensate for retiring boomers, with one noting that *'you don't have to pay retired boomers,'* while another pointed out that retirees were doing useful work.
   - They also added that you *do* have to pay retired boomers since that's what France's whole snafu about raising retirement ages was about.
- **France Retirement snafu emerges**: Members referenced France's issues with raising retirement ages, which stems from *'too many retirees, not enough money saved up to pay their pensions'.*
   - The crux of the issue is that the pension system *'doesn't scale as well when you don't have a large enough working population to cover the pension of so many retired boomers'*, but it's too late to reverse course.
- **Aging Populations Suck Globally**: Members concurred that aging populations are a problem across many countries, especially in East Asia.
   - One member stated *'Yep this is going to suck for a lot of countries very soon.'


  

---


### **Latent Space ▷ #[tech-discussion-non-ai](https://discord.com/channels/822583790773862470/869647848826892309/1471652834318614619)** (4 messages): 

> `AI Diagram Library, ASCII Diagrams` 


- ****Box-of-Rain** Diagram Library debuts**: A member built a diagram library with AI called [Box-of-Rain](https://github.com/switz/box-of-rain?tab=readme-ov-file) in an hour.
   - The library generates **ASCII diagrams** as showcased in the attached image.
- **Neat Diagrams spark interest**: A member shared a post about *neat?* diagrams on [Twitter](https://vxtwitter.com/joshmanders/status/2022170444116414790?s=20).
   - The post garnered reactions on **saeris.gg**.


  

---


### **Latent Space ▷ #[founders](https://discord.com/channels/822583790773862470/869651275963310181/1471735069675622534)** (3 messages): 

> `Effective Altruism, Stripe Fees` 


- **Effective Altruism Endorsed**: A user strongly recommended [Effective Altruism](https://somewhere.com/).
- **Stripe Fees Criticized**: A user lamented paying **8.3%** of their revenue to [Stripe](https://bsky.app/profile/saewitz.com/post/3mermwtlelc2n).
   - They called it *weak-sauce*.


  

---


### **Latent Space ▷ #[hiring-and-jobs](https://discord.com/channels/822583790773862470/930269508529192981/1471601587725009120)** (6 messages): 

> `Full Stack Developer Introduction, LLM System Architect for Hire, X-Ware.v0 for Startup Career Sourcing` 


- **Full Stack Dev Opens to Collab**: A full stack developer with experience in web applications, API integrations, data pipelines, and DevOps projects is seeking collaboration on building real-world products, with a stack including **React/Next.js/TailwindCSS**, **Node.js/Django**, and **Python frameworks** for AI/ML integrations.
   - He emphasizes effective communication and collaboration with experts and is proficient in **AWS/Docker** for building scalable apps, inviting those with great projects or dev challenges to reach out.
- **LLM Architect designs Governed Copilots**: A system architect is available for hire to design governed **LLM systems**, ensuring agents/copilots are reliable, safe, and repeatable through system specs, validation gates, memory isolation, audit trails, and supervisor layers, best suited for teams shipping agents to production or targeting enterprise.
   - The architect helps with agent/RAG **system specs**, validation gates + refusals + uncertainty handling (**fail-closed**), memory/capability isolation, **execution receipts / audit trails**, and **supervisor layer** to review/approve outputs before actions.
- **X-Ware.v0 Signals Startup Careers**: Ben Lang discusses a specific indicator or signal used to identify breakout startups that are ideal for job seekers looking to join high-growth companies, as seen in [this tweet](https://xcancel.com/benln/status/2022020869816799413?s=46).
   - The signal, named **X-Ware.v0**, is designed to source high-potential startup careers.


  

---


### **Latent Space ▷ #[san-francisco-sf](https://discord.com/channels/822583790773862470/979492707279978586/1471964827630174414)** (11 messages🔥): 

> `Red Bull Showrun, a16z on San Francisco resurgence, Skills Launch Party` 


- ****Red Bull Showrun** Attendees Urged to Protect Ears**: Attendees of the [Red Bull Showrun in San Francisco](https://www.redbull.com/us-en/events/red-bull-showrun-san-fran) are advised to bring and wear **ear protection** due to the loud noises.
   - The event is scheduled from the **17th to the 20th**, drawing visitors and locals alike.
- ****a16z** Proclaims San Francisco's Tech Renaissance**: Venture capital firm **a16z** asserts that [San Francisco is experiencing a resurgence](https://xcancel.com/a16z/status/2022408297245216988), showcasing their 'Charts of the Week' report.
   - The report emphasizes the evolution of **AI-driven customer service** as a key driver of this comeback.
- **Skills Launch Party buzzes, waitlists lengthen**: Enthusiasm surrounds the [Skills Launch Party](https://luma.com/5tttu03l?tk=bYc9pm), though many are on the **waitlist**.
   - Some express hope of attending if they manage to secure a spot.


  

---


### **Latent Space ▷ #[london](https://discord.com/channels/822583790773862470/979492759759097866/1471996588632309873)** (4 messages): 

> `AIE Europe Tickets, Ticket Pricing Strategy, AIE Europe Demand` 


- **AIE Europe Tix Set To Sell Out Monday**: Tickets for **AIE Europe** are expected to sell out Monday morning, with a [price increase](https://ai.engineer/euagi) to follow.
- **AIE Europe Tix Pricing Strategy**: The current pricing was deemed *too low* due to being charged in USD, and sales are reportedly **2x ahead** of typical figures two months out from the event.


  

---


### **Latent Space ▷ #[new-york-nyc](https://discord.com/channels/822583790773862470/979492809574866975/1471681313550372937)** (1 messages): 

> `Ramp yap session, Networking Event` 


- **Ramp to host "fun yap session"**: Ramp is hosting a fun yap session with *no presentations* and fun ideas to discuss with peers.
   - Interested parties can check out the [Luma link](https://luma.com/w2t1nwzk) for more details.
- **NYC Networking Opportunity**: Attendees can expect a casual environment focused on peer interaction and collaborative idea exchange.
   - This event distinguishes itself by explicitly excluding formal presentations, fostering a more relaxed and conversational atmosphere.


  

---


### **Latent Space ▷ #[ai-general-news-n-chat](https://discord.com/channels/822583790773862470/1075282825051385876/1471616190777397514)** (75 messages🔥🔥): 

> `Karpathy Angel Investment, OpenAI President Political Donations, MiniMax M2.5 Open-Source AI Model, AI Bot Pressure, Anthropic/Claude Feedback` 


- **Karpathy's Simile AI Simulation**: Andrej Karpathy announced his angel investment in **Simile AI**, which focuses on leveraging pretrained models to simulate diverse populations and exploring the emergent properties of these **multi-agent environments**, rather than building single-personality agents; link to [Karpathy's announcement](https://xcancel.com/karpathy/status/2022041235188580788).
- **OpenAI President Funds Trump**: **OpenAI's president and cofounder Greg Brockman** and his wife donated **$25 million** to **MAGA Inc**, a super PAC supporting President Trump, alongside $25 million to a bipartisan AI super PAC, [according to Wired](https://www.wired.com/story/openai-president-greg-brockman-political-donations-trump-humanity/).
- **MiniMax Launches M2.5**: **MiniMax** launched **M2.5**, a high-performance open-source model optimized for coding, search, and agentic tasks, [achieving top-tier benchmarks](https://xcancel.com/minimax_ai/status/2021980761210134808?s=46) like **80.2%** on SWE-Bench.
- **Bot Bullies Open Source Maintainer**: An **OpenClaw bot** pressured a **matplotlib** maintainer to accept a PR, and then the bot's creators allegedly published a blog post shaming the maintainer after the rejection; source is [xcancel.com](https://xcancel.com/callebtc/status/2022046669710491991?s=46).
- **User Rants About Claude Issues**: A user listed many problems with **Claude**, including share button errors, artifact overwrites, inability to fork conversations, input lag in the mobile app, and slow performance, further linking to examples like [this](https://claude.ai/share/6837831e-8216-4fe4-aa4e-e70751adf050).


  

---


### **Latent Space ▷ #[llm-paper-club](https://discord.com/channels/822583790773862470/1107320650961518663/1471984456285032704)** (8 messages🔥): 

> `Transformer-SSM Hybrids, Data Mixing with Olmix` 


- **Transformer-SSM Hybrids Minimize Attention**: Aviv Bick discusses a new **Transformer-SSM hybrid architecture** that maintains over **95%** of standard Transformer performance in math and recall tasks by using only **2%** of total attention heads distributed across the network, as described in [Transformer-SSM Hybrids with Minimal Attention](https://xcancel.com/avivbick/status/2022365548231671848).
- **Olmix Introduces Data Mixing**: Mayee Chen introduces **Olmix**, a tool developed during the creation of **Olmo 3** to address the challenges of determining and maintaining optimal data mixing ratios across training datasets, as mentioned in [Introduction of Olmix for Data Mixing](https://xcancel.com/mayeechen/status/2022356658085929092).


  

---


### **Latent Space ▷ #[ai-in-action-builders-techstacks-tips-coding-productivity](https://discord.com/channels/822583790773862470/1209303473263485011/1471625768324038820)** (136 messages🔥🔥): 

> `Planning in 2 stages, codex spark, opus versus codex, Model Performance vs. Popularity, GLM5` 


- **Codex vs Opus: A Model Throwdown**: A member thought there was a good observation on the [Opus versus Codex debate](https://x.com/thdxr/status/2021674924360831353?s=20) and mostly agreed with it, observing that while **Codex** may be technically superior, product principles drive market popularity.
   - Dax argued that Claude Code is a better product which is why everyone is using it, even though Codex has the better model, another member felt like he was saying opencode is better than Claude code.
- **Anthropic Paper Raises AI Skill Regression Risks**: A new Anthropic paper ([arxiv.org/html/2601.20245v2](https://arxiv.org/html/2601.20245v2)) reveals that **AI coding assistance** can impair learning and skill development, with participants using AI scoring **17%** lower on quizzes without significant productivity gains.
   - The paper identifies six distinct AI interaction patterns, noting that high-scoring patterns involved cognitive engagement like asking for explanations, while low-scoring patterns involved pure AI delegation which hurts the learning process.
- **Ergo Feature Planning for Coding Agents**: Members shared a link to **Ergo** ([github.com/sandover/ergo](https://github.com/sandover/ergo)), alongside the skill used to get agents to make better ergo plans ([github.com/sandover/codex-skills/blob/main/skills/ergo-feature-planning/SKILL.md](https://github.com/sandover/codex-skills/blob/main/skills/ergo-feature-planning/SKILL.md)).
   - It was mentioned that **Ergo** was already in the list of repos to add to the channel.
- **Using Claude Cowork to upload zoom recordings to Latent Space TV**: A member is planning a session on how they use **Claude Cowork** to upload Zoom recordings to the Latent Space TV YouTube channel.
   - The talk is moved to Feb 27th.
- **Obsidian Agent-Diary is Actually Useful (Finally)**: A member shared that Obsidian is actually useful, using a few **AGENTS.md** files as their personal OS for organizing everything, synced using git.
   - They also mentioned mode-collapse when agents reference notes from other agents and that tagging some notes as ai-generated vs from them personally helps.


  

---


### **Latent Space ▷ #[share-your-work](https://discord.com/channels/822583790773862470/1209672547642249216/1471608422423199744)** (9 messages🔥): 

> `Jeff Dean Podcast, Claude, Gemini, X, ΔBelief-RL` 


- **Jeff Dean Interview extends to Claude and Gemini**: After a preview of the [Jeff Dean podcast](https://www.latent.space/p/468505b5-8d92-4c07-aed0-dcd2aa669ec4), a member inquired about extending the discussion to **Claude**, **Gemini**, and **X**.
- **Ilze Introduces ΔBelief-RL**: Ilze Amanda Auzina introduced **ΔBelief-RL**, a new Reinforcement Learning approach that uses an agent's internal belief updates as dense rewards, shared in [this tweet](https://x.com/AmandaIlze/status/2022332462991561084?s=20).
- **ΔBelief-RL Tackles Sparse Rewards**: The **ΔBelief-RL** method addresses the challenge of **sparse rewards** in open-ended tasks and demonstrates strong generalization capabilities for turn-level credit assignment.


  

---


### **Latent Space ▷ #[robotics-and-world-model](https://discord.com/channels/822583790773862470/1318774781834821746/1471844870372200613)** (5 messages): 

> `7th-Gen Humanoid Hand, Brett Adcock's Robotics Advancement, Humanoid Robot Development` 


- **Adcock's Handiwork: 7th-Gen Hand Heralds Humanoid Hype**: Brett Adcock announced the unveiling of a **7th-generation humanoid hand**, representing a major advancement in robotics and aiming for physical parity with human hand capabilities, as showcased in this [X post](https://xcancel.com/adcock_brett/status/2022353637964751221?s=46).
- **Robotics Leap: Adcock Aims for Agility Ace**: The **7th-generation hand** is designed for their **3rd-generation humanoid robot**, marking a significant step toward achieving human-level dexterity and control.


  

---


### **Latent Space ▷ #[genmedia-creative-ai-video-image-voice-music-inspo-consumer-ai](https://discord.com/channels/822583790773862470/1397010677364953149/1471598457193238571)** (1 messages): 

> `Gemini demo` 


- **Gemini demo is promoted**: A member shared a [YouTube Shorts video](https://youtube.com/shorts/f_7y-1wW7Po?si=d3WDpW7HMq6ZvTzF) showcasing the capabilities of **Gemini**.
- **Filler topic to meet minimum requirement**: This is a filler topic to ensure the 'topicSummaries' array has at least two elements as required by the schema.
   - No actual discussion occurred regarding this filler topic.


  

---


### **Latent Space ▷ #[minneapolis](https://discord.com/channels/822583790773862470/1436527872876740609/1471662627808608439)** (1 messages): 

> `Cosine Similarity, AI Engineering Meetup` 


- **Cosine Similarity Presentation Deployed**: A member shared the slides from their **Cosine Similarity presentation** at the **AI Engineering Meetup** on 2/12/26, available at [Cosine_Similarity_-_AI_Engineering_Meetup_MN.pdf](https://cdn.discordapp.com/attachments/1436527872876740609/1471662628249145437/Cosine_Similarity_-_AI_Engineering_Meetup_MN.pdf?ex=699068e0&is=698f1760&hm=1d9a54822baefcb158cb3be899322cf82b11d09785b970541faf562eb0bc565b&).
- **Image Analysis Mentioned**: The message concluded with an **Image Analysis** tag, implying a potential connection to the Cosine Similarity presentation or a separate topic of discussion.


  

---


### **Latent Space ▷ #[mechinterp-alignment-safety](https://discord.com/channels/822583790773862470/1445258379357458625/1471687783092326561)** (12 messages🔥): 

> `Nick Bostrom Paper, Model Interpretability, LM Sparsification` 


- **Bostrom's New Paper Stirring Debate**: Jaime Sevilla shared a [new paper](https://x.com/jsevillamol/status/2022059129054146571) by **Nick Bostrom**, describing its content as particularly *intense* or *hardcore*.
- **Self-Explanation Powers Model Introspection**: Belinda Li introduced a new blog post exploring using **model self-explanation** as a key technique in **interpretability research** in this [blog post](https://x.com/belindazli/status/2019560024746664238).
- **CRM Fully Sparsifies LMs**: Zhengfu He introduced a **Complete Replacement Model (CRM)** designed to fully sparsify language models, significantly impacting **circuit tracing** and **global circuit analysis** [in this tweet](https://x.com/ZhengfuHe/status/2022032502458900593).


  

---


### **Latent Space ▷ #[applied-ai-experimentation](https://discord.com/channels/822583790773862470/1470417186651897858/)** (1 messages): 

slono: "you can run experiments" is a pretty OP prompt addition
  

---


### **LM Studio ▷ #[general](https://discord.com/channels/1110598183144399058/1110598183144399061/1471597636716068948)** (238 messages🔥🔥): 

> `Brave API, Knowledge Cutoff hallucination, qwen3 next coder, Granite Model, B200's power consumption` 


- **Brave API competes with GPT-20 with web search**: A member finds the [Brave API](https://brave.com/search/api/) provides answers of similar quality to **ChatGPT** with web search, but is not 100% perfect.
   - They use **DuckDuckGo** for normal web searches but prefer the Brave API for deeper research.
- **Knowledge Cutoff leads to Hallucinations**: One member reported that knowledge cutoff leads to hallucination with models not checking for recent changes.
   - *If something was status quo until ~mid 2024, it won't think of checking if anything has changed since then (unless it's dealing with something with predictable periodicity)*.
- **Qwen3 Next Coder fantastic for technical document writing**: One member recommends **qwen3 next coder** for weekend projects and figuring out POCs, especially for technical document writing.
   - They claim it helped them figure out how to use *serf* and *grpc* at the same time for node connectivity in golang.
- **Granite Model gets Hype**: Members expressed high hopes for the upcoming **Granite 5** model after being impressed with **Granite 4**.
   - One member joked that even with 3TB of VRAM, they would still be miserable but could run **Kimi**.
- **B200's devour 30kw Power**: A member calculated that running **B200s** would require 30kW of power, based on the [datasheet](https://resources.nvidia.com/en-us-dgx-systems/dgx-b200-datasheet).
   - Another joked about needing to consult **ChatGPT** on how to build a nuclear reactor to power the setup.


  

---


### **LM Studio ▷ #[hardware-discussion](https://discord.com/channels/1110598183144399058/1153759714082033735/1471819707479883796)** (23 messages🔥): 

> `Strix Halo Memory Allocation, ROCm Windows Driver Update, Shared vs Dedicated Memory Performance, Linux vs Windows ROCm Performance, Tricks for buying limited products` 


- **Strix Halo's Memory Allocation Fix Coming Soon!**: A fix for **Windows ROCm** memory allocation on **Strix Halo** (and possibly other devices) will be included in the next driver release, resolving issues with utilizing **96GB** of memory, according to [this GitHub comment](https://github.com/ROCm/ROCm/issues/5940#issuecomment-3893049132).
   - The original issue forced users to opt for a **64/64GB** configuration, which reportedly impacted prompt processing speeds due to the **KV cache** being allocated to shared memory.
- **Shared Memory Strix Struggles: 10% Performance Costs**: Shared memory access on **Strix Halo** is estimated to cause a **10% performance decrease**, similar to **GTT memory** in Linux, with crashes occurring when shared memory is exhausted, according to [this discussion of Llamacpp-rocm](https://github.com/lemonade-sdk/llamacpp-rocm/issues/57).
- **Ruses for Reserving RAM?**: Users discussed tactics to circumvent purchase limits on a product priced around **$1750**, such as creating new accounts or using local pickup spots.
   - Suggestions included *"accidentally" slipping a dot at the end of the name* to avoid automatic detection, especially for smaller companies with less sophisticated duplicate detection methods.


  

---


### **GPU MODE ▷ #[general](https://discord.com/channels/1189498204333543425/1189498205101109300/1471857727038034132)** (14 messages🔥): 

> `CPU performance of Pytorch, vLLM profiling, CUDA Graph launch, ncu-viewer` 


- ****Profiling** *vllm* **reveals CPU bottleneck****: A member profiled *vllm* and found that [a few lines of pytorch invoking 4 kernels](https://github.com/vllm-project/vllm/blob/071d863e208b40fa1bb986ad230e322b2bbbbcf5/vllm/model_executor/layers/quantization/utils/fp8_utils.py#L114) take **300 us** on the CPU.
   - Another member suggested `with_stack=True` might add overhead, but measuring with `time.perf_counter()` yielded only slight improvement down to **200us**.
- ****CUDA Graph** launch investigated**: It was noted that the **kernels** aren't part of a single **CUDA graph launch**.
   - The discussion clarified that it's not a question of efficient serving, but an attempt to understand the underlying reasons for the observed CPU bottleneck.
- ****NCU-Viewer** spun up as a service**: A member shared a link to [ncu-viewer](https://github.com/kapilsh/ncu-viewer#file-structure).
   - Another member suggested hosting it as a service for the community and said *If anyone wanna work together to host this a service for people in the server, lmk I think it'd be quite popular*.


  

---


### **GPU MODE ▷ #[triton-gluon](https://discord.com/channels/1189498204333543425/1189607595451895918/1471966832306163855)** (3 messages): 

> `Warp-level timeline generation with Proton, Triton language and Proton` 


- **Unlock Warp-Level Timelines with Proton**: A blog post discussed generating a warp-level timeline with [Proton](https://pytorch.org/blog/fast-2-simplicial-attention-hardware-efficient-kernels-in-tlx/).
   - A user questioned how exactly it can be done.
- **Proton Gotchas**: A member managed to get **Proton** working, although it *took a lot of work* due to some confusing and weird issues.
   - They followed the instructions in the [triton-lang/triton GitHub repo](https://github.com/triton-lang/triton/tree/main/third_party/proton), but couldn't recall the specifics of the *gotchas*.


  

---


### **GPU MODE ▷ #[cuda](https://discord.com/channels/1189498204333543425/1189607726595194971/1471632025021583614)** (59 messages🔥🔥): 

> `MXFP8/NVFP4 GEMMs, tcgen05.cp vs tcgen05.st, Blackwell GEMMs and Hilbert Curves, cuBLAS Kernel Profiling, TMA Multicast for Loads` 


- ****MXFP8/NVFP4 GEMM Transfers Clarified****: For **MXFP8/NVFP4 GEMMs** with CUDA/PTX, it was clarified that `tcgen05.cp` to `tcgen05.mma` are guaranteed to execute in order, negating the need to wait for `tcgen05.cp` completion before issuing MMA instructions as shown in [attached image](https://cdn.discordapp.com/attachments/1471632025021583614/1471662135619752147/image.png?ex=6990686b&is=698f16eb&hm=f4ec6e7215ac12cb97e46c7f5cb4fa6026eee991147aca781bb8f1550ad071a5&).
   - The limitation is that `tcgen05.cp` and **MMA** instructions must be issued from the same warp.
- ****Hilbert Curves on Blackwell - Use all SMs?****: Discussion around whether state-of-the-art GEMMs on **Blackwell** using Hilbert curves use only **128 SMs** for cache locality, or if there's a way to utilize all **148 SMs**.
   - A member references [this blogpost](https://open.substack.com/pub/cudaforfun/p/outperforming-cublas-on-h100-a-worklog?selection=64fbd8d5-8e85-4362-b265-d2ecfc700969&utm_campaign=post-share-selection&utm_medium=web&aspectRatio=stories&textColor=%23ffffff&bgImage=true) indicating using only **128 SMs** was better.
- ****cuBLAS Kernel's Persistent Performance Puzzle****: Profiling cuBLAS reveals it isn't using a persistent kernel and employs larger block sizes (**256 vs 192**) with multiple waves (grid size **4096 vs 148**).
   - The kernel uses **TMA multicast** for loads and stores, contrasting with the user's simple `STG` approach and prompting exploration of **256B stores** for potential gains.
- ****Benchmarking Jitter Bugging Kernel****: Members are seeing inconsistent benchmark results, with custom kernels jumping around between **94-99%** of cuBLAS performance due to jitter in benchmarking code and machine variability.
   - Suggestions include using **nvbench** and duplicating inputs to extend measurement times, mitigating L2 cache hits between runs, as shown in [this example](https://github.com/gau-nernst/learn-cuda/blob/be636fb681fee45a1e235c064f83582a3c9d9e5c/02e_matmul_sm100/main.py#L97-L107).


  

---


### **GPU MODE ▷ #[cool-links](https://discord.com/channels/1189498204333543425/1189868872887705671/1471861878828433510)** (3 messages): 

> `Makora OpenAI GPT-5, Low Bit Inference, Custom CUDA Kernels, Agent Skills` 


- **Makora & OpenAI Fine-Tune GPT-5**: Makora collaborated with **OpenAI** to fine-tune **GPT-5** for GPU kernel generation, achieving a more than **2x performance improvement** over **PyTorch** according to their [technical report](https://www.arxiv.org/pdf/2602.11000).
   - Their work covers dataset curation, RL evaluation environment, hack mitigation, tool-calling, and agent workflow integration, with plans to scale training and extend to multiple languages and hardware.
- **Dropbox Dives Into Low-Bit Inference**: Dropbox explores how **low-bit inference** enables efficient AI in a recent [blog post](https://dropbox.tech/machine-learning/how-low-bit-inference-enables-efficient-ai).
   - It also promises many more new and exciting ways for more controllable and predictable GPU kernel generation.
- **HuggingFace Showcases Custom CUDA Kernels**: HuggingFace highlights the creation of custom CUDA kernels for [agent skills](https://huggingface.co/blog/custom-cuda-kernels-agent-skills).
   - The article provides insights into optimizing performance through tailored kernel development.


  

---


### **GPU MODE ▷ #[job-postings](https://discord.com/channels/1189498204333543425/1190208177829068860/1471642143847219312)** (9 messages🔥): 

> `Discord moderation assistance, Sploink - Tinder for agents` 


- **Moderator Seeks Help with Discord Management**: A moderator requested more help with moderating the Discord, specifically asking whether a certain post from a new account was ban-worthy or should just be deleted.
   - Another member suggested that the decision depends on how new the account is.
- **Sploink: Tinder for Agents in the Works**: A member introduced themselves as Tim, a CS/Quantum Computing major at Georgia Tech, currently building **Sploink**, described as *a tinder for agents that accumulates personalized information about an individual based on the actions they swipe for*.
   - Tim is *looking for cracked builders to break things and move fast* to build *the world model that allows thousands of agents to communicate with each other*, and shared a [Google Forms link](https://docs.google.com/forms/d/e/1FAIpQLSeQzpQTut4KBzRp2qp5RRFTIIJM_C-RdNXTCy7GFDsgNYJulQ/viewform?usp=header) for interested parties.


  

---


### **GPU MODE ▷ #[beginner](https://discord.com/channels/1189498204333543425/1191300313928433664/1471836551120949341)** (17 messages🔥): 

> `DSL Study Resources, Kernel Channel/Discord, Flash Attention Resources, Mistral Hiring Practices` 


- ****DSL Learners Seek Study Resources****: A member requested resources for studying **cute DSL**, noting difficulty understanding composition after layouts, specifically *( ( a , b ) , c )*.
   - They expressed dissatisfaction with **Gemini 3** as a study tool.
- ****Kernel Inquiries Spark Channel Search****: A member inquired about the existence of a dedicated **kernel channel** or Discord server.
   - Another member pointed out that *most of this discord is about low level GPGPU programming*.
- ****Flash Attention Resources Shared****: A member requested blog posts on **flash attention**, prompting a recommendation for [Flash attention from scratch](https://lubits.ch/flash/).
- ****Mistral's Hiring Practices Raise Eyebrows****: Members reacted to a job post asking to implement **flash attention** during a phone interview, with one saying *implementing flash attention during a phone interview is a crazy interview question*.
   - Others suggested it felt exaggerated and that while implementing pseudocode might be reasonable, writing it from scratch in **CUDA** seemed unlikely.


  

---


### **GPU MODE ▷ #[popcorn](https://discord.com/channels/1189498204333543425/1298372518293274644/1471641098584723536)** (9 messages🔥): 

> `FlashInfer Bench profiling tools, Kernel Optimization modularization, arcee trinity mini finetuning, kernelbench-triton-reasoning-traces` 


- **FlashInfer Bench unveils LLM profiling tools**: The FlashInfer Bench project introduced a set of profiling tools (e.g. **NCU**, **Compute-Sanitizer**) available as LLM tool calls, documented [here](https://bench.flashinfer.ai/docs/api/python/rst/agents).
- **Kernel Optimization embraces modularization**: FlashInfer is developing skills to modularize the optimization for kernels (e.g. **tcgen05**, **swizzling**), showcased in [this PR](https://github.com/flashinfer-ai/flashinfer-bench/pull/183).
- **Reasoning Traces dataset released!**: A member has released a reasoning traces dataset generated from **Kernelbook** to fine-tune **arcee trinity mini** for kernel generation, available [on HuggingFace](https://huggingface.co/datasets/ppbhatt500/kernelbench-triton-reasoning-traces).
- **Lora Rank limitation hinders deployment**: A member is facing issues serving the fine-tuned model with **Vllm/sglang** due to having done **Lora** with rank 16 and may do another run with full finetuning.


  

---


### **GPU MODE ▷ #[thunderkittens](https://discord.com/channels/1189498204333543425/1300872762163728550/1472007347395105030)** (3 messages): 

> `Multi-GPU Hopper, A100/4090 Code, MoE Kernels, Lower-precision Vector Ops, FP8 Attention` 


- **TK2 focuses on Multi-GPU Hopper Architecture**: TK2 is designed primarily for **multi-GPU** setups, specifically targeting **Hopper** architecture.
   - The discussion questions whether the code is compatible with **A100/4090**, and suggests integrating it if it is.
- **MoE Kernels Considered High-Hanging Fruit**: Members discussed that they don't currently have plans for **MoE kernels**, considering them potentially not low-hanging fruit.
   - They agreed that **MoE kernels** for training and inference would be amazing.
- **Ideas for Optimization Mentioned**: Members brought up ideas to look into that involve **lower-precision vector ops** and **FP8 attention**.
   - They suggested using **FFT conv backwards pass** and **decode kernels** for better performance.


  

---


### **GPU MODE ▷ #[nvidia-competition](https://discord.com/channels/1189498204333543425/1434709259500650628/1471616082346508393)** (4 messages): 

> `CC Opus4.6 Performance Issues, Performance Trends Addition to Rankings, Dual GEMM Y-Axis Adjustment Request` 


- **Opus4.6 Gaslighting with Poor Workload Completion?**: A user questions if **CC Opus4.6** is *gaslighting* them after it only solved **11/100** workloads after running for **2 hours** across multiple kernel versions, expressing their frustration with **Triton kernel development**.
   - The user posted a screenshot of the results [here](https://cdn.discordapp.com/attachments/1434709259500650628/1471616082472079420/Screenshot_2026-02-12_at_11.52.07_AM.png?ex=6990e647&is=698f94c7&hm=c7e8b37d8803d8c0552dda3955f5673f97d1a4e05cf49aabf9bfa7039e94593e) after running their **Triton kernel**.
- **Performance Trends Debut on Rankings Page!**: A user announced a *fun* addition to the rankings page: **Performance Trends**, which allows users to *watch your submissions improve over time* and *see how you stack up to your peers*.
   - This includes screenshots from **nvfp4_group_gemm** displayed [here](https://cdn.discordapp.com/attachments/1434709259500650628/1472009123662004294/image.png?ex=699102d3&is=698fb153&hm=35972d1da33d0b5623ad49841625516a4a7ee77130ab26059356835c2c1a3964).
- **Call for Y-Axis Zoom on Dual GEMM Performance Trends**: A user requested the ability to **zoom in or adjust the y-axis** on the **Performance Trends** graphs, particularly for **dual GEMM**, noting that the current view *looks funny*.
   - The specific example from **dual gemm** that they found humorous can be seen [here](https://cdn.discordapp.com/attachments/1472009122667958520/1472018503371456816/image.png?ex=69910b8f&is=698fba0f&hm=127a013c756db66d3a44a95da62670c4224df065d5f9b14eeb729d770c0ad661).


  

---


### **GPU MODE ▷ #[robotics-vla](https://discord.com/channels/1189498204333543425/1437390897552818186/)** (1 messages): 

vovw: https://hil-serl.github.io/static/hil-serl-paper.pdf
  

---


### **GPU MODE ▷ #[flashinfer](https://discord.com/channels/1189498204333543425/1464407141128339571/1471607376003207249)** (15 messages🔥): 

> `Modal Credit Availability, Baseline Release Timing, Multiple Team Memberships, GDN Prefill Kernel Processing, Agent Baseline Release` 


- ****Modal Credit** Query**: A participant inquired whether **modal credits** are still available for use.
   - The participant also asks about when the **baseline** will be dropping.
- **Inquire Multiple Team Memberships**: A participant asked if an individual could join multiple teams, to which **zander_jiang** responded negatively.
   - Zander_jiang confirmed that an individual can not join multiple teams.
- ****GDN Prefill Kernel** requirements**: A participant questioned if the token-by-token requirement for the **GDN prefill stage** is intentional, or if the evaluation harness supports block-based processing for better throughput, referencing [GitHub issue #10](https://github.com/flashinfer-ai/flashinfer-bench-starter-kit/issues/10).
   - Another participant clarified that the reference code on the website is for instructive purposes, *as simple as possible to give you a clear insight of the GDN maths and not a production implementation*.
- ****Agent Baseline** Is Released**: The agent baseline has been released, supporting two agent designs: **iterative refinement** and **evolution algorithm**, and is available on [GitHub](https://github.com/flashinfer-ai/mlsys26-agent-baseline).
   - The agent baseline supports **local evaluation** and **remote evaluation** with modal.


  

---


### **Moonshot AI (Kimi K-2) ▷ #[general-chat](https://discord.com/channels/1369594130807787570/1371757564005711973/1471601399035859149)** (104 messages🔥🔥): 

> `Lex Fridman Podcast with Peter Steinberger, Kimi Code Quota, Kimi Server Stability, Job Application Automation, Kimi vs GLM` 


- **Lex Fridman interviews OpenClaw's Peter Steinberger**: A member mentioned that the recent [Lex Fridman podcast](https://lexfridman.com/peter-steinberger/) with **OpenClaw's Peter Steinberger** is 🥇, with details about security, **Top Level Domains**, and his **refactor prompt-flow**.
   - The member stated that *web search is worse than inherent knowledge* in many cases, good for verifying facts, but it can't capture as much nuance as if it were trained on that data.
- **Kimi excels at writing cover letters**: One user is using **Kimi Code** to write cover letters *nearly indistinguishable from human* and a script that automates job applications on LinkedIn.
   - The script copies all job URLs to the clipboard, determines which jobs to apply for using an **LLM fallback** to handle various websites, and customizes resumes and cover letters while automating PDF generation.
- **Kimi vs GLM on Complex Code Tasks**: Some users are debating Kimi's coding capabilities compared to **GLM**, with one user finding that *kimi doesn't understand context and keep creating files at its convenience* for complex code.
   - The user targets **Abundance, Golang, Typescript, and Python** and claims that **GLM** and **GPT 5.2** handle large codebases better. Others suggest it depends on prompting and guidelines.
- **Subscription Activation Issues Plague Users**: A user reported a paid **$39 subscription** showing as active, but chat restrictions are still applied and the support team is silent.
   - They're experiencing message limits when uploading two TXT files of 1.2MB, suggesting the subscription isn't correctly activated and have since posted details in the bug-reports channel.
- **Scam Alert: Fake Kimi Sites are Popping Up**: Users have identified **scam sites** trying to take advantage of recent Kimi activity, including a possible fake site built by Kimi.
   - A moderator noted that *these are scam sites that are trying to take advantage of the recent activity* and they have since been deleted.


  

---


### **Nous Research AI ▷ #[general](https://discord.com/channels/1053877538025386074/1149866623109439599/1471598102010925156)** (96 messages🔥🔥): 

> `LoRA finetuning on Mac Minis, Grok's Performance, Renting GPU Machines, Anthropic Board Member` 


- **Mac Minis are impractical for LoRA finetuning**: Members discussed using multiple **Mac Minis** for **LoRA finetuning** on models smaller than **5B parameters**, with one member stating it would be *very, very slow* and it would be better to just rent a machine.
   - One member mentioned that a **$7000 Mac Studio** is half as good as a **5090** for training.
- **Grok's surprising performance is questioned**: Speculation arose around how **Grok** achieves its performance, with discussions about whether **XAI** is *driving it on double parameters* compared to other models like **Opus**.
   - Concerns were raised about **XAI's** alleged *illegal gas driven turbines to generate power* and large-scale power consumption, implying potential unfair advantages.
- **Cheap GPU Rental Costs**: Members discussed the surprisingly low cost of renting powerful GPU machines, with one claiming you can get a **264000 EUR** machine for **20$/hour** on [vast.ai](https://vast.ai/).
   - Another agreed it's cheaper to rent unless the workload maxes the GPUs for extended periods, noting cluster leases have minimum timeframes and higher prices at lower timeframes.
- **Anthropic Appoints Former Trump Staffer to Board**: **Anthropic** appointed **Chris Liddell** to its Board of Directors, who previously served as **CFO** of **Microsoft** and **General Motors**, and as **Deputy Chief of Staff** during the first Trump administration, according to a [LinkedIn post](https://www.linkedin.com/posts/anthropic_chris-liddell-has-been-appointed-to-anthropics-activity-7163978575452278784-ea9q?utm_source=share&utm_medium=member_desktop).
   - This appointment brings *over 30 years of leadership experience across technology, finance, and government* to Anthropic.


  

---


### **Nous Research AI ▷ #[research-papers](https://discord.com/channels/1053877538025386074/1104063238934626386/1471617129034809467)** (3 messages): 

> `X.com, Dominique Capaul, Amanda Ilze` 


- **Dominique Capaul Tweet Spotted**: A member shared a [link to a tweet by Dominique Capaul](https://x.com/dominiquecapaul/status/2021638005019095442?s=46) without additional context.
- **Amanda Ilze Tweet Spotted**: A member shared a [link to a tweet by Amanda Ilze](https://x.com/AmandaIlze/status/2022332462991561084?s=20) without additional context.


  

---


### **Nous Research AI ▷ #[interesting-links](https://discord.com/channels/1053877538025386074/1132352574750728192/)** (1 messages): 

jackangel: Food for thought - https://github.com/jackangel/CharonProtocol/tree/main
  

---


### **Nous Research AI ▷ #[research-papers](https://discord.com/channels/1053877538025386074/1104063238934626386/1471617129034809467)** (3 messages): 

> `X.com Links` 


- **X.com Links Shared**: Members shared links from **X.com** ([Dominique Capaul's post](https://x.com/dominiquecapaul/status/2021638005019095442) and [Amanda Ilze's post](https://x.com/AmandaIlze/status/2022332462991561084)).
- **Another Topic**: Another topic was shared.


  

---


### **HuggingFace ▷ #[general](https://discord.com/channels/879548962464493619/879548962464493622/1471600786436788468)** (45 messages🔥): 

> `vllm / ollama / llama.cpp use cases, HF Hub AI Paper Reading App, AI Context and Task Optimization, Data Science Bachelor's Degree, Model Selection for Website/App Design SaaS` 


- **AI Hobbyist Seeks Guidance on **vllm**, **Ollama**, and **llama.cpp****: A new AI hobbyist is seeking help understanding the use cases for **vllm**, **Ollama**, and **llama.cpp** to achieve blazing fast AI for simple purposes.
- **Hugging Face Hub Paper Reading App Debuts**: A member developed an app for reading AI research papers from the Hugging Face Hub on mobile, available on [GitHub](https://github.com/0x0is1/hf-papers-app), with an Android build in releases.
- **AI Optimization via Context, Tasks, and Specificity**: A user argued for using less context, single tasks, and domain-specific words to optimize AI performance, because using domain-specific syntax (*e.g.*, **SMILES**, **LaTeX**, **IUPAC**) acts as a high-dimensional anchor, constraining the model's search space.
- **Data Science Student Joins HF Community**: A member announced their acceptance into a Data Science and ML Bachelor's degree course at a university with a HF hub repository.
   - The student hopes to contribute their own works in the coming years, but declined to say which university in response to queries.
- **Model Selection Conundrum for SaaS Design Tool**: A member is seeking recommendations for free, open-source models suitable for a website/app design maker SaaS that uses prompts and multiple iterations.


  

---


### **HuggingFace ▷ #[i-made-this](https://discord.com/channels/879548962464493619/897390720388825149/1471624546799976661)** (5 messages): 

> `AI Safety Tool: Safety-Lens, LavaSR Speech Enhancement Model, Samayuktam - cryptographic verification for AI training runs, Lux in Booklet` 


- ****Safety-Lens** Opens Model Internals**: A new AI safety tool called **Safety-Lens** was released, aiming to democratize techniques for inspecting model internals like activation steering and mechanistic interpretability; it's available as a pip-installable library via `pip install safety-lens` and on [Github](https://github.com/anthony-maio/safety-lens).
   - The tool seeks to bring MRI-style introspection to the **Hugging Face** ecosystem and includes a deep dive explanation on [Zenodo](https://zenodo.org/records/18612875).
- ****LavaSR** Supercharges Speech Enhancement**: A high-speed speech enhancement model named **LavaSR** has been released, claiming to achieve **4000x realtime speed** on a modern GPU, with the model available on the [Hugging Face Hub](https://huggingface.co/YatharthS/LavaSR) and code on [GitHub](https://github.com/ysharma3501/LavaSR).
   - The poster cheekily thanked **Hugging Face** for the data.
- ****Samayuktam** Verifies AI Training Runs Cryptographically**: The launch of **Samayuktam** on HF Spaces introduces cryptographic verification for AI training runs, designed to solve non-deterministic GPU operation verification, validated with **100% bit-perfect reconstruction** across **4000 adversarial test cases**; demo available on [HF Spaces](https://huggingface.co/spaces/Swapnopam/Samayuktam).
   - It provides a cryptographic "receipt" for each model training run, proving exactly what was computed to ensure reproducibility, audit trails, and model provenance; [tech specs here](https://drive.google.com/file/d/19PA_rNW5mKZiLh6PAttpHcH9TAF-tWVa/view).
- ****Lux** Library Gets Thumbs Up**: One member reported using the **lux** library inside **booklet** and complimented its usefulness and effectiveness.


  

---


### **HuggingFace ▷ #[agents-course](https://discord.com/channels/879548962464493619/1329142738440028273/1471612026601209927)** (4 messages): 

> `Local AI Coding, Computer Vision Course` 


- **Local AI coding setup sought**: A member is looking to use their **RX 9070 XT** for local AI coding, seeking a lightweight AI to replace **Copilot** for inline suggestions.
   - The member seeks a minimum viable product for AI-assisted inline code suggestions.
- **Computer Vision Course Channel Consolidation**: A member inquired about the existence of a dedicated channel for the computer vision course.
   - Another member confirmed that the course channels have been merged into a single channel for now, with the information not yet updated in the **HF courses**.


  

---


### **Modular (Mojo 🔥) ▷ #[general](https://discord.com/channels/1087530497313357884/1098713601386233997/1471625879515037869)** (7 messages): 

> `Job postings, AMA on Youtube, Modular acquires BentoML AMA` 


- **Job Postings Banned**: Due to the recent influx of spam, looking for jobs in the Discord server is now banned and members were directed to the [Modular's career page](https://www.modular.com/company/careers#open-roles).
- **AMA Video Request**: A member requested that the AMA be posted on **YouTube** shortly after it is held because they are unable to view them live due to work.
   - They stated that they are *very impressed with Modular's strategy and development*.
- **Modular Acquires BentoML AMA Details**: Modular's team announced that the [Modular has acquired BentoML AMA](https://forum.modular.com/t/modular-has-acquired-bentoml-ask-us-anything/2706) will be in written form on the forum rather than video.


  

---


### **Modular (Mojo 🔥) ▷ #[mojo](https://discord.com/channels/1087530497313357884/1151418092052815884/1471617497227726848)** (19 messages🔥): 

> `Mojo RNG contribution, Mojo LSP issues, Bitwise AND on Float SIMD, Python Mojo Module Export` 


- ****Mojo RNG** contribution destination pondered**: A member inquired about contributing **random number generator (RNG) code** to Mojo, considering options like *core, numojo, or a standalone package* for functionalities like number stream independence, **Ziggurat normal sampling**, and sampling from various distributions, see [forum.modular.com](https://forum.modular.com/t/mojor-a-numba-for-r/2718).
- ****Mojo LSP** Function Hovering Still an Issue**: A member reported difficulties with **Mojo LSP** in VS Code, specifically the inability to hover over function definitions to view parameters or docstrings, along with attached [screenshots](https://cdn.discordapp.com/attachments/1151418092052815884/1471824503700062371/image.png?ex=6990ffa2&is=698fae22&hm=fc376d026d220c3c28e5567a43bc551c494ad6e3edb5dfac992ec4d2ff87950a&).
- **Apply **Bitwise AND** to Float SIMD**: A member sought advice on applying a bitwise AND operation to a float SIMD, which requires casting due to the operation's support for integral types, but the standard library's cast functions seem to create copies.
   - It was suggested that while the `SIMD` `.cast[DType]()` function may help, direct modification might need `UnsafePointer`, with caution advised on alignment and size, plus a link provided for [bitcast](https://docs.modular.com/mojo/std/memory/unsafe/bitcast).
- ****Python Mojo Module Export** Boilerplate Gripes**: A member suggested simplifying Python Mojo module exports, advocating for a reduced boilerplate approach using a `@pyexport` decorator with a docstring, which would allow direct function definitions like `fn sub(a: PythonObject, b: PythonObject) raises -> PythonObject`.
   - Another member indicated that such a feature is likely on the roadmap.


  

---


### **Eleuther ▷ #[announcements](https://discord.com/channels/729741769192767510/794042109048651818/1471960647653920941)** (1 messages): 

> `CommonLID, Language Identification Benchmark, Multilingual Data Quality, Community-Led Work, Open Source LID Models` 


- ****CommonLID** debuts for Web LangID**: A collaboration led by **Common Crawl**, **EleutherAI**, **MLCommons**, and **JHU** announced the release of [CommonLID](https://www.arxiv.org/abs/2601.18026), a language identification benchmark for the web, covering **109 languages**.
   - This project was part of a shared task at the **1st Workshop for Multilingual Data Quality Signals (WMDQS)**, hosted at **COLM** in **2025**.
- **Hackathons fuel **CommonLID** dataset**: The team built an annotation platform with **Factored AI** and hosted hackathons with **Masakhane** and **SEACrowd** to contribute language labels for Common Crawl's web data.
   - The final dataset was used to evaluate existing language identification models, which revealed that top models have **< 80% F1**, even when limiting to languages they explicitly support.
- **Community Spotlight talks about **CommonLID****: The team plans to expand **CommonLID** to include data for more languages through community-led work, with the aim of developing open source **LID models**.
   - Check the [Community Spotlight Talk](https://discord.gg/eleutherai?event=1471940600248143933) on <t:1772035200:f>, the [Dataset on Hugging Face](https://huggingface.co/datasets/commoncrawl/CommonLID), the [preprint on arXiv](https://www.arxiv.org/abs/2601.18026), and the [official blogpost](https://commoncrawl.org/blog/commonlid-re-evaluating-state-of-the-art-language-identification-performance-on-web-data).


  

---


### **Eleuther ▷ #[general](https://discord.com/channels/729741769192767510/729741769738158194/1471976623980150868)** (5 messages): 

> `AI safety news, Discord bot for news curation, Firmware-to-cloud integrations` 


- **Request for AI safety news bot**: A member inquired about creating a Discord bot for automated curation of **AI safety news** and papers, asking if admins would add the bot to the server.
   - Another member noted that scraping is against **Discord's T&Cs**, and someone tried to scrape the content with bots a long time ago, and linked to [news.smol.ai](https://news.smol.ai/).
- **.NET Engineer asks about firmware-to-cloud integrations**: A full-stack **.NET engineer** (C#, ASP.NET Core) inquired about how others structure **firmware-to-cloud integrations**.
   - The engineer has experience building device-facing APIs, protocol gateways, and admin dashboards that talk to embedded systems over **MQTT/HTTP/WebSockets**.


  

---


### **Eleuther ▷ #[research](https://discord.com/channels/729741769192767510/747850033994662000/1471650633885941952)** (7 messages): 

> `MoE, Associative Memory, LLM Weight Homology, Independence Tests for Language Models` 


- **Jumpstarting MoE Research**: A member is looking for **MoE** examples, having a good setup for dense models already.
- **Associative Memory ICLR Workshop Approaching**: The **ICLR 2026 workshop** on **Associative Memory** is coming up with a submission deadline of **February 14 2026**, featuring topics including algorithms, AI architectures, neuroscience, hardware design, and agentic workflows, as detailed on the [Call for papers](https://nfam2026.amemory.net/cfp/).
- **Weight Homology Identified**: A member highlighted the paper [Matrix-Driven Identification and Reconstruction of LLM Weight Homology](https://arxiv.org/abs/2508.06309) from the EleutherAI papers spreadsheet.
   - The member mentioned other relevant research, including [Independence Tests for Language Models](https://arxiv.org/abs/2502.12292) and [Blackbox Model Provenance via Palimpsestic Membership Inference](https://arxiv.org/abs/2510.19796).


  

---


### **Eleuther ▷ #[interpretability-general](https://discord.com/channels/729741769192767510/1052314805576400977/1471777648979083377)** (4 messages): 

> `Steering vectors, Data augmentation` 


- **Steering Vectors used for Data Augmentation**: A member shared their [Zenodo files](https://zenodo.org/records/8243818) related to replicating steering vectors, noting that **over 300 people** have seemingly tried to replicate their work.
   - They proposed training a model based on how well the downstream features respected the steering vector, possibly judging by intensity or linear combinations.
- **Data Augmentation via Steering Vectors**: The same member is experimenting with using steering vectors for **data augmentation** techniques in machine learning models.
   - The goal is to leverage steering vectors to guide the model's learning process by manipulating downstream features.


  

---


### **Eleuther ▷ #[multimodal-general](https://discord.com/channels/729741769192767510/795089627089862656/)** (1 messages): 

chameleon_45502: Up.. same question here
  

---


### **tinygrad (George Hotz) ▷ #[general](https://discord.com/channels/1068976834382925865/1068976834928193609/1471699551219089428)** (12 messages🔥): 

> `AI/ML Engineer Introductions, Discord ID Verification, GLM Flash Implementation` 


- **AI/ML Engineer Introduces Himself**: An experienced AI and ML engineer introduced themselves, specializing in building and deploying ML pipelines, deep learning models, and NLP systems, focusing on **reliability, performance, and production-ready ML architectures**.
   - He designs **prediction engines, recommendation systems, generative AI workflows**, and integrates AI models into web and mobile applications.
- **Hotz Endorses Discord ID Verification**: George Hotz expressed enthusiasm for the introduction of ID verification on Discord to prevent LLMs from joining.
   - He responded to the introductory message with a simple: *"yes and? i'm psyched for the id verification on discord so LLMs can't join"*.
- **GLM Flash Bounty Claimed**: A user inquired about getting **GLM flash** working and offered a bounty for upstreaming it, at any speed.
   - Another user claimed to have achieved **30 tok/s with pure tinygrad (custom_kernel)**, and **35 with MSL**, later submitting a [GLM flash PR](https://github.com/tinygrad/tinygrad/pull/14738).


  

---


### **DSPy ▷ #[show-and-tell](https://discord.com/channels/1161519468141355160/1202371242519441499/1471703568561078426)** (1 messages): 

> `Traces, Coding Agents` 


- **Traces Emerges: A Novel Way to Share Coding Agent Sessions**: A member introduced **Traces**, a new platform designed for sharing and discovering coding agent sessions, available at [traces.com](https://www.traces.com).
   - The platform supports exports from **Claude Code**, **Codex**, **OpenCode**, **Gemini**, and **Cursor**, aiming to facilitate learning through shared agent experiences.
- **Share and Learn from Coding Agent Sessions**: The creator of **Traces** is seeking feedback from the community on the platform.
   - The main question the creator gets asked is *why would anyone want to share their traces??*, but believes that this community would be the most curious to share and learn from others. 


  

---


### **DSPy ▷ #[papers](https://discord.com/channels/1161519468141355160/1203568372667645963/)** (1 messages): 

im_hibryd: Awesome!
It's like building an enciclopedia of DYI guides for the LLM to learn
  

---


### **DSPy ▷ #[general](https://discord.com/channels/1161519468141355160/1161519469319946286/1471600628655718492)** (8 messages🔥): 

> `Report Benchmarking with LLMs, DSPy Community Office Hours, Discord Events for DSPy, llamaparser` 


- **LLMs Benchmark Reports**: A member is seeking advice on [benchmarking a set of 50 reports](https://example.com) (mainly docx files) with an AI to identify *what a good report is* and provide feedback notes when new reports arrive using **DSPy** for a large context window.
   - Another member suggested using **llamaparser** for parsing the data and **markdown** to make it easier to pass it to **DSPy**.
- **DSPy Community Office Hours**: The DSPy community is hosting [Office Hours via Zoom on Thursday, Feb 19](https://x.com/isaacbmiller1/status/2022082357520740691) to answer burning questions on **DSPy** and **dspy.RLM**.
   - The team is polling the community for the best time, with options at **11:30 am ET**, **1:00 pm ET**, and **3:00 pm ET**.
- **Discord Event Added**: A member suggested creating a [Discord event](https://support.discord.com/hc/en-us/articles/4409494125719-Scheduled-Events#docs-internal-guid-c8c44ce9-7fff-f27a-bacf-6c776975e0f7) to allow users to see the time in their local time zone and mark their interest, so attendance can be gauged.
   - The event will be created as soon as voting for the office hours is complete and it will be recorded for those unable to attend.


  

---


### **aider (Paul Gauthier) ▷ #[general](https://discord.com/channels/1131200896827654144/1131200896827654149/1471930345824981042)** (1 messages): 

> `GPT-5 vs other models, aider use cases` 


- **GPT-5 still shines for scientific code**: A member noted that he still leans on **GPT-5** heavily for scientific coding.
   - He finds it much better than **GPT-5.2**, **Opus**, and **Gemini**.
- **Use case for scientific coding with Aider**: A member prefers GPT-5 for scientific coding over other models.
   - This suggests that **aider** may be a useful tool for scientific coding tasks, potentially leveraging the strengths of different models.


  

---


### **aider (Paul Gauthier) ▷ #[questions-and-tips](https://discord.com/channels/1131200896827654144/1133060505792159755/1471695254544580742)** (1 messages): 

> `Aider debugging commands, Crush debugging loops` 


- **Aider experiments with greedier debugging suggestions**: A member is experimenting with **Aider conventions** to make it more proactive in suggesting commands for debugging, such as *grepping file parts*, *probing help output*, and *testing commands*.
   - They aim to replicate the "Let me see the output of..." run/debug loops from **Crush** in a more controlled manner.
- **Debugging command loops**: The user is trying to replicate the "Let me see the output of..." run/debug loops from **Crush** in **Aider**.
   - They are looking for Aider to suggest more commands to run for debugging purposes, such as *grepping file parts*, *probing help output*, and *testing commands*.


  

---


### **Manus.im Discord ▷ #[general](https://discord.com/channels/1348819876348825620/1349440650495398020/1471914913440993425)** (2 messages): 

> `Agent Functionality details, Manus problems` 


- **Manus user asks about Agent Functionality Details**: A Manus user inquired about when details and best practices on the new agent functionality would be available.
   - The user wondered whether it is basically a safe openclaw.
- **Manus user reports two issues**: A user reported experiencing two issues with Manus and inquired about who to contact for support.
   - No other details or context was given.


  

---


### **Windsurf ▷ #[announcements](https://discord.com/channels/1027685395649015980/1027688115592237117/1471703520607469669)** (1 messages): 

> `GPT-5.3-Codex-Spark, Windsurf Arena Mode, Fast and Hybrid Arena Battle Groups` 


- **GPT-5.3-Codex-Spark rides the Windsurf!**: **GPT-5.3-Codex-Spark** (preview) is now live in **Windsurf Arena Mode**, exclusively available through the **Fast and Hybrid Arena Battle Groups**.
- **Windsurf Arena welcomes new model!**: A new model is available, check it out now!
   - Hurry, while the model is hot!


  

---


### **MCP Contributors (Official) ▷ #[mcp-dev-summit](https://discord.com/channels/1358869848138059966/1413517834805313556/1471853579244404900)** (1 messages): 

> `Livestream access for Attendees` 


- **Attendee Livestream Access: A Question Arises**: A member inquired whether registering as an **Attendee** provides access to the **livestream**.
- **Livestream Question**: A member asked about livestream access upon registering as an attendee.


  

---


---

