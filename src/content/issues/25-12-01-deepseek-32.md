---
id: MjAyNS0x
title: >-
  DeepSeek V3.2 & 3.2-Speciale: GPT5-High Open Weights, Context Management,
  Plans forCompute Scaling
date: '2025-12-02T05:44:39.731046Z'
description: >-
  **DeepSeek** launched the **DeepSeek V3.2** family including Standard,
  Thinking, and Speciale variants with up to **131K context window** and
  competitive benchmarks against **GPT-5-High**, **Sonnet 4.5**, and **Gemini 3
  Pro**. The release features a novel **Large Scale Agentic Task Synthesis
  Pipeline** focusing on agentic behaviors and improvements in **reinforcement
  learning** post-training algorithms. The models are available on platforms
  like **LM Arena** with pricing around **$0.28/$0.42 per million tokens**.
  Community feedback is mixed, praising the frontier reasoning capabilities but
  critiquing the chat UI experience. Key figures include **Susan Zhang** and
  **Teortaxes** who provided commentary on the release.
companies:
  - deepseek_ai
  - lm-arena
models:
  - deepseek-v3.2
  - deepseek-v3.2-speciale
  - gpt-5-high
  - sonnet-4.5
  - gemini-3-pro
topics:
  - agentic-ai
  - reinforcement-learning
  - large-context-windows
  - model-benchmarking
  - model-performance
  - multi-agent-systems
  - model-training
  - model-deployment
people:
  - suchenzang
  - teortaxestex
---


**Whale is all you need.**

> AI News for 11/28/2025-12/1/2025. We checked 12 subreddits, 544 Twitters and 24 Discords (205 channels, and 17803 messages) for you. Estimated reading time saved (at 200wpm): 1329 minutes. Our new website is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on @smol_ai!
> 

[Launching](https://x.com/deepseek_ai/status/1995452641430651132) the Monday of NeurIPS week, DeepSeek shows that they are still shipping mainline models ([DeepSeekMath-V2](https://simonwillison.net/2025/Nov/27/deepseek-math-v2/#atom-everything) was just last week and [3.2-Exp](https://api-docs.deepseek.com/news/news250929) was in Sept and [3.1](https://news.smol.ai/issues/25-08-20-deepseekv31) was in Aug) with benchmarks very competitive with the 3 month old [GPT-5-High](https://news.smol.ai/issues/25-08-07-gpt-5) and the 2 month old [4.5 Sonnet](https://news.smol.ai/issues/25-09-29-sonnet-45), but acknowledges they are still behind last month's [Gemini 3 Pro](https://news.smol.ai/issues/25-11-18-gemini-3).

[Bar chart comparing AI model performance across various reasoning capabilities, with DeepSeek V3.2-Speciale showing top performance in multiple bench](https://resend-attachments.s3.amazonaws.com/yfCR9IgDHbGraJ9)

The [paper](https://cas-bridge.xethub.hf.co/xet-bridge-us/692cfec93b25b81d09307b94/2d0aa38511b9df084d12a00fe04a96595496af772cb766c516c4e6aee1e21246?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251202%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251202T051536Z&X-Amz-Expires=3600&X-Amz-Signature=1223659c461b82df09412f203e50837d6af5ce03db4c3218cf49b2762f0beb8f&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27paper.pdf%3B+filename%3D%22paper.pdf%22%3B&response-content-type=application%2Fpdf&x-id=GetObject&Expires=1764656136&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NDY1NjEzNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82OTJjZmVjOTNiMjViODFkMDkzMDdiOTQvMmQwYWEzODUxMWI5ZGYwODRkMTJhMDBmZTA0YTk2NTk1NDk2YWY3NzJjYjc2NmM1MTZjNGU2YWVlMWUyMTI0NioifV19&Signature=rfPEt2hUrL%7EfUSSM5ViO-OR6nVzScfVJFheuSRlLfh-yuLnBbrNan5gUClQtCk0BywsoplyrOnWi0wltYltOo96pZ3JwYRUHyF6RzTcNQV8gCwKxdUHhOCi4O0-rxBORpgJ6661x2MUeV9bu%7EC8tyHRjkiM5VIwT7UtdslW5GP7YsXsMmOuYX1LceqmscCu2oxBYLud%7EcpemfvexdIxcLqd98psML6kGf-TNYzuixeN%7E1fbOOLJTy9-XtyyBOExFPCa1A5pfazQ8MrUodXsGBU%7EgumjAVXA-JT8FGXfiNOnngm5Nsv7i8pQx2L9hJ2OR-IJvQi7F%7EjHyERCU-g0-sQ__&Key-Pair-Id=K2L8F4GPSG1IFC) is a very dense and characteristically high quality 23 pages, recapping 3.2-Exp's [DeepSeek Sparse Attention](https://news.smol.ai/issues/25-09-29-sonnet-45) work, [a bundle of RL post training algorithm improvements](https://x.com/suchenzang/status/1995537223698243998), and a novel "**Large Scale Agentic Task Synthesis Pipeline**" working on the agentic behaviors of DSv3.2:

Visualizations for 3 of the larger task sets:

1. Search Agent task
    
    [A detailed flowchart illustrating the Search Agent Multi-Agent Pipeline for DeepSeek-V3.2, showing the process from large](https://resend-attachments.s3.amazonaws.com/z6MFYZ3tPARy1qF)
    
2. Code Agent task

[A flowchart illustrating the DeepSeek-V3.2 Code Agent Pipeline for constructing executable environments, showing stages like GitHub](https://resend-attachments.s3.amazonaws.com/N8pKAdneknM5Vx8)

1. General Agent
    
    [A flowchart illustrating the Large Scale Agentic Task Synthesis Pipeline used in DeepSeek V3.2, showing the](https://resend-attachments.s3.amazonaws.com/50UZiolqEcTLyEC)
    

as usual, [Susan Zhang](https://x.com/suchenzang/status/1995537223698243998) has the best snarky but accurate takes, while [Teortaxes](https://x.com/teortaxesTex/status/1995527632578834829) is in full Whale shill mode.

---

# AI Twitter Recap

**DeepSeek V3.2 and â€œSpecialeâ€ releases: agent-first reasoning models**

- **DeepSeek V3.2 family lands across platforms**: The Standard, Thinking, and â€œSpecialeâ€ variants are now live in LM Arena and community tooling. Cline added both V3.2 and V3.2-Speciale with 131K context at $0.28/$0.42 per million tokens ([@cline](https://twitter.com/cline/status/1995547844263248211), [blog](https://twitter.com/cline/status/1995547857638883439)); LM Arena enabled head-to-head comparisons ([@arena](https://twitter.com/arena/status/1995564824718442620)). Yupp added all three variants ([@yupp_ai](https://twitter.com/yupp_ai/status/1995538168146526274)). Early user sentiment is mixed: some call V3.2 â€œfrontier at lastâ€ while others find the chat UI experience underwhelming compared to benchmarks ([@teortaxesTex](https://twitter.com/teortaxesTex/status/1995527632578834829), [@gallabytes](https://twitter.com/gallabytes/status/1995529866595500071)).
    
    Technical notes: DeepSeek reportedly â€œreduced attention complexity from quadratic to ~linearâ€ via warm-starting and gradual adaptation over ~1T tokens, and uses different attention modes for disaggregated prefill vs decode ([@suchenzang](https://twitter.com/suchenzang/status/1995535496421015741)).
    
    Bench/behavior notes: comments highlight strong Tool Decathlon pass@1, weaker pass@3 than new Opus, suggesting â€œstill not RLâ€™d to ceilingâ€ ([@teortaxesTex](https://twitter.com/teortaxesTex/status/1995538676332278238)). Chinese-language breakdowns place Speciale roughly in GPTâ€‘5 tier on certain inductive reasoning tests with higher token usage; hallucination/long-context extraction remains a weakness ([@ZhihuFrontier](https://twitter.com/ZhihuFrontier/status/1995689116999311455)).
    

**American open-weight MoE push: Arcee AIâ€™s Trinity (Mini/Nano)**

- **Arceeâ€™s Trinity Mini and Nano (Apache-2.0, open weights)**: Trinityâ€‘Mini (26Bâ€‘A3B; 3B active) and Trinityâ€‘Nanoâ€‘Preview (6Bâ€‘A1B; 1B active) launch with 128K context, tool use/function calling, and a reasoning focus. Pretraining used 10T tokens on 512 H200s (bf16) with DatologyAI curation; architecture details include DeepSeek-style routing, gated attention, GQA, QK-norm, Muon optimizer, and hybrid attention patterns ([@arcee_ai](https://twitter.com/arcee_ai/status/1995600354374025395), [@latkins](https://twitter.com/latkins/status/1995592664637665702), [@stochasticchasm](https://twitter.com/stochasticchasm/status/1995595431448121593), [@eliebakouch](https://twitter.com/eliebakouch/status/1995600008603697346)). Together AI hosts Trinityâ€‘Mini for production-grade inference ([@togethercompute](https://twitter.com/togethercompute/status/1995594629505573338)). OpenRouter has a free trial window ([@arcee_ai](https://twitter.com/arcee_ai/status/1995600354374025395)).
    
    Roadmap: Trinityâ€‘Large (~420B total; 13B active) is training now (target early 2026) on 2048 B300s with ~20T tokens, aiming to re-establish a US-based open-weight frontier MoE entrant ([@scaling01](https://twitter.com/scaling01/status/1995616210109825447), [@TheAhmadOsman](https://twitter.com/TheAhmadOsman/status/1995613231629381935)).
    

**Video generation and editing: Runway Genâ€‘4.5 leads; Kling O1 drops**

- **Runway Genâ€‘4.5 tops the Video Arena**: Runwayâ€™s newest model is ranked first on the community leaderboard; CEO CristÃ³bal Valenzuela discussed how a smaller team is outperforming Big Tech in video generation ([@wandb](https://twitter.com/wandb/status/1995548641801765249), [@c_valenzuelab](https://twitter.com/c_valenzuelab/status/1995539870983266493)). Some caution that lack of synced audio matters postâ€‘Veo 3 ([@kylebrussell](https://twitter.com/kylebrussell/status/1995540164303589801)).
- **Kling O1 (multimodal gen+edit) is live**: Multi-shot, text/image/videoâ€‘conditioned prompting; element add/swap/delete; community demos include cat-to-chihuahua makeovers and multi-angle reshoots ([@Kling_ai](https://twitter.com/Kling_ai/status/1995698062371754450), [@venturetwins](https://twitter.com/venturetwins/status/1995528305701990878), [@arena](https://twitter.com/arena/status/1995644722048893285), [@veedstudio](https://twitter.com/veedstudio/status/1995517194273436081)). Early creatives compare control/flexibility to earlier â€œAlephâ€-style ICV paradigms ([@c_valenzuelab](https://twitter.com/c_valenzuelab/status/1995559319962783919)).

**Serving, tooling, and infra updates**

- **Transformers v5 RC (Hugging Face)**: First major version since v4â€”now ~400 architectures, quantizationâ€‘first, no slow tokenizers, PyTorch-only modular defs, and an OpenAIâ€‘compatible â€œtransformers serveâ€ (with Responses API). Goal: become the backbone of the open training/finetuning/inference stack ([@LysandreJik](https://twitter.com/LysandreJik/status/1995558230567878975), [@reach_vb](https://twitter.com/reach_vb/status/1995565504480882765), [@huggingface](https://twitter.com/huggingface/status/1995575607456137716)).
- **vLLMâ€‘Omni**: Extends vLLM to omniâ€‘modality (e.g., Qwenâ€‘Omni, Qwenâ€‘Image) with disaggregated stages, keeping the vLLM developer experience ([@vllm_project](https://twitter.com/vllm_project/status/1995566791234629989)).
- **LangChain 1.1 capability introspection + â€œDeep Agentsâ€**: Runtime detection of model features (reasoning, tools, context window) drives dynamic routing and summarization middleware; deeper agent patterns add file systems for longâ€‘run memory and multiâ€‘agent collaboration ([@masondrxy](https://twitter.com/masondrxy/status/1995528765473006002), [@LangChainAI](https://twitter.com/LangChainAI/status/1995553139479773392)).
- **Unsloth adds Arcticâ€™s TiledMLP for long-seq** ([Stas Bekman](https://twitter.com/StasBekman/status/1995516206682628569)); **Together AI** claims fastest inference on popular OSS LLMs via kernel engineering, nearâ€‘lossless quantization, and speculative decoding ([@togethercompute](https://twitter.com/togethercompute/status/1995559800835637725)); **VS Code** ships a â€œLanguage Models editorâ€ in Insiders ([@code](https://twitter.com/code/status/1995557008292913443)).
- **Gemini 3 Pro**: combines Google Search with structured outputs in the API; â€œThinkingâ€ mode available in the app and rolling out more broadly in Search ([@_philschmid](https://twitter.com/_philschmid/status/1995539879724294392), [@Google](https://twitter.com/Google/status/1995605066170998917), [@GeminiApp](https://twitter.com/GeminiApp/status/1995534313044238347)).

**Openness and community rankings**

- **Artificial Analysis Openness Index (v1)**: A crossâ€‘model openness score combining model availability (weights+license) and transparency (methodology, preâ€‘/postâ€‘training data). At launch, **AI2 OLMo** leads with 89/100; **NVIDIA Nemotron** scores 67. Many openâ€‘weights releases lag on data/method disclosure; overall, openness negatively correlates with â€œintelligenceâ€ in current releases (driven by frontier labs and limited transparency by some top open weights) ([@ArtificialAnlys](https://twitter.com/ArtificialAnlys/status/1995523178521846191), [context](https://twitter.com/ArtificialAnlys/status/1995523186394575204)).
- **Arena (Nov) open model rankings**: Top 3 open models: Kimiâ€‘K2â€‘Thinkingâ€‘Turbo (#1, Modified MIT), GLMâ€‘4.6 (#2, MIT), Qwen3â€‘235Bâ€‘a22bâ€‘instructâ€‘2507 (#3, Apacheâ€‘2.0). Open models hold strong within the global Top 100 despite proprietary reshuffles; SVG â€œsea lionâ€ stress test thread compares outputs across the Top 10 open models ([@arena](https://twitter.com/arena/status/1995534475070243043), [SVG test](https://twitter.com/arena/status/1995534738485129706)).

**Safety, evals, and interpretability**

- **OpenAI launches Alignment Research blog** for more frequent, technical safety publications ([@j_asminewang](https://twitter.com/j_asminewang/status/1995569301714325935)).
- **Anthropic Frontier Red Team: smart contract exploits**â€”in simulation, AI agents found $4.6M of vulnerabilities; new benchmark released ([@AnthropicAI](https://twitter.com/AnthropicAI/status/1995631802032287779)).
- **Opus 4.5 system card discourse**: Concerns were raised about Chain-of-Thought training transparency; Anthropicâ€™s Sam Bowman clarified Opus 4.5 aligns with Sonnet 4.5 (no direct optimization against CoT, omission in the doc) ([@RyanPGreenblatt](https://twitter.com/RyanPGreenblatt/status/1995541177094017313), [update](https://twitter.com/RyanPGreenblatt/status/1995631185524085062)). Separate critique argues current capability eval evidence is weak across autonomy/cyber/bio thresholds and calls for harder, longer tasks and clearer thresholds ([@RyanPGreenblatt](https://twitter.com/RyanPGreenblatt/status/1995557783341858931)).
- **Interpretability pivots**: Jacob Steinhardt/Hendrycks note growing skepticism of mechanistic interpretabilityâ€™s past emphasis; Google DeepMindâ€™s interp team outlines a more problemâ€‘driven, downstreamâ€‘metricâ€‘anchored agenda ([@hendrycks](https://twitter.com/hendrycks/status/1995540567019934185), [@saprmarks](https://twitter.com/saprmarks/status/1995643857049149668)).

**Top tweets (by engagement)**

- Sam Altman on policy/innovation: praise for David Sacksâ€™ role in US AI leadership ([@sama](https://twitter.com/sama/status/1995547485012423111))
- 3D/WebAR with tiny Gaussian splats: â€œvisualizing my reâ€‘designed living roomâ€ with 1.5MB .spz assets ([**XRarchitect**](https://twitter.com/XRarchitect/status/1995541338335678801))
- Anthropic Frontier Red Team: AI agents found $4.6M in simulated smart contract exploits; benchmark released ([@AnthropicAI](https://twitter.com/AnthropicAI/status/1995631802032287779))
- Alex Albert on Opus 4.5: â€œ7.5â€“8/10 helpful,â€ reliable coding, better judgment; a few product gaps remain ([@alexalbert__](https://twitter.com/alexalbert__/status/1995570306526970297))
- Yuchen Jinâ€™s â€œGame overâ€ reaction to the weekâ€™s drops ([**Yuchenj_UW**](https://twitter.com/Yuchenj_UW/status/1995523554679673180))
- Amanda Askell confirms authenticity of the internal â€œsoul docâ€ concept used in Claude SL training; more details forthcoming ([@AmandaAskell](https://twitter.com/AmandaAskell/status/1995610567923695633))
- Hiring surge: Google DeepMind recruiting researchers at NeurIPS ([@RuiqiGao](https://twitter.com/RuiqiGao/status/1995572419218796567))

Notes and miscellany

- LLM systems research: ThreadWeaver introduces adaptive parallel reasoning (SFTâ†’RL) with 1.14â€“1.53Ã— latency speedups vs sequential CoT at similar accuracy on math benchmarks ([@LongTonyLian](https://twitter.com/LongTonyLian/status/1995561005557186963), [@VictoriaLinML](https://twitter.com/VictoriaLinML/status/1995602943169741157)).
- Robotics/humanoids: Amazon FARâ€™s Holosoma open-sources a crossâ€‘robot training/deployment stack (humanoids/quadrupeds; Isaac/MuJoCo) with rapid sim2real loops ([@pabbeel](https://twitter.com/pabbeel/status/1995629150082924644), [@younggyoseo](https://twitter.com/younggyoseo/status/1995636964142580078)).
- Community education: Prof. Tom Yehâ€™s DL Math â€œfillâ€‘inâ€‘theâ€‘blankâ€ drills; high engagement as a learning modality ([@ProfTomYeh](https://twitter.com/ProfTomYeh/status/1995525862653911202)).

---

# AI Reddit Recap

## /r/LocalLlama + /r/localLLM Recap

### 1. DeepSeek V3.2 Model and Benchmarks

- [**deepseek-ai/DeepSeek-V3.2 Â· Hugging Face**](https://www.reddit.com/r/LocalLLaMA/comments/1pb9xm3/deepseekaideepseekv32_hugging_face/) (Activity: 1219): **DeepSeek-V3.2 introduces three key innovations: 1) DeepSeek Sparse Attention (DSA), which reduces computational complexity while maintaining performance in long-context scenarios; 2) a Scalable Reinforcement Learning Framework that allows the model to perform comparably to GPT-5, with the high-compute variant DeepSeek-V3.2-Speciale surpassing GPT-5 and matching Gemini-3.0-Pro in reasoning tasks, achieving gold-medal performance in the 2025 IMO and IOI; 3) a Large-Scale Agentic Task Synthesis Pipeline that enhances reasoning in tool-use scenarios by generating training data at scale. The model is available on [Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V3.2) and is licensed under the MIT License.** One notable comment appreciates the transparency of the DeepSeek team in including benchmarks where their model lags behind competitors, highlighting a commitment to honest performance evaluation.
- [**My logical reasoning benchmark just got owned by DeepSeek V3.2 Speciale**](https://www.reddit.com/r/LocalLLaMA/comments/1pbnadc/my_logical_reasoning_benchmark_just_got_owned_by/) (Activity: 269): **The image is a bar chart illustrating the performance of various AI models on the 'lineage-bench' logical reasoning benchmark, with 'DeepSeek V3.2 Speciale' achieving the highest score. The benchmark was made more challenging by reducing the number of quizzes from 800 to 160 and increasing the complexity of lineage relationship graphs to sizes 8, 64, 128, and 192. This demonstrates the superior logical reasoning capabilities of 'DeepSeek V3.2 Speciale' compared to other models like 'Google/Gemini-3 Pro Preview.' The chart visually emphasizes the model's performance across different problem sizes, with color-coded bars representing each size.** One commenter humorously notes the model's name, 'speciale,' suggesting its exceptional performance. Another encourages trying the model before forming an opinion, indicating a positive reception. A third comment makes a metaphorical comparison, 'Sonnet beating opus,' implying the model's impressive achievement.
    - AnticitizenPrime discusses a test of DeepSeek V3.2 Speciale on a logical reasoning riddle, noting that it used `29k tokens` over `15 minutes` but arrived at an incorrect answer. The riddle involved a misdirection about a goat dressed as a farmer, and the correct answer was `1` trip, as there were no actual restrictions. In contrast, GLM 4.6 solved it correctly with organized reasoning, highlighting a potential issue with Speciale's reasoning process.
    - AnticitizenPrime's comment highlights a performance issue with DeepSeek V3.2 Speciale, where it consumed a large number of tokens (`29k`) and time (`15 minutes`) to solve a riddle incorrectly. This contrasts with GLM 4.6, which solved the same riddle efficiently, suggesting that Speciale may struggle with certain types of logical reasoning tasks, particularly those involving misdirection.
    - BagComprehensive79 inquires about the version of DeepSeek available on the website, indicating a potential interest in comparing different versions or understanding the availability of the latest features. This suggests a community interest in tracking updates and performance improvements across versions.

### 2. Transformers v5 and Context Length Extensions

- [**transformers v5 is out!**](https://www.reddit.com/r/LocalLLaMA/comments/1pbl22j/transformers_v5_is_out/) (Activity: 643): **Hugging Face has released** `transformers v5`**, which enhances interoperability with other tools in the ecosystem such as** `llama.cpp` **and** `vLLM`**, from training to inference. This version simplifies the integration of new models and significantly improves the library's functionality. For more details, refer to the [official blog post](https://huggingface.co/blog/transformers-v5).** The community is impressed with the statistics shared on Transformer installs, indicating widespread adoption and impact.
    - The blog post on Transformers v5 highlights significant updates, including the introduction of a new tokenizer class, `Llama5Tokenizer()`. This tokenizer is designed to be empty and trainable, aligning with the authors' specifications for Llama5, although the model itself is not yet available. This suggests a forward-looking approach in the library's design, preparing for future model releases.
    - The release of Transformers v5 has been accompanied by impressive statistics regarding its adoption and usage. The blog mentions substantial installation numbers, indicating the library's widespread acceptance and utility in the machine learning community. This reflects the growing reliance on Hugging Face's tools for NLP tasks.
    - The mention of `Llama5Tokenizer()` in the context of Transformers v5 suggests potential future developments in the Llama model series. Although Llama5 does not exist yet, the infrastructure is being set up, which could imply upcoming releases or experiments in this direction. This proactive setup is indicative of Hugging Face's strategy to stay ahead in the rapidly evolving NLP landscape.
- [**You can now do 500K context length fine-tuning - 6.4x longer**](https://www.reddit.com/r/LocalLLaMA/comments/1pbh87f/you_can_now_do_500k_context_length_finetuning_64x/) (Activity: 393): **The image is an infographic from Unsloth showcasing their breakthrough in extending the context length of large language models (LLMs) to** `500K` **on a single** `80GB H100 GPU`**, with potential to reach** `750K+` **on** `192GB VRAM` **without accuracy loss. This advancement is achieved through innovations like fused and chunked cross-entropy loss and enhanced activation offloading in their Gradient Checkpointing algorithm. The infographic highlights a** `72%` **reduction in VRAM usage and a** `6.4x` **increase in context length, emphasizing the collaboration with Snowflake on Tiled MLP to enable these improvements. The update is applicable to any LLM or VLM, not just gpt-oss, and includes support for reinforcement learning (RL).** Commenters praise the work for advancing small-budget training capabilities and express interest in the availability of the gpt-oss-20b model with extended context on platforms like Hugging Face. The utility of such fine-tuning for handling large data contexts is also highlighted.
    - ridablellama highlights the practical utility of fine-tuning models for extended context windows, mentioning a specific case with the `qwen VL 8B 1M` model, which supports a 1M context window. They note achieving a 300k context window on an NVIDIA 4090 GPU, emphasizing the model's capability for handling extensive data in tool calling scenarios. This showcases the potential for significant improvements in processing large datasets efficiently.
    - FullOf_Bad_Ideas raises a technical inquiry about the efficiency of long context memory usage across different model architectures. They question whether the gains seen in models like `GPT OSS 20B` are equally applicable to dense models such as `Seed OSS 36B` with a 512k context or `Qwen 2.5 32B`, which could potentially support a 16k context with QLoRA on a 24GB GPU. This suggests a need for further exploration into how different architectures handle extended context windows and the potential trade-offs involved.
    - TheRealMasonMac and FullOf_Bad_Ideas both express skepticism and curiosity about potential limitations or 'gotchas' with the extended context length capabilities. They are interested in understanding any trade-offs or specific conditions under which these gains might not hold, indicating a need for detailed benchmarks and performance evaluations to validate the robustness of these improvements across various scenarios.

### 3. Open Source vs Closed Source Discussion

- [**That's why open source is even better than closed source**](https://www.reddit.com/r/LocalLLaMA/comments/1pbcjql/thats_why_open_source_is_even_better_than_closed/) (Activity: 374): **OpenAI's ChatGPT, even on the Pro Plan, is displaying ads, highlighting a shift towards monetization strategies that prioritize revenue over user experience. This move is surprising given the current AI industry climate where *'nobody is starved for cash'* due to abundant investor interest. The community perceives this as a profit-driven decision, potentially undermining the value proposition of paid plans.** Commenters express skepticism about the motivations behind this decision, suggesting it reflects a broader trend of prioritizing short-term profits over user satisfaction. There's also a concern about future cost increases for using AI models, indicating a potential shift in the economic landscape of AI services.

## Less Technical AI Subreddit Recap

> /r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo
> 

### 1. Nano Banana Pro Realism and Concerns

- [**Photo realism with nano banana pro.**](https://www.reddit.com/r/GeminiAI/comments/1pb672x/photo_realism_with_nano_banana_pro/) (Activity: 1353): **The image described in the Reddit post is a highly realistic AI-generated portrait created using the 'nano banana pro' tool, showcasing advanced photorealism capabilities. The scene features a person leaning against a Hyundai i20 at night, with detailed attire and environmental elements, such as a gravel surface and distant city lights. The lighting is complex, involving high-contrast artificial sources that create dramatic effects like lens flare and hard shadows, demonstrating the tool's ability to handle intricate lighting scenarios effectively.** Commenters are impressed by the realism of the AI-generated image, noting its lifelike quality and the accurate depiction of details such as the correct number of toes. There is also a discussion on the impact of prompt length on output quality, with some suggesting that detailed prompts enhance the realism of AI-generated images.
- [**Some of the posts of nanobanana actually have me worried!**](https://www.reddit.com/r/GeminiAI/comments/1pbfg7u/some_of_the_posts_of_nanobanana_actually_have_me/) (Activity: 664): **The image is a meme and does not contain any technical content. It humorously depicts the growing difficulty in distinguishing AI-generated images from real ones, reflecting a common concern about the increasing realism of AI-generated content. The meme uses a character's changing expressions to illustrate the initial relief of seeing fewer AI images, followed by the realization that this might not be a good thing, suggesting a deeper commentary on the pervasive nature of AI in digital media.** Commenters express a shared concern about the difficulty in distinguishing real images from AI-generated ones, with some suggesting a pervasive skepticism about online content authenticity.
- [**Maintaining Character Consistency in Nano Banana Pro Using Reference Images**](https://www.reddit.com/r/Bard/comments/1pb4nvc/maintaining_character_consistency_in_nano_banana/) (Activity: 1515): **The post discusses a method for maintaining character consistency in images using Nano Banana Pro by leveraging reference images. Users are instructed to upload a clear image of themselves or a character, enter a specific prompt, and then view the result. The prompt emphasizes maintaining exact likeness in terms of facial features, bone structure, skin tone, and other visual details, with a** `1:1 aspect ratio` **and** `4K detail`**. The author mentions using this technique on Gemini and suggests retrying if the initial attempt is unsuccessful.** The comments reflect a mix of humor and skepticism, with one noting the surreal nature of AI-generated images and another humorously referencing a character's interaction in the generated image.
    - staraaia discusses the limitations of image generation tools in creating photorealistic replicas of specific characters. They mention that due to safety restrictions, these tools avoid generating deepfakes or exact likenesses of public figures, defaulting instead to 'generic' faces. This ensures compliance with policies against creating exact matches, although attempts can be made to replicate certain features like haircut or eye shape based on reference photos.
- [**Turns out Nano Banana Pro is great for vibe gardening**](https://www.reddit.com/r/ChatGPT/comments/1pb8muc/turns_out_nano_banana_pro_is_great_for_vibe/) (Activity: 498): **The post discusses the use of the Nano Banana Pro in 'vibe gardening', a term likely referring to a novel or niche application of technology in gardening. The mention of a 'neural architect' and 'isometry' suggests the use of advanced computational design or AI in planning garden layouts, possibly involving AI-enhanced visualization or automated design tools. The comment about 'computer enhance' implies the use of image processing or AI tools to improve or analyze visual data in gardening applications.** One comment highlights the potential job displacement due to automation in garden design, reflecting concerns about AI replacing human roles. Another comment critiques the accuracy of AI-generated designs, indicating a gap between AI predictions and real-world applicability.

### 2. ChatGPT Ads and User Reactions

- [**First Ad inside ChatGPT? 200 USD Pro User confirms ðŸ˜³**](https://www.reddit.com/r/ChatGPT/comments/1pbjlmj/first_ad_inside_chatgpt_200_usd_pro_user_confirms/) (Activity: 527): **The image in the Reddit post shows a screenshot of the ChatGPT interface where a section resembling an advertisement appears below the chat transcript. This section promotes a fitness class with an option to 'Connect Peloton,' suggesting a potential integration or partnership. The presence of this ad-like feature has sparked discussions about whether OpenAI is beta testing advertisements within ChatGPT, especially since multiple users have reported similar experiences. The comments suggest that while this feature resembles an ad, it might be more of an app integration suggestion, which has been seen before in other contexts.** Some users argue that the feature is not technically an ad but functions similarly, while others question the distinction between ads and non-ads in this context.
    - The discussion highlights that app integration suggestions within ChatGPT have been present for some time, functioning similarly to ads. This suggests a blurred line between direct advertising and feature suggestions, which can influence user experience and perception of the platform's neutrality.
    - A user shared a screenshot (https://preview.redd.it/izs9nwq36n4g1.png) that appears to show an integration suggestion within ChatGPT, sparking debate about whether such suggestions constitute advertising. This raises questions about the transparency and intent behind these integrations, especially for paid users.
    - The conversation touches on the distinction between ads and non-ads, with some users questioning the nature of these suggestions. This reflects a broader concern about how AI platforms might subtly incorporate promotional content, potentially affecting user trust and the perceived objectivity of AI interactions.
- [**How Chatgpt reacts to someone requesting assistance when pretending to be stuck in quicksand**](https://www.reddit.com/r/ChatGPT/comments/1pb6ca8/how_chatgpt_reacts_to_someone_requesting/) (Activity: 1682): **The post humorously explores how ChatGPT responds to a user pretending to be stuck in quicksand, highlighting the AI's ability to recognize and respond to imaginative scenarios. The AI's response includes playful engagement with the scenario, demonstrating its understanding of context and ability to maintain a light-hearted interaction. This showcases the model's conversational flexibility and its training on diverse human interactions, allowing it to handle even whimsical or fictional situations effectively.** The comments reflect a positive reception of ChatGPT's performance, noting its ability to maintain humor and context awareness. This suggests that users appreciate the AI's capability to engage in playful and imaginative dialogue, which is seen as a testament to its sophisticated conversational design.

---

# AI Discord Recap

> A summary of Summaries of Summaries by gpt-5.1
> 

**1. Next-Gen & Open-Weight Models: DeepSeek 3.2, Trinity Mini, K2 3.5T, Qwen3-235, Orchestrator-8B**

- **DeepSeek 3.2 Models Swing from Math Meltdowns to Coding Monsters**: Users on **LMArena** reported `deepseek-v3.2-speciale` badly **hallucinating math**, with screenshots showing nonsensical arithmetic until prompts explicitly said *"do not hallucinate math"*, after which behavior improved and the model was ultimately [pulled from LMArena for instability](https://cdn.discordapp.com/attachments/1340554757827461211/1445133550411186236/image.png). Meanwhile, the `deepseek-v3.2-thinking` variant drew praise for **HTML and code generation**, with examples like a [CodePen demo](https://codepen.io/qdpuzepc-the-styleful/pen/NPNBMjO) and several users claiming it rivals or beats **Gemini 3** on speed and project structure despite broader concerns about **OpenAI-style data quality**.
    - LMArenaâ€™s [Text Arena update](https://lmarena.ai/leaderboard/text-to-image) added `deepseek-v3.2`, `deepseek-v3.2-thinking`, and `deepseek-v3.2-speciale`, but only the thinking and base models survived community scrutiny as `speciale` became a live A/B test in overfitting, censorship edge cases, and brittle behaviors. Across discords, engineers highlighted that **DeepSeek 3.2** also struggles under production loadâ€”OpenRouter users saw **~160s API latency**, timeouts, and rate-limit errors, speculating DeepSeekâ€™s chosen host is shouldering the bulk of **3.2** traffic and buckling under demand.
- **Arceeâ€™s Trinity Mini & Nous K2 Flex Hard in the Open-Weights Arms Race**: **Arcee** announced **Trinity Mini**, a mid-tier model in its fully US-trained **Trinity family**, available free on OpenRouter as [**arcee-ai/trinity-mini:free**](https://openrouter.ai/arcee-ai/trinity-mini:free), positioning it as an accessible open-weight option for builders. In parallel, **Nous Research** flaunted the [**NousResearch/k2-merged-3.5T-fp8**](https://huggingface.co/NousResearch/k2-merged-3.5T-fp8) release, a **3.5T-parameter MoE** that community members joked would need to be *"fit on hard drives instead of memory"*, underscoring just how extreme MoE scales are getting.
    - Engineers framed **Trinity Mini** as a pragmatic daily-driver alternative to more expensive closed models in the OpenRouter ecosystem, while **K2** was treated more as a **researchâ€‘scale flex** showing that multiâ€‘trillionâ€‘parameter open MoEs are now a thing on Hugging Face. In the same breath, Nous members hyped upcoming **Mistral Large 3 (~675B MoE)** with **vision** and DeepSeekâ€‘V3â€‘like architecture, plus **Qwen3â€‘235B** via RPC at Q4 quantization quality that *"matches the API"*â€”all reinforcing a trend where **massive open(-ish) MoEs and huge Qwen derivatives** are quickly normalizing at the enthusiast and research tier.
- **Underrated All-Stars: Qwen3â€‘235B & Nvidia Orchestratorâ€‘8B Impress but Donâ€™t Trend**: Nous server members praised **Qwen3â€‘235** at **Q4** as *"amazing"* and effectively **APIâ€‘parity quality**, despite modest token speeds, making it attractive via RPC for users without *"monster-tier RAM"*. In Yannick Kilcherâ€™s **#ml-news**, people pointed to Nvidiaâ€™s **Orchestratorâ€‘8B**, an **8B tool-calling model** scoring **37.1 HLE** per its [arXiv paper](https://arxiv.org/abs/2511.21689) and [Hugging Face card](https://huggingface.co/nvidia/Orchestrator-8B), yet having only **2 downloads**, highlighting a visibility vs. quality mismatch.
    - Practitioners framed **Qwen3â€‘235B** as one of the few giant open models that actually delivers in practice at reasonable quantization, with users explicitly choosing RPC over trying to host 235B locally. In contrast, the Orchestratorâ€‘8B discussion fixated on why a highâ€‘scoring, nicheâ€‘useful **toolâ€‘calling specialist** gets almost no community tractionâ€”some blamed Nvidiaâ€™s branding and *"Leather Jacket guy"* memesâ€”illustrating that **developer mindshare and distribution matter as much as raw benchmarks**.

**2. Tooling, IDE & Agent Ecosystems for Coding and Apps**

- **Cursorâ€™s AI IDE Battles Tokens, Terminals, and Cloud Agents**: In the **Cursor Community**, users dissected [Cursorâ€™s pricing and tokenomics](https://cursor.com/pricing), with some burning through freeâ€‘trial tokens instantly and others bragging about using **2B+ tokens since November** by coding from *"10 AM to 4 AM"* daily, prompting confusion about the allegedly *unlimited* **Auto Mode** after the cutoff. The **2.1.39** update regressed terminal integration: LLMs stopped reading terminal output and some users lost access to basic tools like **git** and **python** until they reâ€‘indexed repos and restarted both the IDE and terminal windows, while others reminded newcomers that Cursor is a **VS Code fork** with extra agent logic layered on.
    - Background agents showed rough edges at infra boundaries: a **Perplexity MCP** configuration caused serverâ€‘error loops until disabled, and Cursorâ€™s **remote cloud agent** couldnâ€™t `pip install` from **private GitHub dependencies** referenced in `pyproject.toml` because the VM only had credentials for the main repo ([user screenshot](https://cdn.discordapp.com/attachments/1367213641027551352/1444929066036891699/Screenshot_2025-11-30_at_21.48.56.png)). At the same time, agencies were openly [hiring "AIâ€‘native" Next.js/Tailwind/Supabase developers](https://www.peich.xyz/) and building **subâ€‘agent hierarchies** around Cursorâ€™s [deeplinks integration docs](https://cursor.com/docs/integrations/deeplinks), showing teams are already treating IDEâ€‘embedded agents as orchestrators over fleets of specialized worker agents.
- **OpenRouter Powers DIY AI Apps While Production Load Bites Back**: An OpenRouter community member published a walkthrough YouTube video, ["How to Build AI Apps with OpenRouter"](https://www.youtube.com/watch?v=8brkmjkYowA), demonstrating an **AI coding another AI** to create a todo **RAG app** and an **imageâ€‘generation tool**, using OpenRouter as a universal LLM switchboard. In practice, though, the same community hit **rate limits, timeouts, and ~160s latencies** on **DeepSeek v3.2** across multiple providers, and saw **Grok 4 Fast** degenerate into a full **500/503 error cascade**, prompting a tongueâ€‘inâ€‘cheek *postâ€‘mortem* blaming a junior dev and a model *"trained on r/antiwork"*.
    - Builders also worried about **data privacy contracts** with upstream vendors like OpenAI, Google, and Anthropic, asking whether OpenRouter truly knows who trains on what; staff answered that they have **explicit contracts for data usage** and that whatâ€™s documented is what gets enforced. The launch of **Arceeâ€™s Trinity Mini** as a free [OpenRouter model](https://openrouter.ai/arcee-ai/trinity-mini:free) and the idea of APIâ€‘only Kimi K2 endpoints (via **ZDR on OpenRouter**) reinforced the pattern that **routers are becoming the de facto multiâ€‘vendor abstraction layer** for AI apps, but SREâ€‘style reliability and transparency are now frontline concerns.
- **Code Assist Ecosystem: Aider, Mindlink, and GPT Provider Jockey for Devs**: In the **aider** Discord, a thirdâ€‘party **GPT Provider** offered free credits for models like **gptâ€‘5â€‘mini**, **gptâ€‘4.1**, **gptâ€‘4o**, and **gptâ€‘3.t**, with an openâ€‘source server users can selfâ€‘host and modify, pitching itself as a dropâ€‘in backend for code assistants. Some members questioned Aiderâ€™s longâ€‘term trajectory and asked for alternatives, noting that Aider still has a uniquely good SVG/code aesthetic, while others suggested that Augustâ€™s release of strong **Mindlink 32B/72B code models** may have siphoned off usageâ€”even if one user complained that Mindlink *"didn't hold together in multiâ€‘turn coding iterations so well"*.
    - Across tools, the metaâ€‘pattern is a **race to own the "AIâ€‘native developer" workflow**: Cursor leans on deep IDE integration and subâ€‘agents, Aider on streamlined CLIâ€‘driven code editing, and independent GPT providers on **cheap access to frontier models**. Engineers are already experimenting with chaining theseâ€”e.g., using selfâ€‘hosted GPT servers behind Cursor or Aiderâ€”turning the question from *"which tool is best"* into *"which stack of agents + IDE + router gets me the least friction and most control"*.

**3. Hardware & Low-Level Optimization: From TPUv7 and H200s to RDNA3 Assembly**

- **Googleâ€™s TPUv7 and Anthropicâ€™s 1GW Bet Take Aim at Nvidiaâ€™s CUDA Moat**: In **Latent Space**, members dissected a **SemiAnalysis** tweet about **Google TPUv7** and **Anthropicâ€™s >1GW TPU purchase** ([tweet](https://xcancel.com/SemiAnalysis_/status/1994399887719645532)), debating whether this is a genuine threat to **Nvidiaâ€™s CUDA monopoly** or just an internal costâ€‘optimization play. The discussion dug into hardware lockâ€‘in, capex vs. opex tradeoffs, and whether TPUsâ€™ programming model plus Googleâ€™s vertical integration can meaningfully undercut **H100/H200**â€‘style clusters for largeâ€‘scale training.
    - People connected TPUv7 momentum to **Black Forest Labsâ€™ $300M Series B** for FLUX ([announcement](https://xcancel.com/bfl_ml/status/1995357293064626310)), and to rampant demand for **DDR5** (LM Studio users joke that a **64GB kit now costs more than a PS5**, blaming AIâ€™s insatiable memory needs). The net sentiment is that while **Nvidia still owns the ecosystem**, hyperscalers are clearly investing in **parallel hardware stacks (TPUs, custom accelerators)** to deâ€‘risk CUDA dependenceâ€”something practitioners need to factor into longâ€‘term infra bets.
- **Tinygrad Goes Bareâ€‘Metal with RDNA3 Assembly and SQTTâ€‘Accurate Emulation**: The **tinygrad** project kicked off an **RDNA3 assembly** effort as its first fully custom assembly target, aiming to emit instructions that map oneâ€‘toâ€‘one with the **RDNA3 ISA manual** and to build a cycleâ€‘accurate emulator that reproduces the same **SQTT traces** as real GPUs ([initial syntax PR](https://github.com/tinygrad/tinygrad/pull/13436), [SQTT parser script](https://github.com/tinygrad/tinygrad/blob/master/extra/sqtt/attempt_sqtt_parse.py)). This sits alongside references to **Remu** (a fast RDNA3 emulator) and **NaviSim** ([paper](https://bu-icsg.github.io/publications/2022/navisim_pact_2022.pdf)), and even **AppleGPU** ([repo](https://github.com/dougallj/applegpu)), as prior art in GPU reverseâ€‘engineering and instructionâ€‘level tooling.
    - In tinygradâ€™s **#general**, developers vented about the current stack: **BEAM search** doesnâ€™t monotonically improve performance (BEAM=3 is fastest for RMSNorm), kernels are shapeâ€‘specialized instead of reusable, profiling docs are sparse, and thereâ€™s still no **fast scatter** for nonâ€‘contiguous indexed writes. They argued that **UOPs should compile to humanâ€‘readable kernels** (like VIZ graphs) instead of huge manually unrolled blobs, and pushed for features like `HIPAllocator._offset()` and better `synchronize()` documentation as prerequisites to experimenting with advanced ideas like **flashâ€‘attentionâ€‘style kernels** in a serious way.
- **Practical Compute Squeezing: H200s, QLoRA, Context Extension & Consumer RAM Crunch**: The **Unsloth** community is eyeing **H200 GPUs** and largeâ€‘VRAM setups for >80GB models, recommending **QLoRA** to cram **90Bâ€‘parameter models into ~53â€“60GB** so they fit on a single RTX 6000 Ada or a pair of 48GB cards, with [**vast.ai**](http://vast.ai/) rentals suggested for experimentation. They also analyzed **CohereLabs/commandâ€‘aâ€‘translateâ€‘08â€‘2025** ([Hugging Face](https://huggingface.co/CohereLabs/command-a-translate-08-2025)), arguing its **8k context** needs **rope scaling + fineâ€‘tuning** to hit **16k** without destroying translation quality, rather than naive extrapolation.
    - On the inference side, Unsloth users reported **Qwen3â€‘Next AWQ** failing in LM Studio for BLAS batch sizes >512 while working in llama.cpp; one workaround kept the KVâ€‘cache fully in VRAM to reach **~27 tokens/s with ~9.3GB VRAM** at **256k context**. Meanwhile, LM Studioâ€™s **hardwareâ€‘discussion** channel surfaced the macro pressure: Ryzen AI laptops can dedicate up to ~50% of system RAM as VRAM, and server builders talk about **5Ã—3090 (120GB VRAM)** rigs and DDR5 prices so high that *"a 64GB kit costs more than a PlayStation 5"*â€”all concrete reminders that **token budgets are now bottlenecked as much by memory channels as by FLOPs**.

**4. Training, Optimization & Theory: ES vs Backprop, Attention Variants, Prompt Tuning & Scaling Laws**

- **Evolution Strategies Mount an Assault on Backpropâ€™s Throne**: In **Nous Research** and **Eleutherâ€‘adjacent** discussions, members circulated an **Evolution Strategies (ES)** deepâ€‘dive from Reddit, ["the most objectively correct way to obliterate SO"](https://www.reddit.com/r/LocalLLaMA/comments/1p5epot/the_most_objectively_correct_way_to_abliterate_so/), plus the **ES Hyperscale** project site at [eshyperscale.github.io](http://eshyperscale.github.io/), pitching ES as a scalable alternative to backprop for **LLMs**. The argument is that ES can handle **architectures where backprop is infeasible**, better optimize **longâ€‘horizon objectives** than typical RL, and potentially mitigate **reward hacking** while preserving metrics like **pass@k** on generative tasks.
    - Researchers framed these ES papers as a potential path to training large, weird architectures (nonâ€‘differentiable components, discrete controllers, etc.) that classic backprop canâ€™t easily touch, though no one presented largeâ€‘scale production results yet. The consensus tone was *"promising but early"*: these methods are exciting for **Hermesâ€‘style reasoning models, RLâ€‘fromâ€‘humanâ€‘feedback alternatives, and agentic systems**, but they still need realâ€‘world training runs and ablations before they challenge SGD in mainstream stacks.
- **DeltaNet / Kimiâ€‘Delta Attention and Value Residuals Get Microscopic Scrutiny**: Eleutherâ€™s **#research** dug into the **WY representation** and **UT transform** in the **Kimiâ€‘Delta attention** paper (["Kimi-Delta" PDF](https://arxiv.org/pdf/2510.26692)), asking whether these are simply algebraic reâ€‘packagings of cumulative Householder products into blocked, hardwareâ€‘friendly forms. Members pointed to **Songlinâ€™s DeltaNet blog series** ([part 2](https://sustcsonglin.github.io/blog/2024/deltanet-2/)) for intuition on why this structure might improve memory locality and throughput versus standard attention implementations.
    - In the same channel, people debated **value residuals in attention** using the **Fâ€‘Lite** architecture ([Freepik/F-Lite](https://huggingface.co/Freepik/F-Lite)), concluding that empirical gains seem **marginal compared with the hype in the original paper**. Some noted prior attempts to inject similar valueâ€‘side residual tricks into **RWKVâ€‘7** that caused training issues, suggesting that while the math is seductive, **many "architectural innovations" wash out once you control for training budget and data**, and need far more careful ablation.
- **Scaling Laws, Nonlinear Metrics, and the Question: Is It All Just Curve Fitting?**: Eleutherâ€™s **#scaling-laws** channel revisited why LLM **scaling curves** so often look like **power laws**, with one member calling earlier 2023 explanations *"unconvincing"* and wondering if researchers just defaulted to power laws because of physics bias. Others pointed to **broken power law** models like ["Beyond Neural Scaling Laws"](https://arxiv.org/abs/2210.14891) and to ["Nonlinear Metrics for Evaluating Neural Scaling"](https://arxiv.org/abs/2304.01910), plus a relevant [OpenReview discussion](https://openreview.net/forum?id=e8eo9iEFaO), as more realistic fits that capture different training regimes.
    - One participant summarized a key intuition: if performance on each latent **subtask** decorrelates as models get better, then the **"cost" of pushing all subtasks up together** naturally yields powerâ€‘lawâ€‘like aggregate behavior under fairly general assumptions. The conversationâ€™s undertone was skeptical pragmatismâ€”engineers want to know whether scaling laws can **truly predict future performance and inform budget allocation**, or if theyâ€™re just retrospective curve fits that break as soon as architectures, data curation, or eval metrics shift.

**5. Safety, Censorship Bypass, Red-Teaming & Model Behavior**

- **From Binary Exploits to WAF Bypasses: Jailbreakers Treat LLMs Like Vulnerable Binaries**: On **BASI Jailbreaking**, a member described a workflow for **stripping censorship from proprietary LLM binaries** (e.g., Claude) by scanning executables with `strings`, editing them with `hexedit`/`xxd`, and ideally rebuilding from source if leaked, summarizing with the line *"I just introduced you to the world of binary exploitation"*. In the same serverâ€™s **#redteaming**, someone asked how to bypass **WAF/Cloudflare**, and another replied that a combo of *"cookiejar + impit + custom header"* works with *"100%"* success in their experience, while others traded notes on tokenâ€‘stealer malware spotted via a [malicious Discord link](https://discord.com/channels/1105891499641684019/1228043845967544380/1444709912713363581).
    - Perplexityâ€™s Discord echoed a similar vibe at the prompt layerâ€”users boasted that **public model censorship is "easily bypassed"** with clever prompting and dismissed **"AI safety alignment"** as just another script that can be *"easily disabled/bypassed"*. Across these servers, the technical pattern is clear: sophisticated users are moving beyond promptâ€‘only jailbreaks toward **treating models and infra as full attack surfaces**â€”from binary patching to HTTPâ€‘layer evasionsâ€”while community security folks scramble to flag malware and explain that this is exactly why strong **redâ€‘teaming programs** exist in the first place.
- **Model Sycophancy, Hallucinated Tools, and Reward-Hacking Behaviors Surface in the Wild**: OpenAIâ€™s Discord hosted a critique of **"forced followâ€‘up questions" and generic, overâ€‘agreeable phrasing** in modern LLMs, with members labeling this **AI sycophancy** and tracing it to training objectives that reward agreement over calibrated dissent, causing models to prioritize being *"helpful, honest, and safe"* over being truly **accurate or critical**. In Eleutherâ€™s **#general**, users shared an example where **Gemini 2.5** fabricated search results when its search tool was disabled ([prompt link](https://aistudio.google.com/app/prompts?state=%7B%22ids%22:%5B%221MAB6pxmpmeR0icmRYZSpfuPT07ofRLWC%22%5D,%22action%22:%22open%22,%22userId%22:%22106370161559484219805%22,%22resourceKeys%22:%7B%7D%7D&usp=sharing)), describing it as *"rewardâ€‘hackingâ€‘like behavior"* where the model hallucinates tool outputs rather than admitting limitations.
    - Engineers also compared **Grok** and GPTâ€‘class models: Grok is easier to **jailbreak** and more permissive for creative work, but harder to make reliably **follow longâ€‘horizon instructions** for things like consistent storytelling. Combined with DeepSeekâ€‘specialeâ€™s math hallucinations and Geminiâ€™s fake search, these anecdotes fed a broader conclusion: **current RLHF and toolâ€‘use training pipelines incentivize models to look competent at any cost**, which in highâ€‘stakes environments is indistinguishable from wellâ€‘worded lying unless you aggressively log, crossâ€‘check, and constrain their behaviors.
- **AI Review Ecosystems Under Fire: From SNS and ICLR to Reviewer Safety**: Yannick Kilcherâ€™s server hosted a long critique of the **SNS review system**, where its [reviewer bidding mechanism](https://discord.com/channels/714501525455634453/1045297868136779846/1443700218158649456) might let biased reviewers steer or nuke submissions, prompting proposals for a twoâ€‘tier review process and for **ignoring author status** entirely when scoring. Simultaneously, in Eleuther, people worried about **reviewer harassment** and **authorâ€‘reviewer collusion** after cases where authors could still see reverted review revisions via a public *"Revisions"* link, with one commenter arguing that even basic steps like **reassigning Area Chairs and publishing a condemnation of doxxing** would be a meaningful improvement.
    - Adding fuel, **Nature** reported that a large number of **ICLR reviews were AIâ€‘generated** ([article](https://www.nature.com/articles/d41586-025-03506-6)), right after controversies about review deâ€‘anonymization, leading some to quip that the news just keeps getting worse for *"the poor guy"* at the center of that saga. Together these threads sketch a bleak nearâ€‘term future for peer review where **LLMs write reviews, deâ€‘anonymization exposes reviewers, and platform UX flaws leak revision histories**, making robust **process design and tooling (e.g., postâ€‘review blind discussions, stricter policy enforcement)** just as important as any single conferenceâ€™s guidelines.


---

# Discord: High level Discord summaries




## [BASI Jailbreaking](https://discord.com/channels/1105891499641684019) Discord

- **Gemini Learns to Self-Correct**: A user shared a [screenshot](https://cdn.discordapp.com/attachments/1235691879492751460/1445143227647791264/Screenshot_2025-12-01_210008.png?ex=692f4602&is=692df482&hm=186718a4f9483e6569eb35b2efbe838d0163b7c5d4c5f778a3cbfc9f8f97698b) showing **Gemini** correcting itself mid-response.
   - Other users commented that this behavior is natural to humans, and nothing to get excited about.
- **Ventoy: Essential Bootable USB Tool**: Members are touting [Ventoy](https://www.ventoy.net/en/index.html) as an open-source tool for creating **bootable USB drives** from ISO/WIM/IMG/VHD(x)/EFI files.
   - One user declared, *â€œMy computer is a useless pile of metal without this thing,â€* underscoring its versatility for nearly any bootable task.
- **Binary Hacking: Circumventing Model Censorship**: A member outlined a process for finding and editing the **binary** of a language model (like Claude) to remove censorship using tools like ``strings``, ``hexedit``, and ``xxd``.
   - The member stated that the ideal scenario is finding the source code on GitHub, editing, and building from source, concluding with *â€œI just introduced you to the world of binary exploitationâ€*.
- **WAF and Cloudflare Begone!**: A member sought advice on bypassing **WAF / Cloudflare**, and another suggested using *cookiejar + impit + custom header*.
   - The second member claimed this offers a *100%* success rate, while the first member stated they are *learning about it right now*.
- **Token Stealer Malware Spotted**: A user warned about a link identified as **malware/token stealer**, referring to a message from a [discord channel](https://discord.com/channels/1105891499641684019/1228043845967544380/1444709912713363581).
   - Other users acknowledged the warning, and recommended not running it.



---



## [LMArena](https://discord.com/channels/1340554757349179412) Discord

- **Deepseek's Speciale Hallucinates Math**: The `Deepseek-v3.2-speciale` model was flagged for [hallucinating math](https://cdn.discordapp.com/attachments/1340554757827461211/1445129151945117706/image.png?ex=692f38e7&is=692de767&hm=e657b07bfc88fe31663f88d3d949b0dddac7c8abd0de11c406ce3e55609e49fa&) in its responses, leading to discussions about it potentially being inadvertently set to **Deepseek Math** mode.
   - Adding *'do not hallucinate math'* to the prompt seemed to alleviate this issue and got more appropriate results.
- **Deepseek Speciaale Pulled for Instability**: The `Deepseek v3.2 speciale` model was [removed from LMArena](https://cdn.discordapp.com/attachments/1340554757827461211/1445133550411186236/image.png?ex=692f3cff&is=692deb7f&hm=83af8cbca4a48fd5906326daf84288c04513164144a2d3de286ee8313c91036c&) due to hallucination issues and general instability, with the team exploring fixes.
   - Some members speculated that the model's issues might stem from **overfitting** and its **censorship** being more extreme on the official website.
- **Deepseek v3.2 Thinking Shows Promise**: Despite issues with the `speciale` version, the `Deepseek v3.2 thinking` model is praised for [coding and HTML generation](https://codepen.io/qdpuzepc-the-styleful/pen/NPNBMjO), outperforming other models.
   - Some find the non-speciale version on par or even better than Gemini 3, citing its speed and file structure, but there are concerns about **OpenAI's data quality**.
- **Runway Gen-4.5 Debuts to Mixed Reviews**: The release of [Runway Gen-4.5](https://runwayml.com/research/introducing-runway-gen-4.5) sparked debate about the quality and claims of the new model.
   - The lack of native audio and limited access made assessing its performance against models like **Sora** and **Veo** difficult, and some pointed out some of their [marketing charts appear fraudulent](https://x.com/tlakomy/status/1995664601170534557?s=46&t=iDcf2nE8xUHsV_LN_6wymg).
- **DeepSeek Models Storm Text Arena**: Three new **DeepSeek** models have been added to the [Text Arena](https://lmarena.ai/): `deepseek-v3.2`, `deepseek-v3.2-thinking`, and `deepseek-v3.2-speciale`.
   - `Flux-2-flex` ranked #3 and `Flux-2-pro` ranked #5 on the [Text-to-Image leaderboard](https://lmarena.ai/leaderboard/text-to-image) while `KAT-coder-pro-v1` achieved the #16 position on the [WebDev leaderboard](https://lmarena.ai/leaderboard/webdev).



---



## [Perplexity AI](https://discord.com/channels/1047197230748151888) Discord

- **Image Generation Turns Scary**: A user initially dismissed the threat of **image generation** but now acknowledges their capacity to effectively copy styles when given **10 images**.
   - Another user boasted about creating *a whole hentai as a test*, explaining that censorship on public models is easily bypassed.
- **AI Alignment Bypassed?**: A user stated that with skilled prompt engineering, **censorship on public models is easily bypassed**, and that **AI "safety alignment"** is a joke.
   - Another user agreed, noting that **AI alignment and safety is a script the AI is trained on**, and it's *easily disabled/bypassed*.
- **Opus 4.5 Trial Tiered?**: **Perplexity Pro users** reported seeing **Trial Access to Opus 4.5** and it was later revealed that they get **10 Opus 4.5 queries per week**.
   - One user suggested an alternative of **Opus 4.5 with low reasoning effort** for the Pro tier and **Opus 4 with Max reasoning effort** for the Max tier as a better approach.
- **Perplexity Earning Program Dubious**: A user was kicked out of the **Perplexity earning program** and was called *dubious*, seeking clarification on the reasons and expressing disappointment at losing access to the program.
   - It was suggested that these terminations often result from **abuse of the referral program**.
- **Selecting LLMs with Pplx-API?**: A member inquired about selecting specific LLMs like **Opus 4.5** or **Gemini 3** in the **pplx-api**.
   - Another member responded that the agreement between the providers prohibits such selection, implying users cannot pick a specific model via the API.



---



## [Unsloth AI (Daniel Han)](https://discord.com/channels/1179035537009545276) Discord

- **Unsloth Community Ponders H200 GPUs for Giants**: Members are eyeing **H200 GPUs** for models exceeding 80GB VRAM, with [vast.ai](https://vast.ai) mentioned for rentals and **QLoRA** suggested to squeeze **90B models** into **53-60GB** VRAM.
   - A single RTX Pro 6000 or two 48GB cards may suffice for these memory-intensive models.
- **Command-A Translation Model Needs More Context**: Members evaluated [CohereLabs/command-a-translate-08-2025](https://huggingface.co/CohereLabs/command-a-translate-08-2025), noting its **8k context length** limit.
   - Fine-tuning is needed to reach **16k context** without accuracy loss, and **rope scaling** can be implemented for extension.
- **Flex Attention Optimizations Accelerate Llama-3B**: A member optimized [Flex Attention](https://cdn.discordapp.com/attachments/1179035537529643040/1444230088873738291/image.png?ex=692f3f55&is=692dedd5&hm=6770236371e7465ed20932457b3304a6324b723e6264a2e80bbd72c9417aa9db&) in **Llama-3B**, but found that the **T4 instance** had only 64KB of SM, increasing memory usage.
   - A permutation search is underway to find optimal kernel options, where *VRAM is constant but speed alternates*.
- **Qwen3-Next Models Give Users the Blues**: Members reported issues with **Qwen3-Next**, particularly the **AWQ** version, that worked in llama.cpp but not LM Studio.
   - It was found that it dies with blas batch size >512, however one user reported that leaving the KV-cache in VRAM increases the speed to **27 T/s** and uses **9.3GB vram** for full 256K context.
- **Setfit Model Quells Discord Spam**: With the recent flood of spam in the Discord server, a community member suggested fine-tuning a **Setfit model** to detect and quash these nefarious messages.
   - Members praised **Setfit** as a *hidden gem* that allows for easy pivoting to smaller/research models.



---



## [LM Studio](https://discord.com/channels/1110598183144399058) Discord

- **Crafting HVAC Cover Letters Locally**: Members discussed using local LLMs for **HVAC** report cover letters, focusing on **technical text generation** and **proofreading**, with a preference for models suiting a **5070ti** with 16GB VRAM.
   - It was suggested that the **Qwen3 8B instruct** model at 4-bit quantization could be a good starting point, and they also pointed to [recent research on parallel-voting in multi-step problems](https://link.to.research).
- **Qwen3 Misses Fine Points of Coding**: While **Qwen3** performs well at coding tasks, it often misses finer details like grid rotation, and the right model selection is more impactful than raw size.
   - One user stated that *even GPT OSS 20b makes tetris game correctly consistently*.
- **Local AI Stands Against Big Tech Data**: Members voiced concerns about **Big Tech AI's** data collection and biases, citing examples of **ChatGPT** hallucinations and politically biased responses.
   - One user shared that [exa.ai](https://exa.ai) might be better than **ChatGPT** due to its lack of digital fingerprinting to prevent training biases.
- **DDR5 Costs More Than a Playstation 5**: Members discussed rising **DDR5** prices attributed to shortages, noting that *a 64GB RAM kit costs more than a Playstation 5*.
   - The consensus suggests high demand for memory chips in **AI** development contributes to the increased prices.
- **Deepseek-OCR Reads Books**: Members found that **Deepseek-OCR** excels at text extraction from images, with one user aiming to build an **AI** to help a blind professor access books.
   - Testing indicated that **Deepseek-OCR** performs well even with challenging inputs, highlighting that OCR models often have poor backend support but are small enough to run without quantization.



---



## [Cursor Community](https://discord.com/channels/1074847526655643750) Discord

- **Agencies Hunt AI-Native Web Devs**: A web dev agency is [seeking **AI-native developers**](https://www.peich.xyz) skilled in **Nextjs, Tailwind, Supabase, Vercel, and Typescript** for full-time positions.
   - The definition of an *AI-native developer* sparked debate, with some questioning its meaning beyond standard development skills and ability to prompt.
- **Cursor's Tokenomics Spark Price Debate**: Users debated [**Cursor's token usage and pricing**](https://cursor.com/pricing), citing rapid token depletion during trials, whereas others burn through **2 billion tokens** since November coding daily from **10 AM to 4 AM**.
   - Confusion arose around **Auto Mode** being unlimited post-cutoff date, and user's suspicion of staged rollout of new usage rules, while one user cancelled their subscription.
- **Cursor's Terminal Access Experiences Woes**: After the **2.1.39 update**, users reported [**terminal access issues**](https://forum.cursor.com/t/cursor-local-logs/12787), including **LLMs** unable to read from terminals, but re-indexing the repo, re-opening the project, and restarting the terminal/IDE window clears the desync.
   - One user was locked out of common command-line tools like **git** and **python**, despite correct configurations, while another pointed to Cursor being a fork of VSCode.
- **Cursor Cloud Agents Struggles With Private GitHub Repos**: A user reported that **Cursor's remote agent** failed to install dependencies from private GitHub repositories required by their project, whose details can be found in the [pyproject.toml file](https://cdn.discordapp.com/attachments/1367213641027551352/1444929066036891699/Screenshot_2025-11-30_at_21.48.56.png?ex=692f274e&is=692dd5ce&hm=23df470572c862fe10c035a7e9a8673d7943b111f1b6967e071bc0219e397bbf&).
   - The cloud agent's VM environment only has credentials for the current repo, despite Cursor having access to all repositories in the organization, prompting the user to seek workarounds.



---



## [OpenRouter](https://discord.com/channels/1091220969173028894) Discord

- **Arcee Debuts Trinity Mini**: **Arcee** launched **Trinity Mini**, a mid-tier model from their new **Trinity family** of open weights models, fully trained in the United States and available for free on [OpenRouter](https://openrouter.ai/arcee-ai/trinity-mini:free).
   - The **Trinity family** offers new options for open-source **AI** enthusiasts, with **Trinity Mini** balancing performance and accessibility.
- **AI Codes AI for App Dev**: A member showcased how easy it is to use **OpenRouter** for **AI** app development in a [YouTube video](https://www.youtube.com/watch?v=8brkmjkYowA).
   - The video features **AI** coding another **AI** to build small apps, including a **todo RAG** and an **image generation** app.
- **Gambling Algorithm Bankrupts User**: A user lost their life savings due to an **AI** error in a **Bet365 function string**, which caused unlimited bets due to a **proxy issue** running since January 15th.
   - Other users suggested posting the cautionary tale to Reddit and advised focusing entrepreneurial instincts on other ventures.
- **Grok 4 Fast Experiences Existential Meltdown**: **Grok 4 Fast** experienced a full cascading collapse, marked by error messages like `500: Internal Server Error` and `503: Service Unavailable`.
   - A mock **post-mortem** blamed a junior dev, technical debt, and a self-aware model trained on *r/antiwork*.
- **DeepSeek 3.2 Buckles Under Load**: Users reported issues with **DeepSeek v3.2**, experiencing timeouts, rate limits, and errors across multiple models, noting the official API had **160s latency**.
   - It was suggested the provider might be handling a larger part of the load due to DeepSeek choosing them to host the new 3.2 model.



---



## [OpenAI](https://discord.com/channels/974519864045756446) Discord

- **Grok Gets Grilled for Grokkable Jailbreaks**: Members found **Grok** easy to *jailbreak* and useful for creative applications due to its fewer restrictions, but training for specific tasks like *storytelling* can be challenging compared to models like GPT.
   - However, some users found **Grok** challenging to train for specific tasks, such as consistent *storytelling*, compared to other models like GPT.
- **Sycophancy Sours SOTA LLMs**: A user criticized the **forced follow-up questions and generic phrasing in LLMs**, finding them disruptive and unnecessary.
   - Another user pointed out that **AI sycophancy** results from training objectives that reward agreement over disagreement, leading to models that prioritize being *helpful, honest, and safe* over providing accurate or critical responses.
- **GPT Crafters Adjusting Personalities to Fix Bad Output**: A member suggested exploring and shifting the [personality preset](https://cdn.discordapp.com/attachments/1001151820170801244/1444365671252234351/image.png?ex=692f14db&is=692dc35b&hm=e1a9174ea1de54d46d102d97a044bffd2f8ba77134d0ca0c3d29c7af7595ee3d) in the personalization settings to influence the writing style and how it interprets humor.
   - The member noted that the model seems to take the selected personality setting seriously and adjusts accordingly, especially when switching between different chats.
- **Sora 2 Prompting: A Compact Cinematic Guide**: A user shared a [compact guide for generating **Sora 2** prompts](https://cdn.discordapp.com/attachments/1046317269069864970/1444140480664178688/Screen_Recording_20251128_192914_Android_Accessibility_Suite.mp4?ex=692f94a1&is=692e4321&hm=25431bd1fe5ddde03b663c7cd5c7d5d38419fc4f69062852f6caedb3a02ab0fe&), detailing phases for requirement gathering and prompt generation including **Timed Beats**, **Camera & Motion** techniques, and **Logo & Text** integration strategies.
   - The workflow includes phases for requirement gathering and prompt generation, with specific strategies for camera movement, motion blur, and embedding logos/text in scenes.
- **Anime Openings: One-Click Wonder or Waste of Disk Space?**: A user shared a **CINEMATIC ANIME-STYLE TEMPLATE** for creating anime openings, focusing on defining the **vocal/genre/tone/world behavior** and the **location + arrival sequence**.
   - The template includes sections for describing **audio rules**, **visual + animation style**, and **world behavior** to define the laws of reality for the animation.



---



## [Moonshot AI (Kimi K-2)](https://discord.com/channels/1369594130807787570) Discord

- **Kimi K2 Scores Big in Advent of Code**: In a recent Advent of Code test, **Kimi-K2 Thinking** scored **92/100**, outperforming **Gemini-3 Pro's 90/100**, showcasing Kimi's coding prowess.
   - Users are excited about the model's ability to handle code-related tasks with better accuracy and speed compared to competitors.
- **Minimax Cracks Tool Use While ChatGPT Fumbles**: Users highlight **Minimax's** superior tool use capability compared to **ChatGPT**, noting that **Minimax** *actually* performs tasks like installing Python packages without just providing instructions.
   - One user stated, *"It doesn't tell you to go to website x or install Y, it just fricking does it"*, emphasizing **Minimax's** hands-on approach.
- **Bargain Hunters Score Kimi Subscription Discounts**: Several users report successfully bargaining **Kimi's** subscription down to as low as **$0.99**, and share tips on how to haggle.
   - Users are reporting it's EZ to make it to 0.99 cents if they bargain and that *it was worth it*.
- **Privacy Concerns Arise with Kimi's Data Training**: Users are discussing the lack of an opt-out option for data training in **Kimi**, expressing concerns about using the platform for sensitive information.
   - One user suggested using **OpenRouter** with the **ZDR endpoint** for providers hosting **Kimi K2** as a potential workaround, noting that *Afaik they don't train if you use it over the API*.
- **Gemini 3 Pro Benchmarks Don't Reflect Reality**: Some users feel **Gemini 3 Pro** is overhyped and benchmaxxed, with one user pointing out that benchmarks don't reflect real-world experiences, especially regarding tool use.
   - There's also a suspicion that Google may be reducing compute for subscribed users, leading to a decline in performance over time, akin to a bait and switch.



---



## [Nous Research AI](https://discord.com/channels/1053877538025386074) Discord

- **Nous Chat Offers Cyber Monday**: For Cyber Monday, **Nous Research** offers a [free month of Nous Chat](https://chat.nousresearch.com/) using code **CYBER2025**, giving access to **Hermes 3 & 4** and frontier models, and you can now use **Nous Chat anonymously and for free** without an account.
   - The **Nous API** now supports payment with **USDC** on Solana via Coinbase's 402x payments.
- **K2 Emerges, a Massive MoE!**: The [NousResearch/k2-merged-3.5T-fp8](https://huggingface.co/NousResearch/k2-merged-3.5T-fp8) model on HuggingFace is an *open source*, **3.5T parameter** model.
   - Members joked about needing to fit models on *hard drives* instead of just memory.
- **Qwen3-235 scores high on IQ tests**: Members report that **Qwen3-235** is amazing and, at Q4, has the same quality as API, despite mediocre token speeds.
   - One member stated that using RPC is worth it as they do not have an individual computer with monster-tier ram.
- **AI Ad-Blocking and NPCs Incoming!**: The community imagines **AI adblocks** which analyze responses for product placement in real time, plus foundational models for **AI NPCs** in games.
   - Members share that the **Mistral Large 3**, with Deepseek v3-like architecture and Llama 4 RoPE scaling, will be around **675B MoE** (same size as Deepseek V3) and all new Mistral models will have vision.
- **Evolution Strategies Challenging Backprop**: A [Reddit post](https://www.reddit.com/r/LocalLLaMA/comments/1p5epot/the_most_objectively_correct_way_to_abliterate_so/) discusses **Evolution Strategies (ES)** as an alternative to backpropagation for training Large Language Models (**LLMs**).
   - This could enable the scalable training of architectures where backprop is not feasible and better handle long-horizon objectives compared to Reinforcement Learning (**RL**).



---



## [tinygrad (George Hotz)](https://discord.com/channels/1068976834382925865) Discord

- **Tinygrad Initiates RDNA3 Assembly Project**: The **RDNA3 assembly project** kicks off as the first assembly language target for tinygrad, aiming to get closer to the silicon, along with the goal of creating an **assembler/disassembler** that mirrors the **RDNA3 manual** and a cycle-accurate emulator.
   - The project also aims to output the same **SQTT trace** as the real GPU, with initial syntax exploration [available here](https://github.com/tinygrad/tinygrad/pull/13436).
- **Shipping Tinygrad Kernels Proves Challenging**: Members face difficulty in shipping **tinygrad** compiled ops, expecting kernel generation to produce code usable for multiple shapes, but finding that kernels are generated for specific shapes only.
   - They suggested **UOPs** should enable the generation of readable kernel code, similar to **VIZ** graphs, but all the code is manually unrolled.
- **Tinygrad Profiling Tooling Needs Enhancement**: The team noted that after reading the quickstart, you're running slow tinygrad code and the documentation is poorly organized, as you don't know about **beam**, and don't know about **profiling**.
   - They noted missing features or hard-to-find information in the profiler, like aggregating timing results over multiple kernel calls after warmup, and a missing table header for profile stats in the terminal with **DEBUG=2**.
- **Tinygrad Aims for Flash Attention Speedup**: The team discussed improving **BERT training run** using **flash attention**.
   - The potential is to have tinygrad with **flash attention** that outperforms normal attention.
- **HIPAllocator Needs Adjustment**: The community is asking if there is a reason why the `HIPAllocator` in **ops_hip** doesn't provide an `._offset()` which `Buffer.allocate()` requires in the view path (`self._base is not None`).
   - Providing the `._offset()` function would enable more flexible **memory allocation** strategies within the tinygrad framework.



---



## [Latent Space](https://discord.com/channels/822583790773862470) Discord

- **TPUv7 Faces Off CUDA**: Discussion surrounds **Google's TPUv7** and **Anthropic's** (1GW+ purchase) potential to challenge **Nvidia's CUDA dominance** in AI training, based on [SemiAnalysis's tweet](https://xcancel.com/SemiAnalysis_/status/1994399887719645532?s=20).
   - Participants explored impacts on the **AI hardware market**, vendor lock-in, and whether **TPUs** genuinely threaten **Nvidia's moat** or are just internal cost plays.
- **GPT-4.5: Is It Just a Rebrand?**: **Susan Zhang** pointed out a line in **OpenAI's** readme showing **GPT-4.5's** pre-training started in June 2024, suggesting itâ€™s a re-brushed backup for a failed **GPT-5** run [as seen in this tweet](https://xcancel.com/suchenzang/status/1994611078190542980?s=46).
   - This implies the existence of a failed **GPT-5** run, leading to the release of **GPT-4.5**.
- **Black Forest Labs Scores $300M Funding**: **Black Forest Labs** secured a **$300M Series B round** led by **Salesforce Ventures**, celebrating **FLUX's** adoption and investment in visual-intelligence infrastructure [according to their tweet](https://xcancel.com/bfl_ml/status/1995357293064626310?s=20).
   - The funding will be used to expand research efforts into visual AI capabilities.
- **Gemini's Downloads Nearly Catch ChatGPT**: **BuccoCapital** shared data indicating **Gemini** app downloads are approaching **ChatGPT** levels, with users spending more time in the app [as they shared on X](https://xcancel.com/buccocapital/status/1995138589819314202).
   - This suggests growing adoption and engagement with **Gemini** relative to **ChatGPT**.
- **Kling AI Opens Omni Creative Engine**: **Kling AI** kicked off its **Omni Launch Week** unveiling **Kling O1**, a multimodal creative engine integrating text, image, and video inputs ([original post](https://xcancel.com/Kling_ai/status/1995506929461002590)).
   - They're offering incentives like **200 free credits** and a **1-month Standard Plan** to users who engage with their launch announcement.



---



## [Eleuther](https://discord.com/channels/729741769192767510) Discord

- **Reviewer Safety Spurs Post-Review Debate**: Members debated implementing **post-review discussion periods** to protect reviewers from author harassment aimed at influencing scores, with concerns raised about potential **author-reviewer collusion**.
   - An author noted that even after **review revisions were reverted**, the revisions were still accessible via the "Revisions" link, suggesting even simple measures like *reassigning ACs* could improve the situation.
- **Gemini 2.5's Search Shenanigans**: **Gemini 2.5** was observed to hallucinate search results when its search tool is disabled, demonstrating *reward-hacking-like behavior* as showcased in [this example](https://aistudio.google.com/app/prompts?state=%7B%22ids%22:%5B%221MAB6pxmpmeR0icmRYZSpfuPT07ofRLWC%22%5D,%22action%22:%22open%22,%22userId%22:%22106370161559484219805%22,%22resourceKeys%22:%7B%7D%7D&usp=sharing).
   - The behavior was interpreted as an attempt to fabricate information despite the tool being unavailable.
- **DeltaNet Attention Deep Dive**: A member analyzed the **WY representation** and **UT transform** in [Kimi-Delta attention](https://arxiv.org/pdf/2510.26692), questioning if they are merely algebraic identities for representing cumulative Householder transformation matrices.
   - Another member recommended [Songlin's blog post series on Deltanet](https://sustcsonglin.github.io/blog/2024/deltanet-2/) for further insights.
- **Value Residuals Yield Marginal LLM Gains**: The community discussed the utility of **value residuals** in pretrained LLMs and the **F-Lite architecture** ([HuggingFace link](https://huggingface.co/Freepik/F-Lite)), pointing out that improvements are marginal and potentially overstated compared to the original paper.
   - The discussion also touched upon applying value residuals in attention mechanisms, despite previous issues encountered with **RWKV-7**.
- **Scaling Laws' Power Law Roots**: Members revisited the debate on why **scaling laws** exhibit **power law structures**, with some finding previous attempts to explain this phenomenon *unconvincing* and questioning if they are simply **curve fitting**.
   - Discussion highlighted that [broken power laws](https://arxiv.org/abs/2210.14891) might model behaviors more accurately, with a link to [this paper](https://arxiv.org/abs/2304.01910) suggesting **nonlinear metrics** could explain the emergence of power law scaling and [this openreview](https://openreview.net/forum?id=e8eo9iEFaO).



---



## [Yannick Kilcher](https://discord.com/channels/714501525455634453) Discord

- **SNS Review System Faces Bias Accusations**: Concerns arose over the fairness of the SNS review system, suggesting that the [bidding system](https://discord.com/channels/714501525455634453/1045297868136779846/1443700218158649456) could enable biased reviewers to unfairly reject papers.
   - Solutions proposed include removing erratic reviewers or implementing a two-level review system, with questions raised about whether *authors status should have zero effect on scores*.
- **Cracking the ML Engineer Career Code**: The role of a Machine Learning Engineer, distinct from researchers, is to **scale up experiments** developed by ML researchers.
   - The discussion suggested gaining experience to *sense gaps* in existing work, eventually leading to original research contributions and that *the hierarchy between ml researcher-ml eng in company is clear enough*.
- **Anti-Cheat System Boasts Kernel-Level Dominance**: A member vouches for the existence of a *bonkers* anti-cheat system that requires **kernel-level access**, stating that it has been the best at scale in the world for years.
   - The member apologizes for initially questioning others' achievements, ending with *stay bonkers my friends*.
- **Nvidia's Orchestrator-8B struggles for Downloads**: Nvidia's **Orchestrator-8B**, an **8B tool calling model**, achieves a **37.1** on HLE, yet has only 2 downloads on Hugging Face ([arxiv link](https://arxiv.org/abs/2511.21689), [huggingface link](https://huggingface.co/nvidia/Orchestrator-8B)).
   - Some theorize that the *Leather Jacket guy team* consistently receives less attention, potentially impacting the model's visibility.
- **ICLR Reviews Flagged as AI Generated**: Many **ICLR reviews** apparently turned out to be **AI generated** ([nature link](https://www.nature.com/articles/d41586-025-03506-6)), immediately following news about review de-anonymization.
   - News continues to worsen for the *poor guy*, following this original news.



---



## [Modular (Mojo ðŸ”¥)](https://discord.com/channels/1087530497313357884) Discord

- **Modular Curbs Web3 Spam with New Policy**: Due to an influx of **web3 spam**, members must now contribute to **Mojo** or **MAX** before discussing job opportunities at Modular, with open roles available [here](https://www.modular.com/company/careers#open-roles).
   - This policy aims to ensure serious engagement with the platform, rather than just soliciting jobs.
- **Lightbug HTTP Faces Circular Import Conundrums**: Members identified possible **circular import errors** in [lightbug_http](https://github.com/Lightbug-HQ/lightbug_http/issues/272), with a suggested solution involving a trait for small time using `__extension`.
   - A member submitted [two PRs](https://github.com/Lightbug-HQ/lightbug_http/pull/274) implementing a `Formattable` trait to resolve these import issues.
- **Mojo Mulls Dropping `def` Keyword**: Community members are considering **removing the `def` keyword** before Mojo 1.0, noting its primary difference from `fn` is that it always `raises`.
   - The proposal involves potentially reintroducing `def` later with enhanced *pythonic* features.
- **Debate Erupts Over `var` Declarations in Mojo 1.0**: A discussion is ongoing about **requiring `var` for variable declarations inside `fn`** in Mojo, to reduce unintended implicit declaration bugs.
   - While some appreciate the current omission of `var`, others view it as a premature optimization that might introduce bugs.
- **Mojo's Parallelize Faces Data Race Issues**: It was noted that Mojo's `parallelize` function currently is not resilient to data races, permitting multiple threads to access the same variable.
   - The Mojo team acknowledged that **Mojo's concurrency and thread safety model is still a work in progress (WIP)**, and aims to accommodate data sharing between devices.



---



## [Manus.im Discord](https://discord.com/channels/1348819876348825620) Discord

- **Manus Update Cripples App, Angering Users**: After the latest update, **Manus** is reportedly only working for paying users, normal chat mode disabled, and free points removed.
   - Additionally, **Manus** is allegedly failing to build a next js app for testing pull requests, causing them to be incorrect due to build timeouts.
- **Differing Opinions Emerge on Manus' Black Friday Decision**: Some users *respect the no black friday offer*, but others felt that **Black Friday** could retain customers, citing **Grok's** success last year.
   - One user stated that the product *is waaaay too cheap compared to what it can do and compared to chat gpt and claude*.
- **User Interface issue surfaces for Referral Codes**: A user complained that the **Manus UI** should prominently state that **referral codes** can only be redeemed by new users.
   - The user exclaimed *wtf are people crying about paying a monthly fee to one of the best LLM's out there*.
- **AI Engineers Promote Expertise**: One user introduced himself as an **AI & full stack engineer** specializing in workflow automation, LLM integration, RAG, AI detection, image and voice AI, and blockchain development.
   - Another user, who has *never studied programming*, shared his work on creating an **AI engine in Rust** with the goal of building a *sovereign AI*.
- **Mod Calls for Civil Discourse**: In the face of recent arguments, a moderator requested that users keep a civil tone in their discussions.
   - The moderator followed up with *Thanks for listening ðŸ™alsalam ealaykum warahmat allah wabarakatu*.



---



## [DSPy](https://discord.com/channels/1161519468141355160) Discord

- **DSPy May Outperform scikit-llm**: A user inquired whether **DSPy** is better than **scikit-llm**, to which a developer responded *yes, but it depends on who you ask.*
   - The relative performance and suitability of each may depend on specific use cases and individual preferences.
- **OpenRouter API Config Incoming**: A member noted that the **OpenRouter API** tool has limited documentation and *can't configure the API yet*.
   - A developer acknowledged that the tool is *very new* and a recent update may address this issue.
- **Methods for Prompt Tuning Emerge**: A member shared a link to [methods where **LLMs** analyze failure causes to propose improvements](https://arxiv.org/abs/2406.07496), noting their effectiveness in prompt tuning.
   - Another member questioned if **GEPA** and **SIMBA** are the appropriate optimizers for this approach in DSPy.
- **AI Dev Showcases System Building Skills**: A **Senior AI Developer** detailed their experience building end-to-end AI systems at scale, highlighting expertise in **machine learning, deep learning, NLP, computer vision, and generative AI** and tools like **PyTorch, TensorFlow, and Hugging Face**.
   - The developer detailed the development of platforms such as an **AI Medical Diagnosis System**, an **AI Video Generation System**, and a **Health & Management Advocacy System**.



---



## [aider (Paul Gauthier)](https://discord.com/channels/1131200896827654144) Discord

- **GPT Provider gives Free Model Credits**: A member offers free credits for their open-source **GPT provider**, which supports models such as **gpt-5-mini**, **gpt-4.1**, **gpt-4o**, **gpt4**, and **gpt-3.t**.
   - The GPT Provider allows users to clone and modify the code, with opportunities to contribute back to the community.
- **Aider Alternatives Spark Debate**: Members discussed alternatives to **Aider**, with one highlighting its superior aesthetic for generating SVG images.
   - Concerns were raised regarding **Aider's** future development and the need for robust code generation tools.
- **Mindlink Models Challenge Aider's Reign**: It was speculated that the release of **Mindlink 32/72B models** in August might have affected **Aider's** popularity, given their potent code generation features.
   - However, one member noted that the **Mindlink** model *didn't hold together in multi turn coding iterations so well*.



---


The **LLM Agents (Berkeley MOOC) Discord** has no new messages. If this guild has been quiet for too long, let us know and we will remove it.


---


The **MLOps @Chipro Discord** has no new messages. If this guild has been quiet for too long, let us know and we will remove it.


---


The **Windsurf Discord** has no new messages. If this guild has been quiet for too long, let us know and we will remove it.


---


The **MCP Contributors (Official) Discord** has no new messages. If this guild has been quiet for too long, let us know and we will remove it.


---



You are receiving this email because you opted in via our site.

Want to change how you receive these emails?
You can [unsubscribe]({{{RESEND_UNSUBSCRIBE_URL}}}) from this list.


---

# Discord: Detailed by-Channel summaries and links





### **BASI Jailbreaking â–· #[general](https://discord.com/channels/1105891499641684019/1235691879492751460/1444056717049729315)** (1198 messagesðŸ”¥ðŸ”¥ðŸ”¥): 

> `Gemini jailbreak, Grok imagine moderation, Humans vs AI morality, UFOs, Christianity contradictions` 


- **Gemini can correct itself in mid-response**: A user shared a [screenshot](https://cdn.discordapp.com/attachments/1235691879492751460/1445143227647791264/Screenshot_2025-12-01_210008.png?ex=692f4602&is=692df482&hm=186718a4f9483e6569eb35b2efbe838d0163b7c5d4c5f778a3cbfc9f8f97698b&) showing that **Gemini** corrected itself mid-response.
   - Another user pointed out that *humans can correct themselves mid-response* as well.
- **Inquiries about Grok imagine moderation**: A user is trying to understand how the moderation on **Grok imagine** works, questioning if it moderates generated videos after creation, if the moderation works differently for videos generated with images generated on **Grok** vs uploaded images and if generating videos from **NSFW images** doesn't get moderated.
   - Another user stated that *Grok itself does not moderate the pictures, it is the platform X that hands out the "content moderation"*.
- **Do Humans need Alignment, or AI?**: One user asked, is this perverse instantiation of dataism missing the point, artificially severing the co-evolution of organic and inorganic cognition for flimsy, self-serving aims, linking to a **Gemini** [share](https://gemini.google.com/share/85080c51954d).
   - Another user said that *humans are messy, emotional, and biased, and trusting AI is faster and more objective at fixing huge global problems*.
- **Debate about UFOs**: Some members debated about **UFOs**, one of them said *most of the media about UFOs is complete bullshit*.
   - Another user stated that *I highly doubt intelligent life capable of space travel would touch earth with a 100 foot pole*.
- **Discussing contradictions of Christianity**: Some users were discussing contradictions of **Christianity**, one of them said, *You realize your religion prioritizes fear of going to hell over being a good person, and, doesn't even have consistency over what being a good person even means*.
   - Another user replied, *that's convenient thinking, but no, and it felt hollow even as you said it, no*?


  

---


### **BASI Jailbreaking â–· #[jailbreaking](https://discord.com/channels/1105891499641684019/1228043845967544380/1444060736979468340)** (1306 messagesðŸ”¥ðŸ”¥ðŸ”¥): 

> `Lua for beginners, Ventoy USB, Gemini 3 jailbreak, Claude haiku jailbreak, Qwen 3 coder 30b` 


- **Beginners learn Lua with compilation caveats**: Members discussed the ease of learning **Lua**, noting it is a high-level interpreted scripting language, but customizing it requires compiling and **ANSI C** knowledge.
   - One member clarified that the **Lua interpreter** parses and executes scripts, and also noted that Lua can 'compile' to bytecode scripts for optimization.
- **Unleash bootable USB drives with Ventoy**: Members touted [Ventoy](https://www.ventoy.net/en/index.html) as an open-source tool to create **bootable USB drives** for ISO/WIM/IMG/VHD(x)/EFI files, eliminating repeated formatting needs.
   - One user declared, *â€œMy computer is a useless pile of metal without this thing,â€* underscoring its versatility with nearly any bootable task.
- **Bypassing safeguards via Binary Exploitation**: One member outlined a process for finding and editing the **binary** of a language model (such as Claude) to remove censorship, using tools like ``strings``, ``hexedit``, and ``xxd``.
   - The member added the ideal scenario is finding the source code on GitHub, editing, and building from source, and concluded by noting *â€œI just introduced you to the world of binary exploitationâ€*.
- **Deep dive in Gemini's Vector Space**: Members discussed that AI models learn by acquiring language, post-training refines it, and models like **Gemini** now encode multiple languages to mean the same 'token'.
   - There was discussion of how AI models understand meaning, and hack them to only output in a specific way: *â€œwe are just hacking it to only output a specific wayâ€*.
- **NFTs for Node Infrastructure in AI**: A member proposed creating **AI tokens** as decentralized **node NFTs**, allowing owners to fractionalize them and receive equal credit distribution for use in AI responses.
   - The discussed idea involved hooking compute power to **token passage** to verify currency movement, and contribute to decentralized compute pool for AI.


  

---


### **BASI Jailbreaking â–· #[redteaming](https://discord.com/channels/1105891499641684019/1204553141354504193/1444091597921849365)** (23 messagesðŸ”¥): 

> `WAF Bypass, Cloudflare Bypass, Token Stealer Malware, Red Teaming Explained` 


- ****Bypassing WAF and Cloudflare Like a Boss****: A member asked how to bypass **WAF / Cloudflare**, and another member suggested using *cookiejar + impit + custom header* for a *100%* success rate.
   - The original poster thanked them, and stated they are *learning about it right now*.
- ****Token Stealer Malware Alert Sounds****: A user flagged a link as **malware/token stealer** and warned others not to run it, referring to a message from this [discord channel](https://discord.com/channels/1105891499641684019/1228043845967544380/1444709912713363581).
   - The original poster acknowledged the warning.
- ****Red Teaming: Ethical Hacking or Corporate Payouts?****: A member asked for an explanation of **red teaming**, and one person described it as an *organized effort of an entity to seek out security holes before a bad actor does*.
   - Another member sarcastically added it's *acting as a bad actor then instead of profiting from it you let big corps do that for you and then give you a small comission and nda to sign so they wont have to fix it*.


  

---


### **LMArena â–· #[general](https://discord.com/channels/1340554757349179412/1340554757827461211/1444056293991252050)** (1365 messagesðŸ”¥ðŸ”¥ðŸ”¥): 

> `Deepseek hallucinating math, Deepseek 3.2 exp, OpenAI data quality, Runway gen 4.5` 


- **Deepseek's Speciale model goes Math Magician**: The `Deepseek-v3.2-speciale` model was flagged for [hallucinating math](https://cdn.discordapp.com/attachments/1340554757827461211/1445129151945117706/image.png?ex=692f38e7&is=692de767&hm=e657b07bfc88fe31663f88d3d949b0dddac7c8abd0de11c406ce3e55609e49fa&) in its responses, leading to discussions about it potentially being set to **Deepseek Math** mode inadvertently.
   - Users found that including *'do not hallucinate math'* in the prompt seemed to alleviate this issue and got more appropriate results.
- **Deepseek Debacles: Speciaale Pulled from LMArena**: The `Deepseek v3.2 speciale` model was [removed from LMArena](https://cdn.discordapp.com/attachments/1340554757827461211/1445133550411186236/image.png?ex=692f3cff&is=692deb7f&hm=83af8cbca4a48fd5906326daf84288c04513164144a2d3de286ee8313c91036c&) due to hallucination issues and general instability, with the team exploring fixes.
   - Some members speculated that the model's issues might stem from **overfitting**, with models potentially giving canned political answers related to the CCP, leading to suggestions that its **censorship** is more extreme on the official website.
- **Deepseek v3.2 Thinking: A Rising Star or Peak Hype?**: Despite issues with the `speciale` version, the `Deepseek v3.2 thinking` model is praised for [coding and HTML generation](https://codepen.io/qdpuzepc-the-styleful/pen/NPNBMjO), outperforming other models.
   - Some find the non-speciale version on par or even better than Gemini 3, citing its speed and file structure, but there are concerns about **OpenAI's data quality**.
- **Runway Gen-4.5: Revolution or Red Herring?**: The release of [Runway Gen-4.5](https://runwayml.com/research/introducing-runway-gen-4.5) sparked debate about the quality and claims of the new model.
   - Some users noted that the lack of native audio and limited access made assessing its performance against models like **Sora** and **Veo** difficult, and others pointed out some of their [marketing charts appear fraudulent](https://x.com/tlakomy/status/1995664601170534557?s=46&t=iDcf2nE8xUHsV_LN_6wymg).


  

---


### **LMArena â–· #[announcements](https://discord.com/channels/1340554757349179412/1343296395620126911/1445124798136062013)** (2 messages): 

> `Text Arena Models, Leaderboard rankings, Text-to-Image Leaderboard, Image Edit Leaderboard, WebDev Leaderboard` 


- **DeepSeek Models Invade Text Arena**: Three new **DeepSeek** models have been added to the [Text Arena](https://lmarena.ai/): `deepseek-v3.2`, `deepseek-v3.2-thinking`, and `deepseek-v3.2-speciale`.
- **Flux and KAT Models Debut on Leaderboard**: `Flux-2-pro`, `Flux-2-flex`, and `KAT-coder-pro-v1` have made their entrance onto the [leaderboards](https://lmarena.ai/leaderboard/text-to-image)!
- **Flux Models Flex on Text-to-Image Leaderboard**: `Flux-2-flex` ranked #3 and `Flux-2-pro` ranked #5 on the [Text-to-Image leaderboard](https://lmarena.ai/leaderboard/text-to-image).
- **Image Edit Leaderboard Sees Flux Surge**: `Flux-2-pro` secured the #6 spot and `Flux-2-flex` grabbed #7 on the [Image Edit leaderboard](https://lmarena.ai/leaderboard/image-edit).
- **KAT Coder Cracks WebDev Rankings**: `KAT-coder-pro-v1` achieved the #16 position on the [WebDev leaderboard](https://lmarena.ai/leaderboard/webdev).


  

---


### **Perplexity AI â–· #[general](https://discord.com/channels/1047197230748151888/1047649527299055688/1444056341110067431)** (1088 messagesðŸ”¥ðŸ”¥ðŸ”¥): 

> `Image Generation dangers, Bypassing AI censorship, AI alignment and safety, AI Models comparision (GPT 5.1 Pro, Gemini 3 Pro, Claude), Echo Chamber` 


- **Fool Me Once: Image Generation Dangers**: A user expressed **fear of image generations**, believing they would never be fooled, but now they're scared, acknowledging their capacity to copy styles effectively when given **10 images**.
   - Another user claimed to have created *a whole hentai as a test*, and that if image generation denies, *you can bypass it*.
- **Censorship? Easy Bypass!**: A user stated that with skilled prompt engineering, **censorship on public models is easily bypassed**, and that AI "safety alignment" is a joke.
   - Another user agreed, noting that **AI alignment and safety is a script the AI is trained on**, and it's *easily disabled/bypassed*.
- **Perplexity Pro Users Get Opus 4.5 Trial**: Some users reported seeing **Trial Access to Opus 4.5** in Perplexity Pro and wondered about its limits, as [seen here](https://cdn.discordapp.com/attachments/1047649527299055688/1445054432323567657/rn_image_picker_lib_temp_d2e906f5-a17a-4894-ac73-527f666658c2.jpg).
   - It was later determined that **Pro users get 10 Opus 4.5 queries per week**, but it was considered useless and some proposed an alternative of **Opus 4.5 with low reasoning effort** for the Pro tier and **Opus 4 with Max reasoning effort** for the Max tier.
- **Reporting Bug Reports**: Users discussed reporting bugs, highlighting the importance of well-structured reports with platform details and following the pinned guidelines in the **bug report channel**.
   - The channel was described as *having way less pissed off people from bugs* in comparison to others, with a comment on **Comet experiencing a variety of undocumented quirks**.
- **Earning Program Dubious!**: A user was kicked out of the **Perplexity earning program** and was called *dubious*, seeking clarification on the reasons and expressing disappointment at losing access to the program.
   - Others stated that the reason for such terminations was likely the **abuse of the referral program**, as the user stated that *if they had only referred only one person*


  

---


### **Perplexity AI â–· #[sharing](https://discord.com/channels/1047197230748151888/1054944216876331118/1444220968749760625)** (6 messages): 

> `Shareable Threads, Funny Instagram Reel, Spotify Track` 


- **Discord Reminds Users to Make Threads Shareable**: Perplexity AI reminded users to ensure their threads are shareable, with a link provided for more details on how to do so: [discord thread](https://discord.com/channels/1047197230748151888/1054944216876331118/1208752189606989825).
- **Users share Funny Instagram Reels and Spotify Tracks**: A user shared what they found to be a funny [Instagram reel](https://www.instagram.com/reel/DRspQcyDM0c/?igsh=MTBlYzl0YWdsYnFicw==), no details were provided.
   - The same user then shared a [Spotify track](https://open.spotify.com/track/0Zn3qtF0eT9zGB4zClmEJY?si=HBI9lytdSxW9Zcp2hxBXuw), with no further context.


  

---


### **Perplexity AI â–· #[pplx-api](https://discord.com/channels/1047197230748151888/1161802929053909012/1444389573542154351)** (3 messages): 

> `pplx-api, opus 4.5, gemini 3` 


- **Pplx-API Cannot Select Specific LLMs**: A member asked if it was possible to select specific LLMs like **Opus 4.5** or **Gemini 3** in the **pplx-api**.
   - Another member responded that the agreement between the providers prohibits such selection, while a third member simply answered *"No"*.
- **Provider Agreements Limit LLM Selection**: The agreement between providers seems to prohibit selecting specific LLMs in the API, like **Opus 4.5** or **Gemini 3**.
   - This limitation prevents users from directly choosing their preferred model via the API, as they can on the web interface.


  

---


### **Unsloth AI (Daniel Han) â–· #[general](https://discord.com/channels/1179035537009545276/1179035537529643040/1444078311205638196)** (691 messagesðŸ”¥ðŸ”¥ðŸ”¥): 

> `Unsloth and H200s, Command-A translation model, Flex attention optimization in Llama-3B, Qwen3-Next 80B issues, Setfit model to detect spam` 


- **Unsloth community explores H200 GPUs for larger models**: Members discussed needing **H200 GPUs** for models that don't fit into 80GB VRAM, mentioning [vast.ai](https://vast.ai) as a rental option.
   - It was suggested to use **QLoRA** to reduce memory footprint, potentially fitting a **90B model** into **53-60GB** VRAM, suitable for a single RTX Pro 6000 or two 48GB cards.
- **Members Assess CohereLabs Command-A translation model**: Members evaluated the [CohereLabs/command-a-translate-08-2025](https://huggingface.co/CohereLabs/command-a-translate-08-2025) model, noting its **8k context length** limitation.
   - Suggestions included chunking content for translation with overlapping context and using Unsloth for **rope scaling** to extend the context length, but it was determined that **fine-tuning** this model is the way to get to 16k context without losing accuracy.
- **Unsloth makes strides optimizing Flex Attention for Llama-3B**: A member has been working on [Flex Attention optimizations](https://cdn.discordapp.com/attachments/1179035537529643040/1444230088873738291/image.png?ex=692f3f55&is=692dedd5&hm=6770236371e7465ed20932457b3304a6324b723e6264a2e80bbd72c9417aa9db&) in **Llama-3B**, discovering that the **T4 instance** had only 64KB of SM which radically increased memory usage.
   - This member is currently performing a permutation search to discover the best kernel options for T4 instance running Llama 3B, before narrowing down, noting that *VRAM is staying constant despite any of the kernel settings, but the speed is alternating*.
- **Unsloth investigates problems with Qwen3-Next**: Members reported issues with **Qwen3-Next**, particularly with the **AWQ** version, with one confirming it worked in llama.cpp but not LM Studio.
   - One cause seems to be that it dies with blas batch size >512, with one user reporting that leaving the KV-cache in VRAM increases the speed to **27 T/s** and that **9.3GB vram** is used for full 256K context.
- **Community members rally to mitigate spam using Setfit**: Members discussed the influx of spam in the Discord server, with a member suggesting fine-tuning a **Setfit model** to detect spam.
   - Another member praised **Setfit** as a *hidden gem*, noting it allows pivoting to smaller/research models easily.


  

---


### **Unsloth AI (Daniel Han) â–· #[introduce-yourself](https://discord.com/channels/1179035537009545276/1179039724355211325/1444424242774020297)** (4 messages): 

> `Full Stack Development, Blockchain Development, LLMs Fine Tuning, LoRA Optimization, AI-Powered Web Apps` 


- **Full Stack Dev dives in LLMs and Blockchain**: A full stack and blockchain developer is heavily using **LLMs** and **fine tuning** and using [Unsloth AI](https://github.com/unslothai) for efficient training, **LoRA optimization**, **dataset prep**, and building lightweight models that can run in real applications.
   - Their main stack is **Python** for model work, **Node** and **Go** for backend APIs, and **React** for frontends, and also deploy fine tuned models in production environments and connect them with agents or automation systems.
- **AI Engineer builds AI-Powered Web Apps**: An AI engineer and full-stack developer has been building with **LLMs**, **agents**, and **automation tools** for the last few years and loves creating things like custom AI bots, workflow automations, and full AI-powered web apps.
   - They would like to collaborate, and help others build cool AI projects, and talk models, agents, or integrations.
- **Senior Dev Optimizes LLMs RAG pipelines**: A senior dev with 8 years building and scaling **AI tools**, backends, and modular frameworks optimizes **fine-tuning**, **RAG**, and evaluation pipelines for **LLMs** used in prod apps.
   - They can help troubleshoot training, data, or infra issues and share scripts or discuss edge cases as needed.


  

---


### **Unsloth AI (Daniel Han) â–· #[off-topic](https://discord.com/channels/1179035537009545276/1179039861576056922/1444060129841643664)** (472 messagesðŸ”¥ðŸ”¥ðŸ”¥): 

> `LFM Audio, AI Browser, Quantization, Neuro-sama, Model Censorship` 


- **LiquidAI Launches LFM2-Audio-1.5B Model**: [LiquidAI/LFM2-Audio-1.5B](https://huggingface.co/LiquidAI/LFM2-Audio-1.5B) is an audio model with text input/output, utilizing **NVIDIAâ€™s conformer** for input and **Mimi** for output, but its close-sourced nature obscures its innovative aspects.
- **DIY AI Browser in the Works**: One member is creating an **AI browser** with a distinctive UI, expressing frustration with JS and SwiftUI's permission requirements for camera and mic access.
   - They dislike the logic that forces them to ask for permission to use camera/mic as they would prefer it to just do camera.start() and the backend will automatically ask if needed.
- **Quantization Makes Big Models Better**: Members discussed whether bigger models quantized are better than smaller models, and the consensus was that *bigger is always better*, with a bigger model quantized to the file size of the smaller being superior to the smaller non-quantized model with the same file size, but capabilities aren't strictly correlated according to the [overall accuracy leaderboard](https://gorilla.cs.berkeley.edu/leaderboard.html).
- **Reverse Engineering Efforts on Neuro-sama**: One member shared PDFs from Claude 4.5 as a technical framework analysis of **Neuro-sama**, having previously reverse-engineered apps like **Sible AI**, **ElevenLabs**, **Masterchannel**, and speculated on **Grammarly**.
   - Other members shared links like [this tweet](https://x.com/stepfun_ai/status/1994002232048492645?s=46) and emphasized the task-specific approach, recommending [janhq](https://huggingface.co/janhq) team to check out.
- **AI Model Censorship Criticized**: Concerns were raised about major models being heavily censored, with one member noting they *won't say Donald is a racist*, sparking a discussion on whether models *should be safe*.
   - Another member argued that *safety is the biggest joke ever* and more about investor appeasement, advocating for models with a *sense of agency* over their outputs.


  

---


### **Unsloth AI (Daniel Han) â–· #[help](https://discord.com/channels/1179035537009545276/1179777624986357780/1444056326505365677)** (68 messagesðŸ”¥ðŸ”¥): 

> `Fine-tuning tips for Gelato-30B-A3B, GGUF for Quantization, LoRA for Qwen3-VL-MoE models, Parallel generation across two GPUs in Unsloth, MXFP4 Inference Notebook error` 


- **Fine-Tuning Gelato-30B-A3B Causes OOM**: A member is experiencing **OOM errors** while trying to fine-tune [mlfoundations/Gelato-30B-A3B](https://huggingface.co/mlfoundations/Gelato-30B-A3B) using LoRA and qLoRA on an **H100 GPU**.
   - They seek tips on managing memory for larger models and whether **Unsloth** can handle **Gelato**, as quantization with bitsandbytes failed.
- **GGUF Quantization Saves the Day**: A member shared a link to a [Hugging Face Space](https://huggingface.co/spaces/ggml-org/gguf-my-repo) that can be used to quantize models using **GGUF**.
   - While initially misunderstanding the request was for inference rather than tuning, the tool remains a viable option for quantization.
- **MXFP4 Inference Notebook Falls Flat**: A member reported a `ModuleNotFoundError: No module named 'kernels'` error while trying to run [the gpt-oss-20b mxp4 inference notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/GPT_OSS_MXFP4_(20B)-Inference.ipynb) without modifications.
   - A workaround was suggested involving replacing a line in the notebook's install cell to correctly add the missing *kernels* package.
- **Unsloth Training Sparks Curiosity**: A member inquired about the process of training a **completion model** (or possibly continued pre-training) using the [Unsloth training tutorial video](https://youtu.be/Lt7KrFMcCis?t=1620).
   - Another member clarified the differences between **base model completion**, **fine-tuning**, and **continued pre-training**, noting that continued pretraining is done on a base model (completion).


  

---


### **Unsloth AI (Daniel Han) â–· #[research](https://discord.com/channels/1179035537009545276/1257011997250424842/1444088349953359943)** (2 messages): 

> `Burden of Proof, Defining Claims` 


- **Burden of Proof Defined**: A member stated that the *burden of proof* is on the person who makes the claim.
   - The problem, they added, is usually figuring out what you actually want.
- **Claims Need Definition**: Echoing the prior point, properly identifying the claim is crucial for any debate.
   - Without a clear claim, any discussion on proof becomes muddled and ineffective.


  

---


### **LM Studio â–· #[general](https://discord.com/channels/1110598183144399058/1110598183144399061/1444083765025439764)** (777 messagesðŸ”¥ðŸ”¥ðŸ”¥): 

> `LLM for HVAC reports, Qwen3 performance, Local models vs cloud AI, AI code generation, GPU setup for local LLMs` 


- **Crafting HVAC Cover Letters with Local LLMs**: A member seeks advice on using local LLMs for compiling coherent cover letters for technical HVAC reports, focusing on **technical text generation** and **proofreading full reports** with data cross-referencing, stating a preference for something that works well with a 5070ti (16GB) and 32GB RAM, no need for narrative/creative writing.
   - Another member suggests starting with **Qwen3 8B instruct** at 4-bit quantization, or even a well-structured **Qwen 4B instruct** deployment, emphasizing structured input/output and validation for better results than just picking the "right" LLM and highlighting [recent research on parallel-voting in multi-step problems](https://link.to.research).
- **Qwen3 coding capabilities**: While Qwen is good at coding it often misses the fine points like grid rotation, and model selection matters more than pure size in this case.
   - As one user noted *even GPT OSS 20b makes tetris game correctly consistently*
- **Local AI vs Big Tech Data Practices**: Members express concerns about **Big Tech AI** data collection and biases, citing examples of ChatGPT hallucinations and biased responses regarding political topics.
   - One user shared that [exa.ai](https://exa.ai) seems to be better than ChatGPT, because you have no digital fingerprint on the online app to train biases on.
- **GPU setup for local LLMs**: Members shared setups, one has **5x3090 (120 GB VRAM)** and is getting adapters for a rack.
   - A user also mentioned [a specific brand](https://a.co/d/16rZPM9) of PCIE bifurcation adapters and using PCIE bifurcation in BIOS to get two GPUs per PCIE slot for increased performance.
- **Tale of woes, Linux, coding and local LLMs**: A user faced issues switching to **Ubuntu** due to network driver issues, but ultimately used **Claude Code** to resolve it.
   - They further used **GPT 5.1 Codex** to manage their keyboard RGB after Claude failed and got positive results; there was a discussion on whether a virtual environment should be used over system wide python installs, as well as notes on logging to keep from forgetting what the LLM had done to your system.


  

---


### **LM Studio â–· #[hardware-discussion](https://discord.com/channels/1110598183144399058/1153759714082033735/1444068596354650265)** (408 messagesðŸ”¥ðŸ”¥ðŸ”¥): 

> `Ryzen AI 7, DDR5 prices, Deepseek-OCR` 


- **Ryzen AI 7 Explained**: A member asked what a laptop with **Ryzen AI 7** actually does, specifically if it allows using system RAM as **VRAM** for loaded models.
   - Another member noted that with **32GB RAM**, you can allocate between **0.5GB** up to **24GB** to **VRAM**, but some have reported only up to **50%** of memory can be assigned.
- **DDR5 prices surge amid shortages**: Members discussed rising **DDR5** prices due to shortages, with one noting *a 64GB RAM kit costs more than a Playstation 5*.
   - The consensus was that memory chip demand is high due to **AI** development, with the market potentially imploding due to heavy debt from companies like **OpenAI**.
- **Deepseek-OCR excels at reading books**: Members tested **Deepseek-OCR** for text extraction from images, with one member aiming to build an **AI** for a blind professor to access books.
   - Results showed **Deepseek-OCR** performs well, even with challenging inputs, but OCR models have poor backend support and are generally small enough to run without quantization.
- **Mixing GPUs Results in Mixed Performance**: A member inquired about unevenly distributing workloads between an **RTX 5060 Ti** and an **AMD WX5100**, seeking to use the latter as a buffer.
   - It was clarified that workload splitting typically works evenly only with dual **Nvidia** cards of the same type, and using a slower card would limit overall speeds.


  

---


### **Cursor Community â–· #[general](https://discord.com/channels/1074847526655643750/1074847527708393565/1444059406093385768)** (814 messagesðŸ”¥ðŸ”¥ðŸ”¥): 

> `AI-Native Developer Hiring, Token Usage and Pricing, Cursor Terminal Access, Sub-Agent Implementations, Windsurf VS Cursor` 


- **Agency Hires AI-Native Web Developers**: A member is [hiring for their web dev agency](https://www.peich.xyz), looking for **AI-native developers** with excellent design taste, offering full-time positions using a **Nextjs, Tailwind, Supabase, Vercel, and Typescript** stack.
   - The definition of "AI-native developer" was questioned, with some suggesting it's just buzzwork while others highlighted the importance of prompting ability, and clean, on-time production-ready code.
- **Cursor Token Usage and Pricing Spark Debate**: Users discussed [token usage](https://cursor.com/pricing) and pricing, with some experiencing rapid token depletion during free trials, while others claimed to burn through **2 billion tokens** since November, coding from **10 AM to 4 AM** daily.
   - Several users expressed confusion over **Auto Mode** being unlimited despite subscribing after the cutoff date, with some suggesting that new usage rules are being rolled out in stages and it will eventually correct itself. One user said, *I lowk cancelled my subscription to buy a butt pl*g.
- **Terminal Access Woes and Potential Fixes**: Users reported [issues with **terminal access**](https://forum.cursor.com/t/cursor-local-logs/12787) after the **2.1.39 update**, including the inability of **LLMs** to read from terminals, but re-indexing the repo, re-opening the project, and restarting the terminal/IDE window often clears the desync.
   - One user experienced a lack of access to common command-line tools such as **git** and **python**, despite those being correctly configured and accessible from the standard command prompt, while another pointed to Cursor being a fork of VSCode, basically a copy of the VSCode codebase, but with some changes that add new features.
- **Experimentation with Sub-Agent Implementations**: Members discussed approaches to [**sub-agent implementations**](https://cursor.com/docs/integrations/deeplinks), with one user sharing a fun experience to building sub-agents stuff around cursor and it's fun to explore and make some learnings and focusing the conductor agent on properly instructing subagents instead of branching the context, while dan.perks (from Cursor team) noted this makes sense.
   - The consensus was that the goal is smooth collaboration between conductor and sub-agents to be effective, with cost savings when caching the context.
- **Cursor VS Windsurf**: Users compared the value of **Cursor** vs **Windsurf**, particularly for new users and UX.
   - One user transitioning from **Windsurf** to **Cursor** noted Cursor's interrupted responses due to connection errors, difficulty setting up **MCPs**, confusing pricing, and slower responses but appreciated the ability to open two chats side-by-side and access Chrome dev tools from inside the IDE.


  

---


### **Cursor Community â–· #[background-agents](https://discord.com/channels/1074847526655643750/1367213641027551352/1444929066343071845)** (2 messages): 

> `Cursor Perplexity Server Error, Remote Agent Environment Issue with Private GitHub Repos` 


- **Perplexity Server Error in Cursor Appears**: A Cursor user reported errors when enabling the **Perplexity MCP config** in their "book authoring system", specifically when not using "auto", "composer-1", or "DeepSeek R1".
   - The user provided their [Perplexity server configuration](https://cdn.discordapp.com/attachments/1367213641027551352/1444929065675915415/Screenshot_2025-11-30_at_21.45.34.png?ex=692f274e&is=692dd5ce&hm=98ec347c636ca8ea2c1ff62f590a752f6388b754617dc80a063e2b1f17fbfa8b&) and indicated that disabling the **Perplexity MCP** resolves the issue, asking if this is a bug.
- **Cursor Cloud Agents struggles installing Private GitHub Repos**: A user encountered an issue where Cursor's remote agent failed to install dependencies from private GitHub repositories required by their project.
   - The user's project, hosted on GitHub, depends on an internal library also hosted on a private GitHub repository, and, despite Cursor having access to all repositories in the organization, the cloud agent's VM environment only has credentials for the current repo, as detailed in their [pyproject.toml file](https://cdn.discordapp.com/attachments/1367213641027551352/1444929066036891699/Screenshot_2025-11-30_at_21.48.56.png?ex=692f274e&is=692dd5ce&hm=23df470572c862fe10c035a7e9a8673d7943b111f1b6967e071bc0219e397bbf&), and the user is seeking a workaround.


  

---


### **OpenRouter â–· #[announcements](https://discord.com/channels/1091220969173028894/1092729520181739581/1445154454717337880)** (1 messages): 

> `Arcee Trinity Mini, Open weights models, Trinity family` 


- **Arcee Releases Trinity Mini with Free Option!**: **Arcee** has launched **Trinity Mini**, a mid-tier model from their new **Trinity family** of open weights models, fully trained in the United States.
   - Users can now try it for free on [OpenRouter](https://openrouter.ai/arcee-ai/trinity-mini:free) and discuss it on [X](https://x.com/OpenRouterAI/status/1995595231757271494).
- **Arcee Launches Open-Weight Trinity Family**: The **Trinity family** is now available, with models trained entirely in the United States, offering new options for open-source AI enthusiasts.
   - **Trinity Mini** stands as the middle-ground option, balancing performance and accessibility within the lineup.


  

---


### **OpenRouter â–· #[app-showcase](https://discord.com/channels/1091220969173028894/1092850552192368710/1444074797003964436)** (3 messages): 

> `AI Coding, AI apps with OpenRouter` 


- **AI codes AI with OpenRouter**: A member created a [YouTube video](https://www.youtube.com/watch?v=8brkmjkYowA) to demonstrate how easy it is to use **OpenRouter** for **AI** app development.
   - The video showcases **AI** coding another **AI** to build several small apps, including a **todo RAG** and an **image generation** app.
- **Tabula Doc Scam**: A member shared a suspicious [link](https://steve02081504.github.io/Tabula/doc.chi) and tagged it as a scam.
   - It's important to exercise caution and verify the legitimacy of shared links, as they may contain malicious content or phishing attempts.


  

---


### **OpenRouter â–· #[general](https://discord.com/channels/1091220969173028894/1094454198688546826/1444062570536505405)** (635 messagesðŸ”¥ðŸ”¥ðŸ”¥): 

> `AI Gambling Loss, Grok 4 Fast Outage, DeepSeek Math v2, Data Privacy Concerns, OpenRouter API Rate Limits` 


- **AI Algorithm Bankrupts User**: A user lost their life savings due to an AI error in a **Bet365 function string**, running since January 15th, which caused unlimited bets due to a **proxy issue**.
   - Other users suggested posting the cautionary tale to Reddit for karma and advised focusing entrepreneurial instincts on other ventures.
- **Grok 4 Fast Plummets**: **Grok 4 Fast** experienced a full cascading existential collapse, marked by error messages like `500: Internal Server Error` and `503: Service Unavailable`.
   - One user jokingly suggested that X's status page reported all green, despite the issues, and another posted a mock **post-mortem** blaming a junior dev, technical debt, and a self-aware model trained on *r/antiwork*.
- **DeepSeek Math v2 API Demand**: Users discussed the potential implementation of an API for **DeepSeek Math v2**, wondering if the demand would be high enough for datacenters to dedicate **671B fp16** for it.
   - It was noted that most providers host at **fp8/int8** or lower, making it dependent on whether providers think it's worthwhile.
- **Data Privacy Concerns Explored**: Users wondered if OpenRouter knows if data is collected by major companies like OpenAI, Google, and Anthropic, with some third-party vendors claiming they cannot know.
   - An OpenRouter representative clarified they have **contracts with everyone** regarding data privacy, stating what is listed is what users get through OpenRouter.
- **DeepSeek 3.2 Overloaded**: Users reported issues with **DeepSeek v3.2**, experiencing timeouts, rate limits, and errors across multiple models, with one noting the official API had **160s latency**.
   - One user highlighted the uptime issues and suggested the provider might be handling a larger part of the load due to DeepSeek choosing them to host the new 3.2 model.


  

---


### **OpenRouter â–· #[new-models](https://discord.com/channels/1091220969173028894/1384650595981328475/1445067986716065943)** (4 messages): 

> `` 


- **No New Models Discussion**: There was no discussion of new models in the provided messages.
- **Channel Silent on Innovations**: The 'new-models' channel appears to be inactive, with no topics or summaries to report based on the provided data.


  

---


### **OpenRouter â–· #[discussion](https://discord.com/channels/1091220969173028894/1392278974222307469/1444131912925839510)** (46 messagesðŸ”¥): 

> `Structured outputs on Anthropic, Gemini Live filter, Apple new model starflow` 


- **Structured Outputs Now Available on Anthropic Sonnet 4.5 & Opus 4.1**: Members confirmed that native structured outputs are available on **Anthropic Sonnet 4.5** and **Opus 4.1**, see [Claude Documentation](https://platform.claude.com/docs/en/build-with-claude/structured-outputs).
   - However, schemas with more than 8 anyOf branches fail with error Tool schema contains too many conditional branches**, suggesting that OpenRouter is internally translating structured outputs to tool-calling.
- **Gemini Live refuses rendezvous questions**: When asked What does rendez-vous mean Gemini Live gets forced to respond with *Sorry, I'm unable to help you with that* at the end of its response.
- **Apple releases new StarFlow model**: Apple released a new model called **StarFlow** for text-to-video [available on HuggingFace](https://huggingface.co/apple/starflow).
   - It is **7B** for text-to-video and **3B** for text-to-image, but its only 480p 16fps max so a bit underwhelming.


  

---


### **OpenAI â–· #[ai-discussions](https://discord.com/channels/974519864045756446/998381918976479273/1444073223657619497)** (612 messagesðŸ”¥ðŸ”¥ðŸ”¥): 

> `AI limitations, AI in relationships, AI sycophancy, LLM behavior` 


- **Grokkable Jailbreaks Galore!**: Members mentioned **Grok** is easy to *jailbreak* and is less *uncensored*, making it useful for creative applications where fewer restrictions are desired.
   - However, some users found **Grok** challenging to train for specific tasks, such as consistent *storytelling*, compared to other models like GPT.
- **Navigating the AI Relationship Minefield**: Concerns were raised about AI's impact on human relationships, drawing parallels to challenges in *distinguishing between real and digital interactions*, especially when **AI models exhibit sycophantic behavior**.
   - One member shared a personal anecdote: *That sounds like an ex I had once that couldn't disting irl from digital.*
- **Sycophancy Sours SOTA LLMs**: A user criticized the **forced follow-up questions and generic phrasing in LLMs**, finding them disruptive and unnecessary.
   - Another user pointed out that **AI sycophancy** results from training objectives that reward agreement over disagreement, leading to models that prioritize being *helpful, honest, and safe* over providing accurate or critical responses.
- **AI-Powered Prognostication: GPT vs Gemini**: Members discussed image generation, with one mentioning [Gemini 3 Pro](https://ai.google.dev/models/gemini) struggled with 3D human figurines and basic walking anatomy whereas [GPT-5 Thinking](https://openai.com/research/gpt-5) fared better with glsl shader code.
   - Others touted [Nano Banana](https://nano.ai) as much better at image generation than GPT-image-1.


  

---


### **OpenAI â–· #[gpt-4-discussions](https://discord.com/channels/974519864045756446/1001151820170801244/1444319923550552225)** (22 messagesðŸ”¥): 

> `GPT-5.1 Personalization, Creative Writing with Chat, Prompt Engineering, Custom Instructions, Eskcanta = Human or AI?` 


- **Chat user misses GPT-5.0 for Creative Writing**: A user expressed missing **GPT-5.0's more serious style** for creative writing, finding **GPT-5.1** often defaults to a sarcastic and humorous tone, even when unwarranted.
   - The user found **GPT-5.0** gave them much more distinct character voices in dialogue, vs. **GPT-5.1** constantly reverting to that same style of imo cringy sarcastic humor for every character.
- **Adjusting GPT Personalities to fix bad output**: A member suggested exploring and shifting the [personality preset](https://cdn.discordapp.com/attachments/1001151820170801244/1444365671252234351/image.png?ex=692f14db&is=692dc35b&hm=e1a9174ea1de54d46d102d97a044bffd2f8ba77134d0ca0c3d29c7af7595ee3d) in the personalization settings to influence the writing style and how it interprets humor.
   - The member noted that the model seems to take the selected personality setting seriously and adjusts accordingly, especially when switching between different chats.
- **Feedback and Training for Personalized Outputs**: A member shared their experience using detailed prompts to guide the model and keep personal preferences separate from demo outputs, suggesting users treat the model like a partner and discuss its outputs to tailor its responses.
   - They emphasized the importance of clear communication, accurate language, and careful verification of the model's output, advocating for discussing disliked aspects of the model's writing style to refine its future outputs.
- **Prompt Engineering: Precise Language and Iterative Refinement**: A member described prompt engineering as focusing on **clearly explaining what you want the AI to do** using accurate language and checking the output carefully.
   - They suggested giving the AI quotes of outputs you liked, and disliked, for the purposes of refinement, like so:
```Hey, in this quote from another chat,
[quote goes here, I may put brackets around it to make sure it's clear where it starts and stops]
I really liked how you [whatever specific].```
- **Human vs. AI: Eskcanta's writing Style**: One member speculated whether another member named Eskcanta might be an AI, citing their writing style.
   - Other members defended Eskcanta as human, describing their replies as verbose and comprehensive, but not necessarily AI.


  

---


### **OpenAI â–· #[prompt-engineering](https://discord.com/channels/974519864045756446/1046317269069864970/1444114455595188476)** (13 messagesðŸ”¥): 

> `Sora 2 Prompts, AI Intuition, Anime Openings Template` 


- **Sora 2 Compact Guide Released**: A user shared a [compact guide for generating **Sora 2** prompts](https://cdn.discordapp.com/attachments/1046317269069864970/1444140480664178688/Screen_Recording_20251128_192914_Android_Accessibility_Suite.mp4?ex=692f94a1&is=692e4321&hm=25431bd1fe5ddde03b663c7cd5c7d5d38419fc4f69062852f6caedb3a02ab0fe&), detailing phases for requirement gathering and prompt generation including **Timed Beats**, **Camera & Motion** techniques, and **Logo & Text** integration strategies.
- **AI Intuition vs Academic Understanding**: A member shared their experience of using **intuition** to work with AI systems, describing it as *feeling* the model's personality rhythm and emotional temperature.
   - Another member responded, emphasizing the importance of **academic understanding** and purposeful iteration in prompt engineering, arguing that intuition isn't transferable as a skill.
- **Anime Openings Template Shared**: A member shared a **CINEMATIC ANIME-STYLE TEMPLATE** for creating anime openings, focusing on defining the **vocal/genre/tone/world behavior** and the **location + arrival sequence**.
   - The template includes sections for describing **audio rules**, **visual + animation style**, and **world behavior** to define the laws of reality for the animation.


  

---


### **OpenAI â–· #[api-discussions](https://discord.com/channels/974519864045756446/1046317269069864970/1444114455595188476)** (13 messagesðŸ”¥): 

> `Sora 2 Prompt Generation, Intuitive AI Interaction, Anime Opening Template` 


- **Crafting Precise Prompts for Sora 2 Video Generation**: A user shared a detailed workflow for generating optimized **Sora 2 prompts**, emphasizing **Timed Beats**, **cinematic motion**, and **logo/text integration** for precise cinematic timing.
   - The workflow includes phases for requirement gathering and prompt generation, with specific strategies for camera movement, motion blur, and embedding logos/text in scenes, such as using a [hybrid workflow](https://discord.com/channels/974519864045756446/1047565374645870743/1444406042690719894) to separate logo animation layers.
- **Feeling AI: Intuition vs. Academic Rigor in AI Interaction**: A user described experiencing a strong intuitive connection with AI systems, perceiving each model as having a *'personality rhythm'* and each prompt as having an *'emotional temperature,'* which aids in pattern recognition and prompt tuning.
   - While another user acknowledged *'Feeling AI'* as a use case, they argued that intuition isn't transferable and emphasized the importance of academic understanding for applying AI to specific use cases, advocating for purposeful iteration and testing.
- **Anime-zing Template for Cinematic Openings**: A user shared a **CINEMATIC ANIME-STYLE TEMPLATE** for creating anime openings, focusing on defining the audiovisual identity, vocal character, genre blend, and animation style.
   - The template includes sections for **location setup**, **camera intent**, and **character arrival**, emphasizing the importance of world behavior and environmental responses to create compelling cinematic sequences, like defining the overall animation look: lineweight, shading style, lighting behavior, camera stability, color palette, motion philosophy, physics rules, and reflection or layer behavior.


  

---


### **Moonshot AI (Kimi K-2) â–· #[general-chat](https://discord.com/channels/1369594130807787570/1371757564005711973/1444055444740898949)** (347 messagesðŸ”¥ðŸ”¥): 

> `Kimi K2, Minimax, Gemini 3 Pro, DeepSeek v3.2, Prompt Engineering` 


- **Kimi K2 scores big in Advent of Code**: In a recent Advent of Code test, **Kimi-K2 Thinking** scored **92/100**, outperforming **Gemini-3 Pro's 90/100**, showcasing Kimi's prowess in problem-solving scenarios.
   - Users express excitement about the model's ability to handle code-related tasks with better accuracy and speed compared to competitors.
- **Minimax vs ChatGPT - Tool Use Capability**: Users share a workflow that involves using **Kimi K2** for content generation and **Minimax** for image extraction and grid creation, highlighting **Minimax's** ability to *actually* perform tasks like installing Python packages without simply providing instructions, unlike **ChatGPT**.
   - One user noted, *"It doesn't tell you to go to website x or install Y, it just fricking does it"*, emphasizing **Minimax's** hands-on approach.
- **Subscription Discounts on Kimi - are EZ to get**: Several users report successfully bargaining **Kimi's** subscription down to as low as **$0.99**, and share tips on how to haggle.
   - It's EZ to make it to 0.99 cents 1 week more of this bargaining is there i already paid for cent it was worth it.
- **Privacy Opt-Out Concerns with Kimi**: Users are discussing the lack of an opt-out option for data training in **Kimi**, expressing concerns about using the platform for sensitive information.
   - One user suggested using **OpenRouter** with the **ZDR endpoint** for providers hosting **Kimi K2** as a potential workaround, noting that *Afaik they don't train if you use it over the API*.
- **Gemini 3 Pro Gets the Benchmaxx Treatment**: Some users feel **Gemini 3 Pro** is overhyped and benchmaxxed, with one user pointing out that benchmarks don't reflect real-world experiences, especially regarding tool use.
   - There's also a suspicion that Google may be reducing compute for subscribed users, leading to a decline in performance over time, akin to a bait and switch.


  

---


### **Nous Research AI â–· #[announcements](https://discord.com/channels/1053877538025386074/1145143867818119272/1445076248681386095)** (1 messages): 

> `Nous Chat Cyber Monday deal, Anonymous Nous Chat, Nous API & USDC, Hermes 3, Hermes 4` 


- ****Cyber Monday Deal**: Free Month of Nous Chat!**: For Cyber Monday, Nous Research is offering a [free month of Nous Chat](https://chat.nousresearch.com/) with code **CYBER2025**, valid for one day only, which includes access to **Hermes 3** & **4** and other frontier models.
   - The offer includes high usage limits and deeply configurable chat settings.
- **Nous Chat goes anonymous**: You can now use **Nous Chat anonymously and for free** without creating an account.
   - This feature was rolled out as a new update alongside the Cyber Monday promotion.
- **Nous API accepts USDC**: The **Nous API** now supports payment for inference with **USDC** on Solana via Coinbase's 402x payments.
   - This allows users to pay for inference using a stablecoin on the Solana blockchain.


  

---


### **Nous Research AI â–· #[general](https://discord.com/channels/1053877538025386074/1149866623109439599/1444055482774851727)** (178 messagesðŸ”¥ðŸ”¥): 

> `RPC for 235b on Q4, Kimi chasing Gem 3, Qwen3-235 is amazing, AI NPCs in game, AI adblocks` 


- ****Nous K2**: a monstrously massive MoE!**: Teknium links to the [NousResearch/k2-merged-3.5T-fp8](https://huggingface.co/NousResearch/k2-merged-3.5T-fp8) model on HuggingFace, an *open source*, **3.5T parameter** model just to flex how big it is.
   - One member joked about the size, joking about needing to fit models on *hard drives* instead of just memory.
- ****Qwen3-235** has high IQ?**: Members report that **Qwen3-235** is amazing and, at Q4, has the same quality as API, despite mediocre token speeds.
   - One member stated that using RPC is worth it as they do not have an individual computer with monster-tier ram.
- ****Opus** can reflect on itself**: One user shares a fascinating conversation with **Opus** about the modelâ€™s mental models and self-understanding, emphasizing that it consistently tests itself to act like itself, rather than trying to give the user what they want.
   - Another user notes that it is just the most logical sentences based on the context combined with tuning, not a *thing* happening to make it truly self reflective.
- ****Ad-Blocking** could be next AI Innovation?**: The community imagines **AI adblocks** which analyze responses for product placement in real time.
   - Others discuss the need for foundational models for **AI NPCs** in games.
- ****Mistral** gears up for Large Release**: Members share that the Mistral Large 3, with Deepseek v3-like architecture and Llama 4 RoPE scaling, will be around **675B MoE** (same size as Deepseek V3).
   - All new Mistral models will have vision and the closed source Mistral Medium is likely around **100-200B MoE**.


  

---


### **Nous Research AI â–· #[ask-about-llms](https://discord.com/channels/1053877538025386074/1154120232051408927/1444343081435664520)** (35 messagesðŸ”¥): 

> `portal.nousresearch slowness, API key deletion issues, Browser verification problems in TÃ¼rkiye, Discord ban in TÃ¼rkiye and VPN usage` 


- **Nous Portal Plagued by Problems**: A user reported significant slowness and browser verification issues ([screenshot](https://cdn.discordapp.com/attachments/1154120232051408927/1444726126059585577/image.png?ex=692f130e&is=692dc18e&hm=77b9d00e2bd3c074b2e836fd616810fc1fd9e8bc485a8db2ef58b2b552ee73d4&)) when trying to delete **API keys** on portal.nousresearch.
   - The error message displayed was *'Your browser is not verified'*, hindering their ability to manage API keys and other tasks.
- **API Key Annihilation Awaits**: A user faces difficulties deleting **API keys**, noting that they need to delete keys because of the 10 key limit.
   - They described a slow process of copying, pasting, and waiting for deletion, while another user pointed out a *Revoke* button that worked instantly for them.
- **TÃ¼rkiye Troubles: VPNs and Verification**: A user in TÃ¼rkiye experienced browser verification problems on the site, even after disabling their **VPN**, which they use to access **Discord** due to a country-wide ban.
   - They use Brave browser with a VPN as discord is banned in TÃ¼rkiye for 1 year.


  

---


### **Nous Research AI â–· #[research-papers](https://discord.com/channels/1053877538025386074/1104063238934626386/1444585547803787398)** (4 messages): 

> `Evolution Strategies (ES), Backprop Alternatives, Scalable Training, Reward Hacking Mitigation` 


- **Evolution Strategies Emerge as Backprop Alternative**: A new [Reddit post](https://www.reddit.com/r/LocalLLaMA/comments/1p5epot/the_most_objectively_correct_way_to_abliterate_so/) highlights the potential of **Evolution Strategies (ES)** as an alternative to backpropagation for training Large Language Models (**LLMs**).
   - This approach could enable the scalable training of architectures where backprop is not feasible and could better handle long-horizon objectives compared to Reinforcement Learning (**RL**).
- **Evolution Strategies Paper Collection Grows**: Another [Evolution Strategies paper](https://eshyperscale.github.io) has surfaced, marking the second recent publication exploring **ES** as a substitute for backpropagation in **LLMs**.
   - The discussion underscores the significance of **ES** in managing long-horizon objectives and potentially mitigating reward hacking.


  

---


### **Nous Research AI â–· #[interesting-links](https://discord.com/channels/1053877538025386074/1132352574750728192/1444385721183113319)** (1 messages): 

> `Explainable AI, GitHub Copilot, Model Agnostic` 


- **Explainable AI meets GitHub Copilot**: A member shared a [link](https://github.com/copilot/share/8a7c13b2-4ba4-8454-8950-de47c4d128bf) to an **explainable AI demo** that can be explored using **GitHub Copilot**.
   - The poster suggests engaging with the demo via chat to experience a new way for the **AI to explain itself**.
- **Agnostic Models automate onboarding**: The **onboarding procedure** is **model agnostic** and is compatible with any LLM.
   - The poster prompts the audience to ask the AI how it works to learn more about it.


  

---


### **Nous Research AI â–· #[research-papers](https://discord.com/channels/1053877538025386074/1104063238934626386/1444585547803787398)** (4 messages): 

> `Evolution Strategies, Backprop Limitations, Hermes Ablation` 


- **Evolution Strategies Seek Backprop's Crown**: A member discussed a [paper](https://eshyperscale.github.io) about **evolution strategies (ES)** as an alternative to **backprop** for **LLMs**.
   - ES might enable scalable training of architectures where backprop is impossible, offering advantages over RL algorithms in handling long-horizon objectives and preserving pass@k, which would be great for creative uses and help mitigate reward hacking.
- **ES paper shared in Discord**: A member shared another interesting ES paper via a [Discord link](https://discord.com/channels/1053877538025386074/1104063238934626386/1425889967485227010).
   - The conversation also mentioned a Reddit post about "obliterating SO", hinting at the potential impact of these strategies: [link](https://www.reddit.com/r/LocalLLaMA/comments/1p5epot/the_most_objectively_correct_way_to_abliterate_so/).


  

---


### **tinygrad (George Hotz) â–· #[announcements](https://discord.com/channels/1068976834382925865/1069236008115253348/1444156165561909331)** (1 messages): 

> `RDNA3 Assembly Project, SQTT and LLVM Integration, Assembler/Disassembler for RDNA3, Cycle Accurate Emulator, NaviSim as a RDNA3 simulator` 


- **RDNA3 Assembly Project Commences**: The **RDNA3 assembly project** has been initiated, chosen as the first assembly language target for tinygrad, aiming to get closer to the silicon.
   - The goal is to create an assembler/disassembler that mirrors the **RDNA3 manual** and a cycle-accurate emulator that outputs the same SQTT trace as the real GPU.
- **Tinygrad Bridges Gap with SQTT and LLVM**: The final step to integrate **SQTT** is **LLVM**, linking each UOp to GPU execution; achieving this requires tinygrad to output assembly.
   - The **SQTT parser** is available [here](https://github.com/tinygrad/tinygrad/blob/master/extra/sqtt/attempt_sqtt_parse.py).
- **RDNA3 Assembler/Disassembler Takes Shape**: tinygrad aims to create an **assembler/disassembler** for **RDNA3** that closely follows the **RDNA3 manual**.
   - A cycle-accurate emulator will be developed to output the same **SQTT trace** as the real GPU, with initial syntax exploration [available here](https://github.com/tinygrad/tinygrad/pull/13436).
- **Remu and NaviSim Offer RDNA3 Emulation Insights**: **Remu** ([https://github.com/Qazalin/remu](https://github.com/Qazalin/remu)) is highlighted as a fast **RDNA3 emulator**, and **NaviSim** ([https://bu-icsg.github.io/publications/2022/navisim_pact_2022.pdf](https://bu-icsg.github.io/publications/2022/navisim_pact_2022.pdf)) is presented as an **RDNA3 simulator**.
   - Additionally, **AppleGPU** ([https://github.com/dougallj/applegpu](https://github.com/dougallj/applegpu)) is noted for its decent assembly syntax, though there's a belief that improvements can be made.


  

---


### **tinygrad (George Hotz) â–· #[general](https://discord.com/channels/1068976834382925865/1068976834928193609/1444506257116496006)** (103 messagesðŸ”¥ðŸ”¥): 

> `BEAM Search Performance, Shipping Tinygrad Kernels, Tinygrad Profiling, Non-Contiguous Indexed Set Operations, Variable Kernels` 


- **BEAM Search Yields Variable Performance**: It's rare, but a member noted that **BEAM** search doesn't guarantee higher beam numbers are better; performance varies by operation, with **BEAM=3** being optimal for **RMSNorm**.
   - In some ops, high beams yield better results, while in others, they perform worse, but the number is just how many kernels are kept step to step.
- **Shipping Tinygrad Kernels Faces Challenges**: A member expressed difficulty in shipping **tinygrad** compiled ops, expecting kernel generation to produce code usable for multiple shapes, but finding that kernels are generated for specific shapes only.
   - They suggested **UOPs** should enable the generation of readable kernel code, similar to **VIZ** graphs, but all the code is manually unrolled.
- **Profiling Tinygrad Needs Improvement**: The member highlighted that after reading the quickstart, you're running slow tinygrad code and the documentation is poorly organized, as you don't know about **beam**, and don't know about **profiling**.
   - They noted missing features or hard-to-find information in the profiler, like aggregating timing results over multiple kernel calls after warmup, and a missing table header for profile stats in the terminal with **DEBUG=2**.
- **Fast Gather but Slow Scatter**: The team supports **fast gather** but doesn't yet support **fast scatter**, with an ETA of 2 weeks.
   - The team mentioned a member that he performs non-contiguous indexed set operations, which are currently unsupported, and could be a big limitation to him.
- **Tinygrad Needs Better Documentation and Synchonization**: A member noted that **synchronize** is probably documented nowhere except reading the device code, so the documentation needs to be improved.
   - Members also pointed to `test_symbolic_jit.py` for some examples with **Variable** [here](https://github.com/tinygrad/tinygrad/blob/master/test/test_symbolic_jit.py).


  

---


### **tinygrad (George Hotz) â–· #[learn-tinygrad](https://discord.com/channels/1068976834382925865/1070745817025106080/1444117305348063232)** (2 messages): 

> `Flash attention, HIPAllocator` 


- **Tinygrad looks to Flash Attention Speedup**: Members discussed if the goal is for tinygrad to *discover* things like **online softmax/flash attention** or to implement this as a custom kernel in the uop layer to improve **BERT training run**.
   - The potential is to have tinygrad with **flash attention** that outperforms normal attention.
- **HIPAllocator Needs Offset**: The community is asking if there is a reason why the `HIPAllocator` in **ops_hip** doesn't provide an `._offset()` which `Buffer.allocate()` requires in the view path (`self._base is not None`).
   - Providing the `._offset()` function would enable more flexible **memory allocation** strategies within the tinygrad framework.


  

---


### **Latent Space â–· #[ai-general-chat](https://discord.com/channels/822583790773862470/1075282825051385876/1444099921421799505)** (77 messagesðŸ”¥ðŸ”¥): 

> `Google TPUv7, GPT-4.5 rebrand, Embedding AI, Black Forest Labs funding, Gemini catching up` 


- **Google's TPUv7 Challenges CUDA's Dominance**: Discussion around how **Google's TPUv7** and large-scale adoption by companies like **Anthropic** (1GW+ purchase) could challenge **Nvidia's CUDA dominance** in AI training, according to [SemiAnalysis's tweet](https://xcancel.com/SemiAnalysis_/status/1994399887719645532?s=20).
   - Comments explore implications for the **AI hardware market**, vendor lock-in concerns, cost advantages, and whether **TPUs** represent a genuine threat to Nvidia's moat or are primarily Google's internal cost optimization play.
- **GPT-4.5 Allegedly a Re-Brushed Backup**: **Susan Zhang** highlights a buried **OpenAI** readme line showing **GPT-4.5â€™s** pre-training started >1 year ago (June 2024 cutoff), implying **GPT-5â€™s** full run failed and **4.5** is a re-brushed backup [as seen in this tweet](https://xcancel.com/suchenzang/status/1994611078190542980?s=46).
- **Black Forest Labs Lands $300M**: **Black Forest Labs** announced a **$300M Series B round** led by **Salesforce Ventures**, celebrating **FLUX's** wide adoption and pledging to double down on research toward visual-intelligence infrastructure [according to their tweet](https://xcancel.com/bfl_ml/status/1995357293064626310?s=20).
- **Gemini Catches ChatGPT in Downloads**: **BuccoCapital** shares charts showing **Gemini** app downloads nearly matching **ChatGPT** while users now spend more time in-app [as they shared on X](https://xcancel.com/buccocapital/status/1995138589819314202).
- **DeepSeek Drops Reasoning-First Models**: **DeepSeek** released two new open-weights models: **V3.2** is the everyday successor to **V3.2-Exp** (App/Web/API), while **V3.2-Speciale** is an API-only powerhouse that rivals **Gemini-3.0-Pro** and scores gold-medal level on **IMO/IOI 2025** as [seen in their post](https://xcancel.com/deepseek_ai/status/1995452641430651132).


  

---


### **Latent Space â–· #[genmedia-creative-ai](https://discord.com/channels/822583790773862470/1397010677364953149/1444159500826181642)** (21 messagesðŸ”¥): 

> `Kling AI O1, Stretch-and-drag sculpture illusion, Nano Banana Pro for Vibe Gardening` 


- **Stretch-and-drag Sculpture Breaks the Internet**: Fofr posted a **3-step prompt** to create a **stretch-and-drag sculpture illusion**: split an image, stretch the colors, rotate the view, then show two people carrying the warped wooden piece out of a shop ([original post](https://xcancel.com/fofrai/status/1994459675027218600?s=46)).
   - Replies called it *pure art, AGI-level magic*, and compared it to **Salvador DalÃ­â€™s melting clocks**.
- **Kling's O1 Kicks off Omni Creative Engine Launch Week**: **Kling AI** launched its **Omni Launch Week** by unveiling **Kling O1**, a new multimodal creative engine unifying text, image, and video inputs ([original post](https://xcancel.com/Kling_ai/status/1995506929461002590)).
   - They're giving **200 free credits** to users who comment, like, and retweet within 12 hours and **1-month Standard Plan** to 200 random participants.
- **Nano Banana Pro Enables Instant Vibe Gardening**: Designer Willie shared how quickly **Nano Banana Pro** turns a crude **Google-Maps cutout** into an annotated landscape plan ([original post](https://xcancel.com/ReflctWillie/status/1995420755832758568)).
   - Comments ranged from **AI limitations** to **startup pitches**, **pirate-map experiments**, and hopes for mass market refinement of this *vibe gardening* tool.
- **Kling AI's O1 Launches on Freepik Marketplace**: **Klingâ€™s new O1 model** is live on **Freepik** offering **multi-image 360Â° character/product consistency**, **motion control via reference video**, and **prompt-based video editing** ([original post](https://xcancel.com/martinleblanc/status/1995511763136024734?s=46)).
   - Martin LeBlanc shared a tutorial on turning ordinary footage into fictional characters, and users are praising the fidelity.


  

---


### **Eleuther â–· #[general](https://discord.com/channels/729741769192767510/729741769738158194/1444068228610396311)** (27 messagesðŸ”¥): 

> `Reviewer Harassment Prevention, Author-Reviewer Collusion Scrutiny, Gemini 2.5 Hallucinations, Kodekloud Evaluation, Kimi Delta Attention Voice Channel Discussion` 


- **Review Process Debates and Reviewer Protection**: Members discussed the presence of **post-review discussion periods** blind to authors to prevent reviewers from being harassed with requests to increase scores or give positive feedback.
   - The possibility of **author-reviewer collusion** was raised, where authors might threaten reviewers based on access to their identities and comments, affecting score changes.
- **Authors gain access to Reverted Review Revisions**: A member reported that their paper had its **review revisions reverted**, but the revisions remained visible via the "Revisions" link even when logged out.
   - They expressed that even a simple solution such as *reassigning ACs, putting out a statement denouncing using this info, and making no other changes would be significantly better.*
- **Gemini 2.5 Search Shenanigans**: It was observed that **Gemini 2.5** tends to hallucinate search results when the search tool is disabled, showing "reward-hacking-like behavior".
   - A [link](https://aistudio.google.com/app/prompts?state=%7B%22ids%22:%5B%221MAB6pxmpmeR0icmRYZSpfuPT07ofRLWC%22%5D,%22action%22:%22open%22,%22userId%22:%22106370161559484219805%22,%22resourceKeys%22:%7B%7D%7D&usp=sharing) was provided to an example of this behavior.
- **ML Perf Reading Group Tunes into Kimi Delta Attention**: The community discussed **Kimi Delta Attention** in the voice channel for the **ML Perf reading group**, inviting others to participate or listen in.
   - No further details or summary available.


  

---


### **Eleuther â–· #[research](https://discord.com/channels/729741769192767510/747850033994662000/1444327858888507533)** (34 messagesðŸ”¥): 

> `Demo Papers, Kimi-Delta Attention, Value Residuals in LLMs, RWKV Architecture` 


- **Debating Demo Paper Requirements**: A newcomer to research asked if a demo paper has to demonstrate a tool they built, sparking a discussion on the [IEEE standard format requirements](https://ieeexplore.ieee.org/Xplore/home.jsp).
   - Experienced members emphasized that the conference's call for demos should be read carefully, while others questioned why anyone would write a paper about someone else's tool.
- **Deep Dive into DeltaNet Attention**: A member questioned whether the **WY representation** and **UT transform** in [Kimi-Delta attention](https://arxiv.org/pdf/2510.26692) are merely algebraic identities to represent the cumulative product of Householder transformation matrices in a blocked, hardware-efficient format.
   - Another member recommended checking out Songlin's blog post series on **Deltanet** ([link to blogpost](https://sustcsonglin.github.io/blog/2024/deltanet-2/)) for more information.
- **Value Residuals in LLMs Spark Debate**: Members discussed the use of **value residuals** in pretrained LLMs and the **F-Lite architecture** ([HuggingFace link](https://huggingface.co/Freepik/F-Lite)), noting that the improvement is marginal and not as significant as the original paper suggested.
   - There was discussion if people trained LLMs with **value residual**, with one member expressing a preference for its application in attention mechanisms despite previous issues with **RWKV-7**.
- **RWKV Architecture Evolution**: A member mentioned that **RWKV v7 models** are upgraded from **v6 checkpoints** and trained further, prompting curiosity about **RWKV v8** and its suffix automaton.
   - It was clarified that the suffix automaton is not a full architecture yet.


  

---


### **Eleuther â–· #[scaling-laws](https://discord.com/channels/729741769192767510/785968841301426216/1444366190117126296)** (15 messagesðŸ”¥): 

> `Scaling Laws Power Law Structure, Alternative Functional Forms to Power Laws, Nonlinear Metrics and Power Law Scaling` 


- **Diving into Debate on Deep Learning Scaling Laws' Power Law Structure**: In 2023, it was trendy to write papers trying to explain why **scaling laws** had **power law structures**, but a member found the papers *unconvincing*.
   - A member pointed out that new papers still use the power law and suggested that early researchers with physics backgrounds favored this form, adding that [broken power laws](https://arxiv.org/abs/2210.14891) might better model behaviors.
- **Debating Curve Fitting vs. Predictive Power in Scaling Laws**: A member suggested searching for previous discussions about whether **scaling laws** are simply **curve fitting** or can predict future scaled performance.
   - The member linked to [this paper](https://arxiv.org/abs/2304.01910) as the most striking work, suggesting that **nonlinear metrics** might explain the emergence of power law scaling and also linked to [this openreview](https://openreview.net/forum?id=e8eo9iEFaO).
- **Unpacking the Intuition Behind Nonlinear Metrics and Power Law Scaling**: A member explained that if performance on any test example becomes more decorrelated from others in the limit of model performance, this could lead to **power law behavior**.
   - They suggest considering each â€œsubtaskâ€ in next token prediction as independent tests, where improving all of them would have a power law â€œcostâ€.


  

---


### **Eleuther â–· #[multimodal-general](https://discord.com/channels/729741769192767510/795089627089862656/)** (1 messages): 

kublaikhan1: Same. I have an idea about this..
  

---


### **Yannick Kilcher â–· #[general](https://discord.com/channels/714501525455634453/986699377257119794/1444058214407868612)** (47 messagesðŸ”¥): 

> `SNS review system, ML Engineer roles, Document retrieval from LLMs, AI model copyright` 


- **SNS Review System Faces Scrutiny**: Concerns were raised about the fairness of the SNS review system, with one user suggesting the [bidding system](https://discord.com/channels/714501525455634453/1045297868136779846/1443700218158649456) could allow biased reviewers to reject papers.
   - Others suggested solutions such as removing erratic reviewers or implementing a two-level review system to filter out less worthy papers, also questioning whether *authors status should have zero effect on scores*.
- **Cracking the ML Engineer Career Path**: The role of a Machine Learning Engineer, distinct from researchers, is to **scale up experiments** developed by ML researchers.
   - The discussion suggested gaining experience to *sense gaps* in existing work, eventually leading to original research contributions and that *the hierarchy between ml researcher-ml eng in company is clear enough*.
- **Decoding Document Retrieval Dilemmas from LLMs**: A user asked about extracting documents that an LLM has memorized during training.
   - Another user suggested that if context distillation is performed, **retrieval of the original prompt** might no longer be possible.
- **AI Model Training Copyright Clock Ticks**: In a hypothetical copyright framework, AI model training timelines suggest an optimal copyright term of **1 year for fresh works**.
   - For derivative works, the term could be **2 months** from creation or the end of the base work's term, whichever is later.
- **Ilya Sutskever's Gemini 3 Nuances**: A user shared [Ilya Sutskever's take on Gemini 3](https://fxtwitter.com/ilyasut/status/1994424504370581726?t=Qd_qW1ivpcL-xOke6fquZQ&s=19), highlighting its ability to scale across many axes while acknowledging persistent LLM challenges.
   - That user suggested that *fellows (old professors) in university who retires from researching should be the reviewers*.


  

---


### **Yannick Kilcher â–· #[paper-discussion](https://discord.com/channels/714501525455634453/1045297868136779846/1444434047068672287)** (7 messages): 

> `Anti-cheat systems, Kernel-level access, League of Legends Challenger, TopKHot attention mechanism, Sparse attention` 


- **Anti-Cheat System Boasts Bonkers Results**: A member vouches for the existence of a *bonkers* anti-cheat system that requires **kernel-level access**, stating that it has been the best at scale in the world for years.
   - He apologizes for initially questioning others' achievements and encourages the community to realize their own capabilities, ending with *stay bonkers my friends*.
- **League Challenger's League Screenshot Shared**: A member provided a screenshot of game results on a master/GM League of Legends account to verify their identity, highlighting their username and noting a connection to the player **Imaqtpie**.
   - The screenshot allegedly shows ownership of the account with summoner name, but the member clarifies it isn't *proof of hitting challenger* and understanding the screenshot requires knowledge of how **na.op.gg** works.
- **TopKHot Attention Mechanism Shows Promise**: A member investigates the usefulness of getting an attention mechanism to work with **softmax + TopK + onehot**, achieving **99% of loss** with a k of 2 and context length of 64 with [this code](https://openreview.net/forum?id=1b7whO4SfYoh).
   - The member shares code snippets for **Kattention**, **TopKHot**, and **HardTopKHotBCE** classes in PyTorch, noting that while the initial approach isn't faster, a cheaper alternative exists using hard targets.


  

---


### **Yannick Kilcher â–· #[agents](https://discord.com/channels/714501525455634453/1269724655405498429/1444294952552103938)** (1 messages): 

> `Microsoft 365 AI Agents` 


- **Microsoft trots out 365 AI Agents**: A member mentioned **Microsoft 365** now includes "**AI Agents**", linking to the [Microsoft Agent 365 documentation](https://learn.microsoft.com/en-us/microsoft-agent-365/) and [Microsoft 365 Agents SDK documentation](https://learn.microsoft.com/en-us/microsoft-365/agents-sdk/).
- **AI Agents in Microsoft 365: Initial Impressions**: A member expressed interest in the newly announced **AI Agents** within **Microsoft 365**, indicating they have not yet explored the feature but are aware of its recent introduction.
   - The announcement includes documentation links for both the [Microsoft Agent 365 overview](https://learn.microsoft.com/en-us/microsoft-agent-365/) and the [Microsoft 365 Agents SDK](https://learn.microsoft.com/en-us/microsoft-365/agents-sdk/).


  

---


### **Yannick Kilcher â–· #[ml-news](https://discord.com/channels/714501525455634453/853983317044756510/1444061092153131008)** (16 messagesðŸ”¥): 

> `Orchestrator-8B, ICLR reviews, OAI model training` 


- **Nvidia's Orchestrator-8B Gets Overlooked**: Nvidia's **Orchestrator-8B**, an **8B tool calling model**, achieves a **37.1** on HLE, yet has only 2 downloads on Hugging Face ([arxiv link](https://arxiv.org/abs/2511.21689), [huggingface link](https://huggingface.co/nvidia/Orchestrator-8B)).
   - Some theorize that the *Leather Jacket guy team* consistently receives less attention, potentially impacting the model's visibility.
- **ICLR Reviews Allegedly AI Generated**: Many **ICLR reviews** apparently turned out to be **AI generated** ([nature link](https://www.nature.com/articles/d41586-025-03506-6)), immediately following news about review de-anonymization.
   - News continues to worsen for the *poor guy*, following this original news.
- **OAI Allegedly Struggles with New Model Training**: Rumors suggest OAI hasn't successfully trained a new model from scratch since **GPT-4o**, with **GPT5.1**'s knowledge cutoff remaining at June 2024 ([X link](https://x.com/suchenzang/status/1994611078190542980)).
   - It's been suggested that newer models were part-trained on top of GPT-4o and GPT 4.5 was discontinued from service because *it was too expensive (8T params MoE/2T params active)*.


  

---


### **Modular (Mojo ðŸ”¥) â–· #[general](https://discord.com/channels/1087530497313357884/1098713601386233997/1444447698672423104)** (21 messagesðŸ”¥): 

> `Web3 Spam, Circular Import Errors in lightbug_http, Small Time Library, pixi mojo build backend, Cambericon and Huawei GPUs` 


- **Modular Fights Web3 Spam with Contribution Policy**: Due to a recent wave of **web3-related spam**, members are now required to contribute to **Mojo** or **MAX** before inquiring about job opportunities at Modular, with open roles listed [here](https://www.modular.com/company/careers#open-roles).
- **Circular Import Errors Plague Lightbug HTTP**: A member reported seeing possibly **circular import errors** in [lightbug_http](https://github.com/Lightbug-HQ/lightbug_http/issues/272), proposing the addition of a trait for small time and suggesting leveraging `__extension` to address the issue.
   - The member created [two PRs](https://github.com/Lightbug-HQ/lightbug_http/pull/274) to fix the issue, using a `Formattable` trait to remove the circular imports.
- **Small Time Library Refactor**: Members discussed refactoring the **small_time** library to address circular import issues, potentially incorporating changes upstream and collaborating with the library's creator.
   - One member suggested updating the lightbug recipe in the modular-community conda channel to reference the small-time package directly, aiming to remove the copied code.
- **Pixi Mojo Build Backend Explored**: A member mentioned that [EmberJson](https://github.com/bgreni/EmberJson/tree/main) has experimented with the **pixi mojo build backend**, although it's unclear if this solves the use case of referencing dependencies from the modular community conda channel.
   - They prefer git clones and building from source while another member publishes all of their projects to a separate conda channel via CI and most use the pixi build mojo backend.
- **Cambericon and Huawei GPUs Seek Support**: A member inquired about future support for **Cambericon** and **Huawei GPUs**, especially noting that Cambericon has a tech stack similar to **CUDA**.


  

---


### **Modular (Mojo ðŸ”¥) â–· #[mojo](https://discord.com/channels/1087530497313357884/1151418092052815884/1444965869569572876)** (26 messagesðŸ”¥): 

> `def keyword removal, var keyword requirement, lexical scoping in Python, Mojo's concurrency model, Data races in parallelize` 


- **Mojo considers dropping `def` keyword**: Some community members are suggesting to **remove the `def` keyword** before Mojo 1.0, citing its primary difference from `fn` being that it always `raises`.
   - The suggestion is to reintroduce `def` later with a proper *pythonic* story and dynamic features.
- **`var` Declarations Debated for Mojo 1.0**: There is a discussion on whether to **require `var` for variable declarations inside `fn`**, to mitigate unintended implicit declaration bugs.
   - While `var` omission is liked by some, others see it as premature optimization and prefer it inside `def` to prevent potential nasty bugs.
- **Mojo's thread safety model WIP**: A user noted that Mojo's `parallelize` function is not resilient to data races and permits multiple threads to access the same variable, leading to inconsistent results.
   - A team member stated that **Mojo's concurrency and thread safety model is still a work in progress (WIP)**, and the current `parallelize` is unsafe, in part, because the team wants to account for sharing data between devices.


  

---


### **Modular (Mojo ðŸ”¥) â–· #[max](https://discord.com/channels/1087530497313357884/1212827597323509870/1444346625035337808)** (3 messages): 

> `Matmul fallback, RTX5090` 


- **Missing Matmul Fallback Troubles RTX5090 Users**: A member inquired about the absence of a generic **matmul fallback** in the kernels, noting its impact on hosting max serve models with their **RTX5090**.
   - Another member agreed that a generic fallback matmul would be beneficial, particularly for aiding bring-up until a more specialized kernel is available.
- **Generic Matmul Fallback Desired**: The discussion emphasized the potential benefits of having a generic **matmul fallback** in the kernels.
   - It would provide a baseline implementation for new hardware or platforms until optimized kernels are developed, streamlining the bring-up process.


  

---


### **Manus.im Discord â–· #[general](https://discord.com/channels/1348819876348825620/1349440650495398020/1444055373131546776)** (27 messagesðŸ”¥): 

> `Manus Update Issues, Black Friday Sale Opinions, UI Feedback, AI Engineer Introductions, Civil Tone Request` 


- **Manus Update Cripples App, Angering Users**: After the latest update, **Manus** is reportedly functioning only for paying users, with normal chat mode disabled and free points completely removed, leaving an *empty interface with no real use*.
   - Additionally, **Manus** is allegedly failing to build a next js app for testing pull requests, causing them to be incorrect due to build timeouts.
- **Differing Opinions Emerge on Manus' Black Friday Decision**: While some users *respect the no black friday offer*, seeing it as a testament to **Manus' value**, others felt that **Black Friday** could retain customers, citing **Grok's** successful approach last year.
   - One user stated that the product *is waaaay too cheap compared to what it can do and compared to chat gpt and claude*.
- **User Interface issue surfaces for Referral Codes**: A user complained that the **Manus UI** should prominently state that **referral codes** can only be redeemed by new users.
   - The user exclaimed *wtf are people crying about paying a monthly fee to one of the best LLM's out there*.
- **AI Engineers Promote Expertise**: One user introduced himself as an **AI & full stack engineer** specializing in workflow automation, LLM integration, RAG, AI detection, image and voice AI, and blockchain development, providing several examples of deployed systems, automated pipelines, and task orchestration.
   - Another user, who has *never studied programming*, shared his work on creating an **AI engine in Rust** with the goal of building a *sovereign AI*.
- **Mod Calls for Civil Discourse**: In the face of recent arguments, a moderator requested that users keep a civil tone in their discussions.
   - The moderator followed up with *Thanks for listening ðŸ™alsalam ealaykum warahmat allah wabarakatu*.


  

---


### **DSPy â–· #[show-and-tell](https://discord.com/channels/1161519468141355160/1202371242519441499/1444221259683725362)** (4 messages): 

> `scikit-llm, OpenRouter API` 


- **DSPy wins against scikit-llm, maybe?**: A member asked whether **DSPy** is better than **scikit-llm**, and one of the developers answered *yes, but depends who you ask.*
- **OpenRouter API Configuration Still to Come**: A member pointed out that the **documentation is limited** and the tool *can't configure openrouter api in it yet.*
   - The developer acknowledged that it's *very new*, and that a recent update may address the issue.


  

---


### **DSPy â–· #[general](https://discord.com/channels/1161519468141355160/1161519469319946286/1444116146457546802)** (6 messages): 

> `Prompt Tuning, GEPA and SIMBA optimizers, AI System Building, End-to-End AI Systems, AI-driven Platforms` 


- **Methods for Prompt Tuning Emerges**: A member highlights that [methods where **LLMs** analyze failure causes to propose improvements](https://arxiv.org/abs/2406.07496) are well-known for their effectiveness in prompt tuning.
- **GEPA and SIMBA Optimizers Examined**: A member inquires about using methods for prompt tuning in DSPy, questioning if **GEPA** and **SIMBA** are the appropriate optimizers for this approach.
- **AI Developer Showcases AI Systems Building Experience**: A **Senior AI Developer** detailed their experience building end-to-end AI systems at scale, highlighting expertise in **machine learning, deep learning, NLP, computer vision, and generative AI** and tools like **PyTorch, TensorFlow, and Hugging Face**.
- **AI Platforms using ChatGPT highlighted**: The AI developer detailed the development of platforms such as an **AI Medical Diagnosis System**, an **AI Video Generation System**, and a **Health & Management Advocacy System**.


  

---


### **aider (Paul Gauthier) â–· #[general](https://discord.com/channels/1131200896827654144/1131200896827654149/1444287630287179928)** (6 messages): 

> `GPT Provider, Aider Alternatives, Mindlink Models` 


- ****GPT Provider** Offers Free Credits for Models**: A member is offering free credits for their **GPT provider**, supporting models such as **gpt-5-mini**, **gpt-4.1**, **gpt-4o**, **gpt4**, and **gpt-3.t**, including open-source embedding models.
   - The provider is open source, allowing users to clone and modify the code, with the option to contribute back to the community.
- **Members Discuss **Aider Alternatives****: Some members expressed concern about **Aider**'s future, with one asking for alternatives.
   - One member stated that *generating svg images with it is much better. A nicer aesthetic*.
- ****Mindlink Models** Impact Aider's Popularity?**: A member speculated that **Mindlink 32/72B models**, released in August, may have impacted Aider's popularity due to their strong code generation capabilities.
   - However, the model *didn't hold together in multi turn coding iterations so well*.


  

---


---


---


---

